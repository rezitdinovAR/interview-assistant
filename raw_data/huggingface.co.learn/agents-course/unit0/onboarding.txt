---
title: Onboarding: Your First Steps ‚õµ
url: https://huggingface.co/learn/agents-course/unit0/onboarding
course: agents-course
chapter: Unit 0. Welcome to the course
chapter_id: unit0/onboarding
---
# Onboarding: Your First Steps ‚õµ

 
![Time to Onboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/time-to-onboard.jpg)
 
Now that you have all the details, let‚Äôs get started! We‚Äôre going to do four things:
 
1. **Create your Hugging Face Account** if it‚Äôs not already done
2. **Sign up to Discord and introduce yourself** (don‚Äôt be shy ü§ó)
3. **Follow the Hugging Face Agents Course** on the Hub
4. **Spread the word** about the course
 

### Step 1: Create Your Hugging Face Account

 
(If you haven‚Äôt already) create a Hugging Face account [here](https://huggingface.co/join).
 

### Step 2: Join Our Discord Community

 
üëâüèª Join our discord server [here.](https://discord.gg/UrrTSsSyjb)
 
When you join, remember to introduce yourself in `#introduce-yourself`.
 
We have multiple AI Agent-related channels:
 
- `agents-course-announcements`: for the **latest course information**.
- `üéì-agents-course-general`: for **general discussions and chitchat**.
- `agents-course-questions`: to **ask questions and help your classmates**.
- `agents-course-showcase`: to **show your best agents**.
 
In addition you can check:
 
- `smolagents`: for **discussion and support with the library**.
 
If this is your first time using Discord, we wrote a Discord 101 to get the best practices. Check [the next section](discord101).
 

### Step 3: Follow the Hugging Face Agent Course Organization

 
Stay up to date with the latest course materials, updates, and announcements **by following the Hugging Face Agents Course Organization**.
 
üëâ Go [here](https://huggingface.co/agents-course) and click on **follow**.
 
![Follow](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/hf_course_follow.gif)
 

### Step 4: Spread the word about the course

 
Help us make this course more visible! There are two ways you can help us:
 
1. Show your support by ‚≠ê [the course‚Äôs repository](https://github.com/huggingface/agents-course).
 
![Repo star](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/please_star.gif)
 
1. Share Your Learning Journey: Let others **know you‚Äôre taking this course**! We‚Äôve prepared an illustration you can use in your social media posts
 
![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)
 
You can download the image by clicking üëâ [here](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png?download=true)
 

### Step 5: Running Models Locally with Ollama (In case you run into Credit limits)

 
1. **Install Ollama**
 
Follow the official Instructions [here.](https://ollama.com/download)
2. **Pull a model Locally**
  
```
ollama pull qwen2:7b
```
 
Here, we pull the [qwen2:7b model](https://ollama.com/library/qwen2:7b). Check out [the ollama website](https://ollama.com/search) for more models.
3. **Start Ollama in the background (In one terminal)**
  
```
ollama serve
```
 
If you run into the error ‚Äúlisten tcp 127.0.0.1:11434: bind: address already in use‚Äù, you can use command `sudo lsof -i :11434` to identify the process
ID (PID) that is currently using this port. If the process is `ollama`, it is likely that the installation script above has started ollama
service, so you can skip this command to start Ollama.
4. **Use LiteLLMModel Instead of InferenceClientModel**
 
To use `LiteLLMModel` module in `smolagents`, you may run `pip` command to install the module.
  
```
pip install 'smolagents[litellm]'
```
  
```
from smolagents import LiteLLMModel

    model = LiteLLMModel(
        model_id="ollama_chat/qwen2:7b",  # Or try other Ollama-supported models
        api_base="http://127.0.0.1:11434",  # Default Ollama local server
        num_ctx=8192,
    )
```
 
1. **Why this works?**
 
- Ollama serves models locally using an OpenAI-compatible API at `http://localhost:11434`.
- `LiteLLMModel` is built to communicate with any model that supports the OpenAI chat/completion API format.
- This means you can simply swap out `InferenceClientModel` for `LiteLLMModel` no other code changes required. It‚Äôs a seamless, plug-and-play solution.
 
Congratulations! üéâ **You‚Äôve completed the onboarding process**! You‚Äôre now ready to start learning about AI Agents. Have fun!
 
Keep Learning, stay awesome ü§ó