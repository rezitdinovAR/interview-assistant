---
title: Thought: Internal Reasoning and the ReAct Approach
url: https://huggingface.co/learn/agents-course/unit1/thoughts
course: agents-course
chapter: Unit 1. Introduction to Agents
chapter_id: unit1/thoughts
---
# Thought: Internal Reasoning and the ReAct Approach

 
> In this section, we dive into the inner workings of an AI agentâ€”its ability to reason and plan. Weâ€™ll explore how the agent leverages its internal dialogue to analyze information, break down complex problems into manageable steps, and decide what action to take next.Additionally, we introduce the ReAct approach, a prompting technique that encourages the model to think â€œstep by stepâ€ before acting.
 
Thoughts represent the **Agentâ€™s internal reasoning and planning processes** to solve the task.
 
This utilises the agentâ€™s Large Language Model (LLM) capacity **to analyze information when presented in its prompt** â€” essentially, its inner monologue as it works through a problem.
 
The Agentâ€™s thoughts help it assess current observations and decide what the next action(s) should be. Through this process, the agent can **break down complex problems into smaller, more manageable steps**, reflect on past experiences, and continuously adjust its plans based on new information.
 

## ðŸ§  Examples of Common Thought Types

 Type of Thought Example Planning â€œI need to break this task into three steps: 1) gather data, 2) analyze trends, 3) generate reportâ€ Analysis â€œBased on the error message, the issue appears to be with the database connection parametersâ€ Decision Making â€œGiven the userâ€™s budget constraints, I should recommend the mid-tier optionâ€ Problem Solving â€œTo optimize this code, I should first profile it to identify bottlenecksâ€ Memory Integration â€œThe user mentioned their preference for Python earlier, so Iâ€™ll provide examples in Pythonâ€ Self-Reflection â€œMy last approach didnâ€™t work well, I should try a different strategyâ€ Goal Setting â€œTo complete this task, I need to first establish the acceptance criteriaâ€ Prioritization â€œThe security vulnerability should be addressed before adding new featuresâ€ 
> Note:In the case of LLMs fine-tuned for function-calling, the thought process is optional. More details will be covered in the Actions section.
 

## ðŸ”— Chain-of-Thought (CoT)

 
**Chain-of-Thought (CoT)** is a prompting technique that guides a model to **think through a problem step-by-step before producing a final answer.**
 
It typically starts with:
 
> â€œLetâ€™s think step by step.â€
 
This approach helps the model **reason internally**, especially for logical or mathematical tasks, **without interacting with external tools**.
 

### âœ… Example (CoT)

  
```
Question: What is 15% of 200?
Thought: Let's think step by step. 10% of 200 is 20, and 5% of 200 is 10, so 15% is 30.
Answer: 30
```
 

## âš™ï¸ ReAct: Reasoning + Acting

 
A key method is the **ReAct approach**, which combines â€œReasoningâ€ (Think) with â€œActingâ€ (Act).
 
ReAct is a prompting technique that encourages the model to think step-by-step and interleave actions (like using tools) between reasoning steps.
 
This enables the agent to solve complex multi-step tasks by alternating between:
 
- Thought: internal reasoning
- Action: tool usage
- Observation: receiving tool output
 

### ðŸ”„ Example (ReAct)

  
```
Thought: I need to find the latest weather in Paris.
Action: Search["weather in Paris"]
Observation: It's 18Â°C and cloudy.
Thought: Now that I know the weather...
Action: Finish["It's 18Â°C and cloudy in Paris."]
```
 
![ReAct](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/ReAct.png)
 (d) is an example of the ReAct approach, where we prompt "Let's think step by step", and the model acts between thoughts. 

## ðŸ” Comparison: ReAct vs. CoT

 Feature Chain-of-Thought (CoT) ReAct Step-by-step logic âœ… Yes âœ… Yes External tools âŒ No âœ… Yes (Actions + Observations) Best suited for Logic, math, internal tasks Info-seeking, dynamic multi-step tasks 
> Recent models likeDeepseek R1orOpenAIâ€™s o1were fine-tuned tothink before answering. They use structured tokens like<think>and</think>to explicitly separate the reasoning phase from the final answer.Unlike ReAct or CoT â€” which are prompting strategies â€” this is atraining-level technique, where the model learns to think via examples.