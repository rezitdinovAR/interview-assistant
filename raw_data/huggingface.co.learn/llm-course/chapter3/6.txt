---
title: Fine-tuning, Check!
url: https://huggingface.co/learn/llm-course/chapter3/6
course: llm-course
chapter: 3. Fine-tuning a pretrained model
chapter_id: chapter3/6
---
[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  

# Fine-tuning, Check!

   
That was comprehensive! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data using modern best practices. To recap, in this chapter you:
 
- Learned about datasets on the [Hub](https://huggingface.co/datasets) and modern data processing techniques
- Learned how to load and preprocess datasets efficiently, including using dynamic padding and data collators
- Implemented fine-tuning and evaluation using the high-level `Trainer` API with the latest features
- Implemented a complete custom training loop from scratch with PyTorch
- Used ðŸ¤— Accelerate to make your training code work seamlessly on multiple GPUs or TPUs
- Applied modern optimization techniques like mixed precision training and gradient accumulation
 
> ðŸŽ‰Congratulations!Youâ€™ve mastered the fundamentals of fine-tuning transformer models. Youâ€™re now ready to tackle real-world ML projects!ðŸ“–Continue Learning: Explore these resources to deepen your knowledge:ðŸ¤— Transformers task guidesfor specific NLP tasksðŸ¤— Transformers examplesfor comprehensive notebooksðŸš€Next Steps:Try fine-tuning on your own dataset using the techniques youâ€™ve learnedExperiment with different model architectures available on theHugging Face HubJoin theHugging Face communityto share your projects and get help
 
This is just the beginning of your journey with ðŸ¤— Transformers. In the next chapter, weâ€™ll explore how to share your models and tokenizers with the community and contribute to the ever-growing ecosystem of pretrained models.
 
The skills youâ€™ve developed here - data preprocessing, training configuration, evaluation, and optimization - are fundamental to any machine learning project. Whether youâ€™re working on text classification, named entity recognition, question answering, or any other NLP task, these techniques will serve you well.
 
> ðŸ’¡Pro Tips for Success:Always start with a strong baseline using theTrainerAPI before implementing custom training loopsUse the ðŸ¤— Hub to find pretrained models that are close to your task for better starting pointsMonitor your training with proper evaluation metrics and donâ€™t forget to save checkpointsLeverage the community - share your models and datasets to help others and get feedback on your work