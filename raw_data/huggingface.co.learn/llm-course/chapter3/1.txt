---
title: Introduction
url: https://huggingface.co/learn/llm-course/chapter3/1
course: llm-course
chapter: 3. Fine-tuning a pretrained model
chapter_id: chapter3/1
---
[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  

# Introduction

   
In [Chapter 2](/course/chapter2) we explored how to use tokenizers and pretrained models to make predictions. But what if you want to fine-tune a pretrained model to solve a specific task? Thatâ€™s the topic of this chapter! You will learn:
 
- How to prepare a large dataset from the Hub using the latest ðŸ¤— Datasets features
- How to use the high-level `Trainer` API to fine-tune a model with modern best practices
- How to implement a custom training loop with optimization techniques
- How to leverage the ðŸ¤— Accelerate library to easily run distributed training on any setup
- How to apply current fine-tuning best practices for maximum performance
 
> ðŸ“šEssential Resources: Before starting, you might want to review theðŸ¤— Datasets documentationfor data processing.
 
This chapter will also serve as an introduction to some Hugging Face libraries beyond the ðŸ¤— Transformers library! Weâ€™ll see how libraries like ðŸ¤— Datasets, ðŸ¤— Tokenizers, ðŸ¤— Accelerate, and ðŸ¤— Evaluate can help you train models more efficiently and effectively.
 
Each of the main sections in this chapter will teach you something different:
 
- **Section 2**: Learn modern data preprocessing techniques and efficient dataset handling
- **Section 3**: Master the powerful Trainer API with all its latest features
- **Section 4**: Implement training loops from scratch and understand distributed training with Accelerate
 
By the end of this chapter, youâ€™ll be able to fine-tune models on your own datasets using both high-level APIs and custom training loops, applying the latest best practices in the field.
 
> ðŸŽ¯What Youâ€™ll Build: By the end of this chapter, youâ€™ll have fine-tuned a BERT model for text classification and understand how to adapt the techniques to your own datasets and tasks.
 
This chapter focuses exclusively on **PyTorch**, as it has become the standard framework for modern deep learning research and production. Weâ€™ll use the latest APIs and best practices from the Hugging Face ecosystem.
 
To upload your trained models to the Hugging Face Hub, you will need a Hugging Face account: [create an account](https://huggingface.co/join)