---
title: Ungraded quiz
url: https://huggingface.co/learn/llm-course/chapter1/7
course: llm-course
chapter: 1. Transformer models
chapter_id: chapter1/7
---
# Ungraded quiz

   
So far, this chapter has covered a lot of ground! Don‚Äôt worry if you didn‚Äôt grasp all the details, but it‚Äôs to reflect on what you‚Äôve learned so far with a quiz.
 
This quiz is ungraded, so you can try it as many times as you want. If you struggle with some questions, follow the tips and revisit the material. You‚Äôll be quizzed on this material again in the certification exam.
 

### 1. Explore the Hub and look for the roberta-large-mnli checkpoint. What task does it perform?

  Summarization  Text classification  Text generation   

### 2. What will the following code return?

  
```
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```
  It will return classification scores for this sentence, with labels "positive" or "negative".  It will return a generated text completing this sentence.  It will return the words representing persons, organizations or locations.   

### 3. What should replace ‚Ä¶ in this code sample?

  
```
from transformers import pipeline

filler = pipeline("fill-mask", model="bert-base-cased")
result = filler("...")
```
  This <mask> has been waiting for you.  This [MASK] has been waiting for you.  This man has been waiting for you.   

### 4. Why will this code fail?

  
```
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
result = classifier("This is a course about the Transformers library")
```
  This pipeline requires that labels be given to classify this text.  This pipeline requires several sentences, not just one.  The ü§ó Transformers library is broken, as usual.  This pipeline requires longer inputs; this one is too short.   

### 5. What does ‚Äútransfer learning‚Äù mean?

  Transferring the knowledge of a pretrained model to a new model by training it on the same dataset.  Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.  Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model.   

### 6. True or false? A language model usually does not need labels for its pretraining.

  True  False   

### 7. Select the sentence that best describes the terms ‚Äúmodel‚Äù, ‚Äúarchitecture‚Äù, and ‚Äúweights‚Äù.

  If a model is a building, its architecture is the blueprint and the weights are the people living inside.  An architecture is a map to build a model and its weights are the cities represented on the map.  An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.   

### 8. Which of these types of models would you use for completing prompts with generated text?

  An encoder model  A decoder model  A sequence-to-sequence model   

### 9. Which of those types of models would you use for summarizing texts?

  An encoder model  A decoder model  A sequence-to-sequence model   

### 10. Which of these types of models would you use for classifying text inputs according to certain labels?

  An encoder model  A decoder model  A sequence-to-sequence model   

### 11. What possible source can the bias observed in a model have?

  The model is a fine-tuned version of a pretrained model and it picked up its bias from it.  The data the model was trained on is biased.  The metric the model was optimizing for is biased.