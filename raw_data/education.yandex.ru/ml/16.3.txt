---
title: Вероятностные распределения
url: https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya
course: ml
chapter: 16. Теормин
chapter_id: 16.3
---
Принимая то или иное решение в условиях недостаточной информации, нам часто приходится взвешивать шансы, просчитывать риски, а то и вовсе уповать на удачу. Теория вероятностей предоставляет математические инструменты для проведения корректных рассуждений в условиях неопределённости, количественного измерения характеристик случайных событий и оценки правдоподобия их реализации.
Этот и последующий параграфы следует рассматривать как расширенный справочник, позволяющий освежить знания по вероятности и статистике, сделав при этом упор на приложении к машинному обучению. За более систематическим курсом по теории вероятностей читателю следует обратиться к серьёзным учебникам вроде Ширяева или Феллера.
Для погружения в статистику смотри, например, книгу Лагутина. А особо нетерпеливым рекомендуем взглянуть на короткий и ёмкий Probability and Statistics Cookbook.
Вероятностное пространство
В учебниках вероятность традиционно поставляется в комплекте с вероятностным пространством. Не увлекаясь чрезмерным формализмом, можно сказать, что для задания вероятностного пространства нужны:
непустое множество
Ω
Ω, называемое пространством элементарных событий (исходов);
алгебра множеств
F
⊂
2
Ω
F⊂2
Ω
— набор подмножеств
Ω
Ω, замкнутый относительно дополнений, объединений и пересечений; каждый элемент
A
∈
F
A∈F называется событием;
вероятностная мера
P:F→[0,1], приписывающая каждому событию
A
∈
F
A∈F некоторую вероятность
P(A)∈[0,1].
К вероятностному пространству
(Ω,F,P) предъявляются следующие требования:
∅
∈
F
∅∈F (невозможное событие),
Ω
∈
F
Ω∈F (достоверное событие);
P(Ω)=1;
P(A∪B)=P(A)+P(B), если
A
,
B
∈
F
A,B∈F и
A
∩
B
=
∅
A∩B=∅ (аддитивность).
Упражнение. Докажите, что
P(∅)=0 и
)=1−P(A), если
A
∈
F
A∈F.
Аддитивность вероятности легко обобщается по индукции до свойства конечной аддитивности: если события
,…,A
n
попарно несовместны, то
k=1
⋃
n
A
k
)=
k=1
∑
n
P(A
k
).
Множество
Ω
Ω часто называют носителем; говорят также, что вероятностная мера (масса)
P
P сосредоточена, или распределена, на носителе
Ω
Ω. В зависимости от типа носителя
Ω
Ω распределения делятся на два типа: дискретные и непрерывные.
Дискретные распределения
Вероятность на не более чем счётном пространстве элементарных событий
Ω={ω
1
,ω
2
,…} задаётся просто приписыванием неотрицательного числа
p
k
p
k
каждому элементарному исходу
ω
k
ω
k
с условием
=1. Для произвольного события
A
⊂
Ω
A⊂Ω полагают
P(A)=
. Набор чисел
} называют также распределением вероятностей на множестве
Ω
Ω. В англоязычной литературе распространён термин probability mass function (сокращённо pmf).
Зачастую в результате эксперимента нас интересуют не вероятности событий сами по себе, а значения некоторой связанной с ними случайной величины, принимающей числовые значения. Например:
сумма очков, выпавших при броске двух кубиков;
число метеоритов диаметром более одного метра, падающих на Землю в течение года;
ежедневный доход от показа рекламных объявлений в интернете.
На каждом элементарном исходе
ω
k
ω
k
случайная величина
ξ
ξ принимает некоторое числовое значение
=ξ(ω
k
). Иными словами, случайная величина — это функция
ξ:Ω→R, принимающая значение
ξ
k
ξ
k
с вероятностью
p
k
p
k
; её математическое ожидание (среднее) и дисперсия (среднеквадратичное отклонение) вычисляются по формулам
Eξ=
и Vξ=E(ξ−Eξ)
2
=Eξ
2
−(Eξ)
2
соответственно. Корень из дисперсии
V
ξ
Vξ
назвают стандартным отклонением случайной величины
ξ
ξ. Стандартное отклонение и дисперсия показывают, насколько далеко значения случайной величины могут отклоняться от среднего значения. Стандартное отклонение хорошо тем, что, в отличие от дисперсии, измеряется в тех же единицах, что и сама случайная величина.
Равномерное распределение
Так называется распределение вероятностней на множестве
Ω={ω
1
,…,ω
n
}, у которого
=P(ω
1⩽k⩽n. Тогда
P(A)=
∣Ω∣
∣A∣
, и мы получили формулу из классического подхода к вероятности, при котором вероятность события равна отношению числа благоприятных исходов к общему их количеству.
Равномерным распределением моделируются различные игровые ситуации:
подбрасывание симметричной монеты (
ω
1
=
«орёл»
ω
1
=«орёл»,
ω
2
=
«решка»
ω
2
=«решка»);
подбрасывание кубика (
=k,
1
⩽
k
⩽
6
1⩽k⩽6);
вращение рулетки в казино (
n
=
37
n=37 для европейской,
n
=
38
n=38 для американской).
Упражнение. У европейской рулетки по
18
18 чёрных и красных секторов и один сектор «зеро». Игрок ставит €10 на чёрное. В случае успеха казино выплачивает ему ещё €10, в противном случае забирает ставку. Чему равно математическое ожидание, дисперсия и стандартное отклонение выигрыша?
Вопрос на подумать. Бывают ли равномерные распределения в пространствах со счётным носителем?
Равномерные распределения преимущественно встречаются в разного сорта играх. В более жизненных ситациях случайность обычно распределена отнюдь не равномерно.
Распределение Бернулли
Так называется очень простое распределение всего лишь с двумя исходами:
P
(
«успех»
«неудача»
P(«успех»)=p,P(«неудача»)=1−p,0⩽p⩽1.
Бернуллиевская случайная величина
ξ∼Bern(p) — это просто индикаторная функция успешного события:
ξ
=
1
ξ=1, если случился «успех»,
ξ
=
0
ξ=0, если нас постигла «неудача». Несложные вычисления показывают, что
Eξ=1⋅p+0⋅(1−p)=p,Vξ=p−p
2
=p(1−p).
Если
, то снова получается равномерное распределение с двумя исходами. При
бернуллиевская случайная величина моделирует подбрасывание несимметричной монеты. В машинном обучении часто встречается задача бинарной классификации, и разбиение на классы обычно кодируется с помощью
Bern(p), например:
диагностика болезни (болен —
1
1, здоров —
0
0);
оценка кредитоспособности клиента (одобрить кредит —
0
0, отказать
1
1);
предсказание поведения пользователя (кликнет на рекламу —
1
1, пропустит —
0
0).
В этих примерах вероятности классов явно не равны, поэтому несимметричное распределение Бернулли — типичная ситуация в реальных задачах.
Биномиальное распределение
Биномиальное распределение
Bin(n,p) имеет сумма независимых бернуллиевских случайных величин
∼Bern(p):
η∼Bin(n,p), если
η=ξ
1
+…+ξ
n
. Другими словами, случайная величина
η
η равна количеству успехов в
n
n независимых испытаниях Бернулли с вероятностью успеха
p
p. Случайная величина
η
η принимает значения от
0
0 до
n
n, и
=P(η=k)=(
k
n
)p
k
(1−p)
n−k
,0⩽k⩽n.
Отметим, что согласно биному Ньютона
k=0
∑
n
p
k
=(p+(1−p))
n
=1,
поэтому числа
} действительно представляют собой распределение вероятностей, называемое также биномиальным. Если
ξ∼Bin(n,p), то
Eξ=np,Vξ=np(1−p).
Пример. Каждый день рекламу компании А поисковой выдаче Яндекса видят ровно
1000
1000 человек. Вчера
50
50 из них кликнули на рекламу. Для прогнозирования объемов продаж компании А хочется знать, с какой вероятностью не менее 50 людей кликнут на ее рекламу сегодня.
Если моделировать наличие или отсутствие клика бернуллиевской случайной величиной, то общее количество кликов за день моделируется случайной величиной
ξ∼Bin(n,p) с параметрами
n
=
1000
n=1000 и
p
=
50
1000
=
0.05
p=
1000
50
=0.05. Тогда с помощью вычислительной техники получаем, что
1000
k
)
0.0
5
k
0.9
5
1000
−
k
≈
0.52.
P(ξ⩾50)=
k=50
(1−p)
n−k
=1−
k=0
∑
49
(
k
1000
)0.05
k
0.95
1000−k
≈0.52.
Отметим, что параметр
p
p в предыдущем примере нам, строго говоря, не был известен, и вместо него мы использовали частотную оценку.
Распределение Пуассона
Это распределение имеет счётный носитель
Ω=N∪{0}. Случайная величина
ξ
ξ имеет пуассоновское распределение с параметром
ξ∼Pois(λ), если
P(ξ=k)=e
−λ
k!
λ
k
,k∈N∪{0}.
Известное разложение экспоненты в ряд Тейлора
k=0
∑
∞
k!
λ
k
позволяет заключить, что вероятности распределения Пуассона действительно суммируются в единицу. Этот же ряд позволяет вычислить, что
Eξ=Vξ=λ.
Пуассоновская случайная величина моделирует число редких событий, происходящих в течение фиксированного промежутка времени: если события наступают со средней скоростью
r
r, то
P
(
k
событий на промежутке
P(k событий на промежутке t)=e
−rt
k!
(rt)
k
,k∈N∪{0}.
Иногда приходится рассматривать биномиальное распределение
Bin(n,p) с большим числом попыток
n
n и вероятностью успеха
p
p с условием
np≈λ>0. Оказывается, что вне зависимости от
n
n такое распределение быстро стабилизируется, сходясь к пуассоновскому распределению с параметром
λ
λ. Точнее говоря, справедлива следующая теорема.
Теорема (Пуассон). Пусть
ξ∼Bin(n,p
n
) и
lim
n→∞
lim
np
n
=λ>0. Тогда
lim
n→∞
lim
P(ξ=k)=e
−λ
k!
λ
k
,k∈N∪{0}.
Пример. Известно, что на поисковой выдаче яндекса на рекламу компании А кликает в среднем примерно 50 пользоваталей в день. Количество показов достаточно большое и может меняться изо дня в день. Требуется оценить вероятность того, что сегодня будет совершено не менее 50 кликов по рекламным объявлениям.
Распределение количества кликов снова будем моделировать биномиальным распределением
Bin(n,p). На этот раз число
n
n нам неизвестно, но сказано, что оно велико и
n
p
≈
50
np≈50 (вспомним, что
E
ξ
=
n
p
Eξ=np, если
ξ∼Bin(n,p)). Поэтому можно воспользоваться теоремой Пуассона и заменить биномиальное распределение пуассоновским с параметром
λ
=
50
λ=50. Тогда искомая вероятность равна
0.518
,
1−
k=0
∑
49
e
−50
k!
50
k
≈0.518,
что практически совпадает ответом, полученным с помощью биномиального распределения при
n
=
1000
n=1000.
Геометрическое распределение
Пусть монетка с вероятностью «успеха»
p
p подбрасывается до тех пор, пока впервые не случится «успех». Случайная величина
ξ
ξ, равная общему количеству попыток на этом пути, имеет геометрическое распределение, т.е.
P(ξ=k)=q
k−1
p,q=1−p,k∈N.
По формуле геометрической прогрессии находим, что
k=1
∑
∞
P(ξ=k)=
k=0
∑
∞
q
k
p=
1−q
p
=1,
поэтому с нормировкой тут всё в порядке. Чем меньше
p
p, тем больше геометрическое распределение похоже на равномерное, что подтверждают и формулы для среднего и дисперсии:
Eξ=
p
1
,Vξ=
p
2
1−p
.
Пример. По оценкам за предыдущие дни пользователь нажимает на рекламу с вероятностью
p
=
0.05
p=0.05. Сегодня компания B планирует показать очень важное рекламное объявление и требует от Яндекса, чтобы с вероятностью не менее
99
%
99% на него кликнули хотя бы раз. Скольким различным людям следует показать это объявление?
Здесь мы имеем дело с геометрическим распределением с вероятностью «успеха» (клика)
p
p: именно так распределена случайная величина
ξ
ξ, равная количеству показов объявления до первого клика по нему. Следовательно,
P(ξ⩽n)=
k=1
∑
n
P(ξ=k)=
k=1
∑
n
q
k−1
p=p⋅
1−q
1−q
n
=1−q
n
.
Эта вероятность должна быть не меньше
99
%
99%, т. е.
0.9
5
n
⩾
0.01
0.95
n
⩾0.01. Отсюда находим, что
n
⩾
log
⁡
0.01
log
⁡
0.95
≈
89.78
n⩾
log0.95
log0.01
≈89.78. Таким образом, рекламу надо показать как минимум
90
90 раз.
Гипергеометрическое распределение
Пример. Известно, что партия из
N
N деталей содержит
K
K бракованных. Какова вероятность того, что среди выбранных наугад
n
n деталей окажется ровно
k
k бракованных?
Всего есть
) способов выбора
n
n деталей из партии. Число вариантов выбрать
k
k деталей из
K
K бракованных и
n
−
k
n−k из
N
−
K
N−K деталей без дефектов равно
n−k
N−K
). По классическому определению вероятности получаем, что искомая вероятность равна
min
n−k
N−K
)
,0⩽k⩽min{K,n}.
Такое распределение называется гипергеометрическим. Равенство
∑
k
=
0
min
k=0
∑
min{K,n}
p
k
=1
следует из тождества Вандермонда. Если случайная величина
ξ
ξ имеет гипергеометрическое распределение с параметрами
N
N,
K
K,
n
n, то
Eξ=
N
nK
,Vξ=n
N
2
(N−1)
K(N−K)(N−n)
.
Гипергеометрическое распределение является аналогом биномиального, при котором моделируется выбор без возвращения с вероятностью успеха
Непрерывные распределения
Вероятностная модель с конечным или счётным носителем не подходит в тех случаях, когда результатом эксперимента удобно считать произвольное действительное число, например, распределение людей по росту или по весу. Для этого требуется пересмотреть подход к построению пространства элементарных событий
Ω
Ω: ведь множество действительных чисел
R
R континуально, и поэтому вероятность события не получится определить как сумму вероятностей всех составляющих исходов, коих тоже может оказаться континуум. Приходится искать другие способы задания вероятности.
Наиболее часто встречающийся на практике класс непрерывных распределений на числовой прямой задаётся с помощью неотрицательной интегрируемой функции плотности (probability density function, pdf)
p
(
x
)
p(x) со свойством
p(x)dx=1.
Вероятность события
A
A определяется как
P(A)=
A
∫
p(x)dx
при условии, что этот интеграл имеет смысл. В частности,
P([a,b))=
a
∫
b
p(x)dx.
Замечание. Связь между вероятностью и плотностью распределения весьма напоминает связь между массой и физической плотностью. Когда плотность объекта всюду одинакова, то масса равна плотности, умноженной на объём. Если же объект неоднороден, то плотность становится функцией, сопоставляющей каждой точке некое число (что-то вроде предела отношения массы малого шарика вокруг этой точки к объёму шарика). Тогда масса любого куска объекта может быть вычислена, как интеграл функции плотности по объёму этого куска.
С плотностью вероятности
p
(
x
)
p(x) автоматически связана случайная величина
ξ:R→R, для которой
P(a⩽ξ<b)=
a
∫
b
p(x)dx. Функция
p
(
x
)
p(x) называется плотностью случайной величины
ξ
ξ, и обозначается также как
(x). Иногда используется запись
ξ∼p(x). Среднее и дисперсия случайной величины
ξ∼p(x) вычисляются по формулам
Eξ=
−∞
∫
∞
xp(x)dx,Vξ=
−∞
∫
∞
x
2
p(x)dx−(Eξ)
2
.
Равномерное распределение
Равномерное распределение на отрезке
[
a
;
b
]
[a;b], которое часто обозначают
U[a,b], имеет постоянную плотность на этом отрезке:
p(x)=
b−a
I(a⩽x⩽b)
={
b−a
1
,
0,
x∈[a,b],
x∈
/
[a,b].
Если
ξ∼U[a,b], то
Eξ=
2
a+b
,Vξ=
12
(b−a)
2
.
Вопрос на подумать. Можно ли задать равномерное распределения на неограниченном промежутке, например, на
R
R или на
[0,+∞)?
Аналогичным образом вводится равномерное распределение в многомерном пространстве: если множество
V
⊂
R
n
V⊂R
n
имеет объём
∣
V
∣
∣V∣, то плотность равномерно распределённой на
V
V случайной величины
ξ
ξ задаётся как
(x)=
∣V∣
I(x∈V)
. Если
A
⊂
V
A⊂V, то
P(A)=
∣V∣
1
A
∫
dx=
∣V∣
∣A∣
,
и мы получили формулу геометрической вероятности.
Нормальное распределение
Случайная величина
ξ
ξ имеет нормальное (гауссовское) распределение
N(μ,σ
2
), если её плотность равна
(x)=
(x−μ)
2
.
Параметры нормального распределения
N(μ,σ
2
) представляют собой его среднее и дисперсию:
Eξ=μ,Vξ=σ
2
.
Параметр
σ
σ отвечает за выраженность «колокола» плотности нормального распределения:
при
σ
→
0
σ→0 «колокол» приобретает очертания резко выраженного пика, то есть практически вся вероятностная масса сосретдоточена в малой окрестности точки
x
=
μ
x=μ;
при
σ
→
+
∞
σ→+∞ «колокол», наоборот, размывается, и распределение становится больше похоже на равномерное.
Гауссиана, у которой
μ
=
0
μ=0 и
σ
=
1
σ=1, называется стандартным нормальным распределением.
Иногда бывает полезно тесно связанное с гауссовским логнормальное распределение.
Случайная величина
ξ:(0,+∞)→R имеет логнормальное распределение,
ξ∼LogN(μ,σ
2
), если
log
logξ∼N(μ,σ
2
). Плотность логнормальной случайной величины равна
log
(x)=
(logx−μ)
2
,x>0,
а её среднее и дисперсию можно вычислить по формулам
Eξ=e
μ+
2
σ
2
,Vξ=(e
σ
2
−1)e
2μ+σ
2
.
Показательное распределение
Плотность показательного (экспоненциального) распределения
Exp(λ) сосредоточена на луче
[0,+∞) и имеет параметр
λ
>
0
λ>0:
p(x)=λe
−λx
,
x
⩾
0
x⩾0. Если
ξ∼Exp(λ), то
Eξ=
λ
1
,Vξ=
λ
2
1
.
Плотность показательного распределения является убывающей функцией на
[0,+∞), а параметр
λ
λ отвечает за скорость этого убывания:
при
λ
→
0
λ→0 убывание очень медленное, и распределение больше похоже на равномерное;
при
λ
→
+
∞
λ→+∞, наоборот, вся вероятностная масса сосредоточена около точки
0
0.
Показательное распределение моделирует временные интервалы между случайными событиями, наступающими с постоянной скоростью, например:
время ожидания автобуса на остановке;
время между телефонными звонками в колл-центре;
время до выхода из строя вычислительного узла в дата-центре.
Гамма-распределение с положительными параметрами
α
α и
β
β имеет плотность
p(x)=
Γ(α)β
α
1
x
α−1
e
−
β
x
,x⩾0,
где
Γ
(
α
)
Γ(α) — гамма-функция Эйлера. При
α
=
1
α=1 гамма-распределение превращается в показательное с параметром
. Среднее и дисперсия случайной величины
ξ
ξ, имеющей гамма-распределение с параметрами
α
α и
β
β, равны
Eξ=αβ,Vξ=αβ
2
.
Бета-распределение
Плотность бета-распределения с параметрами
α
,
β
>
0
α,β>0 равна
p(x)=
B(α,β)
1
x
α−1
(1−x)
β−1
,0<x<1,
где
B(α,β) — бета-функция Эйлера.
Бета-распределение имеет следующее статистическое приложение. Выберем случайным образом точки
,…,x
n
∈[0,1], и упорядочим их по возрастанию. Получим набор значений
0⩽x
(1)
⩽x
(2)
⩽…⩽x
(k)
⩽…⩽x
(n)
⩽1.
Оказывается, что случайная величина
ξ=x
(k)
, называемая
k
k-й порядковой статистикой, имеет бета распределение с параметрами
k
k и
n
+
1
−
k
n+1−k:
(x)=
(k−1)!(n−k)!
n!
x
k−1
(1−x)
n−k
=k(
k
n
)x
k−1
(1−x)
n−k
.
Распределение Стьюдента
При проверке статистических гипотез бывает полезно распределение Стьюдента (t-distribution) с
ν
ν степенями свободы, плотность которого равна
p(x)=
ν+1
)
(1+
ν
x
2
)
−(ν+1)/2
,ν>0,
где
Γ
(
α
)
Γ(α) — гамма-функция Эйлера. Распределение Стьюдента похоже на стандартное нормальное распределение; более того, при
ν
→
+
∞
ν→+∞ оно превращается в
N(0,1).
Однако при малых значениях
ν
ν распределение Стьюдента имеет гораздо более тяжёлые «хвосты»: например, при
ν
⩽
2
ν⩽2 его дисперсия бесконечна, а при
ν
⩽
1
ν⩽1 та же участь постигает и математическое ожидание (всё из-за расходимости соответствующих интегралов). В остальных случаях
Eξ=0,Vξ=
ν−2
ν
,
если
ξ
ξ имеет распределение Стьюдента с
ν
ν степенями свободы.
Распределение Лапласа
Плотность распределения Лапласа с параметрами
μ
,
b
μ,b равна
p(x)=
2b
1
e
−
b
∣x−μ∣
.
Такое распределение иногда обозначают
Laplace(μ,b). Если
ξ∼Laplace(μ,b), то
Eξ=μ,Vξ=2b
2
.
При
μ
=
0
μ=0 распределение Лапласа представляет собой экспоненциальное распределение, плотность которого симметрично отражена на отрицательную полуось: если
ξ∼Laplace(0,b), то
∣ξ∣∼Exp(
b
1
). Распределение Лапласа похоже на нормальное и отличается от него немного более тяжёлыми «хвостами» и тем, что его плотность теряет гладкость в нуле.
Характеристики случайных величин
Моменты
Если
n
∈
N
n∈N, то
n
n-й момент
μ
n
μ
n
случайной величины
ξ
ξ равен
E
ξ
n
Eξ
n
. В зависимости от типа случайной величины моменты вычисляются по-разному:
P(ξ=x
k
), если
ξ
ξ принимает дискретные значения
,…,x
k
,…;
(x)dx, если
ξ
ξ имеет плотность
(x).
Первый момент
μ
1
μ
1
— это в точности математическое ожидание (среднее) случайной величины
ξ
ξ. Дисперсию тоже можно выразить через моменты:
Vξ=Eξ
2
−(Eξ)
Не у всех случайных величин есть конечные среднее и дисперсия. Например, распределение Коши (оно же распределение Стьюдента с одной степенью свободы) имеет плотность
p(x)=
π
1
1+x
2
1
, и если мы попытаемся вычислить первые два момента, то получим расходящиеся интегралы
1+x
2
x
dx и
π
1
−∞
∫
+∞
1+x
2
x
2
dx.
Упражнение. Приведите пример дискретной случайной величины с бесконечным средним.
Свойства математического ожидания
Если
ξ
=
C
ξ=C, то
E
ξ
=
C
Eξ=C.
E(aξ+bη)=aEξ+bEη (линейность).
Если
ξ
⩽
η
ξ⩽η, то
E
ξ
⩽
E
η
Eξ⩽Eη (монотонность).
EI(A)=P(A).
Если случайные величины
ξ
ξ и
η
η независимы, то
Eξη=EξEη.
Если
ξ
⩾
0
ξ⩾0, то
P(ξ⩾a)⩽
a
Eξ
(неравенство Маркова).
Если функция
f
f выпукла вниз, то
f(Eξ)⩽E(f(ξ)) (неравенство Йенсена).
Law of the unconscious statistician (LOTUS)
Если случайная величина
η
η получена применением некоторой детерминированной функцией из случайной величины
η=g(ξ), то
Eη=
k
∑
g(x
k
)P(ξ=x
k
), если
ξ
ξ дискретна;
Eη=
−∞
∫
+∞
g(x)p
ξ
(x)dx, если
ξ
ξ непрерывна.
Дисперсия и ковариация
Ковариация случайных величин
ξ
ξ и
η
η определяется по формуле
cov(ξ,η)=E((ξ−Eξ)⋅(η−Eη))=E(ξ⋅η)−Eξ⋅Eη
В частности,
cov(ξ,ξ)=Vξ. На практике часто применяют коэффициент корреляции, который получается нормированием ковариации:
corr(ξ,η)=
Vξ
Vη
cov(ξ,η)
.
Коэффициент корреляции всегда принимает значения из отрезка
[−1;1]. Если
corr(ξ,η)=0, то случайные величины
ξ
ξ и
η
η называют некоррелированными.
Свойства дисперсии и ковариации
V
ξ
⩾
0
Vξ⩾0, причём
Vξ=0⟺∃a∈R:P(ξ=a)=1.
V(aξ)=a
2
Vξ,
V(ξ+a)=Vξ.
cov(ξ,η)=cov(η,ξ),
cov(aξ,bη)=abcov(ξ,η).
V(ξ+η)=Vξ+Vη+2cov(ξ,η).
Если случайные величины
ξ
ξ и
η
η независимы, то
cov(ξ,η)=0 и
V(ξ+η)=Vξ+Vη.
P(∣ξ−Eξ∣⩾a)⩽
a
2
Vξ
(неравенство Чебышева).
Функции распределения и плотности
Случайная величина
ξ:Ω→R является числовой функцией, заданной на пространстве элементарных событий; однако, больший интерес обычно представляет порождаемое ею распределение вероятностей. В дискретном случае достаточно задать вероятности отдельных значений
P(ξ=x
i
); для непрерывных же случайных величин на помощь приходят функция распределения и функция плотности.
Функцией распределения (cumulative distribution function, cdf) случайной величины
ξ
ξ называется функция
(x)=P(ξ⩽x).
Свойства функции распределения
(−∞)=0,
(+∞)=1;
функция
F
ξ
F
ξ
неубывающая;
функция
F
ξ
F
ξ
непрерывна справа:
lim
h→0+
lim
F
ξ
(x+h)=F
ξ
(x);
P(a<ξ⩽b)=F
ξ
(b)−F
ξ
(a).
Любая дискретная случайная величина имеет ступенчатую функцию распределения. К примеру, вот как выглядит график функции
F
ξ
F
ξ
для
0.5
)
ξ∼Bin(10,0.5):
Если непрерывная случайная величина
ξ
ξ имеет непрерывную плотность
(x), то
(x)−F
ξ
(a)=
a
∫
x
p
ξ
(t)dt,
откуда следует, что
(x)=p
ξ
(x). В типичных случаях непрерывная случайная величина имеет гладкую возрастающую функцию распределения с двумя горизонтальными асимптотами. Вот примеры графиков функций распределения гауссовских случайных величин:
Медиана и мода
Математическое ожидание — не единственная числовая метрика, с помощью которой можно пытаться охарактеризовать, чему равно в среднем значение случайной величины. Медиана разбивает вероятностную массу распределения на две равные части. Если случайная величина
ξ
ξ имеет плотность
(x), то её медиана
m=medξ определяется из условия
P(ξ⩽m)=
−∞
∫
m
p
ξ
(x)dx=
m
∫
+∞
p
ξ
(x)dx=P(ξ⩾m)=
2
1
.
В терминах функции распределения это означает, что
(m)=1−F
ξ
(m), или
(m)=
2
1
. В непрерывном случае функция распределения
(x) строго возрастает, поэтому уравнение
(m)=
2
1
имеет единственное решение. Для дискретных случайных величин это может быть не так, и поэтому в общем случае медиану определяют как число
m
m, удовлетворяющее условиям
P(ξ⩽m)⩾
2
1
,P(ξ⩾m)⩾
2
1
.
Например, если
ξ∼Bern(
2
1
), то
P(ξ=0)=P(ξ=1)=
2
1
, и поэтому любое число
m∈(0,1) является медианой симметричного бернуллиевского распределения. Бесконечное количество медиан будет у всякой дискретной случайной величины
ξ
ξ, для которой
(x)=
2
1
на целом промежутке.
Мода распределения максимизирует его pmf или pdf:
max
или
max
mode(ξ)=arg
k
max
P(ξ=k) или mode(ξ)=arg
x
max
p
ξ
(x).
Мод у распределения может быть больше одной; самое вырожденное в этом смысле распределение — равномерное, каждая точка носителя является его модой. Если плотность случайной величины имеет единственную точку максимума, то она и является модой. Например:
mode(ξ)=μ, если
ξ∼N(μ,σ
mode(ξ)=0, если
ξ∼Exp(λ);
мода t-распределения Стьюдента также равна нулю.
Все такие распределения унимодальны. Если плотность
(x) имеет два или более максимума, то случайная величина
ξ
ξ называется бимодальной или мультимодальной.
image1
Для симметричных распределений вроде нормального математическое ожидание, медиана и мода совпадают, однако, в общем случае это три различные меры типичного среднего значения случайной величины. Смысл каждой из этой мер наглядно демострирует следующая иллюстрация:
mmm
Упражнение. Найдите среднее, медиану и моду экспоненциального распределения с параметром
λ
λ и сравните их между собой.
Классификация случайных величин
У внимательного читателя (отягощённого математическим образованием впридачу) может возникнуть вопрос: а все ли случайные величины относятся к дискретным или непрерывным? В буквально такой постановке ответ, конечно, отрицательный, поскольку можно получить гибридную случайную величину, сложив дискретную и непрерывную. Но, может быть, всякая случайная величина равна сумме непрерывной и дискретной компонент?
В терминах функций распределения этот вопрос можно переформулировать так: верно ли, что всякая монотонная функция
F:R→[0,1] может быть представлена в виде
F=F
jump
+F
smooth
, где
jump
— неубывающая ступенчатая функция (функция скачков), а
smooth
(x)=
−∞
∫
x
p(t)dt
— гладкая возрастающая функция, полученная интегрированием плотности?
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
16.2. Матричная факторизация
Следующий параграф
16.4. Многомерные распределения
