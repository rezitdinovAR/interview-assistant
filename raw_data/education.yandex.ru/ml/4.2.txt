---
title: Экспоненциальный класс распределений и принцип максимальной энтропии
url: https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.2
---
Самые главные семейства распределений в жизни любого data scientist’а
Мотивация: метод моментов
Метод моментов — это ещё один способ, наряду с методом максимального правдоподобия, оценки параметров распределения по данным
,…,x
N
. Суть его в том, что мы выражаем через параметры распределения теоретические значения моментов
=Ex
k
нашей случайной величины, затем считаем их выборочные оценки
, приравниваем их все друг к другу и, решая полученную систему, находим оценки параметров.
Можно доказать, что полученные оценки являются состоятельными, хотя могут быть смещены.
Пример 1. Оценим параметры нормального распределения
N(μ,σ
2
) с помощью метода моментов.
Теоретические моменты равны
=μ,μ
2
=σ
2
+μ
2
Запишем систему:
Из неё очевидным образом находим
Легко видеть, что полученные оценки совпадают с оценками максимального правдоподобия
Пример 2. Оценим параметр
μ
μ логнормального распределения
exp
⁡
(
−
(
log
p(x)=
x
2πσ
2
1
exp(−
2σ
2
(logx−μ)
2
)
при известном
σ
2
σ
2
. Будет ли оценка совпадать с оценкой, полученной с помощью метода максимального правдоподобия?
Теоретическое математическое ожидание равно
exp
exp(μ+
2
σ
2
), откуда мы сразу находим оценку
μ
^
=
log
=log(
Теперь запишем логарифм правдоподобия:
log
log
l(X)=−
i
∑
logx
(logx
i
−μ)
2
+const
Дифференцируя по
μ
μ и приравнивая производную к нулю, получаем
log
⁡
x
i
μ
MLE
=
N
1
i
∑
logx
i
что вовсе не совпадает с оценкой выше.
Несколько приукрасив ситуацию, можно сделать вывод, что первые два выборочных момента позволяют если не править миром, то уверенно восстанавливать параметры распределений. А теперь давайте представим, что мы посчитали
, а семейство распределений пока не выбрали.
Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?
10
Почему-то хочется сказать, что в первом. Почему? Второе не симметрично — но почему мы так думаем? Если мы выберем третье, то добавим дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.
Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Но чтобы эти нестрогие рассуждения превратить в формулы, придётся немного обогатить наш математический аппарат и научиться измерять количество информации.
Энтропия и дивергенция Кульбака-Лейблера
Измерять «знание» можно с помощью энтропии Шэннона. Она определяется как
log
⁡
P
(
x
)
H(P)=−
x
∑
P(x)logP(x)
для дискретного распределения и
log
H(p)=−∫p(x)logp(x)dx
для непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу.
Неформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении.
Пример 1. Рассмотрим схему Бернулли с вероятностью успеха
p
p. Энтропия её результата равна
log
log
⁡
2
p
−(1−p)⋅log
2
(1−p)−p⋅log
2
p
Давайте посмотрим на график этой функции:
10
Минимальное значение (нулевое) энтропия принимает при
p∈{0,1}. В самом деле, для такого эксперимента мы всегда можем наверняка сказать, каков будет его исход; обращаясь к другой интерпретации — чтобы сообщить кому-то о результате эксперимента, достаточно
0
0 бит (ведь получатель сообщения и так понимает, что вышло).
Максимальное значение принимается в точке
1
2
2
1
, что вполне соответствует тому, что при
предсказать исход эксперимента сложнее всего.
Пример 2. Энтропия нормального распределения
N(μ,σ
2
) равна
1
2
log
log(2πσ
2
)+
2
1
, и чем меньше дисперсия, тем меньше энтропия, что и логично: ведь когда дисперсия мала, значения сосредоточены возле матожидания, и они становятся менее «разнообразными».
Энтропия тесно связана с другим важным понятием из теории информации — дивергенцией Кульбака-Лейблера. Она определяется для
p
(
x
)
p(x)
q
(
x
)
q(x) как
log
KL(p∣∣q)=∫p(x)log
q(x)
p(x)
dx
в непрерывном случае и точно так же, но только с суммой вместо интеграла в дискретном.
Дивергенцию можно представить в виде разности:
log
log
KL(p∣∣q)=(−∫p(x)logq(x)dx)−(−∫p(x)logp(x)dx)
Вычитаемое — это энтропия, которая, как мы уже поняли, показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое похоже по виду, и можно показать, что оно говорит о том, сколько в среднем бит потребуется на кодирование случайной величины с плотностью
p
p алгоритмом, оптимизированным для кодирования случайной величины
q
q.
Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений
p
p, если при настройке алгоритма кодирования вместо
p
p использовать
q
q. Более подробно вы можете почитать, например, в этом посте.
Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности,
KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду. Но при этом она не является симметричной: вообще говоря,
KL(p∣∣q)

=KL(q∣∣p).
Вопрос на подумать. Пусть
p
(
x
)
p(x) — распределение, заданное на отрезке
[
a
,
b
]
[a,b]. Выразите энтропию через дивергенцию Кульбака-Лейблера
p
(
x
)
p(x) с равномерным на отрезке распределением
(x)=
b−a
1
I
[a,b]
(x).
Принцип максимальной энтропии
Теперь наконец мы готовы сформулировать, какие распределения мы хотим искать.
Принцип максимальной энтропии. Среди всех распределений на заданном носителе
X
X, удовлетворяющих условиям
(x)=μ
1
, ...,
(x)=μ
k
, где
u
i
u
i
— некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию.
В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории.
Давайте рассмотрим несколько примеров, которые помогут ещё лучше понять, почему некоторые распределения так популярны:
Пример 1. На конечном множестве
1
,
…
,
n
1,…,n наибольшую энтропию имеет равномерное распределение (носитель — конечное множество из
n
n элементов, других ограничений нет).
Доказательство: Пусть
i=1,…,n — некоторое распределение,
— равномерное. Запишем их дивергенцию Кульбака-Лейблера:
log
log
⁡
q
i
=
KL(p∣∣q)=
i
∑
p
i
logp
logq
log
=−H(p)+logn
=1
i
∑
p
i
Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что
H
(
p
)
⩽
log
⁡
n
H(p)⩽logn. При этом равенство возможно, только если распределения совпадают.
Пример 2. Среди распределений, заданных на всей вещественной прямой и имеющих заданные матожидание
μ
μ и дисперсию
σ
2
σ
2
наибольшую энтропию имеет нормальное распределение
N(μ,σ
2
).
Доказательство: Пусть
p
(
x
)
p(x) — некоторое распределение,
q(x)∼N(μ,σ
2
). Запишем их дивергенцию Кульбака-Лейблера:
log
log
KL(p∣∣q)=∫p(x)logp(x)dx−∫p(x)logq(x)dx=
log
=−H(p)−∫p(x)(−
2
1
log(2πσ
2
)−
2σ
2
1
(x−μ)
2
)dx=
log
=−H(p)+
2
1
log(2πσ
2
)⋅
=1
∫p(x)dx
+
2σ
2
1
=Vp=σ
2
∫(x−μ)
2
p(x)dx
log
=−H(p)+
=H(q)
2
1
log(2πσ
2
)+
2
1
Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что
H(p)⩽H(q). При этом равенство возможно, только если распределения
p
p и
q
q совпадают почти всюду, а с точки зрения теории вероятностей такие распределения различать не имеет смысла.
Пример 3. Среди распределений, заданных на множестве положительных вещественных чисел и имеющих заданное матожидание
λ
λ наибольшую энтропию имеет показательное распределение с параметром
1
λ
λ
1
(его плотность равна
exp
p(x)=
λ
1
exp(−
λ
1
x)I
(0;+∞)
(x)).
Все хорошо знакомые нам распределения, не правда ли? Проблема в том, что они свалились на нас чудесным образом. Возникает вопрос, можно ли их было не угадать, а вывести как-нибудь? И как быть, если даны не эти конкретные, а какие-то другие ограничения?
Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе.
Экспоненциальное семейство распределений
Говорят, что семейство распределений относится к экспоненциальному классу, если оно может быть представлено в следующем виде:
exp
p(x∣θ)=
h(θ)
1
g(x)⋅exp(θ
T
u(x))
где
θ
θ — вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства),
h
,
g
>
0
h,g>0,
u
u — некоторая вектор-функция, и, разумеется, сумма или интеграл по
x
x равняется единице. Последнее, в частности, означает, что
exp
h(θ)=∫g(x)exp(θ
T
u(x))dx
(или сумма в дискретном случае).
Пример 1. Покажем, что нормальное распределение принадлежит экспоненциальному классу. Для этого мы должны представить привычную нам функцию плотности
exp
p(x∣μ,σ
2
)=
2π
σ
1
exp(−
2σ
2
(x−μ)
2
)
в виде
exp
⁡
(
∑
i
(параметр)
i
⋅
(функция от x)
i
)
что-то, не зависящее от
x
p(x∣θ)=
что-то, не зависящее от x
g(x)⋅exp(∑
i
(параметр)
i
⋅(функция от x)
i
)
Распишем
1
2
π
σ
exp
exp
exp(−
2σ
2
(x−μ)
2
)=
2π
σ
1
exp(−
exp
exp
σexp(−
2σ
2
μ
2
)
exp(−
Определим
(x)=x,u
2
(x)=x
exp
h(θ)=
2π
σexp(−
2σ
2
μ
2
)
Если теперь всё-таки честно выразить
h
h через
θ
θ (это мы оставляем в качестве лёгкого упражнения), то получится
exp
p(x∣μ,σ
2
)=
h(θ)
1
exp(θ
T
u(x))
В данном случае функция
g
(
x
)
g(x) просто равна единице.
Пример 2. Покажем, что распределение Бернулли принадлежит экспоненциальному классу. Для этого попробуем преобразовать функцию вероятности (ниже
x
x принимает значения
0
0 или
1
1):
exp
⁡
(
x
log
log
P(x∣p)=p
x
(1−p)
1−x
=exp(xlogp+(1−x)log(1−p))
Теперь мы можем положить
u(x)=(x,1−x),
θ=(p,1−p), и всё получится. Единственное, что смущает, — это то, что компоненты вектора
u
(
x
)
u(x) линейно зависимы. Хотя это не является формальной проблемой, но всё же хочется с этим что-то сделать. Исправить это можно, если переписать
exp
⁡
(
x
log
log
(1−p)
1−x
=(1−p)exp(xlogp+(−x)log(1−p))=
exp
⁡
(
x
log
=(1−p)exp(xlog
1−p
p
)
и определить уже минимальное представление с
u(x)=x,
θ
=
log
⁡
p
1
−
p
θ=log
1−p
p
— мы ведь уже сталкивались с этим выражением, когда изучали логистическу регрессию, не так ли?
Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках
U[a,b]? Казалось бы, да: так как:
exp
⁡
(
0
)
p(x)=
b−a
1
I
[a,b]
(x)exp(0)
В чём может быть подвох?
Как мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать
Y
∣
X
Y∣X.
В том числе:
нормальное;
распределение Пуассона;
экспоненциальное;
биномиальное, мультиномиальное (с фиксированным числом испытаний);
геометрическое;
χ
2
χ
2
-распределение;
бета-распределение;
гамма-распределение;
распределение Дирихле.
К экспоненциальным семействам не относятся, к примеру:
равномерное распределение на отрезке;
t
t-распределение Стьюдента;
распределение Коши;
смесь нормальных распределений.
MLE для семейства из экспоненциального класса
Возможно, вас удивил странный и на первый взгляд не очень естественный вид
p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.
Запишем функцию правдоподобия выборки
X=(x
1
,…,x
exp
p(X∣θ)=h(θ)
−N
⋅(
i=1
∏
N
g(x
i
))⋅exp(θ
T
[
i=1
∑
N
u(x
i
)])
Её логарифм равен
log
log
l(X∣θ)=−Nlogh(θ)+
i=1
∑
N
logg(x
i
)+θ
T
[
i=1
∑
N
u(x
i
)]
Дифференцируя по
θ
θ, получаем
log
l(X∣θ)=−N∇
θ
logh(θ)+[
i=1
∑
N
u(x
i
)]
Тут нам потребуется следующая
Лемма.
∇
θ
log
logh(θ)=Eu(x)
Доказательство:
Как мы уже отмечали в прошлом пункте:
exp
h(θ)=∫g(x)exp(θ
T
u(x))dx
Следовательно,
∇
θ
log
exp
exp
logh(θ)=
∫g(x)exp(θ
T
u(x))dx
∇
θ
∫g(x)exp(θ
T
u(x))dx
exp
h(θ)
∫u(x)g(x)exp(θ
T
u(x))dx
exp
=∫u(x)⋅
h(θ)
1
g(x)exp(θ
T
u(x))dx=Eu(x)
Кстати, можно ещё доказать, что
log
Cov
logh(θ)=Cov(u
i
(x),u
j
(x))
Приравнивая
l(X∣θ) к нулю и применяя лемму, мы получаем, что
Eu(x)=
N
1
[
i=1
∑
N
u(x
i
)]
Таким образом, теоретические матожидания всех компонент
(x) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для
(x) в качестве моментов.
И в следующем пункте выяснится, что распределения из семейств, относящихся к экспоненциальному классу, это те самые распределения, которые имеют максимальную энтропию из тех, что имеют заданные моменты
(x).
**Пример.**Рассмотрим вновь логнормальное распределение:
exp
⁡
(
−
(
log
p(x)=
x
2πσ
2
1
exp(−
2σ
2
(logx−μ)
exp
log
log
2πσ
2
1
exp(−
2σ
2
1
log
2
x+
σ
2
μ
logx−
exp
exp
log
log
2πσ
2
exp(
exp
(x)
logx
(x)
log
exp
exp
−πθ
2
−1
⋅exp−
exp(θ
1
u
1
(x)+θ
2
u
2
(x))
Как видим, логнормальное распределение тоже из экспоненциального класса. Вас может это удивить: ведь выше мы обсуждали, что для него метод моментов и метод максимального правдоподобия дают разные оценки.
Но никакого подвоха тут нет: мы просто брали не те моменты. В данном случае
log
⁡
x
u
1
(x)=logx,
log
⁡
2
x
u
2
(x)=log
2
x, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться
E
log
log
⁡
x
i
,
E
log
log
⁡
2
x
i
Elogx=
N
1
i
∑
logx
i
,Elog
log
2
x
i
Матожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле:
E
log
log
Elogx=
∂θ
1
∂
logh(θ)=
log
⁡
π
+
1
2
log
logπ+
2
1
logθ
)=−
log
log
Elog
2
x=
∂θ
2
∂
logh(θ)=
Теорема Купмана-Питмана-Дармуа
Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.
В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.
Теорема. Пусть
exp
p(x)=
h(θ)
1
exp(θ
T
u(x)) — распределение, причём
θ
θ — вектор длины
n
n и
(x)=α
i
для некоторых фиксированных
i=1,…,n. Тогда распределение
p
(
x
)
p(x) обладает наибольшей энтропией среди распределений с тем же носителем, для которых
(x)=α
i=1,…,n. При этом оно — единственное с таким свойством: в том смысле, что любое другое распределение, обладающее этим свойством, совпадает с ним почти всюду.
Рассмотрим несколько примеров:
Пример 1. Среди распределений на множестве
{1,2,3,…} неотрицательных целых чисел с заданным математическим ожиданием
μ
μ найдём распределение с максимальной энтропией.
В данном случае у нас лишь одна функция
(x)=x, которая соответствует фиксации матожидания
E
x
Ex. Плотность будет вычисляться только в точках
x
=
k
x=k,
k=1,2,… и будет иметь вид
exp
=p(k)=
h(θ)
1
exp(θk)
В этой формуле уже безошибочно угадывается геометрическое распределение с
p=1−e
θ
. Параметр
p
p можно подобрать из соображений того, что математическое ожидание равно
μ
μ. Матожидание геометрического распределения равно
1
p
p
1
, так что
. Окончательно,
(1−
μ
1
)
k−1
Пример 2. Среди распределений на всей вещественной прямой с заданным математическим ожиданием
μ
μ найдём распределение с максимальной энтропией.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
4.1. Вероятностный подход в ML
Как описать привычные модели на языке статистики. Оптимизация функции потерь vs оценка максимального правоподобия
Следующий параграф
4.3. Обобщённые линейные модели
Как прокачать линейную модель с помощью распределений из экспоненциального класса
