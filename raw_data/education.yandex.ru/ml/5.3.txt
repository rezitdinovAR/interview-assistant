---
title: Метод обратного распространения ошибки
url: https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki
course: ml
chapter: 5. Глубинное обучение - введение
chapter_id: 5.3
---
Как эффективно посчитать градиенты по весам нейронной сети
Нейронные сети обучаются с помощью тех или иных модификаций градиентного спуска, а чтобы применять его, нужно уметь эффективно вычислять градиенты функции потерь по всем обучающим параметрам. Казалось бы, для какого-нибудь запутанного вычислительного графа это может быть очень сложной задачей, но на помощь спешит метод обратного распространения ошибки.
Метод обратного распространения ошибки (backward propagation)
Открытие метода обратного распространения ошибки стало одним из наиболее значимых событий в области искусственного интеллекта. В актуальном виде он был предложен в 1986 году Дэвидом Э. Румельхартом, Джеффри Э. Хинтоном и Рональдом Дж. Вильямсом, а также независимо и одновременно красноярскими математиками С. И. Барцевым и В. А. Охониным.
С тех пор для нахождения градиентов параметров нейронной сети используется метод вычисления производной сложной функции, и оценка градиентов параметров сети стала хоть и сложной инженерной задачей, но уже не искусством. Несмотря на простоту используемого математического аппарата, появление этого метода привело к значительному скачку в развитии искусственных нейронных сетей.
Суть метода можно записать одной формулой, тривиально следующей из формулы производной сложной функции: если
f(x)=g
m
(g
m−1
(…(g
1
(x))…)), то
m−1
∂g
m
∂g
m−2
∂g
m−1
. Уже сейчас мы видим, что градиенты можно вычислять последовательно, в ходе одного обратного прохода, начиная с
m−1
∂g
m
и умножая каждый раз на частные производные предыдущего слоя.
Backward propagation в одномерном случае
В одномерном случае всё выглядит особенно просто. Пусть
w
0
w
0
— переменная, по которой мы хотим продифференцировать , причём сложная функция имеет вид
f(w
0
)=g
m
(g
m−1
(…g
1
(w
0
)…)),
где все
g
i
g
i
скалярные. Тогда
)=g
m
′
(g
m−1
(…g
1
(w
0
)…))⋅g
m−1
′
(g
m−2
(…g
1
(w
0
)…))⋅…⋅g
1
′
(w
0
)
Суть этой формулы такова. Если мы уже совершили прямой проход (forward propagation), значит мы уже знаем
),g
2
(g
1
(w
0
)),…,g
m−1
(…g
1
(w
0
)…),
Поэтому мы можем действовать следующим образом:
берём производную
g
m
g
m
в точке
m−1
(…g
1
(w
0
)…);
умножаем на производную
g
m
−
1
g
m−1
в точке
m−2
(…g
1
(w
0
)…);
и так далее, пока не дойдём до производной
g
1
g
1
в точке
w
0
w
0
.
Проиллюстрируем это на картинке, расписав по шагам дифференцирование по весам
w
i
w
i
функции потерь логистической регрессии на одном объекте (то есть для батча размера 1):
17
Собирая все множители вместе, получаем:
=(−y)⋅e
−y(w
1+e
−y(w
⋅(−y)⋅e
−y(w
1+e
−y(w
⋅(−y)⋅e
−y(w
1+e
−y(w
Таким образом, сперва совершается forward propagation для вычисления всех промежуточных значений (да, все промежуточные представления нужно будет хранить в памяти), а потом запускается backward propagation, на котором в один проход вычисляются все градиенты.
Почему же нельзя просто пойти и начать везде вычислять производные?
В параграфе, посвящённом матричным дифференцированиям, мы поднимаем вопрос о том, что вычислять частные производные по отдельности — это зло, лучше пользоваться матричными вычислениями. Но есть и ещё одна причина: даже и с матричной производной в принципе не всегда хочется иметь дело.
Рассмотрим простой пример. Допустим, что
r+1
— два последовательных промежуточных представления
N
×
M
N×M и
N
×
K
N×K, связанных функцией
r+1
=f
r+1
(X
r
). Предположим, что мы как-то посчитали производную
r+1
∂L
функции потерь
L
L, тогда
i,j
r+1
∂X
ij
r+1
∂L
И мы видим, что, хотя оба градиента
r+1
— это просто матрицы, в ходе вычислений возникает «четырёхмерный кубик»
r+1
. Его болезненно даже хранить: уж больно много памяти он требует —м
MK по сравнению с безобидными
N
M
+
N
K
NM+NK, требуемыми для хранения градиентов.
Поэтому хочется промежуточные производные
r+1
рассматривать не как вычисляемые объекты
r+1
, а как преобразования, которые превращают
r+1
Целью следующих параграфов будет именно это: понять, как преобразуется градиент в ходе error backward propagation при переходе через тот или иной слой.
Вы спросите себя: надо ли мне сейчас пойти и прочитать параграф учебника про матричное дифференцирование?
Короткий ответ: Зависит от ваших знаний.
Длинный ответ: Найдите производную функции по вектору
— матрица размера
n
×
n
f(x)=x
T
Ax, A∈Mat
n
R — матрица размера n×n
А как всё поменяется, если
A
A тоже зависит от
x
x? Чему равен градиент функции, если
A
A является скаляром?
Если вы готовы прямо сейчас взять ручку и бумагу и посчитать всё, то вам, вероятно, не надо читать про матричные дифференцирования. Но мы советуем всё-таки заглянуть в этот параграф, если обозначения, которые мы будем дальше использовать, покажутся вам непонятными: единой нотации для матричных дифференцирований человечество пока, увы, не изобрело, и переводить с одной на другую не всегда легко.
А мы же сразу перейдём к интересующей нас вещи: к вычислению градиентов сложных функций.
Градиент сложной функции
Напомним, что формула производной сложной функции выглядит следующим образом:
(u∘v)](h)=[D
v(x
0
)
u]([D
x
0
v](h))
Теперь разберёмся с градиентами. Пусть
f(x)=g(h(x)) – скалярная функция. Тогда
f](x−x
0
)=⟨∇
x
0
f,x−x
0
⟩.
С другой стороны,
h(x
0
)
g]([D
x
0
h](x−x
0
))=⟨∇
h
x
0
g,[D
x
0
h](x−x
0
)⟩=⟨[D
x
0
h]
∗
∇
h(x
0
)
g,x−x
0
⟩.
То есть
f=[D
x
0
h]
∗
∇
h(x
0
)
g — применение сопряжённого к
h линейного отображения к вектору
h(x
0
)
g.
Эта формула — сердце механизма обратного распространения ошибки. Она говорит следующее: если мы каким-то образом получили градиент функции потерь по переменным из некоторого промежуточного представления
X
k
X
k
нейронной сети и при этом знаем, как преобразуется градиент при проходе через слой
f
k
f
k
между
X
k
−
1
X
k−1
и
X
k
X
k
(то есть как выглядит сопряжённое к дифференциалу слоя между ними отображение), то мы сразу же находим градиент и по переменным из
X
k
−
1
X
k−1
:
17
Таким образом слой за слоем мы посчитаем градиенты по всем
X
i
X
i
вплоть до самых первых слоёв.
Далее мы разберёмся, как именно преобразуются градиенты при переходе через некоторые распространённые слои.
Градиенты для типичных слоёв
Рассмотрим несколько важных примеров.
Пример №1
f(x)=u(v(x)), где
x
x — вектор, а
v
(
x
)
v(x) – поэлементное применение
v(x
1
)
⋮
v(x
N
)
Тогда, как мы знаем,
f](h)=⟨∇
x
0
f,h⟩=[∇
x
0
f]
T
h.
Следовательно,
v(x
0
)
u]([D
x
0
v](h))
=[∇
v(x
)⊙h)=
=
i
∑
[∇
v(x
=⟨[∇
v(x
0
)
u]⊙v
′
(x
0
),h⟩.
где
⊙
⊙ означает поэлементное перемножение. Окончательно получаем
f=[∇
v(x
0
)
u]⊙v
′
(x
0
)=v
′
(x
0
)⊙[∇
v(x
0
)
u]
Отметим, что если
x
x и
h
(
x
)
h(x) — это просто векторы, то мы могли бы вычислять всё и по формуле
)⋅(
∂z
j
∂h
).
В этом случае матрица
) была бы диагональной (так как
z
j
z
j
зависит только от
x
j
x
j
: ведь
h
h берётся поэлементно), и матричное умножение приводило бы к тому же результату. Однако если
x
x и
h
(
x
)
h(x) — матрицы, то
) представлялась бы уже «четырёхмерным кубиком», и работать с ним было бы ужасно неудобно.
Пример №2
f(X)=g(XW), где
X
X и
W
W — матрицы. Как мы знаем,
f](X−X
0
)=tr([∇
X
0
f]
T
(X−X
0
)).
Тогда
g]([D
X
0
(∗W)](H))=[D
X
0
W
g](HW)=
=tr([∇
X
0
W
g]
T
⋅(H)W)=
=tr(W[∇
X
0
W
(g)]
T
⋅(H))=tr([[∇
X
0
W
g]W
T
]
T
(H))
Здесь через
∗
W
∗W мы обозначили отображение
Y
↪
Y
W
Y↪YW, а в предпоследнем переходе использовалось следующее свойство следа:
tr(ABC)=tr(CAB),
где
A
,
B
,
C
A,B,C — произвольные матрицы подходящих размеров (то есть допускающие перемножение в обоих приведённых порядках). Следовательно, получаем
f=[∇
X
0
W
(g)]⋅W
T
Пример №3
f(W)=g(XW), где
W
W и
X
X — матрицы. Для приращения
H=W−W
0
имеем
f](H)=tr([∇
W
0
f]
T
(H))
Тогда
g]([D
W
0
(X∗)](H))=[D
XW
0
g](XH)=
=tr([∇
XW
0
g]
T
⋅X(H))=tr([X
T
[∇
XW
0
g]]
T
(H))
Здесь через
X
∗
X∗ обозначено отображение
Y
↪
X
Y
Y↪XY. Значит,
f=X
T
⋅[∇
XW
0
(g)]
Пример №4
f(X)=g(softmax(X)), где
X
X — матрица
N
×
K
N×K, а
softmax — функция, которая вычисляется построчно, причём для каждой строки
softmax(x)=(
,…,
В этом примере нам будет удобно воспользоваться формализмом с частными производными. Сначала вычислим
для одной строки
x
x, где через
s
l
s
l
мы для краткости обозначим
softmax(x)
. Нетрудно проверить, что
(1−s
j=l,
j

=l
Так как softmax вычисляется независимо от каждой строчки, то
(1−s
r=i,j=l,
r=i,j

=l,
r

=i
,
где через
s
r
l
s
rl
мы обозначили для краткости
softmax(X)
rl
.
Теперь пусть
=∇g=
∂s
rl
∂L
(пришедший со следующего слоя, уже известный градиент). Тогда
r,l
Так как
=0 при
r
≠
i
r

=i, мы можем убрать суммирование по
=−s
i1
s
ij
∇
i1
−…+s
ij
(1−s
ij
)∇
ij
−…−s
=−s
Таким образом, если мы хотим продифференцировать
f
f в какой-то конкретной точке
X
0
X
0
, то, смешивая математические обозначения с нотацией Python, мы можем записать:
=−softmax(X
0
)⊙sum (softmax(X
0
)⊙∇
softmax(X
0
)
g, axis=1)+
softmax(X
0
)⊙∇
softmax(X
0
)
g
Backward propagation в общем виде
Подытожим предыдущее обсуждение, описав алгоритм error backward propagation (алгоритм обратного распространения ошибки). Допустим, у нас есть текущие значения весов
и мы хотим совершить шаг SGD по мини-батчу
X
X. Мы должны сделать следующее:
Совершить forward propagation, вычислив и запомнив все промежуточные представления
X=X
0
,X
1
,…,X
m
=
y
.
Вычислить все градиенты с помощью backward propagation.
С помощью полученных градиентов совершить шаг SGD.
Проиллюстрируем алгоритм на примере двухслойной нейронной сети со скалярным output. Для простоты опустим свободные члены в линейных слоях.
17
Обучаемые параметры – матрицы
U
U и
W
W. Как найти градиенты по ним в точке
L=∇
W
0
(
2
1
L∘h∘[W↦g(XU
0
)W])=
=g(XU
0
)
T
∇
g(XU
0
)W
0
(L∘h)=
k×N
g(XU
N×1
h
′
(∫
0
1
g(XU
0
)W
0
)
⊙
N×1
∇
h(∫
0
1
g(XU
0
)W
0
)
L
Итого матрица
k
×
1
k×1, как и
L=∇
U
0
(
2
1
L∘h∘[Y↦YW
0
]∘g∘[U↦XU])=
L∘h∘[Y↦YW
0
]∘g)=
(XU
0
)⊙∇
g(XU
0
)
[∈
0
1
L∘h∘[Y↦YW
0
])
=
…
=
XTD×N
=…=
D×N
X
T
⋅
2
1
N×K
g
′
(XU
0
)
⊙
N×K
∫
0
1
N×1
h
′
(∫
0
1
g(XU
0
)W
0
)
⊙
N×1
∇
h(∫
0
1
g(XU
1×K
W
T
Итого
D
×
K
D×K, как и
U
0
U
0
Схематически это можно представить следующим образом:
17
Backward propagation для двухслойной нейронной сети
Если вы не уследили за вычислениями в предыдущем примере, давайте более подробно разберём его чуть более конкретную версию (для
g
=
h
=
σ
g=h=σ)
Рассмотрим двуслойную нейронную сеть для классификации. Мы уже встречали её ранее при рассмотрении линейно неразделимой выборки. Предсказания получаются следующим образом:
=σ(X
1
W
2
)=σ((σ(X
0
W
1
))W
2
).
Пусть
— текущее приближение матриц весов. Мы хотим совершить шаг по градиенту функции потерь, и для этого мы должны вычислить её градиенты по
в точке
Прежде всего мы совершаем forward propagation, в ходе которого мы должны запомнить все промежуточные представления:
=σ(X
=σ(X
=σ(σ(X
. Они понадобятся нам дальше.
Для полученных предсказаний вычисляется значение функции потерь:
log
log
l=L(y,
y
)=ylog(
y
)+(1−y)log(1−
y
).
Дальше мы шаг за шагом будем находить производные по переменным из всё более глубоких слоёв.
Градиент
L
L по предсказаниям имеет вид
1−y
=
y
(1−
y
)
y−
y
,
где, напомним,
=σ(X
3
)=σ((σ(X
0
W
0
1
))W
0
2
) (обратите внимание на то, что
тут именно те, из которых мы делаем градиентный шаг).
Следующий слой — поэлементное взятие
σ
σ. Как мы помним, при переходе через него градиент поэлементно умножается на производную
σ
σ, в которую подставлено предыдущее промежуточное представление:
l=σ
′
(X
3
)⊙∇
y
l=σ(X
3
)(1−σ(X
3
))⊙
y
(1−
=σ(X
3
)(1−σ(X
3
))⊙
σ(X
3
)(1−σ(X
3
))
y−σ(X
l=(X
l=(X
2
)
T
⋅(y−σ(X
3
))=
=(σ(X
⋅(y−σ(σ(X
Аналогичным образом
l=∇
X
3
l⋅(W
0
2
)
T
=(y−σ(X
3
))⋅(W
=(y−σ(X
2
W
0
2
))⋅(W
0
2
)
T
Следующий слой — снова взятие
l=σ
′
(X
1
)⊙∇
X
2
l=σ(X
1
)(1−σ(X
1
))⊙((y−σ(X
2
W
0
2
))⋅(W
=σ(X
1
)(1−σ(X
1
))⊙((y−σ(σ(X
1
)W
0
2
))⋅(W
0
2
)
T
)
Наконец, последний слой — это умножение
. Тут мы дифференцируем только по
l=(X
l=(X
0
)
T
⋅(σ(X
1
)(1−σ(X
1
))⊙(y−σ(σ(X
1
)W
0
2
))⋅(W
=(X
0
)
T
⋅(σ(X
0
W
0
1
)(1−σ(X
0
W
0
1
))⊙(y−σ(σ(X
))⋅(W
0
2
)
T
)
Итоговые формулы для градиентов получились страшноватыми, но они были получены друг из друга итеративно с помощью очень простых операций: матричного и поэлементного умножения, в которые порой подставлялись значения заранее вычисленных промежуточных представлений.
Автоматизация и autograd
Итак, чтобы нейросеть обучалась, достаточно для любого слоя
k−1
↦X
k
с параметрами
W
k
W
k
уметь:
превращать
L в
k−1
L (градиент по выходу в градиент по входу);
считать градиент по его параметрам
При этом слою совершенно не надо знать, что происходит вокруг. То есть слой действительно может быть запрограммирован как отдельная сущность, умеющая внутри себя делать forward propagation и backward propagation, после чего слои механически, как кубики в конструкторе, собираются в большую сеть, которая сможет работать как одно целое.
Более того, во многих случаях авторы библиотек для глубинного обучения уже о вас позаботились и создали средства для автоматического дифференцирования выражений (autograd). Поэтому, программируя нейросеть, вы почти всегда можете думать только о forward-проходе, прямом преобразовании данных, предоставив библиотеке дифференцировать всё самостоятельно.
Это делает код нейросетей весьма понятным и выразительным (да, в реальности он тоже бывает большим и страшным, но сравните на досуге код какой-нибудь разухабистой нейросети и код градиентного бустинга на решающих деревьях и почувствуйте разницу).
Но это лишь начало
Метод обратного распространения ошибки позволяет удобно посчитать градиенты, но дальше с ними что-то надо делать, и старый добрый SGD едва ли справится с обучением современной сетки. Так что же делать? О некоторых приёмах мы расскажем в следующем параграфе.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
5.2. Первое знакомство с полносвязными нейросетями
Основные понятия глубинного обучения. Базовые слои и функции активации
Следующий параграф
5.4. Тонкости обучения
Инициализация весов. Регуляризация нейросетей. Dropout и Batchnorm
