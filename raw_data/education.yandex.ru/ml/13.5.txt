---
title: Ландшафт функции потерь
url: https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter
course: ml
chapter: 13. Теория глубокого обучения
chapter_id: 13.5
---
ставится как задача минимизации эмпирического риска
(θ)=E
x,y∈S
m
r(y,f
θ
(x)),
где
S
m
S
m
– выборка размера
m
m, а
r
r – функция риска, например,
r(y,
y
^
)=I[y

=
y
^
]. Часто интересующая нас функция риска не дифференцируема по второму аргументу, что делает градиентную оптимизацию неприменимой. По этой причине вместо исходной функции риска
r
r вводят её дифференцируемый выпуклый суррогат, то есть некоторую выпуклую и дифференцируемую по второму аргументу функцию
ℓ
≥
r
ℓ≥r. Новый функционал эмпирического риска имеет вид
(θ)=E
x,y∈S
m
ℓ(y,f
θ
(x)).
Если
f
θ
f
θ
дифференцируема по
θ
θ, то из дифференцируемости
ℓ
ℓ следует дифференцируемость
, что делает возможной градиентную оптимизацию. А если
f
θ
f
θ
выпукла по
θ
θ, то из выпуклости
ℓ
ℓ следует выпуклость
, что даёт гарантии на сходимость градиентного спуска в глобальный минимум.
Увы, в общем случае нейронные сети не выпуклы как функции своих весов. Это можно увидеть на простом примере. Пусть
(x)=uvx, где
u
u,
v
v и
x
x – скаляры, а
θ=(u,v). Гессиан
f
f как функции
θ
θ в любой точке равен
); его собственные числа равны
x
x и
(
−
x
)
(−x), что и означает, что для любого ненулевого
x
x функция
f
f не выпукла.
Таким образом, даже для выпуклой
ℓ
ℓ функция потерь
нейронной сети не обязана быть выпуклой функцией весов.
У невыпуклых функций могут быть минимумы, не являющиеся глобальными, в которых может «застревать» градиентный спуск. Тем не менее, на практике часто оказывается, что градиентный спуск всегда находит точку со сколь угодно близким к глобальному минимуму значением функции потерь.
Это наблюдение приводит к гипотезе, что, хотя поверхность функции потерь не обязана быть выпуклой, все её минимумы глобальны для используемых нами сетей и тех наборов данных, на которых мы их обучаем.
Известны два случая, для которых эту гипотезу удаётся доказать. Первый – это линейные сети. Второй – это достаточно широкие нелинейные сети (ширина одного из слоёв не меньше числа примеров в выборке). К сожалению, оба примера нереалистичны: выразительная способность линейных сетей не выше, чем у обыкновенной линейной модели, а ширина реальных нейронных сетей не настолько велика (порядка
1
0
3
10
3
нейронов против
1
0
6
10
6
примеров в ImageNet), причём улучшить оценку на ширину в общем случае невозможно, см. Q. Nguyen A note on connectivity of sublevel sets in deep learning. Возможно, для получения лучших оценок исследователям предстоит научиться учитывать структуру данных обучающей выборки.
Исторически первое доказательство глобальности всех локальных минимумов линейной сети содержится в работе Deep learning without poor local minima. Более простое доказательство в немного более общем случае можно найти в работе Depth creates no bad local minima. Ещё более простое доказательство есть в работе Deep linear networks with arbitrary loss: All local minima are global, но оно подходит только для сетей без боттлнеков (
n
l
≥
min
≥min(n
0
,n
L+1
∀l∈1,…,L). Последнее подробно разобрано в конспекте лекций автора этого параграфа.
Здесь мы разберём только второй случай (достаточно широкие нелинейные сети) как потенциально более перспективный.
Все минимумы достаточно широкой нелинейной сети глобальны
Рассмотрим нейронную сеть с одним скрытым слоем:
f(x)=W
1
x
1
(x)∈R
n
2
,x
1
(x)=ϕ(h
1
(x))∈R
n
1
,h
1
(x)=W
0
x∈R
n
1
,x∈R
n
0
,
где функция активации
ϕ
ϕ применяется поэлементно. Рассмотрим набор данных
(
X
,
Y
)
(X,Y) размера
m
m, где
X∈R
n
0
×m
, а
Y∈R
n
2
×m
. Применяя соотношения выше к этому набору, получим следующие значения выходов слоёв:
=ϕ(H
1
)∈R
X∈R
n
1
×m
,X∈R
n
0
×m
.
Поставим задачу оптимизации квадратичной функции потерь
min
L(W
0:1
)=∣∣Y−
0:1
min
,
где
∣∣⋅∣∣
F
– норма Фробениуса.
Теорема 1 (On the local minima free condition of backpropagation learning) Если
ϕ
ϕ аналитична, ограничена и не тождественно равна нулю, ширина скрытого слоя
n
1
n
1
не меньше
m
m и все столбцы матрицы
X
X различны, то все локальные минимумы
L(W
0:1
) глобальны.
Доказательство. Пусть
0:1
∗
– локальный минимум
L(W
0:1
), и пусть
– соответствующие ему скрытые представления. Тогда
– локальный минимум
)=∣∣Y−W
Задача оптимизации
) выпуклая, поэтому
– глобальный минимум
Если
rkX
1
∗
=m, то система
1,i
X
1
∗
, где
W
1
,
i
W
1,i
– неизвестная матрица, гарантировано имеет решение для каждого
i∈1…,n
2
. Следовательно,
min
L(W
0:1
∗
)=L
)=minL
W
0
∗
(W
1
)=0,
а значит,
0:1
∗
– глобальный минимум
L(W
0:1
).
Заметим, что для выполнения равенства
rkX
1
∗
=m необходимо
≥m.
Пусть теперь
rkX
1
∗
<m. Если тем не менее
min
minL
)=0, то
0:1
∗
– по-прежнему глобальный минимум
L(W
0:1
). Пусть
min
L(W
0,1
∗
)=L
)=minL
W
0
∗
(W
1
)>0.
Докажем, что
0,1
∗
не может быть локальным минимумом
L
L до тех пор, пока выполнены условия следующей леммы, которую мы докажем позже:
Лемма 1. Если
≥m, функция
ϕ
ϕ аналитична, ограничена и не тождественно равна нулю, а все столбцы матрицы
X
X различны, то лебегова мера множества
:rkX
1
<m} равна нулю.
Так как
L(W
0,1
∗
)>0 и
L
L – непрерывна как функция от
W
0
,
1
W
0,1
, существует
ϵ
>
0
ϵ>0, для которого
0,1
∈B
ϵ
(W
0,1
∗
):L(W
0,1
)>0,
где через
0,1
∗
) мы обозначили
ϵ
ϵ-окрестность точки
0,1
∗
в пространстве весов.
Из леммы 1 следует, что для любого
δ
>
0
δ>0 найдётся
), для которого
rkX
1
′
=m. Возьмём
δ∈(0,ϵ). Для соответствующего
имеем
L(W
0
′
,W
1
∗
)>0; при этом
rkX
1
′
=m. Как было отмечено выше, задача минимизации
) выпуклая, и оптимум её равен нулю, так как
rkX
1
′
=m. Поэтому градиентный спуск, применённый к
и стартующий в
, сойдётся в некоторую точку
∗,′
, для которой
∗,′
)=0.
Мы знаем, что в нашей эпсилон-окрестности функция потерь положительна, значит, найденная точка находится вне её:
∗,′
)∈
/
B
ϵ
(W
0,1
∗
).
Таким образом, найдётся
ϵ
>
0
ϵ>0 такое, что для любых
δ∈(0,ϵ) существует пара
)∈B
δ
(W
0,1
∗
) такая, что градиентный спуск, примененный к
L
L, стартующий в
) и действующий только на
W
1
W
1
, сходится в точку
∗,′
)∈
/
B
ϵ
(W
0,1
∗
).
Очевидно, что если «для любых
δ∈(0,ϵ)» заменить на «для любых
δ
>
0
δ>0», утверждение выше останется верным. Это означает, что динамика градиентного спуска, действующего только на
W
1
W
1
, не устойчива по Ляпунову в точке
0,1
∗
. Следовательно,
0,1
∗
не может быть точкой минимума (иначе градиентный спуск был бы устойчив), а значит, условие
L(W
0,1
∗
)>0 невыполнимо в условиях леммы 1. Таким образом, все локальные минимумы
L
L глобальны. Теорема 1 доказана.
Доказательство леммы 1. Пусть
I
m
I
m
– наборов индексов из
1,…,n
1
длины
m
m. Рассмотрим
1,I
m
∈R
m×m
– подматрицу матрицы
X
1
X
1
, состоящую из строк
X
1
X
1
, проиндексированных набором
I
m
I
m
. В терминах
I
m
I
m
условие
rk
X
1
<
m
rkX
1
<m эквивалентно
det
detX
1,I
Так как
ϕ
ϕ аналитична, а определитель – аналитическая функция элементов матрицы,
det
detX
1,I
m
– аналитическая функция от
W
0
W
0
для любого
I
m
I
m
.
Нам понадобится следующая лемма, доказательство которой вы можете найти в The loss surface of deep and wide neural networks (лемма 4.3):
Лемма 2. В условиях леммы 1, найдётся
W
0
W
0
, для которого
rk
X
1
=
m
rkX
1
=m.
Из леммы 2 и эквивалентности выше следует, что найдётся
W
0
W
0
, такой что для некоторого
I
m
I
m
имеет место неравенство
det
detX
1,I
m

=0. Так как определитель
X
1
,
m
X
1,m
– аналитическая функция
W
0
W
0
, а всякая не тождественно нулевая аналитическая функция принимает значение ноль лишь на множестве меры ноль по Лебегу, то лебегова мера множества
{
W
0
:
det
:detX
1,I
m
=0} равна нулю. Таким образом, лемма 1 доказана.
Обобщения
При доказательстве теоремы 1 мы воспользовались следующими условиями:
Все обучающие примеры (столбцы матрицы
X
X) различны;
Число скрытых слоёв
L
L равно одному;
Ширина (последнего) скрытого слоя не меньше числа примеров:
≥m;
Функция активации
ϕ
ϕ аналитична, ограничена и не тождественно равна нулю;
Функция ошибки квадратична.
Можем ли мы ослабить какие-то из них?
Если какие-то из примеров совпадают и соответствующие метки также одинаковы, теорема обобщается тривиально. Если же метки не совпадают, то нулевая ошибка, вообще говоря, недостижима. Тем не менее, доказательство меняется по большому счёту лишь в том, что вместо
m
m будет фигурировать число различных примеров.
Рассмотрим сеть с
L
L скрытыми слоями, действующую на набор данных
X
0
X
0
размера
L+1
×m
,X
l
=ϕ(H
l
)∈R
l−1
X
l−1
∈R
n
l
×m
∀l∈[L],X
Для обобщения теоремы 1 на глубокие сети с широким последним скрытым слоем, достаточно обобщить лемму 1. Например, можно воспользоваться следующим результатом (лемма 4.4 из The loss surface of deep and wide neural networks)
Лемма 3. Пусть
ϕ
ϕ аналитична, ограничена и не тождественно равна нулю, и пусть
l∈1,…,L. Тогда если
≥m и все строки матрицы
X
0
X
0
различны, то лебегова мера множества
0:l−1
:rkX
l
<m} равна нулю.
Третье предположение можно попытаться ослабить с двух сторон.
Во-первых, можно требовать меньшего числа нейронов в скрытом слое. В общем случае этот подход не работает: в статье A note on connectivity of sublevel sets in deep learning доказывается, что
m
m – это наименьшая ширина, при которой теорема выполняется для набора данных общего вида с различными примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки.
Во-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных:
≥m для некоторого
l
<
L
l<L. Но тогда задача
0:l−1
∗
(W
l:L
) не выпукла, а значит, из
rkX
l
∗
=m не следует, что
L(W
0:L
∗
)=0, и градиентный спуск, действующий на
W
l
:
L
W
l:L
, не обязан сходиться в точку, в которой
L
=
0
L=0 (он может застрять в локальном минимуме). Тем не менее, поставив ряд дополнительных условий, теорему 1 можно обобщить:
Теорема 2. Пусть
0:L
∗
– локальный минимум
L(W
0:L
)=∥
Y
^
−Y∥
F
2
и выполнены следующие условия:
ϕ
ϕ аналитична, ограничена, не тождественно равна нулю;
производная
ϕ
ϕ нигде не обращается в ноль;
≥m;
rkW
∈{l+1,…,L};
det
det(∇
W
l+1:L
2
L(W
0:L
∗
))

=0.
Тогда
0:L
∗
– глобальный минимум
L(W
0:L
).
Условие 4 необходимо, чтобы из
rkX
l
∗
=m следовало
L(W
0:L
∗
)=0. Отметим, что из условия 4 также следует, что
l+1
>L, то есть нейронная сеть должна сужаться, начиная со следующего после самого широкого слоя.
Условие 5 необходимо, чтобы в случае
rkX
l
∗
<m построить малое возмущение минимума
0:L
∗
, которое снова является минимумом, но для которого
rk
X
l
=
m
rkX
l
=m; невырожденный гессиан позволяет применить для этого теорему об обратной функции.
Если функция активации
ϕ
ϕ не аналитична, то лемма 3 неверна. В самом деле, для однородной
ϕ
ϕ (например, для ReLU или leaky ReLU) паттерны активаций,
), не меняются при малом возмущении весов. Значит, мы, вообще говоря, не можем найти такое малое возмущение, для которого ранг
X
1
X
1
будет полным.
Вместо малого возмущения в работе On Connected Sublevel Sets in Deep Learning явно строятся пути в пространстве весов, на которых функция потерь не возрастает и достигает нуля. Если такой путь можно построить из произвольной точки, то все (строгие) локальные минимумы глобальны. Оказывается, что для построения такого пути аналитичность функции активации не требуется. Более того, при определённых условиях можно доказать, что из любых двух точек в пространстве весов можно построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, что множество подуровня
((−∞,E)) связно при любом
E
>
0
E>0 – эффект, впервые эмпирически обнаруженный в работах Loss surfaces, mode connectivity, and fast ensembling of DNNs и Essentially No Barriers in Neural Network Energy Landscape.
Связность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня
E
>
0
E>0, то для достаточно малого
ϵ
>
0
ϵ>0 ему бы соответствовала отдельная связная компонента множества подуровня
E
+
ϵ
E+ϵ. С другой стороны, если все локальные минимумы глобальны, но изолированы, то множество подуровня
ϵ
ϵ несвязно для достаточно малого
ϵ
>
0
ϵ>0.
Вместо того, чтобы строить пути, на которых функция потерь достигает нуля, можно строить пути, на которых функция потерь достигает сколь угодно малого значения
ϵ
>
0
ϵ>0. Это позволяет обобщить результат на функции потерь
ℓ(y,
y
^
), для которых минимум по второму аргументу, ответу сети, не достигается. Пример такой функции – кросс-энтропия. Так мы приходим к следующей теореме:
Теорема (On Connected Sublevel Sets in Deep Learning). Пусть выполнены следующие условия:
ϕ(R)=R,
ϕ
ϕ строго монотонна и не найдётся ненулевых
i=1
=j, таких что
∀
x
∈
R
∀x∈R
ϕ(x)=∑
i=1
p
λ
i
ϕ(x−a
ℓ(y,
y
^
) выпукла по второму аргументу и
inf
inf
y
^
ℓ(y,
y
^
)=0 для любого
y
y;
Существует
l∈{1,…,L}, для которого
rk
X
l
=
m
rkX
l
=m и
для всех
∈{l+1,…,L}.
Тогда если
L(W
0:L
)=∑
j=1
m
ℓ(y
j
,f
W
0:L
(x
j
)), то
Для каждого
ϵ
>
0
ϵ>0 найдутся веса
W
0
:
L
W
0:L
, для которых
L(W
0:L
)<ϵ;
Множество подуровня
((−∞,E)) связно для каждого
E
>
0
E>0.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
13.4. Сети бесконечной ширины
Следующий параграф
13.6. Implicit bias
