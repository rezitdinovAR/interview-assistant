---
title: Байесовский подход к оцениванию
url: https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.6
---
Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии
Априорное знание
Начнём с простого вопроса: как нам внести в модель априорные знания.
Представьте, что мы обучаем модель линейной регрессии
y∼⟨x,w⟩+ε,
ε∼N(0,σ
2
). С помощью MLE мы получили некоторую оценку
w
^
w
на веса
w
w — всякие ли их значения мы встретим с покорностью и смирением? Наверное, мы удивимся, если какие-то компоненты вектора
w
^
w
будут очень большими по сравнению с элементами
X
X: пожалуй, наша физическая интуиция будет бунтовать против этого, мы задумаемся о том, что из-за потенциальных ошибок сокращения вычисление предсказаний
) окажутся неточным — в общем, хотелось бы по возможности избежать этого. Но как?
Будь мы приверженцами чисто инженерного подхода, мы бы сделали просто: прибавили бы к функции потерь слагаемое
α∥ω∥
2
2
, или
α
∥
ω
∥
1
α∥ω∥
1
, или ещё что-то такое — тогда процедура обучения стала бы компромиссом между минимизацией исходного лосса и этой добавки, что попортило бы слегка близость
y∼⟨x,w⟩, но зато позволило бы лучше контролировать масштаб
w
^
w
. Надо думать, вы узнали в этой конструкции старую добрую регуляризацию.
Но наша цель — зашить наше априорное знание о том, что компоненты
w
w не слишком велики по модулю, в вероятностную модель. Введение в модель априорного знания соответствует введению априорного распределения на
w
w. Какое распределение выбрать? Ну, наверное, компоненты
w
w будут независимыми (ещё нам не хватало задавать взаимосвязи между ними!), а каждая из них будет иметь какое-то непрерывное распределение, в котором небольшие по модулю значения более правдоподобны, а совсем большие очень неправдоподобны.
Мы знаем такие распределения? Да, и сразу несколько. Например, нормальное. Логично было бы определить
p(w)=
i=1
∏
D
N(w
i
∣0,τ
2
)
где
τ
2
τ
2
— какая-то дисперсия, которую мы возьмём с потолка или подберём по валидационной выборке. Отметим, что выбор нормального распределение следует и из принципа максимальной энтропии: ведь у него наибольшая энтропия среди распределений на всей числовой оси с нулевым матожиданием и дисперсией
τ
2
τ
2
.
Контроль масштаба весов — это, вообще говоря, не единственное, что мы можем потребовать. Например, мы можем из каких-то физических соображений знать, что тот или иной вес в линейной модели непременно должен быть неотрицательным. Тогда в качестве априорного на этот вес мы можем взять, например, показательное распределение (которое, напомним, обладает максимальной энтропией среди распределений на положительных числах с данным матожиданием).
Оцениваем не значение параметра, а его распределение
Раз уж мы начали говорить о распределении на веса
w
w, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из которых мы пытаемся построить регрессию
y∼ax+b:
14
Несмотря на то, что в каждом из случаев «точная формула» или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность — оценить распределение параметров. Так, для примеров выше распределения на параметр
a
a могли бы иметь какой-то такой вид:
14
Дальше мы постараемся формализовать процесс получения таких оценок.
Построение апостериорного распределения
Давайте ненадолго забудем про линейную регрессию и представим, что мы подобрали с пола монету, которая выпадает орлом с некоторой неизвестной пока вероятностью
θ
θ. До тех пор, пока мы не начали её подкидывать, мы совершенно ничего не знаем о
θ
θ, эта вероятность может быть совершенно любой — то есть априорное распределение на
θ
θ является равномерным (на отрезке
[
0
,
1
]
[0,1]):
p(θ)=I
[0;1]
(θ)
Теперь представим, что мы подкинули её
n
n раз, получив результаты
Y=(y
1
,…,y
n
) (
0
0 — решка,
1
1 — орёл), среди которых
=n−∑
i=1
n
y
i
решек и
i=1
n
y
i
орлов. Определённо наши познания о числе
p
p стали точнее: так, если
n
1
n
1
мало, то можно заподозрить, что и
p
p невелико (уже чувствуете, запахло распределением!).
Распределение мы посчитаем с помощью формулы Байеса:
p(θ∣Y)=
p(Y)
p(θ,Y)
=
∫p(Y∣ψ)p(ψ)dψ
p(Y∣θ)p(θ)
в нашем случае:
p(θ∣Y)=
∫
0
1
∏
i=1
n
ψ
y
i
(1−ψ)
1−y
i
dψ
∏
i=1
n
θ
y
i
(1−θ)
1−y
i
I
[0,1]
(θ)
(1−ψ)
(1−θ)
n
0
I
[0,1]
(θ)
В этом выражении нетрудно узнать бета-распределение:
Beta
Beta(n
1
+1,n
0
+1). Давайте нарисует графики его плотности для нескольких конкретных значений
Как можно заметить, с ростом
n
n мы всё лучше понимаем, каким может быть
θ
θ, при этом если орёл выпадал редко, то пик оказывается ближе к нулю, и наоборот. Ширина пика в каком-то смысле отражает нашу уверенность в том, какими могут быть значения параметра, и не случайно чем больше у нас данных — тем уже будет пик, то есть тем больше уверенности.
Распределение
p(θ∣Y) параметра, полученное с учётом данных, называется апостериорным. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные
=(y
1
′
,…,y
m
′
), в которых
m
0
m
0
решек и
m
1
m
1
орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса:
p(θ∣Y∪Y
′
)=p([θ∣Y]∣Y
′
)=
p(Y
′
)
p(Y
′
∣θ)p(θ∣Y)
злой интеграл
=
=
злой интеграл
θ
m
1
(1−θ)
m
0
B(n
1
+1,n
0
+1)
θ
n
1
(1−θ)
n
0
I
[0,1]
(θ)
константа
∼
Beta
константа
θ
n
1
+m
1
(1−θ)
n
0
+m
0
∼Beta(n
1
+m
1
+1,n
0
+m
0
+1)
Вопрос на подумать. Пусть
p(y∣μ)=N(y∣μ,σ
2
) — нормальное распределение с фиксированной дисперсией
σ
2
σ
2
, а для параметра
μ
μ в качестве априорного выбрано также нормальное распределение
N(μ∣λ,θ
2
). Каким будет апостериорное распределение при условии данных
Y=(y
1
,…,y
n
)?
Сопряжённые распределения
В двух предыдущих примерах нам очень сильно повезло, что апостериорные распределения оказались нашими добрыми знакомыми. Если же взять случайную пару распределений
p(y∣θ) и
p
(
θ
)
p(θ), результат может оказаться совсем не таким приятным.
В самом деле, нет никакой проблемы в том, чтобы посчитать числитель формулы Байеса, но вот интеграл в знаменателе может и не найтись. Поэтому выбирать распределения нужно с умом. Более того, поскольку апостериорное распределение само станет априорным, когда придут новые данные, хочется, чтобы априорное и апостериорное распределения были из одного семейства; пары (семейств) распределений
p(y∣θ) и
p
(
θ
)
p(θ), для которых это выполняется, называются сопряжёнными
p
(
θ
)
p(θ) называется сопряжённым к
p(y∣θ). Полезно помнить несколько наиболее распространённых пар сопряжённых распределений:
p(y∣θ) — распределение Бернулли с вероятностью успеха
p(θ) — бета распределение;
p(y∣μ) — нормальное с матожиданием
μ
μ и фиксированной дисперсией
p(θ) также нормальное;
p(y∣λ) — показательное с параметром
p(λ) — гамма распределение;
p(y∣λ) — пуассоновское с параметром
p(λ) — гамма распределение;
p(y∣θ) — равномерное на отрезке
[
0
,
θ
]
[0,θ],
p
(
θ
)
p(θ) — Парето;
Возможно, вы заметили, что почти все указанные выше семейства распределений (кроме равномерного и Парето) относятся к экспоненциальному классу. И это не случайность! Экспоненциальный класс и тут лучше всех: оказывается, что для
p(y∣θ) из экспоненциального класса можно легко подобрать сопряжённое
p
(
θ
)
p(θ). Давайте же это сделаем.
Пусть
p(y∣θ) имеет вид
exp
p(y∣θ)=
h(θ)
1
g(y)exp(θ
T
u(y))
Положим
exp
p(θ)=
h
ν
(θ)
1
exp(η
T
θ)⋅f(η,ν)
где
f(η,ν) — множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение:
exp
exp
злой интеграл
=
p(θ∣Y)=
злой интеграл
[∏
i=1
n
h(θ)
1
g(y
i
)exp(θ
T
u(y
i
))]
h
ν
(θ)
1
exp(η
T
θ)⋅f(η,ν)
exp
что-то, где нет
θ
=
что-то, где нет θ
h
ν+n
(θ)
1
exp(θ
T
[η+∑
i=1
n
u(y
i
)])
Это распределение действительно из того же семейства, что и
p
(
θ
)
p(θ), только с новыми параметрами:
new
=η+
i=1
∑
n
u(y
i
),ν
new
=ν+n
Пример. Пусть
p(y∣q)=q
y
(1−q)
1−y
подчиняется распределению Бернулли. Напомним, что оно следующим образом представляется в привычном для экспоненциального класса виде:
exp
log
p(y∣q)=
=
h(q)
1
(1−q)
exp
=u
1
(y)
y
=θ
log
1−q
q
Предлагается брать априорное распределение вида
exp
⁡
(
η
log
что-то, где нет
q
p(q)=
что-то, где нетq
(1−q)
ν
exp(ηlog
1−q
q
)
Тогда апостериорное распределение будет иметь вид (проверьте, посчитав по формуле Байеса!)
exp
log
что-то, где нет
q
p(q∣Y)=
что-то, где нетq
(1−q)
ν+n
exp([η+∑
i=1
n
y
i
]log
1−q
q
)
Превратив логарифм частного в сумму, а экспоненту суммы в произведение, легко убедиться, что получается то самое бета распределение, которое мы уже получали выше.
Оценка апостериорного максимума (MAP)
Апостериорное распределение — это очень тонкий инструмент анализа данных, но иногда надо просто сказать число (или же интеграл в знаменателе не берётся и мы не можем толком посчитать распределение). В качестве точечной оценки логично выдать самое вероятное значение
θ
∣
Y
θ∣Y (интеграл в знаменателе от
θ
θ не зависит, поэтому на максимизацию не влияет):
argmax⁡θ
argmax⁡θ
MAP
=
θ
argmax
p(θ∣Y)=
θ
argmax
p(Y∣θ)p(θ)
Это число называется оценкой апостериорного максимума (MAP).
Если же в формуле выше перейти к логарифмам, то мы получим кое-что, до боли напоминающее старую добрую регуляризацию (и не просто так, как мы вскоре убедимся!):
argmax⁡θ
argmax⁡θ
log
argmax
p(Y∣θ)p(θ)=
θ
argmax
log(p(Y∣θ)p(θ))=
=
argmax⁡θ
(
log
log
argmax
(
2
1
logp(Y∣θ)+logp(θ))
Пример. Рассмотрим снова распределение Бернулли
p(y∣q) и априорное распределение
p
(
q
)
∼
Beta
p(q)∼Beta(q∣a,b). Тогда MAP-оценка будет равна
argmax⁡q
argmax⁡q
argmax
p(Y∣q)p(q)=
q
argmax
q
∑
i=1
n
y
i
(1−q)
n−∑
i=1
n
y
i
⋅q
a−1
(1−q)
b−1
=
argmax⁡q
log
log
argmax
((a−1+
i=1
∑
n
y
i
)logq+(b−1+n−
i=1
∑
n
y
i
)log(1−q))
Дифференцируя по
q
q и приравнивая производную к нулю, мы получаем
a+b+n−2
a+∑
i=1
n
y
i
−1
В отличие от оценки максимального правдоподобия
i=1
n
y
i
мы здесь используем априорное знание: параметры
(
a
−
1
)
(a−1) и
(
b
−
1
)
(b−1) работают как «память о воображаемых испытаниях», как будто бы до того, как получить данные
y
i
y
i
, мы уже имели
(
a
−
1
)
(a−1) успехов и
(
b
−
1
)
(b−1) неудач.
Связь MAP- и MLE-оценок
Оценка максимального правдоподобия является частным случаем апостериорной оценки.
В самом деле, если априорное распределение является равномерным, то есть
p
(
θ
)
p(θ) не зависит
θ
θ (если веса
θ
θ вещественные, могут потребоваться дополнительные усилия, чтобы понять, как такое вообще получается), и тогда
argmax⁡θ
log
argmax⁡θ
(
log
log
MAP
=
θ
argmax
logp(Y∣θ)p(θ)=
θ
argmax
logp(Y∣θ)+
=const
logp(θ)
=
=
argmax⁡θ
log
argmax
logp(y∣θ)=
θ
MLE
Байесовские оценки для условных распределений
В предыдущих разделах мы разобрали, как байесовский подход работает для обычных, не условных распределений. Теперь вернёмся к чему-то более близкому к машинному обучению, а именно к распределениям вида
y
∣
x
,
w
y∣x,w, и убедимся, что для них байесовских подход работает точно так же, как и для обычных распределений.
Имея некоторое распределение
p(y∣x,w), мы подбираем для него априорное распределение на веса
p
(
w
)
p(w) (и да, оно не зависит от
x
x: ведь априорное распределение существует ещё до появления данных) и вычисляем апостериорное распределение на веса:
p(w∣X,y)
Вычислять его мы будем по уже привычной формуле Байеса:
p(w∣X,y)=
p(y)
p(y,w∣X)
=
p(y)
p(y∣X,w)p(w)
Повторим ещё разок, в чём суть байесовского подхода: у нас было некоторое априорное представление
p
(
w
)
p(w) о распределении весов
w
w, а теперь, посмотрев на данные
i=1
n
, мы уточняем своё понимание, формулируя апостериорное представление
p(w∣X,y).
Если же нам нужна только точечная оценка, мы можем ограничиться оценкой апостериорного максимума (MAP):
argmax⁡w
argmax⁡w
MAP
=
w
argmax
p(w∣X,y)=
w
argmax
p(y∣X,w)p(w)=
=
argmax⁡w
(
log
log
argmax
(
2
1
logp(y∣X,w)+logp(w))
что уже до неприличия напоминает регуляризованную модель
Пример: линейная регрессия с
L
2
L
2
-регуляризацией как модель с гауссовским априорным распределением на веса
В модели линейной регрессии
y=⟨x,w⟩+ε,
ε∼N(0,σ
2
) введём априорное распределение на веса вида
p(w)=N(w∣0,τ
2
I)=
j=1
∏
D
N(w
j
∣0,τ
2
)=
j=1
∏
D
p(w
j
)
Тогда
MAP
— точка минимума следующего выражения:
−
log
log
−logp(y∣X,w)−logp(w)=−
i=1
∑
N
p(y
i
∣x
i
,w)−
j=1
∑
D
p(w
log
log
i=1
∑
N
(−
2
1
log(2πσ
−(w,x
i
))
2
)−
j=1
∑
D
(−
2
1
log(2πτ
не зависящие от
w
члены
=
2σ
2
1
i=1
∑
N
(y
i
−(w,x
j=1
∑
D
w
j
2
+ не зависящие от w члены
Получается, что
argmin⁡w
MAP
=
w
argmin
(
2
1
i=1
∑
N
(y
i
−(w,x
∥w∥
2
)
а это же функция потерь для линейной регрессии с
L
2
L
2
-регуляризацией! Напомним на всякий случай, что у этой задачи есть «точное» решение
MAP
=(X
Для этого примера мы можем вычислить и апостериорное распределение
p(w∣X,y). В самом деле, из написанного выше мы можем заключить, что
log
log
log
logp(w∣X,y)=log(p(y∣X,w)p(w))−logp(y)=
не зависящие от
w
члены
=
2σ
2
1
(y−Xw)
T
(y−Xw)+
2τ
2
1
w
T
w+ не зависящие от w члены
Таким образом,
log
logp(w∣X,y) — это квадратичная функция от
w
w, откуда следует, что апостериорное распределение является нормальным. Чтобы найти его параметры, нужно немного преобразовать полученное выражение:
y−w
T
X
T
y−y
T
Wx+w
T
X
T
Xw)+
2τ
2
1
w
T
w+const(w)=
I)w−
Wx+const(w)=
(w−
w
MAP
I)(w−
w
MAP
)+const(w)=
Таким образом,
p(w∣X,y)=N(
w
MAP
Как видим, от априорного распределения оно отличается корректировкой как матожидания
MAP
, так и ковариационной матрицы
. Отметим, что
X
T
X
X
T
X — это, с точностью до численного множителя, оценка ковариационной матрицы признаков нашего датасета (элементы матрицы
X
T
X
X
T
X — это скалярные произведения столбцов
X
X, то есть столбцов значений признаков).
Иллюстрация. Давайте на простом примере (датасет с двумя признаками) посмотрим, как меняется апостериорное распределение
w
w с ростом размера обучающей выборки:
14
Как видим, не только мода распределения, то есть
MAP
приближается к своему истинному значению, но и дисперсия распределения постепенно уменьшается.
Ещё иллюстрация. Теперь рассмотрим задачу аппроксимации неизвестной функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом третьей степени. Её, разумеется, тоже можно решать, как задачу линейной регрессии на коэффициенты многочлена. Давайте нарисуем, как будут выглядеть функции, сгенерированные из распределения
p(w∣X,y) для разного объёма обучающей выборки:
14
Тут тоже видим, что функции не только становятся ближе к истинной, но и разброс их уменьшается.
Пример: линейная регрессия с
L
1
L
1
-регуляризацией как модель с лапласовским априорным распределением на веса
Другое распределение, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения
w
j
w
j
были правдоподобными, а большие не очень, — распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса.
exp
p(w)=
j=1
∏
D
p(w
j
)=
j=1
∏
D
2
λ
exp(−λ∣w
m
∣)
Проводя такое же вычисление, получаем, что
argmin⁡w
MAP
=
w
argmin
(
2
1
i=1
∑
N
(y
i
−(w,x
i
))
2
+λ
j=1
∑
D
∣w
j
∣)
а это же функция потерь для линейной регрессии с
L
1
L
1
-регуляризацией!
Как делать предсказания
Все изложенные выше рассуждения проводились в ситуации, когда
X=X
train
— обучающая выборка. Для неё мы можем посчитать
p(w∣X
train
,y
train
)=
p(y)
(y∣X,w)p(w)
и точечную апостериорную оценку
argmax⁡w
MAP
=
w
argmax
p(y∣X,w)p(y). А теперь пусть нам дан новый объект
∈X. Какой таргет
y
0
y
0
мы для него предскажем?
Было бы естественным, раз уж мы предсказываем распределение для
w
w, и для
y
0
y
0
тоже предсказывать распределение. Делается это следующим образом:
p(y
0
∣x
0
,X
train
,y
train
)=∫p(y
0
∣x
0
,w)p(w∣X
train
,y
train
)dw
Надо признать, что вычисление этого интеграла не всегда посильная задача, поэтому зачастую приходится «просто подставлять
MAP
». В вероятностных терминах это можно описать так: вместо сложного апостериорного распределения
p(w∣X
train
,y
train
) мы берём самое грубое на свете приближение
p(w∣X
train
,y
train
)≈δ(w−
w
MAP
),
где
δ
(
t
)
δ(t) — дельта-функция, которая не является честной функцией (а является тем, что математики называют обобщёнными функциями), которая определяется тем свойством, что
∫f(t)δ(t)dt=f(0) для достаточно разумных функций
f
f. Если не мудрствовать лукаво, то это всё значит, что
p(y
0
∣x
0
,X
train
,y
train
)≈p(y
0
∣x
0
,
w
MAP
)
Пример. Пусть
y∼Xw+ε,
ε∼N(0,σ)
2
— модель линейной регрессии с априорным распределением
p(w)=N(0,τ
2
) на параметры. Тогда, как мы уже видели раньше,
p(w∣X,y)=N(w∣
w
MAP
Попробуем для новой точки
x
0
x
0
посчитать распределение на
y
0
y
0
. Рекомендуем читателю попробовать самостоятельно посчитать интеграл или же обратиться к пункту 7.6.2 книжки «Machine Learning A Probabilistic Perspective» автора Kevin P. Murphy, убедившись, что
p(y
0
∣x
0
,X
train
,y
train
)=N(y
0
∣x
0
w
MAP
что, очевидно, более содержательно, чем оценка, полученная с помощью приближения
p(w∣X
train
,y
train
)≈δ(w−
w
MAP
p(y
0
∣x
0
,
w
MAP
)=N(y
0
x
0
w
MAP
,σ
2
)
Собственно, видно, что в этом случае
Пример в примере. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше
3
3 (в которой мы строим модели с
=1). Для
N
=
8
N=8 мы получали такую картинку:
14
Если оценить по приведённым выше формулам
p(y
0
∣x
0
,X
train
,y
train
) для разных
x
0
x
0
, то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из обучающей выборки:
14
Байесовский подход и дообучение моделей
До сих пор мы в основном рассуждали о моделях машинного обучения как о чём-то, что один раз обучается и дальше навсегда застывает в таком виде, но в жизни такое скорее редкость. Мы пока не будем обсуждать изменчивость истинных зависимостей во времени, но даже если истина неизменна, к нам могут поступать новые данные, которые очень хотелось бы использовать для дообучения модели.
Обычные, не байесовские вероятностные модели не предоставляют таких инструментов. Оценку максимального правдоподобия придётся пересчитывать заново (хотя, конечно, можно схитрить, использовав старое значение в качестве начального приближения при итеративной оптимизации). Байесовский же подход позволяет оформить дообучения в виде простой и элегантной формулы: при добавлении новых данных
N+1
,y
N+1
),…,(x
M
,y
M
) имеем
p(w∣(x
i
,y
i
)
i=1
M
)=
p((y
i
)
i=N+1
M
)
p((y
i
)
i=N+1
M
∣(x
i
)
i=N+1
M
)p(w∣(x
i
,y
i
)
i=1
N
)
Байесовский подход к выбору модели: мотивация
Нам часто приходится выбирать: дерево или случайный лес, линейная модель или метод ближайших соседей; да, собственно, и внутри наших вероятностных моделей есть параметры (скажем, дисперсия шума
), которые надо бы подбирать. Но как?
В обычной ситуации мы выбираем модель, обученную на выборке
train
,y
train
) в зависимости от того, как она себя ведёт на валидационной выборке
val
,y
val
) (сравниваем правдоподобие или более сложные метрики) — или же делаем кросс-валидацию. Но как сравнивать модели, выдающие распределение?
Ответим вопросом на вопрос: а как вообще сравнивать модели? Назначение любой модели — объяснять мир вокруг нас, и её качество определяется именно тем, насколько хорошо она справляется с этой задачей. Тестовая выборка — это хороший способ оценки, потому что она показывает, насколько вписываются в модель новые данные. Но могут быть и другие соображения, помогающие оценить качество модели.
Пример №1
Аналитик Василий опоздал на работу. Своему руководителю он может предложить самые разные объяснения — и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания и потенциально позволяющая руководителю принять решение о том, карать ли Василия.
Конечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров:
Модель «Василий опоздал, потому что так получилось», то есть факт опоздания — это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому факту, что Василий опоздал, а его коллега Надежда не опоздала и (б) совершенно не помогает решить, наказывать ли за опоздание. Наверное, такое не удовлетворит руководителя.
Модель «Василий опоздал, потому что рядом с его домом открылся портал в другой мир, где шла великая битва орков с эльфами, и он почувствовал, что просто обязан принять в ней участие на стороне орков, которых привёл к победе, завоевав руку и сердце орочьей принцессы, после чего был перенесён обратно в наш скучный мир завистливым шаманом». Чем же она плоха? Битва с эльфами — это, безусловно, важное и нужное дело, и на месте руководителя мы бы дружно согласились, что причина уважительная. Но заметим, что в рамках этой модели можно объяснить множество потенциальных исходов, среди которых довольно маловероятным представляется наблюдаемый: тот, в котором Василий не погиб в бою, не остался со своей принцессой и не был порабощён каким-нибудь завистливым шаманом. Отметим и другой недостаток этой модели: её невозможно провалидировать. Если в совершенно случайной модели можно оценить вероятность опоздания и впоследствии, когда накопятся ещё примеры, проверить, правильно ли мы её посчитали, то в мире, где открываются порталы и любой аналитик может завоевать сердце орочьей принцессы, возможно всё, и даже если больше никто не попадёт в такую ситуацию, Василий всё равно сможет бить себя в грудь кулаком и говорить, что он избранный. Так что, наверное, это тоже не очень хорошая модель.
Модель «Василий опоздал, потому что проспал» достаточно проста, чтобы в неё поверить, и в то же время даёт руководителю возможность принять решение, что делать с Василием.
Пример №2
Обратимся к примеру из машинного обучения. Сравним три модели линейной регрессии:
14
Даже и не запрашивая тестовую выборку, мы можем сделать определённые выводы о качестве этих моделей. Средняя (квадратичная) явно лучше левой (линейной), потому что она лучше объясняет то, что мы видим: тот факт, что облако точек обучающей выборки выглядит вогнутым вниз.
А что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой — многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как — многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки:
14
Обратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах сотни? Очевидно, она очень мала. Поэтому мы можем сказать, что выбор в качестве модели многочлена десятой степени не очень обоснован.
Попробуем резюмировать
Слишком простая модель плохо объясняет наблюдаемые нами данные, тогда как слишком сложная делает это хорошо, но при этом описывает слишком многообразный мир, в котором имеющиеся у нас данные оказываются уже слишком частным случаем. В каком-то смысле наш способ выбора модели оказывается переформулировкой бритвы Оккама: из моделей, пристойно описывающих наблюдаемые явления, следует выбирать наиболее минималистичную.
Байесовский подход к выбору модели: формализация
Пусть у нас есть некоторое семейство моделей
J
J и для каждого
j
∈
J
j∈J задана какая-то своя вероятностная модель. В духе байесовского подхода было бы оценить условное распределение моделей
p(j∣y,X)=
j∈J
∑
p(j,y∣X)
p(y∣X,j)p(j)
и в качестве наилучшей модели взять её моду. Если же считать все модели равновероятными, то мы сводим всё к максимизации только лишь
p(y∣X,j)=p
j
(y∣X):
ȷ
^
=
argmax⁡j
argmax⁡j
argmax
∫p
j
(y∣X,w)p
j
(w)dw=
j
argmax
p
j
(y∣X)
Величина
(y∣X) называется обоснованностью (evidence, marginal likelihood) модели.
Отметим, что такое определение вполне согласуется с мотивацией из предыдущего подраздела. Слишком простая модель плохо описывает наблюдаемые данные, и потому будет отвергнута. В свою очередь, слишком сложная модель способна описывать гораздо большее многообразие явлений, чем нам было бы достаточно. Таким образом, компромисс между качеством описания и сложностью и даёт нам оптимальную модель.
Пример
Вернёмся к нашей любимой задаче аппроксимации функции одной переменной многочленом небольшой степени по нескольким точкам, значение в которых было искажено нормальным шумом. Построим несколько моделей, приближающих многочленом степени не выше некоторого
d
e
g
deg (будет принимать значения 1, 3 и 6), положив в вероятностной модели
=1.
Мы не будем приводить полный вывод обоснованности для задачи регрессии
p(y∣X,w)=N(y∣Xw,σ
2
I)p(w∣τ
2
I), а сразу выпишем ответ:
p(y∣X)=N(0,σ
2
I+τ
2
XX
T
)
Посмотрим, какой будет обоснованность для разного числа обучающих точек:
14
Можно убедиться, что для регрессии по двум точкам наиболее обоснованная — линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте.
Аппроксимация обоснованности и байесовский информационный критерий
Точно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой.
Напомним, что
p(y∣X)=∫p(y∣X,w)p(w)dw
Воспользуемся приближением Лапласа, то есть разложим
p(y∣X,w) (как функцию от
w
w) вблизи своего максимума, то есть вблизи
MLE
в ряд Тейлора:
log
log
logp(y∣X,w)≈logp(y∣X,
w
)−
2
1
(w−
)(w−
w
),
где линейный член отсутствует, поскольку разложение делается в точке локального экстремума, а
) — знакомая нам матрица Фишера
log
)=−E∇
w
2
logp(y∣X,w)∣
w
=NI
1
(
w
).
Далее,
p
(
w
)
p(w) мы можем с точностью до второго порядка приблизить
MAP
). Получается, что
log
p(y∣X)≈∫e
logp(y∣X,
w
)−
2
N
(w−
)(w−
w
)
p(
w
MAP
)dw=
=
e
log
logp(y∣X,
w
)
p(
w
MAP
)∫e
−
2
N
(w−
)(w−
w
)
dw=
=
e
log
logp(y∣X,
w
)
p(
w
MAP
)⋅(2π)
D/2
N
D/2
exp
⁡
(
log
log
⁡
N
+
всякие штуки
)
=exp(logp(y∣X,
w
)−
2
D
logN+всякие штуки)
Несмотря на то, что
MAP
) и
, сгруппированные нами во «всякие штуки», существенным образом зависят от модели, при больших
N
N они вносят в показатель гораздо меньше вклада, чем первые два слагаемых. Таким образом, мы можем себе позволить вместо трудновычисляемых
p(y∣X) использовать для сравнения модели
байесовский информационный критерий
байесовский информационный критерий(BIC):
B
I
C
=
D
log
⁡
N
−
2
log
BIC=DlogN−2logp(y∣X,
w
)
Фреквентисты против байесиан: кто кого?
Мы с вами познакомились с двумя парадигмами оценивания:
фреквентистской (frequentist, от слова "frequency", частота) — в которой считается, что данные являются случайным (настоящая случайность!) семплом из некоторого фиксированного распределения, которое мы стараемся оценить по этому семплу, и
байесовской — в которой данные считаются данностью и в которой мы используем данные для обновления наших априорных представлений о распределении параметров (здесь случайности нет, а есть лишь нехватка знания).
У обеих есть свои достоинства и недостатки, поборники и гонители. К недостаткам байесовской относится, безусловно, её вычислительная сложность: возможно, вы помните, в пучину вычислений сколь мрачных нас низвергла банальная задача линейной регрессии, и дальше становится только ещё трудней. Если мы захотим байесовский подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере.
Одно известное свойство оценки максимального правдоподобия — асимптотическая нормальность. Если оценивать наши веса
w
w по различным наборам из
N
N обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка
MLE
тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при
N
→
∞
N→∞
MLE
∼N(w
где
w
∗
w
∗
— истинное значение весов, а
) — матрица информации Фишера, которая определяется как
log
log
)=E[(
∂w
i
∂
logp(y∣X,w)
logp(y∣X,w)
w
∗
)]
что при некоторых не слишком обременительных ограничениях равно
log
)=−E[
logp(y∣X,w)
w
∗
]
При этом поскольку
log
log
logp(y∣X,w)=∑
i=1
N
logp(y∣X,w), матрица тоже распадается в сумму, и получается, что
)=NI
1
(w
∗
), то есть с ростом
N
N ковариация
(NI
1
(w
∗
))
−1
оценки максимального правдоподобия стремится к нулю.
На интуитивном уровне можно сказать, что матрица информации Фишера показывает, сколько информации о весах
w
w содержится в
X
X.
Поговорим о проблемах. В реальной ситуации мы не знаем
w
∗
w
∗
и тем более не можем посчитать матрицу Фишера, то есть мы с самого начала вынуждены лукавить. Ясно, что вместо
w
∗
w
∗
можно взять просто
w
^
w
, а вместо
) — матрицу
), которую можно даже при желании определить как
log
log(p(y∣X,w))
w
∗
)
безо всякого математического ожидания. Итак, хотя мы можем теперь построить доверительный интервал для оцениваемых параметров, по ходу нами было сделано много упрощений: мы предположили, что асимптотическая оценка распределения уже достигнута, от
w
∗
w
∗
перешли к
w
^
w
, а для полноты чувств ещё и избавились от математического ожидания. В байесовском подходе мы такого себе не позволяем.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
4.5. Генеративный подход к классификации
Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес
Следующий параграф
4.7. Модели с латентными переменными
