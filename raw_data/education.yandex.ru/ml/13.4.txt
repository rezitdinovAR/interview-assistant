---
title: Сети бесконечной ширины
url: https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny
course: ml
chapter: 13. Теория глубокого обучения
chapter_id: 13.4
---
Во введении обсуждалось, что истинный риск нейронной сети выходит на асимптоту при стремлении ширины сети (то есть числа нейронов в слое) к бесконечности. Это намекает нам на то, что существует предельная модель, «бесконечно широкая сеть». В этом параграфе мы обсудим подходы к анализу её поведения.
Динамика обучения нейронной сети описывается эволюцией в пространстве весов – например, правилом обновления весов в градиентном спуске. Но в каком виде можно записать эволюцию бесконечно широкой сети, в которой весов бесконечно много? Есть два способа это сделать.
Первый способ – ввести меру в пространстве весов
В качестве примера рассмотрим нейронную сеть с одним скрытым слоем, скалярным выходом и скалярным входом:
f(x)=
n
1
i=1
∑
n
a
i
ϕ(w
i
x).
Это выражение можно представить в виде
f(x)=∫
R
2
aϕ(wx)dμ
n
(a,w), где мера
сосредоточена в весах, ассоциированных с каждым из нейронов скрытого слоя:
(a,w)=
n
1
i=1
∑
n
δ
a
i
(a)δ
w
i
(w).
Здесь
δ
x
δ
x
– мера, сосредоточенная в
x
x.
При стремлении ширины
n
n к бесконечности
μ
n
μ
n
может иметь предел. Так, если все веса насемплированы независимо из стандартного нормального распределения
N(0,1), предельная мера принимает вид двумерного стандартного нормального распределения
N(0,I
2×2
), а предсказание предельной сети можно записать в виде
f(x)=E
a,w∼N(0,1)
aϕ(wx).
Заметим, что множитель
1
/
n
1/n в определении модели выше принципиально важен для того, чтобы предельная мера и представление предельной сети в виде интеграла по мере были определены. Такая параметризация носит название mean-field parameterization.
Динамику эволюции весов также можно представить в виде эволюции меры. В самом деле, в случае конечной ширины градиентный спуск говорит нам о том, как за один шаг оптимизации меняются веса, ассоциированные с каждым из нейронов, или, что то же самое, как меняется мера
μ
n
μ
n
. Заменив в этом выражении меру
μ
n
μ
n
на предельную, можно получить эволюцию предельной меры.
К сожалению, представление эволюции предельной сети в виде эволюции меры не даёт сказать много о свойствах предельной модели. Так, известно, что предельная модель всегда сходится в глобальный минимум на обучающей выборке, см статью On the global convergence of gradient descent for over-parameterized models using optimal transport, но мало что известно о её обобщающей способности.
Более того, лишь сети с одним скрытым слоем допускают простую формулировку в форме эволюции меры в пределе бесконечной ширины, см. статьи Mean field analysis of neural networks: A central limit theorem, On the global convergence of gradient descent for over-parameterized models using optimal transport и Trainability and accuracy of neural networks: An interacting particle system approach. Для сетей с большим числом слоёв подобная формулировка также возможна, см. статьи A mean-field limit for certain deep neural networks и A rigorous framework for the mean field limit of multilayer neural networks, но анализ усложняется.
Так, сходимость в глобальный минимум для сети с двумя скрытыми слоями была доказана лишь совсем недавно в работе Global convergence of three-layer neural networks in the mean field regime; для более глубоких сетей подобные результаты пока неизвестны.
Второй способ – вместо эволюции весов рассматривать эволюцию предсказаний модели в каждой точке
x
x
Для простоты рассмотрим задачу минимизации квадратичной функции потерь на наборе данных
) размера
min
⁡
θ
.
2
1
j=1
∑
m
(y
j
−f(x
j
;θ))
2
→
θ
min
.
Будем оптимизировать эту функцию потерь градиентным спуском с шагом
k+1
−θ
k
=−η∇
θ
(
2
1
j=1
∑
m
(y
j
−f(x
j=1
∑
m
(y
j
−f(x
j
;θ
t
))∇
θ
f(x
j
;θ
k
).
Ниже нам будет удобнее рассматривать градиентный спуск с непрерывным временем вместо дискретного:
j=1
∑
m
(y
j
−f(x
j
;θ
t
))∇
θ
f(x
j
;θ
t
).
Переход к непрерывному времени соответствует устремлению к нулю шага
η
η, если при этом число шагов растёт как
k=[t/η], где округление применяется в любую сторону.
Обозначим через
(x) предсказание в точке
x
x модели в момент времени
t
t. Оно зависит от времени следующим образом:
(x)=
(x)=
j=1
))∇
(x).
Введём обозначение:
(x,x
′
)=∇
θ
T
f
t
(x)∇
С помощью него уравнение выше можно записать более коротко:
(x)=
Θ
^
t
(x,
)).(1)
Здесь и дальше мы будем считать, что
(x,
x
) имеет размерность
1
×
m
1×m.
Функция
(x,x
′
) называется эмпирическим нейрокасательным ядром (Neural Tangent Kernel, NTK); подробнее о ядрах мы поговорим ниже в параграфе про ядровые методы.
Заметим, что в уравнении
(
1
)
(1) вся информация о весах содержится в ядре, которое является отображением из
X
×
X
X×X в
R
R. Как мы увидим ниже, при определённых условиях, при стремлении ширины сети к бесконечности ядро имеет предел и он не зависит от
t
t.
Обозначив этот предел через
Θ
Θ, мы приходим к следующему виду эволюции предсказаний бесконечно широкой сети:
(x)=Θ(x,
)).(2)
Здесь и далее будем называть
Θ
Θ (не эмпирическим) нейрокасательным ядром или NTK. Такой термин был введён в оригинальной работе Neural tangent kernel: Convergence and generalization in neural networks.
В этом случае динамика предсказаний интегрируется следующим образом. На обучающей выборке
)=Θ(
)),
что даёт
)=f
0
(
x
)−(I−e
−Θ(
x
,
x
)t
)(f
Подставляя решение в
(
2
)
(2), получаем
(x)=Θ(x,
x
)e
−Θ(
)),
и, наконец,
(x)=f
0
(x)−Θ(x,
)(I−e
−Θ(
x
,
x
)t
)(f
0
(
x
)−
y
).(3)
Прежде, чем доказывать сходимость ядра, мы обсудим, как может применяться предельное ядро и представление эволюции предсказаний в форме (1).
Применение NTK-анализа
NTK как математический аппарат
Нам удалось проинтегрировать динамику предсказаний в явном виде. Что это даёт?
Во-первых, мы получаем достаточное условие на сходимость в глобальный минимум на обучающей выборке. Таким условием является положительная определённость матрицы Грама ядра:
)≥λ
0
для некоторого
>0.
В самом деле, в этом случае,
)=−(
))≤−λ
что даёт
при
→0при t→∞.
Во-вторых, раз явное решение известно, можно написать оценку на обобщающую способность.
Оба этих результата опираются на то, что ядро постоянно. Как мы покажем ниже, постоянство нейрокасательного ядра нейронной сети можно гарантировать лишь в пределе бесконечной ширины. Тем не менее, если сеть конечна, но достаточно широка, можно показать, что её ядро достаточно близко к предельному, и оценки сохраняют силу.
Например, для обоснования сходимости в глобальный минимум достаточно показать, что наименьшее собственное значение эмпирического ядра с высокой вероятностью остаётся отделённым от нуля в течение обучения:
∀t≥0 с вероятностью
≥
1
−
δ
≥1−δ для
n≥n
∗
(δ). В самом деле, из этого следует, что
)=−(
))≤−
а значит,
при
t/2
→0при t→∞.
Формальное доказательство вы можете найти в работе Gradient Descent Provably Optimizes Over-parameterized Neural Networks, а также в конспекте лекций автора этого параграфа.
Вот ещё несколько результатов, полученных в этом направлении:
улучшенные оценки на минимальную ширину в работе Quadratic suffices for over-parametrization via matrix chernoff bound;
оценки для случая глубоких сетей в работе Gradient descent finds global minima of deep neural networks;
оценки на обобщающую способность, полученные через близость ядра к предельному, в работе Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.
Определение патологий обучения
Как мы увидим позже, NTK реальных, стандартно параметризованных, имеющих конечную ширину сетей может меняться за время обучения существенным образом: см. эмпирическую работу Deep learning versus kernel learning и теоретический анализ для сетей с одним скрытым слоем Dynamically Stable Infinite-Width Limits of Neural Classifiers.
Тем не менее, ядро в инициализации может выявить определённые патологии соответствующей нейронной сети. Рассмотрим один из примеров применения.
В некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения.
Необходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или, наоборот, к бесконечности. В первом случае какие-то из компонент выборки никогда не выучатся, во втором обучение невозможно ни при каком конечном темпе обучения.
Чтобы в этом убедиться, рассмотрим разложение матрицы Грама ядра по собственным векторами:
j=1
где
≥…≥λ
m
≥0, а векторы
,…,
v
m
образуют ортонормированный базис. Разложим предсказание сети по этому базису:
)=∑
j=1
m
u
t,j
v
j
. Так как базис ортонормированный, каждая из компонент эволюционирует независимо от других. В самом деле, для дискретного градиентного спуска с шагом
η
η имеем
k+1,j
=u
k,j
+ηλ
t,j
).
Таким образом, если
=0, то
u
t
,
j
u
t,j
никогда не сойдётся к
Кроме того, для того, чтобы процесс сходился, шаг
η
η должен убывать обратно пропорционально наибольшему собственному числу
λ
1
λ
1
. Если последнее стремится к бесконечности, то
η
η стремится к нулю, а значит,
η
λ
j
ηλ
j
будем мало для всех
j
j, для которых
λ
j
λ
j
конечен; соответствующие компоненты также никогда не сойдутся.
Подробности см. в работе Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping, а также в более ранних работах Exponential expressivity in deep neural networks through transient chaos, Deep information propagation, Resurrecting the sigmoid in deep learning through dynamical isometry, Dynamical isometry and a mean field theory of cnns, в которых использовалась похожая идея, но не использовалось понятие NTK явно. См. также главу про инициализацию в конспекте лекций.
NTK и Ядровые методы
Предельное NTK нейронной сети можно использовать в любом ядровом методе, например, в SVM. Обсудим это поподробнее и заодно разберёмся, почему NTK вообще называют ядром.
Рассмотрим задачу линейной регрессии:
θ
^
λ
=
argmin
=argmin
θ∈R
d
j=1
∑
m
L(y
j
,θ
T
x
j
)+λ∥θ∥
2
2
(4)
Эту же задачу можно эквивалентно переписать следующим образом:
f
^
λ
=
argmin
=argmin
f∈H
j=1
∑
m
L(y
j
,f(x
j
))+λ∥f∥
H
2
,(5)
где
H
H – пространство линейных отображений
(x)=θ
T
x с некоторой нормой
∥
f
∥
H
∥f∥
H
на нём.
Сделаем линейное пространство
H
H евклидовым, введя на нём следующее скалярное произведение. Для
f(x)=θ
T
x и
(x)=
θ
~
T
x определим
⟨f,
f
~
⟩=θ
T
θ
~
.
Это скалярное произведение порождает норму
∥f∥
H
=∥θ∥
2
, что и делает формулировку (5) эквивалентной формулировке (4).
Пространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть задачу вида (5), в которой
H
H будет произвольным нормированным пространством функций. Наиболее хорошо изучен случай, когда функции из
H
H являются линейными моделями в некотором (возможно, бесконечномерном) гильбертовом пространстве признаков:
f(x)=⟨Φ(x),θ⟩, где
Φ
Φ отображает
x
x в это пространство. Если последнее всё же конечномерно, то мы можем использовать матричную запись
f(x)=θ
T
Φ(x); элементы
θ
θ в этой записи обычно называют первичными переменными (primal variables).
Пространство функций
H
H также оказывается гильбертовым: соответствующее скалярное произведение имеет вид
=⟨θ,θ
′
⟩
Таким образом,
=⟨f
θ
,f
θ
⟩
H
=⟨θ,θ⟩, если
(x)=⟨Φ(x),θ⟩.
Любому отображению
Φ
Φ можно сопоставить симметричную положительно-определённую функцию
K(x,x
′
)=⟨Φ
T
(x),Φ(x
′
)⟩; функции такого вида называются ядрами.
В силу фундаментальной теоремы о представителе любое решение задачи (4) принимает вид
f(x)=
j=1
∑
m
α
j
K(x,x
j
)=K(x,
x
)
α
.
В отличие от
θ
θ, вектор
α
α всегда конечномерен: его размерность равна размеру обучающей выборки. Элементы
α
α называют двойственными (dual) переменными.
Упомянутый результат позволяет перейти от минимизации
f
f в бесконечномерном пространстве функций (или, что то же самое, минимизации
θ
θ в бесконечномерном пространстве признаков), к минимизации в конечномерном пространстве двойственных переменных:
α
⃗
=
argmin
=argmin
α
∈R
m
j=1
∑
m
L(y
j
,K(x
j
,
x
)
α
)+λ
.(6)
Если в качестве функции потерь взять квадратичную
L(y,z)=
2
1
(y−z)
2
, то получим ядровую регрессию; если же взять hinge loss
L(y,z)=[1−yz]
+
, то SVM.
Заметим, что двойственная задача полностью сформулирована в терминах ядра
K
K: отображение в потенциально бесконечное пространство признаков
Φ
Φ более нигде не возникает. Поэтому мы можем использовать в качестве
K
K любую симметричную положительно определённую функцию двух переменных, не думая о том, для какого пространства признаков оно будет ядром (есть теорема, что такие функции всегда являются ядрами). Это может быть очень полезно. Так, если для эмпирического NTK в инициализации
(x,x
′
) имеем
Φ(x)=∇
θ
f(x;θ
0
), но совершенно неочевидно, какое отображение
Φ
Φ соответствует предельному NTK:
lim
Θ(x,x
′
)=lim
n→∞
Θ
^
0
(x,x
′
).
Таким образом, мы можем использовать
Θ
Θ в качестве ядра
K
K в двойственной задаче (6) наряду с линейным
K(x,x
′
)=x
T
x
′
или гауссовским ядром
K(x,x
′
)=e
−
2σ
2
1
∥x−y∥
2
2
. Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения ядрового метода с NTK в качестве ядра будет близок к результату обучения соответствующей нейронной сети.
Основная проблема ядровых методов в том, что они требуют вычисления матрицы Грама ядра на обучающем наборе данных
). Её размер
m
×
m
m×m (где
m
m – размер выборки), так что применение ядровых методов на больших данных сильно усложняется. Более того, наивное вычисление динамики
f
t
f
t
из формулы (3) требует обращения матрицы Грама, которое занимает
O
(
m
3
)
O(m
3
) времени.
Тем не менее, определённые оптимизации существуют. Так например, в работе Kernel methods through the roof предлагается способ приближённого вычисления (
f
t
f
t
) за
log
⁡
m
)
O(m
3/2
logm) памяти и времени. Другие подходы см. в работах Fast Finite Width Neural Tangent Kernel и Neural tangents: Fast and easy infinite neural networks in python.
Так или иначе, на малых наборах данных выражение (3) можно вычислить точно, см. результаты в работе Harnessing the power of infinitely wide deep nets on small-data tasks. Существуют также примеры задач, в которых матрицу Грама ядра достаточно посчитать только для малых
m
m, см., например, Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks.
Ещё одна проблема использования NTK в ядровых методах состоит в том, что явный подсчёт предельного NTK доступен только для сетей, состоящих из слоёв из определённого класса. В этот класс входят полносвязные и свёрточные слои, average pooling, ряд нелинейностей с одним аргументом (включая, например, ReLU и erf), layer norm, но не входят max pooling и batch norm, часто используемые в реальных архитектурах. Явный подсчёт предельного NTK для «хороших» сетей реализован в библиотеке NeuralTangents; часть явных формул для подсчёта можно найти в статье On exact computation with an infinitely wide neural net.
Тем не менее, даже в тех случаях, когда посчитать предельное NTK не представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации
(x,x
′
)=∇
θ
T
f(x;θ
0
)∇
θ
f(x
′
;θ
0
)
Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работе Neural tangents: Fast and easy infinite neural networks in python.
NTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро
K(x,x
′
)=Ef(x)f(x
′
) – это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работах Deep Neural Networks as Gaussian Processes, Wide neural networks of any depth evolve as linear models under gradient descent, Random neural networks in the infinite width limit as Gaussian processes или в конспекте лекций. Можно показать, что оно соответствует NTK-ядру для сети, в которой учится лишь выходной слой.
Так как, в отличие от NTK, для подсчёта NNGP-ядра не требуется обратный проход (backward pass), последнее более вычислительно эффективно; Towards nngp-guided neural architecture search – пример работы, в которой предпочтение отдаётся NNGP-ядру именно по этой причине.
Сходимость эмпирического ядра
Вы этом параграфе мы покажем, что при определённой параметризации эмпирическое NTK не зависит ни от времени, ни от инициализации. Мы начнём с иллюстративного примера, прежде чем формулировать строгую теорему.
Рассмотрим сеть с одним скрытым слоем, скалярным выходом и гауссовской инициализацией весов; вход для простоты тоже положим скалярным:
f(x;a
1:n
,w
1:n
)=
i=1
∑
n
a
i
ϕ(w
i
x),a
1:n
∼N(0,n
−1
I),w
1:n
∼N(0,I).
Здесь
n
n – ширина скрытого слоя.
Следуя одной из стандартных схем инициализации из статьи Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, дисперсия каждого слоя выбирается обратно пропорционально числу входных нейронов (подробнее см. в параграфе про тонкости обучения нейросетей).
Назовём описанную выше параметризацию стандартной.
Для сходимости ядра нам придётся несколько её видоизменить:
f(x;a
1:n
,w
1:n
)=
n
1
i=1
∑
n
a
i
ϕ(w
i
x),a
1:n
∼N(0,I),w
1:n
∼N(0,I).
Назовём новую параметризацию NTK-параметризацией.
Отметим, что распределение выходов нейронов в инициализации остаётся неизменным при переходе от стандартной к NTK-параметризации. Что меняется – это динамика градиентного спуска:
j=1
∑
m
ϕ(w
k
x
j
)(y
j
−f
t
(x
j
)),
j=1
)).
При
t
=
0
t=0 приращения весов для такой параметризации имеют порядок
O(n
−1/2
), в то время как сами веса имеют порядок
O
(
1
)
O(1) при
t
=
0
t=0. Поэтому
(t)→a
k
(0) и
(t)→w
k
(0) при
n
→
∞
n→∞ для любого данного
k
∈
N
k∈N и
t
∈
R
+
t∈R
+
. Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке.
Сравним с градиентным спуском в стандартной параметризации:
j=1
∑
m
ϕ(w
k
x
j
)(y
j
−f
t
(x
j
)),
j=1
В этом случае веса выходного слоя имеют порядок
O(n
−1/2
) при
t
=
0
t=0, но получают приращения порядка
O
(
1
)
O(1) в этот момент времени, в то время как веса входного слоя имеют порядок
O
(
1
)
O(1) при
t
=
0
t=0, но получают в этот момент времени приращения порядка
O(n
−1/2
).
В новой параметризации эмпирическое NTK выглядит следующим образом:
(x,x
′
)=
i=1
∑
n
(∂
a
i
f(x)∂
a
i
f(x
′
)+∂
w
i
f(x)∂
w
i
f(x
′
))=
i=1
∑
n
(ϕ(w
i
(t)x)ϕ(w
i
(t)x
′
)+a
i
2
(t)ϕ
′
(w
i
(t)x)ϕ
′
(w
i
(t)x
′
)xx
′
).
Так как
(t)→a
k
(0) и
(t)→w
k
(0) при
n
→
∞
n→∞ для любых заданных
k
∈
N
k∈N и
t
∈
R
+
t∈R
+
, выражение выше асимптотически эквивалентно
(x,x
′
)=
n
1
i=1
∑
n
(ϕ(w
i
(0)x)ϕ(w
i
(0)x
′
)+a
i
2
(0)ϕ
′
(w
i
(0)x)ϕ
′
(w
i
(0)x
′
)xx
′
),
а значит, сходится к
Θ(x,x
′
)=E
a,w∼N(0,1)
(ϕ(wx)ϕ(wx
′
)+a
2
ϕ
′
(wx)ϕ
′
(wx
′
)xx
′
)
при
n
→
∞
n→∞ в силу закона больших чисел.
Предельное ядро
Θ(x,x
′
) не зависит ни от времени
t
t, ни от инициализации. Мы будем называть это ядро нейрокасательным или просто NTK (его не стоит путать с эмпирическим NTK
Ещё раз подчеркнём, что это работает для NTK-параметризации, но не для стандартной. Для стандартной параметризации эмпирическое NTK в инициализации расходится с шириной:
(x,x
′
)=
i=1
∑
n
(ϕ(w
i
(0)x)ϕ(w
i
(0)x
′
)+a
i
2
(0)ϕ
′
(w
i
(0)x)ϕ
′
(w
i
(0)x
′
)xx
∼n⋅E
w∼N(0,1)
ϕ(wx)ϕ(wx
′
).
Подробнее мы поговорим об этом в одном из следующих параграфов.
Для NTK-параметризации сходимость эмпирического ядра выполняется не только для сетей с одним скрытым слоем. Так, рассмотрим полносвязную сеть с
L
L слоями:
f(x)=h
L
(x),h
l
(x)=
n
l−1
1
W
l
x
l−1
(x),x
l−1
(x)=ϕ(h
l−1
(x)),x
0
(x)=x.
Здесь
1×n
L−1
l−1
для всех остальных
l
l.
Положим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь
=−∇
θ
(
j=1
∑
m
L(y
j
,f(x
j
;θ
t
)))=
j=1
∑
m
∂z
∂L(y
j
,z)
z=f(x
f(x
j
;θ
t
),
где
θ
θ – объединение всех весов
W
1
:
L
W
1:L
сети.
Теорема ниже доказана в оригинальной работе по NTK:
Теорема. В предположениях выше, если
ϕ
ϕ из
C
2
C
2
и липшицева и
L
L из
C
1
C
1
и липшицева, то
(x,x
′
) сходится к
Θ(x,x
′
) по вероятности при
1:L−1
→∞ последовательно
∀x,x
∀t≥0.
Оказывается, что эта теорема верна не только для полносвязных сетей с гладкими активациями.
Определим тензорную программу как начальный набор переменных определённых типов и последовательность команд. Каждая команда порождает новую переменную, действуя на уже имеющиеся.
Переменные бывают трёх типов:
A
A:
n
×
n
n×n матрицы с независимыми элементами из
N(0,1);
G
G: вектора размера
n
n с асимптотически независимыми нормальными элементами;
H
H: образы
G
G-переменных относительно поэлементных нелинейностей.
Для переменной
W
W запись
W
:
A
W:A будет означать, что
W
W имеет тип
A
A.
Команды бывают следующие:
trspop:
W:A→W
T
:A (перевести переменную типа
A
A со значением
W
W в переменную типа
A
A со значением
W
T
W
T
);
matmul:
(W:A, x:H)→
n
1
Wx:G;
lincomb:
({x
i
:G,a
i
∈R}
i=1
k
)→∑
i=1
k
a
i
x
i
:G;
nonlin:
({x
i
:G}
i=1
k
,ϕ:R
k
→R)→ϕ(x
1:k
):H (здесь мы несколько выходных векторов
x
i
x
i
агрегируем в один с помощью покоординатной, возможно, нелинейной функции).
Формализм тензорных программ позволяет представить прямой и обратный проход широкого класса нейронных архитектур, который включает свёрточные сети, рекуррентные сети, сети с residual слоями. Хотя и ни одна из операций выше не может порождать новые
A
A-переменные (веса), любое наперёд заданное число шагов градиентного спуска можно представить в рамках одной тензорной программы (посредством «развёртывания» шагов градиентного спуска).
Назовём величину
n
n шириной тензорной программы.
Основная «предельная» теорема тензорных программ представлена ниже:
Master theorem (G. Yang, Tensor programs III: Neural matrix laws). Рассмотрим тензорную программу с
M
M
G
G-величинами, удовлетворяющую определённым начальным условиям. Пусть все нелинейности
ϕ
ϕ и функция
ψ:R
M
→R полиномиально ограничены. Тогда
α=1
∑
n
ψ(g
α
1
,…,g
α
M
)→E
Z∼N(μ,Σ)
ψ(Z)
почти наверное при
n
→
∞
n→∞, где
μ
μ и
Σ
Σ могут быть вычислены по некоторым рекурентным правилам.
Оказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в виде
α=1
n
ψ(g
α
1
,…,g
α
M
) для некоторой функции
ψ
ψ, см. Tensor programs II: Neural tangent kernel for any architecture.Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для ядра в любой фиксированный момент времени, см. Tensor Programs IIb.
В качестве иллюстрации обратимся вновь к сети с одним скрытым слоем. Рассмотрим тензорную программу, вычисляющую прямой и обратный проходы на входах
x
x и
x
′
x
′
. Такая программа порождает следующие
G
G-величины:
=w(0)x,
=w(0)x
=a(0)x и
=a(0)x
′
. Напомним, что эмпирическое NTK равно
(x,x
′
)=
n
1
i=1
∑
n
(ϕ(w
i
(0)x)ϕ(w
i
(0)x
′
)+a
i
2
(0)ϕ
′
(w
i
(0)x)ϕ
′
(w
i
(0)x
′
)xx
′
).
Положив
ψ(g
α
1
,…,g
α
4
)=ϕ(g
α
1
)ϕ(g
α
2
)+ϕ
получим выражение как раз в виде, требуемом Master Theorem.
Стандартная параметризация и эволюция ядра
Как было отмечено в предыдущем параграфе, эмпирическое NTK двухслойной сети расходится с шириной при стандартной параметризации.
(x,x
′
)=
i=1
∑
n
(ϕ(w
i
(t)x)ϕ(w
i
(t)x
′
)+a
i
2
(t)ϕ
′
(w
i
(t)x)ϕ
′
(w
i
(t)x
′
)xx
′
).
При
t
=
0
t=0, так как
w
i
w
i
независимы и имеют порядок
O
(
1
)
O(1), сумма расходится пропорционально
n
n.Так как для квадратичной функции потерь
(x)=
Θ
^
t
(x,
)), предсказание модели в любой точке
x
x получает приращение порядка
O
(
n
)
O(n) на первом же шаге обучения; для задачи регрессии такая модель теряет смысл.
Однако для классификации величина предсказаний не играет роли: для бинарной классификации важен лишь знак, а для многоклассовой – индекс максимального логита. Таким образом, в этом случае, несмотря на расходящееся ядро, предел при бесконечной ширине имеет смысл, см. Dynamically Stable Infinite-Width Limits of Neural Classifiers.
Рассмотрим нормализованное эмпирическое NTK
(x,x
′
)=
Θ
^
t
(x,x
′
)/n. Его предел в инициализации равен
w∼N(0,1)
ϕ(wx)ϕ(wx
′
). Назовём этот предел нормализованным NTK и обозначим
(x,x
′
).
В отличие от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени:
(x,x
′
)
=
n
1
i=1
∑
n
(ϕ(w
i
(t)x)ϕ
′
(w
i
(t)x
(t)x)ϕ(w
i
(t)x
′
)x)
dt
dw
i
(t)
(x,x
′
)
+
n
1
i=1
∑
n
a
i
2
(t)xx
′
(ϕ
′
(w
i
(t)x)ϕ
′′
(w
i
(t)x
(t)x)ϕ
′
(w
i
(t)x
′
)x)
dt
dw
i
(t)
(x,x
′
)
+
n
1
i=1
∑
n
2a
i
(t)ϕ
′
(w
i
(t)x)ϕ
′
(w
i
(t)x
′
)xx
′
dt
da
i
(t)
.
Напомним, как выглядит градиентный спуск в стандартной параметризации:
(t)
=
j=1
∑
m
ϕ(w
k
(t)x
j
),
dt
w
k
(t)
=
j=1
∑
m
a
k
(t)ϕ
′
(w
k
(t)x
j
)x
j
.
При
t
=
0
t=0,
=O(1), в то время как
=O(n
−1/2
). Так как
(0)=O(n
−1/2
) и
(0)=O(1), для любого
t
>
0
t>0, не зависящего от
(t)=O(1),
(t)=O(1),
(t)=O(1) и
(t)=O(1).
Наивная оценка сумм даёт
(x,x
′
)
=O(1)+O(1)+O(1)=O(1) для любого
t
>
0
t>0, не зависящего от
n
n. Таким образом, нормализованное ядро зависит от времени даже в пределе бесконечной ширины. Экспериментальный анализ эволюции ядра реальной нейронной сети в стандартной параметризации см. в работе Deep learning versus kernel learning.
Преимущество нейронных сетей над ядровыми методами, в том числе с NTK, может быть связано, в частности, с зависимостью предельного ядра от времени. В самом деле, ядро измеряет «похожесть» в некотором пространстве признаков. Для NTK это пространство фиксировано, в то время как нейронная сеть меняет своё ядро по ходу обучения, возможно, делая его более подходящим для задачи.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
13.3. PAC-байесовские оценки риска
Следующий параграф
13.5. Ландшафт функции потерь
