---
title: Обобщённые линейные модели
url: https://education.yandex.ru/handbook/ml/article/obobshyonnye-linejnye-modeli
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.3
---
Как прокачать линейную модель с помощью распределений из экспоненциального класса
Мотивация
До сих пор мы рассматривали в основном модели вида
y∼f(x)+ε
с шумом
ε
ε из того или иного распределения. Но у этих моделей:
шум не зависит от
x
x;
y
y может принимать любые значения.
А что, если мы захотим предсказывать время ожидания доставки? Казалось бы, чем дольше время потенциального ожидания, тем больше его дисперсия. А как корректно предсказывать таргет, который принимает только целые значения?
Один из подходов мы обсудим в этом параграфе. Грубо говоря, вместо того, чтобы прибавлять один и тот же шум, мы зафиксируем семейство распределений
p(y∣μ(x)), в котором изменяемым параметром будет зависящее от
x
x математическое ожидание
μ
(
x
)
μ(x).
Вот как могут выглядеть такие модели для случаев, если
p
p нормальное с фиксированной дисперсией, экспоненциальное или пуассоновское соответственно:
11
Как видим, такой подход позволяет получать и модели с меняющейся дисперсией шума, и модели с целочисленным таргетом.
Определение
Мы рассмотрим достаточно широкий класс моделей — обобщённые линейные модели (generalized linear models, GLM). К ним относятся, в частности, линейная и логистическая регрессии. В итоге мы научимся подбирать подходящую регрессионную модель для самых разных типов данных.
Вспомним, что вероятностную модель линейной регрессии можно записать как
y∣x∼N(⟨x,w⟩,τ
2
),
а вероятностную модель логистической регрессии — как
y∣x∼Bern(σ(⟨x,w⟩)),
где
Bern(p) — распределение Бернулли с параметром
p
p, а
σ(u)=
1+e
−u
1
.
Итак, чем в этих терминах отличаются вероятностные модели линейной и логистической регрессии? Во первых, параметризованное семейство распределений для
y
∣
x
y∣x, а именно,
N(∗,σ
2
) в случае линейной регрессии и
B
e
r
n
Bern в случае логистической.
Во-вторых, в обоих случаях математическое ожидание условного распределения
y
∣
x
y∣x является функцией от
⟨
x
,
w
⟩
⟨x,w⟩. На это можно посмотреть и по-другому: для каждой из задач выбрана функция
g
g такая, что
g(E(y∣x))=⟨x,w⟩. Эта функция называется функцией связи (link function).
В случае линейной регрессии
g(u)=u. В самом деле,
E(y∣x)=EN(⟨x,w⟩,τ
2
)=⟨x,w⟩. В случае логистической регрессии
logit
(
u
)
=
log
⁡
u
1
−
u
g(u)=σ
−1
(u)=logit(u)=log
1−u
u
. Давайте это тоже проверим. В модели логистической регрессии условное распределение
y
∣
x
y∣x — это распределение Бернулли с вероятностью успеха
σ(⟨x,w⟩), и этой же вероятности равно его математическое ожидание. Следовательно,
g(σ(⟨x,w⟩))=σ
−1
(σ(⟨x,w⟩))=⟨x,w⟩.
Обобщая, можно сказать, что, если данные таковы, что
E(Y∣X) не является линейной функцией от
x
x, мы линеаризуем
E(Y∣X) с помощью функции связи
g
g.
Замечание: Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного
x
x оценить все параметры распределения
y
∣
x
y∣x, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов
w
w, ни на итоговые предсказания
E(Y∣X), которые выдаёт обученная модель.
Продолжим. Задав эти две составляющие — параметризованное семейство распределений и функцию связи — мы получим обобщённую линейную модель (GLM).
Для нового объекта
x
x она выдаст предсказание
=E(y∣x)=g
−1
(⟨x,w⟩), а выбор класса распределений
y
∣
x
y∣x потребуется нам для подбора весов
w
w. В принципе, можно выбрать любой класс распределений
y
∣
x
y∣x и любую монотонную функцию связи
g
g, получив некоторую вероятностную модель. Однако обычно для упрощения поиска оптимальных весов
w
w в GLM предполагают, что
y
∣
x
y∣x принадлежит одному из достаточно простых семейств экспоненциального класса.
Что даёт нам принадлежность экспоненциальному классу?
В контексте GLM обычно рассматривают подкласс экспоненциального класса, состоящий из семейств, представимых в виде
exp
p(y∣θ,ϕ)=exp(
ϕ
yθ−a(θ)
+b(y,ϕ))
где
θ
θ и
ϕ
ϕ — скалярные параметры, причём
ϕ
ϕ — нечто фиксированное, обычно дисперсия, которая чаще всего полагается равной
1
1, а значения
θ
θ параметризуют распределения из семейства. Нетрудно переписать плотность в более привычном для нас виде, чтобы стало очевидно, что это семейство действительно из экспоненциального класса:
exp
exp
exp
p(y∣θ,ϕ)=
exp(
ϕ
a(θ)
)
1
exp(b(y,ϕ))exp(
ϕ
yθ
)
Действительно, если вспомнить, что
φ
φ — это константа, а не параметр, то получается очень похоже на
exp
p(y∣ν)=
h(ν)
1
g(y)⋅exp(ν
T
u(y))
В частности, мы видим, что
u
(
y
)
u(y) состоит из единственной компоненты
(y), равной
y
ϕ
ϕ
y
. По доказанной в предыдущем разделе лемме имеем тогда, что математическое ожидание
μ
μ такой случайной величины равно
μ=ϕEu
1
(y)=ϕ
∂θ
∂
(
ϕ
a(θ)
)=a
′
(θ)
До сих пор мы рассуждали о распределении
p
(
y
)
p(y) без
x
x в условии. Что будет, если его добавить? Параметр
ϕ
ϕ мы договорились сохранять постоянным, тогда от
x
x должен зависеть единственный оставшийся параметр
θ
θ.
Самый естественный в нашей ситуации вариант — это положить
θ=⟨x,w⟩. В GLM мы вводили функцию
g
g, для которой
g(E(y∣x))=⟨x,w⟩, то есть
E(y∣x)=g
−1
(⟨x,w⟩). Но ведь матожидание
y
y равно
(θ), то есть
(⟨x,w⟩). Это позволяет нам однозначно определить функцию связи
g=(a
′
)
−1
. Такая функция связи называется канонической функцией связи (canonical link function).
Примеры
Поговорим немного о том, как на практике подбирать
ϕ
,
a
,
b
ϕ,a,b, чтобы по классу распределений
y
∣
x
y∣x определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров.
Пример 1. Пусть мы решили применить к данным линейную регрессию. Тогда
exp
p(y∣x,w,σ
2
)=
2π
σ
1
exp(−
2σ
2
(y−⟨x,w⟩)
2
)
Обозначим для краткости
μ=⟨x,w⟩ и будем рассматривать
p(y∣μ,σ
2
).
Мы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но давайте выразим эту плотность в описанном выше более частном виде:
exp
log
p(y∣μ,σ
2
)=exp(−
2σ
2
(y−μ)
2
−log(2πσ
2
))
В формуле экспоненциального семейства распределений единственная часть, не зависящая от
θ
θ, — это функция
b
b. Поскольку
μ=a
′
(θ), функция
b
b также не должна зависеть от
μ
μ. Так что внутри экспоненты выделим в качестве функции
b
b всё, что не зависит от
exp
log
p(y∣μ,σ
2
)=exp
=ϕ
σ
2
yμ
=yθ
−
μ
2
/2
=a(θ)
=b(y,ϕ)
+log(2πσ
2
))
Эта формула уже похожа на формулу экспоненциального семейства распределений и видно, что
ϕ
=
σ
2
ϕ=σ
θ=g(μ)=μ (коэффициент при
y
y),
a(θ)=μ
2
/2=θ
2
/2,
log
b(y,ϕ)=−
2σ
2
y
2
−log(2πσ
2
).
Каноническая функция связи является обратной к
(θ)=θ, то есть
⟨x,w⟩=g(μ)=μ, как мы и привыкли.
Пример 2. Проделаем то же самое, но теперь для распределения Бернулли.
Пример 3. Хорошо, про линейную и логистическую регрессию мы и так знали. Давайте попробуем решить с помощью GLM новую задачу.
Пусть мы хотим по каким-то признакам
X
X предсказать количество «лайков», которое пользователи поставят посту в социальной сети за первые 10 минут после публикации. Конечно, можно использовать для этого линейную регрессию. Однако предположение линейной регрессии, что
Y
∣
X
∼
N
Y∣X∼N, в данном случае странное по нескольким причинам.
Во-первых, количество лайков заведомо не может быть отрицательным, а нормальное распределение всегда будет допускать ненулевую вероятность отрицательного значения.
Во-вторых, количество лайков — всегда целое число.
В-третьих, у распределения количества лайков, скорее всего, положительный коэффициент асимметрии (skewness). То есть, если модель предсказывает, что под постом будет 100 лайков, мы скорее можем ожидать, что под ним окажется 200 лайков, чем 0. Нормальное распределение симметрично и не может описать такие данные.
С другой стороны, если мы предположим, что в первые 10 минут после публикации есть какая-то постоянная частота (своя для каждого поста, зависящая от
x
x), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем:
11
Но какая будет каноническая функция связи, если мы считаем, что
Y
∣
X
∼
Poisson
Y∣X∼Poisson? Аналогично первому и второму примерам:
exp
⁡
(
y
log
⁡
μ
−
μ
−
log
⁡
y
!
)
p(y∣μ)=
y!
e
−μ
μ
y
=exp(ylogμ−μ−logy!)
Откуда
ϕ
=
1
ϕ=1,
log
⁡
μ
θ=g(μ)=logμ,
exp
⁡
(
θ
)
a(θ)=μ=exp(θ),
log
⁡
y
!
b(y,ϕ)=−logy!
Значит, эта модель (она называется пуассоновская регрессия), будет предсказывать с помощью формулы
exp
E(y∣x)=g
−1
(⟨x,w⟩)=exp(⟨x,w⟩).
Вопрос на подумать. В каких ситуациях была бы полезной функция связи complementary log-log link (cloglog)
g
(
x
)
=
log
g(x)=log(−log(1−x))?
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
4.2. Экспоненциальный класс распределений и принцип максимальной энтропии
Самые главные семейства распределений в жизни любого data scientist’а
Следующий параграф
4.4. Как оценивать вероятности
Как правильно оценить вероятности классов в задаче классификации
