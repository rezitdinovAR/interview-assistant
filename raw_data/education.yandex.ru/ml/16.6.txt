---
title: Параметрические оценки
url: https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki
course: ml
chapter: 16. Теормин
chapter_id: 16.6
---
Различные типы распределений, описанные в предыдущих параграфах, применяются в качестве теоретических моделей в задачах, связанных со случайностью и неопределённостью. Однако на практике далеко не всегда ясно, какое именно распределение моделирует имеющиеся в наличии данные. А если из каких-либо соображений тип распределения всё же установлен, то следующая задача — оценить параметры этого распределения, например, среднее и/или дисперсию в случае гауссовского распределения
N(μ,σ
2
).
Подобными обратными по отношению к теории вероятностей задачами занимается математическая статистика. Типичный пример статистической задачи: по числовой выборке
,…,X
n
оценить параметры распределения, из которого они были получены. Обычно предполагается, что выборка i.i.d. (independent and identically distributed), то есть представляет собой независимые реализации случайной величины с одним и тем же распределением. Параметр этого определения
θ
θ может быть числом или вектором; оценку этого параметра по выборке
,…,X
n
обычно обозначают
,…,X
n
) или просто
θ
^
θ
.
Предельные теоремы
Как правило, чем больше размер выборки, тем более информативны параметрические оценки вида
,…,X
n
). Теоретические свойства таких оценок при
n
→
∞
n→∞ устанавливаются с помощью предельных теорем теории вероятностей.
Закон больших чисел
Внимательный читатель мог обратить внимание, что в ряде примеров из предыдущих параграфов параметры некоторых распределений почему-то молчаливо подменялись средними значениями. Так мы поступили в задаче о показе рекламы, взяв в качестве параметра пуассоновского распределение среднее количество кликов пользователей. Фактически мы оценили неизвестный параметра
λ
λ средним по выборке:
k=1
∑
n
X
k
.
В общем-то это кажется логичным, поскольку
λ
=
E
ξ
λ=Eξ, если
ξ∼Pois(λ). Однако у такой оценки есть также мощное теоретическое обоснование.
Теорема (Закон больших чисел, ЗБЧ). Пусть
,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием
μ
μ. Тогда для любого
ε
>
0
ε>0
lim
где
n→∞
lim
P(∣
X
n
−μ∣>ε)=0, где
X
n
=
n
1
k=1
∑
n
X
k
.
Таким образом, чем больше размер выборки
n
n, тем менее вероятно отклонение выборочного среднего
X
‾
n
X
n
от истинного среднего
μ
μ на любое число
ε
>
0
ε>0.
Закон больших чисел особенно легко обосновать для случая конечных дисперсий:
<+∞. Имеем
k=1
∑
n
EX
k
=μ,V
k=1
Отсюда видно, что
lim
n→∞
lim
V
X
n
=0, поэтому при больших
n
n распределение случайной величины
всё больше похоже на распределение, сосредоточенное в одной лишь точке
μ
μ. Формально же утверждение ЗБЧ получается с помощью неравенства Чебышева:
P(∣
X
n
−μ∣>ε)⩽
→0,n→∞.
Закон больших чисел допускает следующее усиление.
Теорема (Усиленный закон больших чисел, УЗБЧ). Пусть
,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием
μ
μ. Тогда выборочное среднее
X
‾
n
X
n
почти наверное сходится к
μ
μ, т.е.
P
(
lim
n→∞
lim
X
n
=μ)=1.
Теорема Муавра-Лапласа
Доска Гальтона иллюстрирует биномиальное распределение. До поворота на ее дне лежит множество маленьких шариков. Сразу после переворота шарики проходят через 10 рядов гладких круглых препятствий. Преодоление каждого препятствия можно рассматривать как испытание Бернулли: с равными вероятностями шарик может пойти как налево, так и направо. Поэтому финальное положение шарика в одной из 10 корзин является приблизительной реализацией биномиального распределения
0.5
)
Bin(10,0.5).
Уже при
n
=
10
n=10 биномиальное распределение напоминает нормальное. И действительно, чем больше
n
n, тем лучше дискретная случайная величина
ξ∼Bin(n,p) аппроксимируется непрерывной гауссианой
N(np,np(1−p)).
Теорема Муавра-Лапласа. Пусть
ξ∼Bin(n,p),
q
=
1
−
p
q=1−p, тогда
lim
n→∞
lim
P(a<
npq
ξ−np
⩽b)=
dx.
Из теоремы Муавра-Лапласа вытекает, что при больших
n
n вероятность попадания биномиальной случайной величины
ξ∼Bin(n,p) в заданный интервал можно оценить как
P(A<ξ⩽B)≈Φ(
npq
B−np
)−Φ(
npq
A−np
).
где
Φ
(
z
)
Φ(z) — функция распределения стандартного нормального распределения.
Центральная предельная теорема
При выводе закона больших чисел мы видели, что выборочное среднее
X
‾
n
X
n
имеет среднее
μ
μ и дисперсию
. Но как именно выглядит распределение случайной величины
X
‾
n
X
n
при увеличении
n
n? Оказывается, что оно становится всё больше похоже на
N(μ,
n
σ
2
). Вот как, например, выглядят нормализованные гистограммы
5000
5000 выборочных средних, построенных по i.i.d. выборкам
0.3
)
X
1
,…,X
n
∼Bin(30,0.3) для разных значений
n
n:
Эти гистограммы и впрямь очень напоминают гауссианы, и это прямое следствие следующей теоремы.
Центральная предельная теорема, ЦПТ. Пусть
,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием
μ
μ и дисперсией
σ
2
σ
2
. Тогда
при
−μ)
≈N(0,1) при n≫1.
Точнее говоря,
lim
n→∞
lim
P(Z
n
⩽z)=Φ(z). Таким образом, случайная величина
Z
n
Z
n
сходится по распределению к
N(0,1):
N(0,1).
Если применить центральную предельную теорему к бернуллиевским случайным величинам с вероятностью успеха
p
p, то вновь получим теорему Муавра-Лапласа.
Свойства параметрических оценок
Оценивать параметры можно по-разному, хочется делать это хорошо. Ценные свойства оценок, которые обычно желательны – это несмещенность и состоятельность.
Несмещённость
Каждый элемент i.i.d выборки
,…,X
n
можно рассматривать как значение случайной величины из некоторого распределения с неизвестным параметром
θ
θ.
А раз так, то всякую оценку этого параметра
,…,X
n
) также можно считать случайной величиной, у которой можно пытаться вычислять математическое ожидание, например.
Оценка
,…,X
n
) параметра
θ
θ называется несмещенной, если
=θ. Несмещённость оценки означает, что она в среднем будет равна истинному значению параметра.
Интуитивно можно представлять себе несмещённость следующим образом: если мы нагенерим большое количество выборок
(i)
,X
2
(i)
,…,X
n
(i)
1⩽i⩽N, и для каждой посчитаем оценку
(i)
, то в среднем получится более или менее истинное значение параметра
i=1
∑
N
θ
(i)
≈θ.
Простейший пример несмещённой оценки среднего значения
θ
θ даёт выборочное среднее
, поскольку
k=1
⋅nθ=θ.
Медианой выборки
,…,X
n
называется средний член вариационного ряда, состоящего из отсортированных по возрастанию элементов выборки:
(1)
⩽X
(2)
⩽…⩽X
(n)
.
Если
n
n нечётно,
n=2m+1, то есть ровно один элемент в середине вариационного ряда, именно он называется медианой:
med(X
1
,…,X
n
)=X
(m)
=X
(
2
n+1
)
. При чётном
n
=
2
m
n=2m в качестве медианы берут среднее двух центральных элементов вариационного ряда:
med(X
1
,…,X
n
)=
2
1
(X
(m)
+X
(m+1)
+1)
).
Упражнение. Дана i.i.d. выборка
,…,X
n
из равномерного распределения
U[0,2θ]. Докажите, что выборочная медиана даёт несмещённую оценку медианы распределения
U[0,2θ].
В некоторых случаях оценка
,…,X
n
) смещена, но с ростом
n
n это смещение нивелируется. Если
lim
n→∞
lim
E
θ
n
=θ, то оценка
θ
^
n
θ
n
называется асимптотически несмещённой.
Упражнение. Пусть
,…,X
n
∼U[0,θ] — i.i.d. выборка. Оценим параметр
θ
θ как максимальное значение выборки:
max
(n)
=max{X
1
,…,X
n
}.
Является ли эта оценка несмещённой? Асимптотически несмещённой?
Состоятельность
Оценка
,…,X
n
) называется состоятельной, если она сходится по вероятности к
θ, то есть
lim
для любого
ε
>
0.
n→∞
lim
P(∣
θ
n
−θ∣>ε)=0 для любого ε>0.
Cостоятельность означает, что с ростом размера выборки всё менее вероятны хоть сколько нибудь значимые отклонения оценки от истинного значения параметра.
Если i.i.d. выборка
,…,X
n
получена из распределения с конечным математическим ожиданием
θ
θ, то в силу закона больших чисел выборочное среднее
X
‾
n
X
n
является состоятельной оценкой для
θ
θ.
Состоятельность оценки – независимое от несмещенности свойство: оценки могут быть состоятельными, но не несмещенными и наоборот. Например, оценка
(n)
из предыдущего упражнения оказалась смещённой, однако, она состоятельна:
P(∣X
(n)
−θ∣>ε)=P(X
(n)
<θ+ε)=
θ−ε
)
n
=(1−
θ
ε
)
n
→0,n→∞.
Упражнение. Приведите пример несмещённой оценки, не являющейся состоятельной.
Имея i.i.d. выборку
,…,X
n
из невырожденного распределения с конечным средним
θ
θ, оценим это среднее как
. Эта оценка, очевидно, несмещённая:
=EX
1
=θ. Состоятельной, однако, она не является, ведь выражение
P(∣
θ
−θ∣>ε)=P(∣X
1
−θ∣>ε)
никоим образом не зависит от
n
n. Следовательно, состоятельность оценки
θ
^
θ
означала бы, что
P(∣X
1
−θ∣>ε)=0 для любого
ε
>
0
ε>0. Такое возможно только для вырожденного распределения, сосредоточенного в одной лишь точке
P(X
1
=θ)=1.
Bias-variance decomposition
Смещение (bias) оценки
,…,X
n
) определяется как
bias(
θ
)=E
θ
−θ.
Смещение показывает, насколько оценка в среднем отклоняется от истинного значения. Оценка
θ
^
n
θ
n
несмещённая, если
bias(
θ
n
)=0;
асимптотически несмещённая, если
lim
n→∞
lim
bias(
θ
n
)=0.
Среднеквадратичной ошибкой (mean squared error, MSE) оценки называется величина
MSE(
θ
)=E(
θ
−θ)
2
.
Смещение, дисперсия и среднеквадратичная ошибка связаны между собой следующим соотношением (bias-variance decomposition):
bias
MSE(
θ
)=bias
2
(
θ
)+V(
θ
).
Упражнение. Докажите, что оценка
θ
^
n
θ
n
состоятельная, если она асимптотически несмещённая и
lim
n→∞
lim
V(
θ
n
)=0.
Таким образом, если
lim
n→∞
lim
MSE(
θ
n
)=0, то оценка
θ
^
n
θ
n
параметра
θ
θ асимптотически несмещённая и состоятельная.
Асимптотическая нормальность
Стандартным отклонением оценки
θ
^
n
θ
n
параметра
θ
θ называется корень из дисперсии:
se(
Оценка
θ
^
n
θ
n
асимптотически нормальна, если
se(
N(0,1), т.е.
lim
n→∞
lim
P(
se(
⩽z)=Φ(z).
Согласно центральной предельной теореме выборочное среднее i.i.d. выборки из распределения с конечными средним
μ
μ и дисперсией
σ
2
σ
2
является асимптотически нормальной оценкой параметра
μ
μ.
Эффективность
Пусть
— несмещённые оценки параметра
θ
θ. Оценка
θ
^
θ
эффективнее оценки
θ
~
θ
~
, если
. Такое определение эффективности вполне логично, ведь чем меньше дисперсия несмещённой оценки, тем меньше у неё шансов удалиться куда-то далеко от истинного значения параметра.
Пример. Пусть
,…,X
n
— i.i.d. выборка из распределения
U[0,2θ]. Какая оценка параметра
θ
θ эффективнее: выборочное среднее или медиана?
Несмещённость оценок
=med(X
1
,…,X
n
) уже была показана выше.
Найдём дисперсию наших оценок. Диспресия случайной величины
ξ∼U[0,2θ] равна
Vξ=
3
θ
2
, следовательно,
Найти дисперсию медианы несколько сложнее. Ограничимся случаем
n=2m+1. Тогда
(m+1)
, и
=EX
(m+1)
=
2θ
1
(m!)
2
(2m+1)!
(1−
2θ
x
)
m
dx.
С помощью замены
отсюда находим, что
(m!)
2
(2m+1)!
m+2
(1−t)
dt=4θ
2
(m!)
2
(2m+1)!
B(m+3,m+1)=
=4θ
2
(m!)
2
(2m+1)!
(2m+3)!
(m+2)!m!
=2θ
2
2m+3
m+2
=θ
2
+
n+3
θ
2
.
Следовательно,
n+3
θ
2
, что при
n
>
1
n>1 больше, чем
, так что выборочное среднее эффективнее
медианы (примерно в
3
3
раз при больших
n
n, если считать по отношению стандартных отклонений).
Несмотря на то что в плане эффективности среднее оказалось предпочтительнее в этом примере,
в статистике медиану любят за бОльшую устойчивость к выбросам.
Ниже приведён scatter-plot, по которому можно наглядно оценить меру разброса среднего и медианы выборки из равномерного распределения на отрезке
[0,2θ] для
θ
=
5
θ=5. Для построения этого графика были взяты
200
200 i.i.d. выборок из
U[0,10] размера
n
=
10
,
100
,
1000
,
10000
n=10,100,1000,10000, и для каждого
n
n посчитаны выборочное среднее и медиана. Эти статистики и задают координаты точки на графике. Разумеется, чем больше значение
n
n, тем кучнее локализованы точки вокруг среднего значения
θ
=
5
θ=5, совпадающего в данном случае с медианой. Как видно, облако точек сосредоточено вдоль прямой
y=θ+
3
(x−θ).
Выборочная дисперсия
Как мы уже убедились, выборочное среднее
k=1
∑
n
X
k
представляет собой несмещённую и состоятельную оценку для математического ожидания. Можно ли то же самое сказать про выборочную дисперсию
k=1
в предположении, что i.i.d. выборка
,…,X
n
состоит из реализаций случайной величины
ξ
ξ с конечными моментами
E
ξ
=
θ
1
Eξ=θ
Прежде всего раскроем скобки и перепишем
S
‾
n
S
n
в виде
k=1
∑
n
(X
k
2
−2X
k=1
∑
n
X
k
2
−2(
где
k=1
∑
n
X
k
2
— выборочное среднее, построенное по выборке
,…,X
n
2
. Оно несмещённое, поэтому
. Заметим также, что
k=1
k=1
1⩽i<j⩽n
откуда в силу независимости
при
i
≠
j
i

=j получаем
1⩽i<j⩽n
n−1
θ
1
2
.
Итак,
n−1
θ
1
2
=
n
n−1
Vξ.
Таким образом, оценка дисперсии
S
‾
n
S
n
смещённая (хотя и асимптотически несмещённая). По этой причине для оценки дисперсии часто используют аналогичную несмещённую оценку
n−1
n
S
n
=
n−1
1
k=1
которую также называют выборочной дисперсией.
Обоснуем теперь состоятельность оценки
. Согласно закону больших чисел
. Здесь нам потребуется пара свойств сходимости по вероятности.
Упражнение. Пусть
η. Докажите, что
ξ+η.
Упражнение. Пусть
ξ. Докажите, что
Пользуясь результатами этих упражнений, заключаем, что
=Vξ, и, стало быть, оценка
S
‾
n
S
n
состоятельна.
Методы оценки параметров
До этого мы обсуждали разные приятные свойства оценок, а теперь рассмотрим некоторые методы, позволяющие систематически получать по выборке оценки параметров с нужными свойствами.
Метод моментов
Пусть выборка
,…,X
n
получена сэмплированием из некоторого семейства распределений
(x) с параметрами
θ=(θ
1
,…,θ
m
). Метод моментов для оценки этих параметров заключается в приравнивании выборочных моментов
j=1
∑
n
X
j
k
к теоретическим
(θ)=
(x).
Решая полученную систему уравнений
(θ)=
1⩽k⩽m, находим оценки параметров
Пример. Оценим параметры нормального распределения
N(μ,σ
2
) с помощью метода моментов.
Упражнение. Оцените по методу моментов параметры
a
a и
b
b для выборки
,…,X
U[a,b].
При некоторых условиях на регулярность семейства распределений
(x) оценка по методу моментов получается состоятельной и асимптотически нормальной.
Метод максимального правдоподбия
Пусть, как обычно, выборка
,…,X
n
∼F
θ
(x).
Правдоподобие (функция правдоподобия, likelihood) выборки
,…,…X
n
— это просто её совместная pmf или pdf. Вне зависимости от типа распределения будем обозначать правдоподобие как
L(θ)≡L(X
1
,…,X
n
∣θ)=p(X
1
,…,X
n
∣θ).
Если выборка i.i.d., то функция правдоподобия распадается в произведение одномерных функций:
L(X
1
,…,X
n
∣θ)=
k=1
∏
n
p(X
k
∣θ).
Оценка максимального правдоподобия (maximum likelihood estimation, MLE) максимизирует правдоподобие:
θ
^
M
L
=
arg
⁡
max
=arg
θ
max
L(θ)
Поскольку максимизировать сумму проще, чем произведение, обычно переходят к логарифму правдоподобия (log-likelihood). Это особенно удобно в случае i.i.d. выборки, тогда
θ
^
M
L
=
arg
⁡
max
⁡
θ
log
arg
⁡
max
log
=arg
θ
max
logL(θ)=arg
θ
max
k=1
∑
n
logp(X
k
∣θ).
Пример. В результате
n
n подбрасываний монеты выпало
k
k «орлов» и
n
−
k
n−k «решек».
Оценим вероятность выпадения «орла» методом максимального правдоподобия.
Пусть
p
p — вероятность выпадения «орла», тогда правдоподобие равно
L(p)=p
k
(1−p)
n−k
.
Дифференцируя логарифм правдоподобия
log
log
log
logL(p)=klogp+(n−k)log(1−p)
и приравнивая к нулю производную, находим
1−p
n−k
⟺k(1−p)=(n−k)p⟺p=
n
k
.
Нетрудно убедиться, что это точка максимума. Итак, оценка максимального правдоподобия
вероятности «успеха» в схеме Бернулли вполне ожидаемо оказалась равна доле «успехов» в серии из
n
n испытаний.
Упражнение. Пусть i.i.d. выборка
,…,X
n
взята из пуассоновского распределения с параметром
λ
λ. Найдите его оценку максимального правдоподобия.
Методом максимального правдоподобия можно оценить сразу несколько параметров.
Пример. Найдём MLE-оценки параметров распределения
N(μ,τ) по i.i.d. выборке
,…,X
n
.
Запишем правдоподобие:
exp
L(μ,τ)=
k=1
∏
n
2πτ
1
exp
2τ
−(X
k
−μ)
2
.
Перейдём к log-likelihood:
log
log
logL(μ,τ)=−
2
n
(logτ+ln2π)−
2τ
1
k=1
∑
n
(X
k
−μ)
2
.
Приравняем частные производные по
μ
μ и
τ
τ к нулю:
∂
log
∂logL
=
τ
1
k=1
∑
N
(X
k
−μ)=0,
∂
log
∂logL
k=1
∑
n
(X
k
−μ)
2
=0,
откуда
– выборочное среднее,
k=1
– выборочная дисперсия.
Упражнение. Пусть i.i.d. выборка
,…,X
n
∼U[a,b]. Найдите оценки максимального правдоподобия для параметров
a
a и
b
b.
Свойства оценки максимального правдоподобия
состоятельность:
инвариантность относительно параметризации: если
— MLE-оценка для
θ
θ, то
) — MLE-оценка для
φ
(
θ
)
φ(θ);
асимптотическая нормальность:
N(0,1);
асимптотическая оптимальность: при достаточно больших
n
n оценка
имеет минимальную дисперсию.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
16.5. Независимость и условные распределения вероятностей
Следующий параграф
16.7. Энтропия и семейство экспоненциальных распределений
