---
title: Нормализующие потоки
url: https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki
course: ml
chapter: 8. Генеративные модели
chapter_id: 8.4
---
В главе Генеративный подход к классификации мы уже познакомились с типом моделей, которые оценивают совместное распределение
p(X,Y). Такие модели называют генеративными. Для простоты предположим, что мы имеем всего один класс, тогда задача моделирования
P(X,Y) сводится к задаче моделирования
p
(
X
)
p(X). Научившись моделировать это распределение, мы сможем:
генерировать объекты
x∼p
θ
data
, где
θ
θ – параметры модели;
оценивать вероятность встретить данный объект
x
x среди набора наблюдаемых данных
D
D;
выучивать скрытые представления для объекта
x
x.
Известными примерами генеративных моделей являются:
Авторегрессионные модели:
(x)=
i=1
x∈R
n
,
Вариационные автокодировщики:
(x)=∫p
θ
(x,z)dz,
z
∈
R
m
.
z∈R
m
.
Но оба эти метода не позволяют одновременно:
получать скрытые представления для объектов
точно вычислять функцию правдоподобия
Нормализующие потоки способны решить обе эти задачи.
Мотивация
flow
Пусть
x∼p
x
(x), где
(x) неизвестно, а
z∼p
z
(z)=N(0,I). Мы хотим найти отображение
, для которого
x=f
θ
(z) и
z=f
θ
−1
(x).
Отображение
f
f преобразует базовую функцию плотности
p
z
p
z
к более сложной
p
x
p
x
. С его помощью мы можем генерировать сложный объект путем сэмплинга простого объекта
z
z (скрытой переменной) из распределения
p
z
p
z
и применения «генератора»
f(z)=x. Обратное отображение
f
−
1
f
−1
«нормализует» сложное распределение
p
x
p
x
, приводя его к простому
p
z
p
z
.
Найдя такое отображение
f
f, мы сможем генерировать новые объекты
x
x, а оценить плотность
(x) поможет формула преобразования плотности случайной величины. Давайте её вспомним.
Формула замены переменной
Пусть
x∼p
x
(x),
z∼p
z
(z), при этом отображение
f:R
n
→R
n
дифференцируемо, обратимо и
x=f(z). Тогда:
det
(x)=p
z
(f
−1
(x))⋅
det(J
f
−1
)
,
где
det
det(J
f
−1
) – якобиан отображения
Определение
Итак, нормализующий поток – это обратимое дифференцируемое отображение
, которое переводит исходные представления объектов в скрытые:
x=f
θ
(z) и
z=f
θ
−1
(x).
При этом функция правдоподобия
(x) вычисляется по формуле:
det
(x)=p
z
(f
−1
(x))⋅
det(J
f
−1
)
Умея вычислять функцию правдоподобия, мы можем обучать нашу модель
f
θ
f
θ
методом максимума правдоподобия (ММП):
log
log
log
⁡
∣
det
log p
x
(D;θ)=
i=1
∑
M
log p
z
(f
θ
−1
(x))+log
det(J
f
−1
)
.
где
D={x
(i)
}
i=1
M
– выборка наблюдаемых данных из распределения
p
x
p
x
.
Обычно модель нормализующего потока составляет композицию из
K
K не очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной:
f=f
1
∘f
2
∘⋯∘f
K
Тогда якобиан вычисляется по формуле:
det
det
det(J
f
−1
)=
i=1
∏
K
det(J
f
i
−1
)
Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение
f
f, подсчет якобиана которого был бы эффективен!
В отличе от VAE и GANов, нормализующие потоки требуют вычисления функции правдоподобия, воэтому важно уметь эффективно вычислять функцию правдоподобия. Метод максимального правдоподобия позволяет обучать нормализующие потоки стабильнее в сравнении с GAN-ами, а возможность быстро и точно вычислять значение функции правдоподобия выделяет нормализующие потоки на фоне VAE и диффузионных моделей.
Примером такого отображения является планарный поток (Planar Flow), где отображение
f
f принадлежит следующему семейству функций:
x=f
θ
(z)=z+u
θ
h(w
θ
⊤
z+b
θ
),
где
– обучаемые параметры, а
h
h – гладкая нелинейная функция, например,
tanh
tanh.
Якобиан такого отображения можно будет посчитать за
O
(
n
)
O(n). Обозначим
ψ(z)=h
′
(w
θ
⊤
z+b
θ
)w
θ
.
Тогда
∣
det
det
det(J
f
)
=
det(I+u
θ
ψ(z)
⊤
)
=
(1+u
θ
⊤
ψ(z))
.
planar
Действие планарного нормализующего потока на нормальное и равномерное распределение (ссылка на статью).
Развитие идеи
В планарных потоках нам удалось быстро посчитать якобиан, потому что матрица имела специальный вид (сумма единичной и низкоранговой). Но мы знаем и другие случаи, когда определитель можно посчитать быстрее – треугольные матрицы. Их определитель равен произведению элементов на диагонали.
Следующие модели активно использовали этот трюк.
NICE: Non-linear Independent Component Estimation и RealNVP
Авторы модели NICE предложили использовать в качестве
f
θ
f
θ
следующее семейство преобразований:
x=f
θ
(z)={
x
1:d
=z
1:d
x
d+1:n
=z
d+1:n
+m
θ
(z
1:d
)
,
где
1
<
d
<
n
1<d<n, а
m
θ
m
θ
– произвольная нейросеть с
d
d входами и
n
−
d
n−d выходами. Такое преобразование называют аддитивным связыванием (additive coupling).
Обратное преобразование вычисляется с такой же легкостью, а якобиан равен
1
1. То есть,
(x)=p
z
(f
−1
(x)), что является довольно сильным ограничением модели.
Далее, из-за того, что
1:d
=z
1:d
,
первые
d
d каналов вектора
x
x совпадают с координатами нормального шума
z
z, то есть моделирования этих каналов
x
x не происходит. Из-за этого выразительная способность модели NICE была относительно невысокой.
Позже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов
x
x, что стало основой работы RealNVP. Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием
(z); при этом градиент перестановки вычисляется легко.
exp
x=f
θ
(z)={
x
1:d
=z
1:d
x
d+1:n
=exp(s
θ
(z
1:d
))⊙z
d+1:n
+m
θ
(z
1:d
)
,
где
⊙
⊙ – поэлементное умножение, а
s
θ
s
θ
– нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и
m
θ
m
θ
. Такое преобразование называют аффинным связыванием (affine coupling).
Получившееся отображение тоже легко обращается, а его якобиан равен:
det
exp
det(J
f
−1
)=exp
i=d+1
∑
n
(s
θ
(z
1:d
))
i
Заметим, что, как и в случае аддитивного связывания, значительная часть каналов остается неизменной при использовании аффинного связывания. Для того, чтобы преобразование
(x) моделировало распределение
x
x во всех каналах, на разных слоях неизменными оставляют разные подмножества из
d
d каналов.
Чтобы улучшить сходимость глубоких (
K
>
1
K>1) нормализующих потоков, авторы предложили использовать Batch Normalization. Данное преобразование тоже является обратимым, а его якобиан вычисляется крайне просто.
В результате, выразительная способность модели сильно повысилась, и она стала способна выучивать сложные распределения:
realnvp
Masked Autoregressive Flows
Ссылка на статью
Данный вид нормализующих потоков также обладает нижнетреугольным якобианом, но он использует другое семейство функций:
x
i
=
z
i
exp
exp(f
α
i
(x
1:i−1
))+f
μ
i
(x
1:i−1
),
где
1:i−1
) и
1:i−1
) – нейросети произвольной архитектуры.
Как видно из формулы,
x
i
x
i
напрямую зависит от
1:i−1
. Таким образом, элементы генерируются авторегрессивно, что и дало название архитектуре.
Якобиан такого преобразования вычисляется по следующей формуле:
det
exp
det(J
f
−1
)=exp(−
i=1
1:i−1
))
Таким образом, шаг генерации выглядит следующим образом:
z∼N(0,I)
x
1
=
z
1
exp
exp(α
1
)+μ
exp
exp(f
α
2
(x
1
))+f
μ
2
(x
1
)
...
Однако вычисление скрытых переменных
z
z не является авторегрессивным:
exp
=(x
i
−f
μ
i
(x
1:i−1
))exp(−f
α
i
(x
1:i−1
))
Несмотря на то, что данная разновидность нормализующих потоков кажется более мощной моделью, её трудно применить на практике к данным высокой размерности. Это происходит из-за того, что генерация нового объекта осуществляется авторегрессивно по координатам, что становится слишком затратным при обучении на высокоразмерных данных, например, на изображениях.
Inverse Autoregressive Flows
Ссылка на статью
Чтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные:
exp
=(x
i
−f
μ
i
(z
1:i−1
))exp(−f
α
i
(z
1:i−1
))
Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объекты
x
x быстрее:
exp
⋅exp(f
α
i
(z
1:i−1
))+f
μ
i
(z
1:i−1
)
Но вычисление
z
z, а значит и правдоподобия, становится долгим, и обучение занимает больше времени.
Звук
Нормализующие потоки стали наиболее актуальны в задаче генерации звука, поскольку они обладают достаточно высокой выразительностью и эффективностью, чтобы быстро генерировать аудиозаписи высокого качества. В этом контексте, модель нормализующего потока должна генерировать аудио, получая на вход описание того, что ей необходимо сгенерировать. То есть модель обуславливается на дополнительные признаки.
Нормализующие потоки могут быть обусловлены на входные данные путем использования дополнительных входных данных в качестве переменной, от которой зависят преобразования, применяемые к данным. Обусловливающей переменной может быть любая дополнительная информация, имеющая отношение к задаче генерации, такая как текстовые описания, изображения или другие характеристики данных.
В контексте генерации аудио обуславливающей переменной обычно служит mel-спектрограмма, которая позволяет отобразить интенсивность различных частот аудио-сигнала в разные моменты времени.
melspec
Нормализующий поток учится генерировать сигнал в виде waveform-а на основе спектрограммы путем обратного преобразования.
waveform
Чтобы генерировать более длинные фрагменты звука, модель генерирует короткие звуковые кадры (фреймы) за раз, которые затем объединяются для формирования полного waveform-а.
Artboard
Теперь мы готовы узнать про применение нормализующих потоков в генерации аудио!
Probability Density Distillation и Parallel WaveNet
Ссылка на статью
Архитектура Inverse Autoregressive Flow (IAF) была изначально предложена для задачи генерации аудио. Она позволяет генерировать объекты крайне эффективно, но обучение методом максимального правдоподобия занимает много времени из-за авторегрессивности вычислений. Метод Probability Density Estimation позволяет решить эту проблему с помощью использования второй предобученной авторегрессивной модели в качестве учителя. IAF обучается в качестве модели-студента, минимизируя KL дивергенцию
KL(p
S
∣∣p
T
), где
– распределения студента и учителя соответственно. Ключевым достижением данного подхода является то, что вычисление функции потерь требует вычисления кросс-энтропии между учителем и студентом, а не правдоподобия, что позволяет максимально распараллелить все вычисления ввиду отсутствия авторегрессивности в вычислениях.
Вместе с тем в данной работе в качестве учителя выбирается не случайная модель, а оригинальная авторегрессивная модель WaveNet, которая в 2016 году позволила достичь state-of-the-art качества генерации аудио. Эта модель является не нормализующим потоком, а обыкновенной авторегрессивной моделью, которая обучается предсказывать следующий кусочек аудио (фрейм) длиной в несколько миллисекунд.
wavenet
Таким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества!
Artboard
На картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примером задачи условной генерации, где на вход модели подается спектрограмма, сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе про Waveglow
Glow
Исследователи из OpenAI в 2018 году опубликовали работу Glow: Generative Flow with Invertible 1×1 Convolutions, которая значительно улучшает результаты модели RealNVP. Опишем два главых улучшения.
Во-первых, для перемешивания каналов Glow использует обратимые свертки с ядром 1x1 вместо фиксированной матрицы перестановок каналов в RealNVP;
Это нововведение является по-настоящему красивым, так как в нем предлагается способ вычисления якобиана 2D-свертки за
O
(
n
)
O(n). А именно, логарифм якобиана такой 1x1-свертки с числом каналов
n
n для тензора размера
h
×
w
×
n
h×w×n равен
h
w
⋅
log
⁡
∣
det
⁡
(
W
)
∣
hw⋅log∣det(W)∣, где
W
W – матрица свёртки 1х1.
Авторы предлагают использовать следующий вариант LU-разложения для матрицы
diag
(
s
)
)
,
W=PL(U+diag(s)),
где
P
P – фиксированная матрица перестановок,
L
L – нижнетреугольная матрица с единицами на диагонали,
U
U – верхнетреугольная матрица с нулями на диагонали, а
s
s – обучаемый вектор.
Нетрудно показать, что
log
⁡
∣
det
sum
(
log
⁡
(
s
)
)
log∣det(W)∣=sum(log(s))
Благодаря этому авторам удалось снизить сложность вычислений якобиана с
O
(
n
3
)
O(n
3
) до
O
(
n
)
O(n)
Кроме того, для улучшения сходимости использовали собственно разработанный actnorm-слой (activation normalization). Поскольку нормализующие потоки требуют много вычислительных ресурсов, для обучения используются мини-батчи маленького размера, из-за чего батч-нормализация работает не очень хорошо. Авторы предлагают использовать следующий тип нормализации – actnorm:
i,j
′
=s⊙x
i,j
+b
Нормализуем входной тензор (промежуточное изображение) по размерности каналов;
Инициализируем параметры смещения
b
b и разброса
s
s статистиками с первого батча;
Далее обучаем их в качестве обычных параметров.
Таким образом, один блок нормализующего потока выглядит так:
Artboard
WaveGlow
Ссылка на статью
Вторым важным с практической точки зрения применением нормализующих потоков стала модель WaveGlow. Она представляет собой версию модели Glow, адаптированную для генерации речи по тексту.
Как мы помним, эта задача также является примером задачи условной генерации:
log
log
log
⁡
∣
det
log p
x∣c
(D;θ)=
i=1
∑
M
log p
z∣c
(f
θ
−1
(x,c)∣c)+log
det(J
f
−1
)
.
На практике это приводит к тому, что все распределения в нашей формуле становятся условными. Таким образом, при генерации мы также сэмплируем из условного распределения
z∼p
z∣c
(z∣c), а в слоях нормализующих потоков используем преобразования
i−1
,c).
В качестве обуславливающего фактора
c
c для WaveGlow мы имеем сгенерированную по тексту mel-спектрограмму, а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В качестве модели, которая производит параметры афинного преобразования, используется похожая на WaveNet архитектура с dilated-свертками.
Правая часть схемы ниже более подробно показывает строение слоя affine coupling:
Artboard
split
(
x
)
(
log
mel-spectrogram
concat
)=split(x)
(logs,t)=WN(x
a
,mel-spectrogram)
x
b
′
=s⊙x
a
+t
f
coupling
−1
(x)w=concat(x
a
,x
b
′
)
Операция
s
p
l
i
t
split разделяет тензор
x
x пополам на два тензора меньшей размерности
для их последующего участия в слое аффинного связывания (affine coupling).
Пример генерации:
Источник
Ground truth	WaveNet	WaveGlow
Out-of-distribution detection
Может показаться, что способность точно и эффективно вычислять функцию правдоподобия может позволить без труда обнаруживать аномалии в данных, что может пригодиться во многих приложениях. Однако в работе Kirichenko et al. на примере задачи генерации изображений было показано, что нормализующие потоки выучивают отображение картинок в латентное пространство, основываясь на локальных корреляциях пикселей и графических деталях, а не на семантическом контенте. Из-за этого правдоподобие OOD-объектов может быть выше, чем правдоподобие in-distribution сэмплов.
flows
Однако позже было предложено использовать ряд эвристик для того, чтобы улучшить способность к детекции аномалий за счет подсчета значения функции правдоподобия:
Использовать значение правдоподобия второй модели потока, обученного на отличном от исходного датасете (например, ImageNet при исходном CelebA). А затем вычислять отношенение этих двух значений для вынесения вердикта об аномальности объекта. Schirrmeister et al.
В работе Serrà et al. показали, что проблема качества нормализующих потоков в задаче детекции аномалий связана с чрезмерным влиянием сложности входных данных на значение функции правдоподобия. Поэтому авторы предложили использовать в качестве поправки размер сжатого изображения с помощью одного из алгоритмов компрессии (JPEG2000/PNG).
flows
Сравнение с другими типами генеративных моделей
Обратимся к статье Bond-Taylor et al., в которой приводится количественный анализ всех существующих семейств генеративных моделей в задаче генерации изображений из датасета CIFAR-10.
nf
В таблице выше указано, насколько представители каждого из популярных семейств генеративных моделей эффективны в следующих аспектах решения задачи:
скорость обучения;
скорость генерации;
число обучаемых параметров;
разрешение генерируемого изображения;
ограничение на форму якобиана;
возможность вычислять правдоподобие объекта;
FID (Fréchet Inception Distance) тестовой выборки;
Отрицательный логарифм правдоподобия тестовой выборки.
За расшифровкой обратимся к таблице ниже:
nf
Подведя итог, можно сказать, что нормализующие потоки:
требуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления;
имеют скорость генерации, сравнимую с GAN-ами;
менее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели;
позволяют быстро вычислять точное значение функции правдоподобия объекта;
обладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям.
Итак, нормализующие потоки явно выделяются среди других семейств генеративных моделей своими свойствами – обратимостью и способностью вычислять правдоподобие объекта. Но если для решения задачи они не требуются, то имеет смысл попробовать другие модели – в первую очередь, GAN-ы и диффузионные модели.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
8.3. Генеративно-состязательные сети (GAN)
Следующий параграф
8.5. Диффузионные модели
