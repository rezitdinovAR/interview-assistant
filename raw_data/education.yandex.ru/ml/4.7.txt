---
title: Модели с латентными переменными
url: https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.7
---
Предположим, что мы делаем анализ данных для банка, и нам предоставили данные о годовых зарплатах клиентов.
ml
В этом графике заметны три моды, которым, наверное, соответствуют три кластера клиентов. Неопытный аналитик мог бы проигнорировать это и попытаться описать график в отчете для руководства с помощью двух чисел — средней зарплаты и стандартного отклонения зарплат.
Однако данные с гистограммы ниже имеют точно такое же среднее и стандартное отклонение, как и мультимодальные данные выше. Распределение совсем другое, правда?
Очевидно, что графики выглядят совершенно по-разному, и правильная интерпретация первого графика может принести бизнесу дополнительные деньги (скажем, если банк научится предлагать клиентам из каждого кластера более кастомизированные предложения). Так что не надо пытаться описывать мультимодальные данные с помощью унимодальных распределений.
ml
Проблема заключается в том, что нам неизвестно, к какому кластеру относится каждый клиент, и неизвестны характеристики кластеров — как же их тогда описать?
Для каждого кластера можно попытаться задать свои параметры (среднее и дисперсию). Но как определить, из какого кластера конкретный клиент в выборке? Более того, один клиент может, например, «на 0.7» относиться к одному кластеру и «на 0.3» к другому.
ml
Как решать такую задачу «мягкой» кластеризации («мягкой», потому что один объект может относиться к нескольким кластерам)? Мы могли бы действовать итерационно. Сначала зададим начальное приближение на параметры распределений.
Например, в нашем случае с клиентами банка из графика можно предположить, что среднее для первой кластера 30, для второго — 40, для третьей — 50, а стандартные отклонения у всех равняются, скажем, 10. Зная эти начальные параметры, мы можем для каждого клиента посчитать степень принадлежности к каждому из трёх кластеров (важно не забыть отнормировать эти числа, чтобы их сумма действительно равнялась единице).
Дальше мы бы могли пересчитать наши средние и дисперсии, «взвешивая» вклад объектов пропорционально степени их принадлежности к каждому кластеру, и таким образом уточнить средние и дисперсии для всех трёх кластеров. Повторяя эти два шага последовательно, мы получили бы средние и дисперсии кластеров, а для каждого объекта — степени принадлежности к кластерам. Это — EM-алгоритм, подробнее о нём мы поговорим ниже.
Кстати, оказывается, что и метод кластеризации K-средних во многом сродни EM-алгоритму (и на самом деле представляет собой его предельный случай). Действительно, мы сначала случайно расставляем центры кластеров.
Затем мы для каждого объекта пересчитываем расстояние до центра каждого кластера, после чего получаем «вес» объекта в каждом кластере («вес» в том смысле, что чем ближе объект к центру кластера, тем больше этот объект учитывается при пересчете центра этого кластера) через нормировку расстояний.
Теперь, если мы применяем настоящий метод K-средних, то приписываем объект к кластеру с самым большим «весом», после чего опять пересчитываем центры кластеров и потом опять измеряем вес для каждого объекта кластера. Строгое же применение EM-алгоритма даёт «мягкую» версию метода K-средних.
Смеси распределений
Говорят, что распределение
p
(
x
)
p(x) является смесью распределений, если его плотность имеет вид
p(x)=
k=1
(x),
k=1
∑
K
π
k
=1,π
k
≥0,
где:
K
K — число компонент;
π
k
π
k
— вероятности компонент;
(x) — функции правдоподобия, то есть функции вероятности компонент (в дискретном случае) или их плотности (в абсолютно непрерывном случае).
Проиллюстрируем это понятие на примере с банком. Будем считать, что распределения компонент смеси принадлежат некоторому параметрическому семейству:
(x)=ϕ(x∣θ
k
) (например, гауссовскому с параметром
=(μ
k
,σ
k
)).
Мы можем говорить, что каждое из распределений
(x) задаёт свой кластер, причём каждый кластер имеет некоторую априорную вероятность
π
k
π
k
. Если у нас нет дополнительных данных, разумно положить
Если же нам, к примеру, известно, что какой-нибудь кластер описывает сравнительно малочисленную группу людей, эти вероятности окажутся различными. Таким образом, мы проинтерпретировали нашу мягкую кластеризацию в терминах смеси распределений.
Как генерировать из смеси распределений
Рассмотрим следующий эксперимент: сначала из дискретного распределения
,…,π
K
} выбирается номер
k
k, а затем из распределения
ϕ(x∣θ
k
) выбирается значение
x
x. Покажем, что распределение переменной
x
x будет представлять собой смесь.
Введем скрытую переменную
z
z, отвечающую за то, к какой компоненте смеси будет относиться очередной
x
x. Пусть она представляет собой
K
K-мерный бинарный случайный вектор, ровно одна компонента которого равна единице:
z∈{0,1}
K
,
k=1
∑
K
z
k
=1.
Вероятность того, что единице будет равна
k
k-я компонента, положим равной
p(z
k
=1)=π
k
.
Запишем распределение сразу всего вектора:
p(z)=
k=1
Теперь, когда номер компоненты смеси известен, сгенерируем
x
x из распределения
ϕ(x∣θ
p(x∣z
k
=1)=ϕ(x∣θ
k
),
или, что то же самое,
p(x∣z)=
k=1
∏
K
[ϕ(x∣θ
k
)]
z
k
.
Проверим, что
x
x имеет нужное нам распределение. Запишем совместное распределение переменных
x
x и
p(x,z)=p(z)p(x∣z)=
k=1
∏
K
[π
k
ϕ(x∣θ
k
)]
z
k
.
Чтобы найти распределение переменной
x
x, нужно избавиться от скрытой переменной:
p(x)=
z
∑
p(x,z).
Суммирование здесь ведется по всем возможным значениям
z
z, то есть по всем
K
K-мерным бинарным векторам с одной единицей:
p(x)=
z
∑
p(x,z)=
k=1
∑
K
π
k
ϕ(x∣θ
k
).
Мы получили, что распределение сгенерированной переменной
x
x в описанном эксперименте представляет собой смесь
K
K компонент.
Модели со скрытыми переменными
Рассмотрим вероятностную модель с наблюдаемыми переменными
X
X и параметрами
Θ
Θ, для которой задано правдоподобие
log
logp(X∣Θ).
Предположим, что в модели также существуют скрытые переменные
Z
Z, описывающие её внутреннее состояние и, возможно, недоступные для непосредственного наблюдения (как то, к какому из кластеров относится клиент). Тогда правдоподобие
log
logp(X∣Θ) называется неполным, а правдоподобие
log
logp(X,Z∣Θ) — полным. Они связаны соотношением
log
log
logp(X∣Θ)=log{
Z
∑
p(X,Z∣Θ)}.
Нашей основной целью будет создать хорошую модель
X
X, то есть оценить параметры
Θ
Θ. И оказывается, что с помощью введения скрытых переменных нередко удаётся существенно упростить правдоподобие и эффективно решить задачу.
Рассмотрим пример со смесями распределений. В качестве наблюдаемых переменных здесь выступает выборка
X={x
1
,…,x
ℓ
}, в качестве скрытых переменных
Z={z
1
,…,z
ℓ
} — номера компонент, из которых сгенерированы объекты (здесь каждый из
K-мерный вектор), в качестве параметров — априорные вероятности и параметры компонент
Θ=(π
1
,…,π
K
,θ
1
,…,θ
K
).
Неполное правдоподобие выглядит так:
log
log
logp(X∣Θ)=
i=1
∑
l
log{
k=1
∑
K
π
k
p(x
i
∣θ
k
)}.
Правдоподобие здесь имеет вид логарифма суммы. Если приравнять нулю его градиент, то получатся сложные уравнения, не имеющие аналитического решения. Данное правдоподобие сложно вычислять: оно не является выпуклым (а точнее, вогнутым) и может иметь много локальных экстремумов, поэтому применение обычных итерационных методов для его непосредственной максимизации приводит к медленной сходимости.
Рассмотрим теперь полное правдоподобие:
log
log
⁡
π
k
+
log
logp(X,Z∣Θ)=
i=1
∑
l
p(x
i
,z
i
∣Θ)=
i=1
∑
l
k=1
∑
K
z
ik
{logπ
k
+logϕ(x
i
∣θ
k
)}.
Оно имеет вид суммы логарифмов, и это позволяет аналитически найти оценки максимального правдоподобия на параметры
Θ
Θ при известных переменных
X
X и
Z
Z. В общем случае
Z
Z также стараются выбирать таким способом, чтобы распределение
p(X,Z∣Θ) оказалось «лучше» исходного. В каком именно смысле, мы увидим дальше.
Проблема, впрочем, заключается в том, что нам не известны скрытые переменные
Z
Z, поэтому их необходимо оценивать одновременно с параметрами, что никак не легче максимизации неполного правдоподобия. Осуществить это позволяет EM-алгоритм.
EM-алгоритм
EM-алгоритм решает задачу максимизации полного правдоподобия путём попеременной оптимизации по параметрам и по скрытым переменным.
Опишем сначала наивный способ оптимизации. Зафиксируем некоторое начальное приближение для параметров
Θ
old
Θ
old
. При известных наблюдаемых переменных
X
X и параметрах
Θ
old
Θ
old
мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения:
Z
∗
=
argmax⁡Z
old
)
=
argmax⁡Z
old
argmax
p(Z∣X,Θ
old
)=
Z
argmax
p(X,Z∣Θ
old
).
Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров:
Θ
new
=
argmax⁡Θ
new
=
Θ
argmax
p(X,Z
∗
∣Θ).
Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров
Θ
*
Θ
*
.
Данная процедура, однако, далека от идеальной — и ниже мы предложим решение, которое приводит к более качественным результатам.
Воспользуемся байесовским подходом. Точечные оценки параметров несут меньше информации, чем их распределение; учтём это и будем оптимизировать не
Z
Z, а условное распределение
Z
Z.
Как и прежде, зафиксируем вектор параметров
Θ
old
Θ
old
, но вместо точечной оценки вычислим апостериорное распределение на скрытых переменных
old
)
p(Z∣X,Θ
old
), которое будет в некотором смысле оптимальным образом описывать распределение
Z
Z при известных
X
X и
Θ
Θ. В этом заключается E-шаг EM-алгоритма.
Отметим, что вычислить
p(Z∣XΘ) аналитически возможно не для всех распределений, и скрытые переменные стоит подбирать так, чтобы это всё-таки получилось.
Теперь мы должны произвести оптимизацию по
Θ
Θ. Для этого возьмём логарифм полного правдоподобия
log
logp(X,Z∣Θ) и усредним его по всем возможным значениям скрытых переменных
old
old
)
log
old
)
log
Q(Θ,Θ
old
)=E
Z∼p(Z∣X,Θ
old
)
logp(X,Z∣Θ)=
Z
∑
p(Z∣X,Θ
old
)logp(X,Z∣Θ)
Формально говоря, мы нашли матожидание логарифма полного правдоподобия по апостериорному распределению на скрытых переменных.
На M-шаге новый вектор параметров находится как максимизатор данного матожидания:
Θ
new
=
argmax⁡Θ
Q
(
Θ
,
Θ
old
)
=
argmax⁡Θ
old
)
log
new
=
Θ
argmax
Q(Θ,Θ
old
)=
Θ
argmax
Z
∑
p(Z∣X,Θ
old
)logp(X,Z∣Θ).
EM-алгоритм состоит в чередовании E-шага и M-шага.
Можно показать, что такой итерационной процесс всегда не уменьшает правдоподобие и сходится.
Жёсткий EM-алгоритм
Не всегда получается подобрать латентные переменные
Z
Z так, чтобы
p(Z∣X,Θ) можно было выразить аналитически, то есть на E-шаге не удаётся минимизировать
L(q,Θ) по
q
q.
В такой ситуации иногда приходится брать оптимум не по всему пространству распределений, а только по некоторому семейству — например, параметрическому, в котором оптимизацию можно проводить градиентными методами. В максимально упрощённой ситуации мы возьмём семейство дельта-функций, то есть вместо распределения на
Z
Z будем брать просто точечную оценку. Такая модификация EM-алгоритма называется жёстким EM-алгоритмом.
В начале параграфа мы упоминали кластеризацию методом K-средних и отмечали, что EM-алгоритм даёт «мягкую» версию алгоритма: на E-шаге мы не приписываем однозначно точку к какому-то из кластеров (то есть не берём точечную оценку скрытой переменной «номер кластера, к которому принадлежит точка»), а сопоставляем ей вероятности принадлежности каждому из кластеров (то есть распределение на скрытые переменные). Настоящий метод K-средних как раз таки соответствует жёсткому EM-алгоритму.
Разделение смеси гауссиан
Пусть теперь нам известно, что
N
N точек были семплированы из K разных гауссовских распределений и нам неизвестно, какая точка из какого распределения пришла в выборку. Нам нужно оценить параметры (
) для первого распределения, (
) для второго и, соответственно, (
) для
k
k-го распределения.
Если мы знаем, что точка
x
i
x
i
пришла из распределения
z
i
z
i
, то её правдоподобие в равно:
exp
p(x
i
∣z
i
,θ)=
2π
σ
z
i
1
exp(−
Напомним, что
z
i
z
i
— это латентная переменная, обозначающая номер гауссианы (от 1 до
K
K), из которой была просемплирована точка
x
i
x
i
. Например, если точка
x
i
x
i
просемплирована из распределения с номером 3, то в формулу вместо (
) нужно подставлять
Воспользуемся EM-алгоритмом для нахождения параметров (
). Сначала инициализируем их:
Θ
old
=
μ
1
_
old
,
σ
1
_
old
,
μ
2
_
old
,
σ
2
_
old
old
,
σ
K
_
old
Θ
old
=μ
1_old
,σ
1_old
,μ
2_old
,σ
2_old
,…,μ
K_old
,σ
K_old
Зная
Θ
_
old
Θ_old, выполним E-шаг: нужно найти
p(Z∣X,Θ) или что то же самое, для каждого объекта
x
i
x
i
найти распределение на вероятности
old
)
p(z
i
=k∣x
i
,Θ_old).
Как найти апостериорную вероятность
old
)
p(z
i
=k∣x
i
,Θ_old), если мы знаем
x
i
x
i
и у нас есть приближение
Θ
_
old
Θ_old?
Ответ — по формуле Байеса:
old
old
old
old
old
p(z
i
=k∣x
i
,Θ
old
)=
p(x
i
∣Θ
old
)
p(x
i
∣z
i
=k,Θ
old
)⋅p(z
i
=k)
=
∑
k=1
K
p(x
i
∣z
i
=k,Θ
old
)⋅p(z
i
=k)
p(x
i
∣z
i
=k,Θ
old
)⋅p(z
i
=k)
где
p(z
i
=k) — априорная вероятность того, что объекта
x
i
x
i
получен из распределения с номером
k
k. На первом шаге априорную вероятность можно положить равной
p(z
i
=k)=
K
1
для всех гауссиан.
Введём обозначение
old
old
exp
old
)
2
2
σ
k
old
2
)
u
ik
:=p(x
i
∣z
i
=k,Θ
old
)=
(2π)
σ
k
old
1
exp(−
2σ
k
old
2
(x
i
−μ
k
old
)
2
)
— правдоподобие того, что объект
x
i
x
i
пришел из нормального распределения с параметрами
(
μ
k
old
,
σ
k
old
2
)
(μ
k
old
,σ
k
old
2
).
Тогда по формуле Байеса:
old
p(z
i
=k∣x
i
,θ
old
)=
∑
k=1
Вот так для каждого объекта
x
i
x
i
по начальному приближению
θ
old
θ
old
мы посчитаем распределение
p
(
z
i
)
p(z
i
) — с какими вероятностями объект
x
i
x
i
приходит из той или иной компоненты смеси.
Теперь выведем формулы для М-шага.
θ
=
argmax⁡Θ
E
q
(
Z
)
log
argmax⁡Θ
log
argmax
E
q(Z)
logp(X∣Z,Θ)=
Θ
argmax
i=1
∑
N
k=1
∑
K
p(z
i
=k∣x
i
,Θ)⋅logp(x
i
∣z
i
=k,Θ)
log
log
i=1
∑
N
k=1
∑
K
p(z
i
=k∣x
i
,Θ)⋅(log
logσ
k
2
)+const
Запишем производную и приравняем к
0
0, чтобы найти экстремум:
log
q(Z)
logp(X∣Z,Θ)
=−
i=1
∑
N
p(z
i
=k∣x
i
,Θ)⋅
Отсюда
i=1
N
p(z
i
=k∣x
i
,θ)
∑
i=1
N
p(z
i
=k∣x
i
,θ)⋅x
i
Мы получили конечную формулу для пересчета
и предыдущему значению
θ
θ. Причем у этой формулы есть простая интерпретация — каждый объект мы взвешиваем с его вероятностью принадлежности к этому классу
p(z
i
=k∣x,θ).
Теперь посчитаем производную по
(обратите внимание, что именно по квадрату
log
q(Z)
logp(X∣Z,Θ)
=
i=1
∑
N
p(z
i
=k∣x
i
,θ)⋅(
)=0
Стало быть,
i=1
N
p(z
i
=k∣x
i
,Θ)
∑
i=1
N
p(z
i
=k∣x
i
,Θ)(x
i
−μ
k
)
2
Мы снова получили интерпретируемый результат: подсчитывая дисперсию для
k
k-ой гауссианы, мы учитываем вес каждого объекта при подсчете среднеквадратичноого отклонения. То есть веса — вероятности происхождения из той или иной компоненты смеси. Сравните эту формулу с формулой для подсчета выборочной дисперсии, где каждый из
N
N объектов вносит одинаковый вклад в дисперсию с весом
−μ)
2
Вы можете «пощупать» EM-алгоритм в задаче разделения вероятностной смеси с помощью интерактивной визуализации — попробуйте сделать E и M шаги и последить за изменениями параметров: после одной итераций алгоритма можно выбрать точку на графике и наблюдать за вероятностью её принадлежности к разным кластерам.
Вероятностный PCA
Теперь давайте рассмотрим простой пример того, как введение латентных переменных может помочь выделять новые информативные признаки в данных.
Предположим, что мы имеем выборку данных
x
i
x
i
(вектор-строку), где каждый объект имеет
D
D признаков (предположим, что число
D
D очень большое). Это достаточно типичная ситуация, например, при работе с текстами или изображениями.
Теперь введём следующую вероятностную модель
+ε, где
z
i
z
i
— латентный вектор-строка меньшей размерности
T
T, а
ε∼N(0,σ
2
E), где
E
E — единичная матрица размером
D
×
D
D×D,
σ
σ — скаляр больший 0.
Что означает эта модель? Она означает, что наши сложные многоразмерные данные
x
i
x
i
могут иметь более простое малоразмерное представление
z
i
z
i
, а отображение
линейно с точностью до нормально распределенного шума.
Заметим, что так как
ε∼N(0,σ
2
E), отсюда следует, что
∼N(z
i
W
T
,σ
2
E). Зададим априорное распределение на
z
i
z
i
как стандартное нормальное
∼N(0,E) и распишем совместное распределение
) через условное и априорное:
p(x
i
,z
i
∣W,σ)=p(x
i
∣z
i
,W,σ)p(z
i
)
Чтобы восстановить параметры
W
W,
σ
σ и латентные переменные
z
i
z
i
, снова воспользуемся EM-алгоритмом.
На E-шаге мы оцениваем распределение на
z
i
z
i
при фиксированных
W
W и
σ
σ:
По формуле Байеса распределение на
z
i
z
i
при условии
p(z
i
∣x
i
,W,σ)=
p(x
i
∣W,σ)
p(x
i
∣z
i
,W,σ)⋅p(z
i
)
С точностью до констант и слагаемых, которые не зависят от
z
i
z
i
, логарифм правдоподобия равен:
log
logp(z
i
∣x
i
,W,σ)∼−
)(x
log
logp(z
i
∣x
i
,W,σ)∼−
log
logp(z
i
∣x
i
,W,σ)∼−
W+σ
2
E)z
Обозначим
M:=W
T
W+σ
2
E, тогда
log
logp(z
i
∣x
i
,W,σ)∼−
⋅(σ
log
logp(z
i
∣x
i
,W,σ)∼−
)⋅(σ
2
M)
−1
⋅(z
Если теперь взять от этого экспоненту, увидим, что
p(z
i
∣x
i
,W,σ)∼N(x
i
WM
−1
,σ
2
M).
M-шаг.
Теперь мы оптимизировать по
W
W и
log
log
min
⁡
W
,
σ
E
p(Z∣X,W,σ)
logp(X,Z∣W,σ)=
i
∑
n
E
p(z
i
∣x
i
,W,σ)
logp(x
i
,z
i
∣W,σ)→
W,σ
min
Приравняв производные к
0
0, можно найти:
W
new
new
=(
i
∑
n
(Ez
i
)x
i
T
)⋅(
i
∑
n
E(z
new
new
new
T
W
new
new
[∣∣x
i
∣∣
2
−2x
i
W
new
Ez
i
T
+tr(W
new
T
W
new
E(z
i
T
z
i
))]
Вероятностный PCA хорош тем, что:
как любая байесовская модель, может служить промежуточным участком в более сложной вероятностной модели;
если в данных есть пропуски, то вероятностный PCA легко обобщается и на этот случай с добавлением дополнительных скрытых переменных;
так как параметры
W
,
σ
W,σ и оценки на
z
i
z
i
получаются через итерационный EM-алгоритм, то вероятностный PCA может быть вычислительно эффективнее. Так, в вычислениях и промежуточных формулах нигде не используется матрица
X∈R
D×D
, и все рассматриваемые матрицы имеют меньший размер.
Связь с обычным PCA
Как вероятностный PCA связан с обычным, который мы изучили в теме про разложение матриц?
Напомним, что в обычном SVD-разложении мы полагали, что
. Давайте опять положим, что разница между
есть гауссовский шум с нулевым средним
ε∼N(0,σ
2
E):
или
=N(z
Если зададим априорное распределение на
z
i
z
i
как стандартное нормальное
p(z
i
)∼N(0,E), тогда
∼N(0,
) и соответственно
∼N(0,
E).
Теперь сделаем обратную замену
и убедимся, что оценка максимального правдоподобия в точности равна
log
log
⁡
det
logp(x
i
∣W,σ)=−
2
N
logdet(W
T
W+σ
2
E)−
W+σ
+const
(напомним, что
это вектор-строки). Заметим, что число есть след матрицы, состоящей из этого числа, поэтому можно преобразовать вторую часть, как
W+σ
tr(
W+σ
tr((W
T
W+σ
))=
tr((W
T
W+σ
2
E)
−1
⋅XX
T
)
Отсюда следует, что
log
log
⁡
det
logp(x
i
∣W,σ)=−
2
N
logdet(W
T
W+σ
2
E)−
2
1
tr((W
T
W+σ
2
E)
−1
⋅XX
T
)
Приравняв производную по
W
W к нулю, найдем:
Оценка максимума правдоподобия на
D−T
1
j=T+1
Эту оценку можно интерпретировать как среднюю потерю дисперсии по всем проигнорированным сингулярным направлениям. Если же
σ
2
σ
2
— константа, то при
σ
→
0
σ→0 получаем обычный PCA.
Другой способ получить обычный PCA — это вместо обычного EM-алгоритма воспользоваться его жёсткой модификацией.
Теперь предлагаем вам потренировать изученный материал на практике. Предлагаем вам выполнить лабораторную работу, которая покрывает большинство тем главы “Вероятностные модели”. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике!
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Выполните задачи урока
0 / 9 выполнено
Сообщить об ошибке
Предыдущий параграф
4.6. Байесовский подход к оцениванию
Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии
Следующий параграф
5.1. Нейронные сети
Краткий путеводитель по разделу
