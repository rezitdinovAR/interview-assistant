---
title: Как оценивать вероятности
url: https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.4
---
Как правильно оценить вероятности классов в задаче классификации
Мы уже упоминали, что оценивать вероятности классов как
softmax(f
w
(x
i
)) для какой-то произвольной функции
f
w
f
w
— это дело подозрительное.
В этом разделе мы поговорим о том, как это делать хорошо и правильно.
Что же такое вероятность класса, если объект либо принадлежит этому классу, либо нет?
Ограничимся пока случаем двухклассовой классификации — с классами 0 и 1. Если утверждается, что мы предсказываем корректную вероятность класса 1 (обозначим её
q
(
x
i
)
q(x
i
)), то прогноз «объект
x
i
x
i
принадлежит классу 1 с вероятностью
2
3
3
2
» должен сбываться в
2
3
3
2
случаев.
То есть, условно говоря, если мы возьмём все объекты, то среди них что-то около двух третей действительно имеет класс 1.
На математическом языке это можно сформулировать так: Если
p
^
p
— предсказанная вероятность класса 1, то
P(y
i
=1∣q(x
К сожалению, в реальной жизни
p
^
p
— это скорее всего вещественные числа, которые будут различными для различных
y
i
y
i
, и никаких вероятностей мы не посчитаем, но мы можем разбить отрезок
[
0
,
1
]
[0,1] на бины, внутри каждого из которых уже вычислить, каковая там доля объектов класса 1, и сравнить эту долю со средним значением вероятности в бине:
12
У модели, которая идеально предсказывает вероятности (как обычно говорят, у идеально калиброванной модели) жёлтые точки на диаграмме калибровки должны совпадать с розовыми.
А вот на картинке выше это не так: жёлтые точки всегда ниже розовых. Давайте поймём, что это значит. Получается, что наша модель систематически завышает предсказанную вероятность (розовые точки), и порог отсечения нам, выходит, тоже надо было бы сдвинуть вправо:
12
Но такая картинка, пожалуй, говорит о какой-то серьёзной патологии классификатора. Гораздо чаще встречаются следующие две ситуации:
Слишком уверенный (overconfident) классификатор:
12
Такое случается с сильными классификаторыми (например, нейросетями), которые учились на метки классов, а не на вероятности: тем самым процесс обучения стимулировал их всегда давать ответ, как можно более близкий к 0 или 1.
Неуверенный (underconfident) классификатор:
12
Такое может случиться, например, если мы слишком много обращаем внимания на трудные для классификации объекты на границе классов (как, скажем, в SVM), в каком-то смысле в ущерб более однозначно определяемым точкам. Этим же могут и грешить модели на основе бэггинга (например, случайный лес). Грубо говоря, среднее нескольких моделей предскажет что-то близкое к единице только если все слагаемые предскажут что-то, близкое к единице — но из-за дисперсии моделей это будет случаться реже, чем могло бы. Подробнее можно почитать в статье.
Вам скажут: логистическая регрессия корректно действительно предсказывает вероятности
Вам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера.
Рассмотрим датасет c двумя классами (ниже на картинке обучающая выборка)
12
Обучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть
L
2
L
2
-регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе справляется. Ниже изображена часть тестовой выборки вместе с предсказанными вероятностями классов для всех точек области
12
Видим, что модель не больно-то уверена в себе, и ясно почему: признаковое описание достаточно бедное и не позволяет нам хорошо разделить классы, хотя, казалось бы, это можно довольно неплохо сделать.
Попробуем поправить дело, добавив полиномиальные фичи, то есть все
для
0⩽j,k⩽5 в качестве признаков, и обучив поверх этих данных логистическую регрессию. Снова нарисуем некоторые точки тестовой выборки и предсказания вероятностей для всех точек области:
12
Видим, что у нас сочетание двух проблем: неуверенности посередине и очень уверенных ошибок по краям.
Нарисуем теперь калибровочные кривые для обеих моделей:
12
Калибровочные кривые весьма примечательны; в любом случае ясно, что с предсказанием вероятностей всё довольно плохо. Посмотрим ещё, какие вероятности наши классификаторы чаще приписывают объектам:
12
Как и следовало ожидать, предсказания слабого классификатора тяготеют к серединке (та самая неуверенность), а среди предсказаний переобученного очень много крайне уверенных — и совсем не всегда правильных.
Но почему же все твердят, что логистическая регрессия хорошо калибрована?!
Попробуем понять и простить её.
Как мы помним, логистическая регрессия учится путём минимизации функционала
log
log
l(X,y)=−
i=1
∑
N
(y
i
log(σ(⟨w,x
i
⟩))+(1−y
i
)log(1−σ(⟨w,x
i
⟩)))
Отметим между делом, что каждое слагаемое — это кроссэнтропия распределения
P
P, заданного вероятностями
P(0)=1−σ(⟨w,x
i
⟩) и
P(1)=σ(⟨w,x
i
⟩), и тривиального распределения, которое равно
y
i
y
i
с вероятностью
1
1.
Допустим, что мы обучили по всему универсуму данных
X
X идеальную логистическую регрессию с идеальными весами
w
∗
w
∗
. Пусть, далее, оказалось, что у нас есть
n
n объектов
,…,x
n
с одинаковым признаковым описанием (то есть по сути представленных одинаковыми векторами
x
i
x
i
), но, возможно, разными истинными метками классов
,…,y
n
. Тогда соответствующий им кусок функции потерь имеет вид
log
log
i=1
∑
n
y
i
)log(σ(⟨w,x
1
⟩))−(
i=1
∑
n
(1−y
i
))log(1−σ(⟨w,x
1
⟩))=
log
log
=−n(
2
1
p
0
log(σ(⟨w,x
1
⟩))+p
1
log(1−σ(⟨w,x
1
⟩)))
где
p
j
p
j
— частота
j
j-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда
σ(⟨w,x
1
⟩)=p
0
,1−σ(⟨w,x
1
⟩)=p
1
Результат, полученный для
n
n совпадающих точек будет приблизительно верным и для
n
n достаточно близких точек в случае, когда:
признаковое описание данных достаточно хорошее — классы не перемешаны как попало и всё-таки близки к разделимым;
модель не переобученная — то есть, предсказания вероятностей не скачут очень уж резко — вспомните второй пример.
На всех этих точках модель будет выдавать примерно долю положительных, то есть тоже хорошую оценку вероятности.
Как же всё-таки предсказать вероятности: методы калибровки
Пусть наша модель (бинарной классификации) для каждого объекта
x
i
x
i
выдаёт некоторое число
q(x
i
)∈[0,1]. Как же эти числа превратить в корректные вероятности?
Гистограммная калибровка. Мы разбиваем отрезок
[
0
,
1
]
[0,1] на бины
,…,B
k
(одинаковой ширины или равномощные) и хотим на каждом из них предсказывать всегда одну и ту же вероятность:
θ
j
θ
j
, если
q(x
i
)∈B
j
. Вероятности
θ
i
θ
i
подбираются так, чтобы они как можно лучше приближали средние метки классов на соответствующих бинах. Иными словами, мы решаем задачу
min
j=1
i=1
N
I{q(x
i
)∈B
,…,θ
k
)
min
Вместо разности модулей можно рассматривать и разность квадратов.
Метод довольно простой и понятный, но требует подбора числа бинов и предсказывает лишь дискретное множество вероятностей.
Изотоническая регрессия. Этот метод похож на предыдущий, только мы будем, во-первых, настраивать и границы
0=b
0
,b
1
,…,b
k
=1 бинов
={t∣b
j−1
⩽b
j
}, а кроме того, накладываем условие
⩽…⩽θ
k
. Искать
мы будем, приближая
y
i
y
i
кусочно постоянной функцией
g
g от
q
(
x
i
)
q(x
min
⁡
g
i=1
∑
N
(y
i
−g(q(x
i
)))
2
⟶
g
min
12
Минимизация осуществляется при помощи pool adjacent violators algorithm, и эти страницы слишком хрупки, чтобы выдержать его формулировку.
Калибровка Платта представляет собой по сути применение сигмоиды поверх другой модели (то есть самый наивный способ получения «вероятностей»). Более точно, если
q
(
x
i
)
q(x
i
) — предсказанная вероятность, то мы полагаем
P(y
i
=1∣x
i
)=σ(aq(x
i
)+b)=
1+e
−aq(x
i
)−b
1
где
a
a и
b
b подбираются методом максимального правдоподобия на отложенной выборке:
log
log
min
⁡
a
,
b
−
i=1
log(σ(q(x
i
)))+(1−y
i
)log(1−σ(q(x
i
))))⟶
a,b
min
Для избежания переобучения Платт предлагал также заменить метки
(1−y
i
) на регуляризованные вероятности таргетов:
#{i∣y
i
=0}+2
1
,t
1
=
#{i∣y
i
=0}+2
#{i∣y
i
=1}+1
Калибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из истинных классов предсказанные вероятности
q
(
x
i
)
q(x
i
) распределы нормально с одинаковыми дисперсиями. Подробнее об этом вы можете почитать в этой статье. Там же описано обобщение данного подхода — бета-калибровка.
С большим количеством других методов калибровки вы можете познакомиться в этой статье
Как измерить качество калибровки
Калибровочные кривые хорошо показывают, что есть проблемы, но как оценить наши усилия по улучшению предсказания вероятностей? Хочется иметь какую-то численную метрику. Мы упомянем две разновидности — прямое воплощение описанных выше идей.
Expected/Maximum calibration error. Самый простой способ, впрочем — он наследник идеи с калибровочной кривой. А именно, разобьём отрезок
[
0
,
1
]
[0,1] на бины
,…,B
k
по предсказанным вероятностям и вычислим
j=1
или
max
j=1,…,k
max
где
) — среднее значение
y
i
y
i
, а
) — среднее значение
q
(
x
i
)
q(x
i
) для
x
i
x
i
, таких что
q(x
i
)∈B
j
. Проблема этого способа в том, что мы можем очень по-разному предсказывать в каждом из бинов вероятности (в том числе константой) без ущерба для метрики.
Brier score. Одна из популярных метрик, которая попросту измеряет разницу между предсказанными вероятностями и
i=1
∑
N
(y
i
−q(x
i
))
2
Казалось бы, в чём смысл? Немного подрастить мотивацию помогает следующий пример. Допустим, наши таргеты совершенно случайны, то есть
P(y
i
=1∣x
i
)=P(y
i
). Тогда хорошо калиброванный классификатор должен для каждого
x
i
x
i
предсказывать вероятность
1
2
2
1
; соответственно, его brier score равен
1
4
4
1
. Если же классификатор хоть в одной точке выдаёт вероятность
, то в маленькой окрестности он должен выдавать примерно такие же вероятности.
Поскольку же таргет случаен, локальный кусочек суммы из brier score будет иметь вид
(1−p)
2
<
2
N
′
, что хуже, чем получил бы всегда выдающий
1
2
2
1
классификатор.
Не обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss:
log
log
i=1
logq(x
i
)+(1−y
i
)log(1−q(x
i
)))
Это же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета brier score и log-loss будут адекватными средствами оценки, но если нет — возможно всякое.
Вопрос на засыпку: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности?
Мы не будем про это рассказывать, но призываем читателя подумать над этим самостоятельно или, например, посмотреть туториал с ECML KDD 2020.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
4.3. Обобщённые линейные модели
Как прокачать линейную модель с помощью распределений из экспоненциального класса
Следующий параграф
4.5. Генеративный подход к классификации
Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес
