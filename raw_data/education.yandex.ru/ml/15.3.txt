---
title: Регуляризация в онлайн-обучении
url: https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii
course: ml
chapter: 15. Онлайн-обучение и стохастическая оптимизация
chapter_id: 15.3
---
В этом параграфе мы поговорим о регуляризации, но использовать мы её будем не для стабилизации обучения, а для того, чтобы накладывать ограничение на получаемое нами решение. Чтобы отличать их от стабилизирующих слагаемых, для таких регуляризаторов будем использовать обозначение
(w)
В теории от регуляризатора требуется только выпуклость, но на практике широко используются лишь три вида:
=∣∣w∣∣
1
и его собрат
L
1
/
2
L
1/2
=∣∣w∣∣
2
2
;
Проекция на выпуклое множество
ψ(w)=I
χ
(w)={
∞
0
w

∈χ
w∈χ
Классическим способом введения регуляризации является прибавление к оптимизируемому функционалу:
(w)=f
t
(w)+ψ
t
(w)
с последующим применением любых методов оптимизации «из коробки». Яркий пример —
L
2
L
2
регуляризация:
(w)=f
t
(w)+
2λ
2
1
∣∣w∣∣
2
2
,
которая не портит гладкости функционала.
Идея неразложения регуляризаторов в субградиентную оценку
Вспомним вывод linearized FTRL. В ходе линеаризации мы заменяли все функции
(w) на их субградиентную оценку в точке
w
t
w
t
. Для регуляризованного функционала
(w)=f
t
(w)+ψ
t
(w) получалась бы такая оценка:
(w)≥
f
^
t
(w
t
)+(g
t
+∂ψ
t
)
T
(w−w
t
),
где через
∂
ψ
t
∂ψ
t
мы обозначили для краткости субградиент
ψ
t
ψ
t
в точке
w
t
w
t
. Теперь субградиентную оценку можно подставить в метод FTRL:
min
t+1
=arg
w
min
[(g
1:t
+∂ψ
1:t
)
T
w+
s=1
∑
t
∣∣w−w
Идея неразложения состоит в следующем: заменим на субградиентную оценку только
(w), а регуляризатор будем подбирать так, чтобы задача FTRL решалась аналитически. Интуитивно, оценка
(w)=f
t
(w)+ψ
t
(w)≥f
t
(w
t
)+g
t
T
(w−w
t
)+ψ
t
(w)
должна быть точнее оценки
(w)=f
t
(w)+ψ
t
(w)≥f
t
(w
t
)+ψ
t
(w
t
)+(g
t
+∂ψ
t
)
T
(w−w
t
)
а значит, и метод оптимизации будет точнее и эффективнее.
Эта идея очень важна для построения регуляризованных алгоритмов онлайн-обучения.
Давайте выпишем, как будут выглядеть с учётом этой идеи регуляризованные алгоритмы.
Composite Objective FTRL
min
t+1
=arg
w
min
[g
1:t
T
w+ψ
1:t
(w)+
s=1
∑
t
∣∣w−w
Online Mirror Descent, Proximal Gradient Descent, (F)ISTA
min
t+1
=arg
w
min
[g
t
T
w+ψ
t
(w)+∣∣w−w
Напомним, что три названия в заголовке соответствуют трём способам восприятия этой формулы:
Online Mirror Descent — метод онлайн-обучения;
Proximal Gradient Descent — метод (стохастической) батч-оптимизации. В стохастическом случае он неотличим от Mirror Descent;
(F)ISTA — по сути, это название аналитического решения указанного уравнения для
L
1
L
1
-регуляризации.
Связь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления
В этом подразделе мы будем проводить рассуждения на примере
L
1
L
1
-регуляритора. для других регуляризаторов выкладки будут аналогичными.
Выпишем Proximal (он же Mirror) Gradient Descent с
L
1
L
1
-регуляризацией:
min
t+1
=arg
w
min
g
t
T
w+λ
1
∣∣w∣∣
1
+
2η
t
1
∣∣w−w
t
∣∣
2
2
Необходимым условием минимума явняется равенство нулю градиента (а в данном случае субградиента) всего выражения:
0=g
t+1
−w
t
)
где
- субградиент регуляризатора
∣∣w∣∣
1
в точке
w
t
w
t
. Отсюда получаем
t+1
Если же переписать формулы в духе FTRL, мы получим
min
t+1
=arg
w
min
g
1:t
T
w+
g
^
1:t−1
T
w+λ∣∣w∣∣
1
+
2
1
s=0
∑
t
∣∣w−w
s
∣∣
σ
s
2
Получился метод, который оптимизирует
L
1
L
1
-регуляризатор в явном виде только на текущей итерации
t
t, а для остальных использует субоптимальные субградиентные оценки. Заметим, что тем же выражением можно ограничить сверху и функционал:
min
t+1
=arg
w
min
g
1:t
T
w+tλ∣∣w∣∣
1
+
2
1
s=0
∑
t
∣∣w−w
s
∣∣
σ
s
2
Мы получили метод FTRL с incremental
L
1
L
1
— более сильным и стабильным вариантом регуляризации, чем Mirror Descent. Подробнее его анализом мы займемся в параграфе про продвинутую
L
1
L
1
-регуляризацию.
L
1
L
1
-регуляризация
Отбор параметров разреженных моделей
Предположим, что мы хотим обучить модель минимального размера и при этом как можно лучшего качества. В этом нам поможет отбор параметров. А именно, давайте постараемся оставить только те из них, которые оказывают наиболее влияние на лосс
1:T
(w).
Определение. Будем называть параметр
w
i
w
i
разреженным, если он не используется (пропускается) при предсказании некоторых
(w). «Некоторых» может означать как десятую часть, так и
0.99999
0.99999 прогнозов
(w), главное — что такие объекты просто есть. Частым мы будем называть параметр, у которого частота пропусков низкая (например,
10
%
10% пропусков), а редким — тот, у которого она высокая (второй случай).
Пример. Рассмотрим модель разреженной линейной регрессии
(w)=(w
. Обычно она применяется в ситуациях, когда элементы вектора признаков
x
t
,
i
x
t,i
— это
0
0 или
1
1 (например, «встретилось ли
i
i-е слово в
t
t-м документе»), причем на практике доля единиц обычно бывает очень маленькой. Поэтому существенная часть параметров
w
i
w
i
при прогнозе на шаге
t
t будет умножаться на нули и, таким образом, не будет использоваться.
Обратите внимание: как правило, в литературе по онлайн-обучению говорят о разреженных параметрах, а не признаках. Впрочем, подавляющее большинство моделей на разреженных признаках устроены так, что каждому такому признаку сопоставляется некий набор параметров, поэтому определения «разреженный признак» и «разреженные параметры» взаимозаменяемы. В линейной модели, как в примере выше, каждому признаку
x
i
x
i
сопоставляется параметр
w
i
w
i
. В более сложных моделях признаку
x
i
x
i
может сопоставляться вектор параметров
w
i
w
i
— эмбеддинг этого признака.
Давайте теперь поймём, что означает фраза «признак влияет на лосс
1:T
(w)». Оказывать влияние можно двумя способами:
Качеством. Если параметр
w
i
w
i
редкий, но очень хорошо прогнозирует свой небольшой набор объектов, его стоит оставить. За счет того, что мы оставим достаточное количество таких параметров, мы можем покрыть большое число объектов. Такие параметры называются memorization parameters (они как будто запоминают «свои» объекты).
Количеством. Если параметр
w
i
w
i
часто встречается, то он в любом случае должен остаться в модели и помогать с суммарным качеством прогноза.
Убирать мы хотим только слабые и редкие параметры. Таких, как правило, больше
99
%
99%.
Обратите внимание: мы не хотим убирать слабые, но часто встречающиеся параметры. Тому есть две причины:
Места они много не занимают, а количества данных в large scale задачах достаточно, чтобы правильно выучить эти параметры. Они будут вносить свой, пусть и небольшой, вклад в общее качество;
Частые параметры хорошо запоминают среднее поведение на всех данных, а разреженные — поведение на конкретных объектах. Если наша цель — оставить как можно меньше параметров, то выгоднее хорошо выучить среднее поведение на всех данных, а отклонения от среднего запомнить с помощью memorization parameters. Если в модели есть только супер-разреженные параметры, то из-за огромной вариативности в их возможных комбинациях в данных каждому параметру придется доучивать среднее поведение. Подробнее на этой проблеме мы остановимся в конце параграфа.
Инициализация разреженных параметров
В обучении разреженных моделей все параметры, на которые накладывается
L
1
L
1
-регуляризация, инициализируются нулями. С точки зрения здравого смысла такая инициализация довольно естественна, однако есть и более формальное обоснование;
Если параметры инициализируются нулями, то мы по мере обучения смотрим на градиенты этих параметров и в зависимости от градиентов принимаем решение, нужен нам параметр для прогноза или не нужен. Все параметры стартуют в равных условиях, и модель понемногу выходит из состояния «абсолютная разреженность», выучивая что-то содержательное.
Если же параметры инициализируются случайно, то нам надо сначала доучить все параметры до какого-то более или менее разумного значения, а потом уже пытаться понять, нужен ли он нам. Момент, когда модель начинает эффективно разреживаться, тем самым очень сильно отдалается.
Composite-objective FTRL с
L
1
L
1
-регуляризацией
Напомним формулировку задачи:
min
t+1
=arg
w
min
g
1:t
T
w+λ
1,t
∣∣w∣∣
1
+
2
1
s=1
∑
T
∣∣w−w
s
∣∣
σ
s
2
Решение можно выписать в явном виде. Для этого введём следующие обозначения:
z
t
z
t
будет аккумулировать сумму градиентов,
=0,
n
t
n
t
будет аккумулировать сумму поэлементных квадратов градиентов,
=0,
α
α — это learning rate.
Следующие формулы выписаны отдельно для каждой координаты. В них
i
i — индекс параметра модели,
t
t — номер итерации.
t,i
=
η
t,i
1
−
η
t−1,i
t,i
+g
t,i
2
−
n
t,i
t+1,i
=z
t,i
+g
t,i
−σ
t,i
w
t,i
t+1,i
=n
t,i
+g
t,i
t+1,i
={
0
−
n
t+1,i
+αλ
2,t
α
(z
t+1,i
−sgn(z
t+1,i
)λ
1,t
)
∣z
t+1,i
∣≤λ
1,t
∣z
t+1,i
∣>λ
1,t
(∗)
Вывод этих формул хорошо расписан в конспекте курса Д. А. Кропотова.
Анализ аналитического решения
При регуляризаторе
∣∣w∣∣
1
в оптимизируемом функционале стоят коэффициенты
λ
1
,
t
λ
1,t
, которые могут как-то зависеть от
t
t. Обычно рассматривают три вида зависимости:
Fixed:
1,t
=λ.
Squared incremental:
1,t
=
t
λ
Linear incremental:
1,t
=tλ
Их также можно комбинировать, получая коэффициенты регуляризации
1,t
=λ
1,global
+
t
λ
1,sqrt
+tλ
1,incremental
Напомним, что все веса
w
i
w
i
мы инициализируем нулями. По формулам
(
∗
)
(∗) из нуля на шаге
t
t выводятся веса
w
i
w
i
, для которых
1:t,i
−∑σ
s
w
s,i
∣>λ
1,t
.
Таким образом, начальное условие выхода параметров из нуля имеет вид
1:t,i
∣>λ
1,t
.
Попробуем понять физический смысл этого неравенства.
Напоминание. Говорят, что функция
f
(
w
)
f(w) имеет липшиц-непрерывный градиент с константой
L
L, если
∣∣∇f(x)−∇f(y)∣∣
2
2
≤
2
L
∣∣x−y∣∣
2
2
Предположим, что это выполняется (ниже мы покажем, что это не слишком обременительное ограничение). Тогда, подставив в качестве
y
y точку оптимума функции
(w) (не путайте с глобальным
из regret!), мы получим
∣∣∇f(x)∣∣
2
2
≤
2
L
∣∣x−x
∗
∣∣
2
2
Это означает, что для достаточно хорошей функции норма градиента является оценкой снизу на расстояние до точки оптимума в пространстве параметров. Чем больше норма градиента, тем дальше мы от оптимальных параметров
w
w.
Вернемся к выражению
1:t,i
∣>λ
1,t
. Здесь мы имеем дело (а) отдельно с каждой из координат и (б) с нормой суммы градиентов (а не с суммой норм). Хорошая новость: утверждение выше верно и для функций одной переменной, то есть
s,i
∣, грубо говоря, показывает, насколько мы далеки от оптимума по
i
i-й координате. Знак
g
s
,
i
g
s,i
говорит о том, в какую сторону мы будем сдвигаться по
i
i-й координате
w
w на
s
s-м шаге. Если сдвиги были в основном в одну сторону, то
1:t,i
будет больше, а если они всё время в разную сторону, то отдельные слагаемые могут скомпенсировать друг друга, и
1:t,i
может быть малым.
Отметим ещё, что абсолютная величина компоненты
1:t,i
на первых итерациях может отражать прогнозирующую силу параметра
w
i
w
i
: в самом деле, неверное значение важного для предсказания параметра может вести к большим ошибкам, что будет давать большие градиенты.
Посмотрим теперь, как будет вести себя разреженная модель в зависимости от вида
λ
1
,
t
λ
1,t
.
Linear incremental (
1,t
=tλ
1
)
Условие выхода
w
i
w
i
из нуля принимает вид
1:t,i
∣>tλ
1
,
что равносильно
1:t,i
>λ
1
Ограничение на среднее значение компоненты градиента означает, что для выхода из нуля параметр
w
i
w
i
должен иметь определённую прогнозирующую силу. Это противоречит нашему требованию о том, чтобы частые маломощные параметры все равно присутствовали в модели и выучивали среднее поведение.
Обратите внимание. Выше мы показали, что проксимальный градиентный спуск с обычным
min
t+1
=arg
w
min
g
t
T
w+λ
1
∣∣w∣∣
1
+
η
t
1
∣∣w−w
t
∣∣
2
2
в некотором смысле эквивалентен Composite-Objective FTRL с инкрементальным
L
1
L
1
. Таким образом, обычная
L
1
L
1
-регуляризация в классическом градиентном спуске эквивалентна именно инкрементальному
L
1
L
1
, который, как мы выяснили, субоптимален. Ниже мы рассмотрим специфический для FTRL вариант
L
1
L
1
-регуляризации, который лишен этих недостатков.
Фиксированный (
1,t
=tλ
1
)
Это самый мощный и полезный на практике режим.
Здесь мы не нормируем на
1
t
t
1
(то есть не берём среднее), и это означает, что выйти из нуля может и слабый, но частый параметр, который за много итераций накопит достаточно большую сумму частных производных.
Свойства фильтрации с фиксированным регуляризатором в точности совпадают с продуктовыми требованиями:
Редкий параметр с мощной прогнозирующей силой на старте будет иметь большие по модулю градиенты одного знака, и он выйдет из нуля;
Редкий параметр с малой прогнозной силой не выйдет из нуля;
Частые параметры в любом случае выйдут из нуля.
Squared incremental: (
1,t
=
t
λ)
В этой статье было теоретически обосновано, что если параметр частый, но нерелевантный и абсолютно шумный, то дисперсия
1:t
∣ будет иметь асимптотику
). Из этого следует, что, если сделать регуляризацию порядка
t
t
, мы лишим такой случайный шум почти любых шансов выйти из нуля.
К сожалению, ни в игрушечных примерах вроде Avazu, ни в продакшен задачах улучшений качества прогноза или степени разреживания модели без потери качества достичь не удалось. Возможно, вам повезет больше.
Полезность частых параметров для разреживания модели
Рассмотрим две линейных модели
(w)=w
T
x
t
+b,g
t
(w)=w
T
x
t
,
в которых все параметры
w
i
w
i
разреженные. Давайте считать, что в первой модели есть константный (и совсем даже не разреженный) признак
=1, которому и соответствует параметр
b
b.
Теперь в каждой из моделей наложим на
w
w регуляризацию
L
1
L
1
и сравним, что получится:
В модели
f
t
f
t
параметрам
w
i
w
i
нужно запомнить «отклонение» от среднего
b
b;
В модели
g
t
g
t
параметрам
w
i
w
i
нужно запомнить абсолютное значение предсказания.
Нетрудно понятно, что при наличии bias нормы градиентов в первой модели в среднем будут намного меньше, потому что мы на каждом шаге оптимизации будем стартовать с точки, которая в среднем ближе к точке оптимума (bias и есть наше среднее). Поэтому меньше весов смогут преодолеть порог по модулю суммы градиентов и выйти из нуля. Таким образом, несмотря на одинаковый оптимум без регуляризации, при введении
L
1
L
1
-регуляризации модель с bias будет обладать более хорошим соотношением разреженность/качество прогноза.
Эта логика легко обобщается на более сложные случаи, когда вместо bias у нас есть неразреженные контентные признаки. Вывод такой: модели, в которых есть только очень разреженные параметры, обладают гораздо худшим соотношением разреженность/качество, чем модели, в которых есть и контентные, и разреженные параметры.
Убедиться в этих эффектах мы сможем в разделе с практикой на линейных моделях.
L
2
L
2
регуляризация
Weight decay
Рассмотрим обыкновенный SGD.
t+1
=w
t
−αg
t
Weight decay состоит во введение штрафа на размер текущих весов:
t+1
=(1−λ)w
t
−αg
t
,0≤λ<1
Внимательные читатели уже заметили, что в случае с SGD это эквивалентно введению
L
2
L
2
-регуляризации. Давайте разберёмся, как это сделать правильно.
Decoupled weight decay
Попробуем заменить
(w) на
(w)=f
t
(w)+λ
2
∣∣w∣∣
2
2
и запустить любой адаптивный метод, например, AdaGrad. Если мы беспечно заменим на градиентную оценку всю функцию
(w) (забыв, что с регуляризатором этого делать не стоит), то алгоритм примет вид
t+1
где
s=1
В этих формулах нехороши две вещи:
Коэффициенты
α
α и
λ
2
λ
2
нетривиальным образом взаимодействуют. Это крайне неудобно при переборе гиперпараметров: изменение learning rate
α
α должно влечь за собой переподбор коэффициента регуляризации
λ
2
λ
2
по полной сетке;
В квадратах градиентов мы хотим видеть только адаптивность к кривизне самой функции
f
t
f
t
, но теперь там ещё добавка
Эта проблема была впервые замечена в Decoupled weight decay regularization. Авторы также рассматривали влияние на momentum, к этому мы вернёмся в параграфе про AdamW.
Авторы статьи предлагают модифицировать метод AdaGrad следующим образом:
t+1
=w
t
−
s=1
Сразу отметим сходство с исходными формулами weight decay — его и добивались авторы.
Decoupled weight decay — это адаптивный
L
2
L
2
Легко видеть, что формула
t+1
=w
t
−
s=1
описывает обыкновенный покоординатный градиентный спуск с некоторым линеаризованным
L
2
L
2
-регуляризатором. Давайте «проинтегрируем» это выражение обратно до аргминимума, из которого бы получились такие формулы обновления весов:
min
t+1
=arg
w
min
∣∣w−w
t
∣∣
2
2
]
Получается, что decoupled weight decay — это адаптивный
L
2
L
2
-centered регуляризатор. Его можно усовершенствовать, вспомним наше важное правило не заменять регуляризатор на субградиентную оценку. Перейдём к задаче
min
t+1
=arg
w
min
∣∣w∣∣
∣∣w−w
t
∣∣
2
2
]
Она отличается от предыдущей заменой
w на
w=∣∣w∣∣
2
. Её решение имеет вид
t+1
=
1+λ
Поскольку мы меньше огрубляем оптимизируемый функционал, обучение может стать немного стабильнее.
Обратите внимание, что в оптимизационной задаче у нас теперь стоит не просто
λ
2
λ
2
, а
Decoupled
L
2
L
2
-регуляризация в Composite-Objective FTRL
Теперь посмотрим, как decoupled weight decay будте работать с Composite-Objective FTRL. Линеаризованная задача имеет вид:
min
t+1
=arg
w
min
[g
1:t
T
w+
2
λ
2
∣∣w∣∣
σ
1:s
2
+
2
1
s=1
∑
t
∣∣w−w
Перепишем её:
min
t+1
=arg
w
min
[g
1:t
T
w+
2
1
s=1
∑
t
(∣∣w−w
∣∣w∣∣
σ
s
2
)]
Нетрудно показать, что решение имеет вид
t+1
t+1
=−
1+λ
Для
z
t
z
t
можно написать и явную формулу:
1:t
−∑
s=1
Замечание. Чтобы оценить Regret такого метода, мы не сможем механически воспользоваться оценкой для AdaGrad: ведь она базированась на оценке на Regret, выведенной либо для целиком Proximal, либо для целиком Centered
L
2
L
2
-регуляризаторов. Composite objective из теоремы 10 тут не годится, так как Centered регуляризатор в этом случае не поедет в оценку норм градиентов, а мы в текущем представлении рассматриваем Proximal и Centered как равноправные члены. Интуитивно, мы должны применить Lemma 7 к обоим регуляризаторам и получить точно такую же оценку с такой же двойственной нормой (напомним, что centered и proximal регуляризаторы имеют одинаковую двойственную норму). Двойственная норма такая же -> формулы оптимального метода AdaGrad будут такие же. Мы оставляем это читателям в качестве упражнения.
(w): проекция на выпуклое множество
χ
χ
Напоминание: множество
χ
χ называется выпуклым, если
∀x,y∈χ, ∀α∈[0;1]:αx+(1−α)y∈χ
Проекцией на это множество называют функцию
(w)={
∞
0
w

∈χ
w∈χ
Докажем, что
(w) — выпуклый регуляризатор. Для этого нам нужно проверить неравенство
αI(x)+(1−α)I(y)≥I(αx+(1−α)y).
Единственный шанс, когда это может быть нарушено — это
I(αx+(1−α)y)=∞,
I(x)=0,
I(y)=0. Это значит, что
x
,
y
∈
χ
x,y∈χ, а
αx+(1−α)y∈
/
χ, что противоречит выпуклости
χ
χ.
Вернемся к формулам FTRL. Здесь ситуация сильно проще — от накидывания любых последовательностей
α
1
:
T
α
1:T
на регуляризатор ничего не изменится, так что его всегда оставляют просто as is
min
t+1
=arg
w
min
g
1:t
T
w+I
χ
(w)+
2
1
s=1
∑
T
∣∣w−w
s
∣∣
σ
s
2
Аналитические решения для каждого вида
χ
χ нужно искать отдельно. Примерно все решения получаются путем выноса
(w) из оптимизируемого функционала и превращения его в ограничение, после чего можно применить метод множителей Лагранжа.
Проекция на шар
x:∣∣x∣∣≤c
Решим аналитически задачу проекции на шар
min
1:t
T
w+
2
1
s=1
∑
T
∣∣w−w
s
∣∣
σ
s
2
⟶min
w
,
∣∣w∣∣
2
≤c.
Функция Лагранжа будет иметь вид
L(w,λ)=g
1:t
T
w+
2
1
s=1
∑
T
∣∣w−w
s
∣∣
σ
s
2
+λ(∣∣w∣∣
2
−c),
а её градиент равен
L(w,λ)=g
1:t
T
+
s=1
∑
T
(w−w
s
)⊙σ
s
+λ
∣∣w∣∣
2
w
,
где
σ
s
σ
s
- вектор, а
⊙
⊙ — поэлементное умножение векторов. Приравнивая к нулю градиент, получаем
1:s
⊙w+λ
∣∣w∣∣
2
w
=0,
где мы, как обычно, обозначили
1:t
−∑
s=1
Проанализируем условие дополняющей нежесткости
λ(∣∣w∣∣−c)=0. Если
λ
=
0
λ=0, то решение
w
w уже находится внутри шара и имеет вид
1:s
−z
t
При практической реализации мы просто сначала посчитаем это выражение и проверим, не попадаем ли мы в шар. Если попадаем — отлично, если нет — то дальше говорим, что
∣∣w∣∣=c и решаем продолжаем решение
1:s
∗w+λ
1:s
+
c
λ
−z
t
Теперь подставим это в
∣∣w∣∣=c и получим
∣∣w∣∣=
σ
1:s
+
c
λ
∣∣z
λ=∣∣z
t
∣∣−σ
1:s
w=c
∣∣z
t
∣∣
−z
t
Получаем, что если мы находимся внутри шара, то мы действуем согласно обыкновенному adaptive алгоритму со всеми хорошими свойствами, иначе — проекция побеждает.
Аналогично
L
1
L
1
регуляризации, здесь тоже есть различия между lazy и greedy представлением этого регуляризатора. Однако, в классических DL задачах эти методы встречаются не слишком часто и здесь сложно привести какой-нибудь значимый успех, который мог бы улучшить качество в важной задача. Навскидку мы можем вспомнить разве что Adversatial White-Box learning, в котором можно было бы это попробовать.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
15.2. Адаптивный FTRL
Следующий параграф
15.4. Методы оптимизации в Deep Learning
