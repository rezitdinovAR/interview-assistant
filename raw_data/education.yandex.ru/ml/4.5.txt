---
title: Генеративный подход к классификации
url: https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii
course: ml
chapter: 4. Вероятностные модели
chapter_id: 4.5
---
Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес
Классификационные модели, которые мы рассматривали в предыдущих параграфах, нацелены непосредственно на оценку
P(Y∣X). Такие модели называются дискриминативными.
К ним относится, например, логистическая регрессия: она предлагает оценку
(y=1∣x)=σ(w
T
ˆ
x). В процессе обучения дискриминативные модели подбирают разделяющую поверхность (гиперплоскость в случае логистической регрессии). Новые объекты дискриминативная модель классифицирует в зависимости от того, по какую сторону от разделяющей поверхности они лежат.
Например, обучившись на изображениях домашних кошек (y=0) и рысей (y=1), дискриминативная модель будет определять, новое изображение больше похоже на кошку или на рысь. При этом, если на вход такой модели дать изображение собаки (объект класса, которого не было в обучении, выброс), дискриминативная модель заведомо не сможет обнаружить, что это и не кошка, и не рысь, и отнесёт такой объект к одному из «знакомых» ей классов.
В этом параграфе мы поговорим о другой группе моделей, которые нацелены на оценку
P(X,Y)=P(X∣Y)P(Y). Такая модель описала бы, как обычно выглядят кошки, как они могут выглядеть, а каких кошек точно не бывает. Так же она описала бы и рысей. Она также определила бы по обучающим данным, насколько изображения кошек встречаются чаще, чем изображения рысей, т.е. оценила бы
P
(
Y
)
P(Y).
Генеративный и дискриминативный подходы к обучению
Если модель позволила точно оценить распределение
P(X∣Y), с её помощью можно генерировать объекты из этого условного распределения, в нашем примере — изображения кошек и рысей соответственно.
А вместе распределение
P(X,Y) дало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире. Поэтому модели, оценивающие
P(X,Y), называют генеративными. Ещё одно достоинство генеративных моделей — их способность находить выбросы в данных: объект
x
x можно считать выбросом, если
P(x∣y) мало для каждого класса
y
y.
Заметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один — то есть никакие метки классов не доступны. Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае
P(X∣Y)=P(X), мы сможем для нового аудио
x
x определить, похоже ли оно на аудиозапись человеческой речи (значение
P
(
x
)
P(x) велико), или это что-то другое: синтезированная речь, посторонний шум и т.п. (значение
P
(
x
)
P(x) мало).
Если мы знаем, что «выбросы», с которыми модели предстоит сталкиваться, — это, как правило, синтезированная речь, то, мы можем дополнить датасет вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов.
Чтобы использовать генеративную модель для классификации, необходимо выразить
P(Y∣X) через
P(X∣Y) и
P
(
Y
)
P(Y). Сделать это позволяет формула Байеса:
P(y∣x)=
y
′
∈Y
∑
P(y
′
)P(x∣y
′
)
P(x,y)
=
y
′
∈Y
∑
P(y
′
)P(x∣y
′
)
P(y)P(x∣y)
Классификация в генеративных моделях осуществляется с помощью байесовского классификатора:
a
(
x
)
=
arg
⁡
max
arg
⁡
max
arg
⁡
max
a(x)=arg
y∈Y
max
P(y∣x)=arg
y∈Y
max
y
′
∈Y
∑
P(y
′
)P(x∣y
′
)
P(y)P(x∣y)
=arg
y∈Y
max
P(y)P(x∣y)
Оценить
P
(
Y
)
P(Y), как правило, несложно. Для этого используют частотные оценки, полученные в обучающей выборке:
Выражение (1)
(Y=y)=
N
#(Y=y)
Отметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о
P
(
y
)
P(y). Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст.
Таргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение.
Но как смоделировать распределение
P(X,Y)? Пространство всех возможных функций распределения
P(X,Y) бесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкой
P(X,Y) на это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах.
Gaussian discriminant analysis
Модель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого класса
y
y подчиняется многомерному нормальному закону со средним
μ
y
μ
y
и ковариационной матрицей
exp
p(x∣y)=
(2π)
n/2
∣Σ
y
∣
1/2
1
exp(−
2
1
(x−μ
(x−μ
y
))
Тогда функция правдоподобия
L(P(Y),μ,Σ)=
i=1
∏
N
p(x
)P(y
i
)
достигает максимума при
i=1
i=1
i=1
i=1
)(x
(Y), представленной выше см. выражение
(
1
)
(1).
Рассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классы
выполняется
P(y
i
∣x)=P(y
j
∣x)⇔
p(x∣y
i
)P(y
i
)=p(x∣y
j
)P(y
j
)⇔
log
log
log
log
logp(x∣y
i
)+logP(y
i
)−logp(x∣y
j
)−logP(y
j
)=0⇔
Выражение (2)
log
log
log
log
(x−μ
(x−μ
y
i
)−log(2π)
n/2
∣Σ
y
i
∣
1/2
+logP(y
i
)+
2
1
(x−μ
(x−μ
y
j
)+log(2π)
n/2
∣Σ
y
j
∣
1/2
−logP(y
j
)=0
Поскольку левая часть уравнения (2) квадратична по
x
x, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён на рис.
13
Плотность классов и разделяющая поверхность в многоклассовой модели LDA см. рисунок.
13
Linear Discriminant Analysis
В выражении (2) член второго порядка
)x зануляется при
. Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу
Σ
Σ, разделяющая поверхность между любыми двумя классами будет линейной (см. рисунок). Поэтому такая модель называется линейным дискриминантным анализом (LDA).
На этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы:
i=1
)(x
Заметим, что в модели GDA для каждого класса требовалось оценить порядка
d
2
d
2
параметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядка
d
d параметров (значение
P
(
y
)
P(y) и элементы вектора
μ
y
μ
y
), и ещё
d
2
d
2
общих для всех классов параметров (элементы матрицы
Σ
Σ).
Таким образом, основное преимущество модели LDA перед GDA — её меньшая склонность к переобучению, недостаток — линейная разделяющая поверхность.
Метод наивного байеса
Предположим, что признаки
X
X объектов каждого класса
y
y — независимые случайные величины:
∀y∈Y∀U,V:U⊔V={1,...d},∀x
u
⊂R
∣U∣
,x
v
⊂R
∣V∣
P(X
∣Y=y)=P(X
U
∈x
u
∣Y=y)P(X
V
∈x
u
∣Y=y).
В таком случае говорят, что величины
X
X условно независимы относительно
Y
Y. Тогда справедливо
Выражение (3)
P(X∣Y)=P(X
1
,X
2
,...,X
d
∣Y)=P(X
1
∣Y)P(X
2
,...,X
d
∣Y)=...=P(X
1
∣Y)P(X
2
∣Y)...P(X
d
∣Y)
То есть для того, чтобы оценить плотность многомерного распределения
P(X∣Y) достаточно оценить плотности одномерных распределений
P(X
i
∣Y), см. рисунок.
13
На рисунке приведён пример условно независимых относительно
Y
Y случайных величин
. Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей.
Рассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории:
Y
=
0
Y=0 — отрицательный отзыв, клиент остался не доволен, и
Y
=
1
Y=1 — положительный отзыв. Пусть признак
X
w
X
w
равен 1, если слово
w
w присутствует в отзыве, и 0 иначе. Тогда условие выражения
(
3
)
(3) означает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон».
На практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия выражения
(
3
)
(3), называют наивным байесовским классификатором (Naive Bayes classifier, NB).
Обучение модели NB заключается в оценке распределений
P
(
Y
)
P(Y) и
P(X
i
∣Y). Для
P
(
Y
)
P(Y) можно использовать частотную оценку выражения
(
1
)
(1).
P(X
i
∣y) — одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения.
Оценка одномерного распределения
Пусть мы хотим оценить одномерное распределение
P
(
X
)
P(X).
Если распределение
P
(
X
)
P(X) дискретное, требуется оценить его функцию массы, то есть вероятность того, что величина
X
X примет значение
x
j
x
j
. Метод максимума правдоподобия приводит к частотной оценке:
Выражение (4)
(X=x
j
)=
N
#(X=x
j
)
Где
N
N — размер выборки, по которой оценивается распределение
X
X (количество объектов класса
y
y в случае оценки плотности класса
y
y).
При этом может оказаться, что некоторое значение
x
j
x
j
ни разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово «амбивалентно» не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использование оценки выражения
(
4
)
(4) приведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа:
(X=x
j
)=
N+mα
#(X=x
j
)+α
,
где
m
m — количество различных значений, принимаемых случайной величиной
X
X,
α
α — гиперпараметр.
Для оценки плотности
p
p абсолютно непрерывного распределения в точке
a
a можно разделить количество объектов обучающей выборки в окрестности точки
a
a на размер этой окрестности:
(a)=
2h
j
∑
1
a−h<X
j
<a+h
=
2h
j
∑
1
−h<X
j
−a<h
.
Обычно объекты, лежащие дальше от точки
a
a, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид
(a)=
−a)
,
где функция
K
h
K
h
, называемая ядром, обычно имеет носитель
(−h,h) (см. рисунок ниже). Такой способ оценки плотности называют непараметрическим.
13
Результат оценки плотности с разными ядрами. Использованы изображения из:
13
При параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид
exp
p(x)=
σ
2π
1
exp(−
2σ
2
(x−μ)
2
)
Таким образом, чтобы оценить плотность
p
(
x
)
p(x), достаточно оценить параметры
μ
,
σ
μ,σ. Метод максимума правдоподобия в этом случае даст такие оценки:
— выборочное среднее,
j=1
— выборочное стандартное отклонение.
Если в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс
y
y описать нормальным распределением со средним
μ
p
μ
p
и диагональной ковариационной матрицей, значения на диагонали которой обозначим
σ
p
σ
p
.
Таким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна модели GDA с дополнительным ограничением на диагональность ковариационных матриц.
Наивный байесовский подход и логистическая регрессия
Предположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом
=σ.
Посмотрим, как будет выглядеть
P(Y∣X) в этом случае. По теореме Байеса имеем
P(Y=1∣X)=
P(Y=1)P(X∣Y=1)+P(Y=0)P(X∣Y=0)
P(Y=1)P(X∣Y=1)
Разделим числитель и знаменатель полученного выражения на числитель:
exp
P(Y=1∣X)=
1+
P(Y=1)P(X∣Y=1)
P(Y=0)P(X∣Y=0)
1
=
1+exp(ln
P(Y=1)P(X∣Y=1)
P(Y=0)P(X∣Y=0)
)
1
Из условной независимости
X
i
X
i
относительно
Y
Y получаем
Формула (5)
exp
P(Y=1∣X)=
1+exp(ln
P(Y=1)
P(Y=0)
+
i=1
∑
d
ln
P(X
i
∣Y=1)
P(X
i
∣Y=0)
)
1
Перепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения
exp
exp
i=1
∑
d
ln
P(X
i
∣Y=1)
P(X
i
∣Y=0)
=
i=1
∑
d
ln
2πσ
i
2
1
exp(
2σ
i
2
−(X
i
−μ
1,i
)
2
)
2πσ
i
2
1
exp(
2σ
i
2
−(X
i
−μ
0,i
i=1
1,i
)
2
−(X
i
−μ
0,i
)
2
=
i=1
0,i
−μ
1,i
1,i
2
−μ
0,i
2
)
Подставляя это выражение в формулу (5) , получаем
exp
P(Y=1∣X)=
1+exp(ln
P(Y=1)
P(Y=0)
+
i=1
0,i
−μ
1,i
1,i
2
−μ
0,i
2
))
1
Таким образом,
P(Y=1∣X) представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии:
Формула (6)
exp
P(Y=1∣X)=
1+exp(w
0
+
i=1
где в случае GNB
=ln
P(Y=0)
P(Y=1)
+
i=1
1,i
2
−μ
0,i
0,i
−μ
1,i
i=1,…,l
Однако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение
P(X,Y), чем GNB.
Так,
X
i
X
i
могут не являться условно независимыми относительно
Y
Y, а распределения
P(X∣Y=y) могут не удовлетворять нормальному закону, но
P(y∣X) может при этом всё равно представляться в виде формулы (6) .
В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
4.4. Как оценивать вероятности
Как правильно оценить вероятности классов в задаче классификации
Следующий параграф
4.6. Байесовский подход к оцениванию
Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии
