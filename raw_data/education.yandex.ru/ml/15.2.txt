---
title: Адаптивный FTRL
url: https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl
course: ml
chapter: 15. Онлайн-обучение и стохастическая оптимизация
chapter_id: 15.2
---
В данном разделе мы рассмотрим широкое семейство алгоритмов, позволяющее делать улучшения в способах введения регуляризации, которые невозможно добиться в классическом градиентном спуске.
Полезные ссылки
Все написанное ниже (за исключением вывода AdaGrad) — сокращенный пересказ обзора H. Brendan McMahan A Survey of Algorithms and Analysis for Adaptive Online Learning. Везде, где мы обозначаем Lemma 4, Theorem 10 и т.д. — мы ссылаемся на соответствующие теоремы из этой статьи. То же самое с доказательствами: если мы что-то опускаем, подробности можно найти в обзоре
Интуитивный вывод AdaGrad взят из статьи Adaptive Subgradient Methods for Online Learning and Stochastic Optimization . Вместо оригинальных оценок на метод Regularized Dual Averaging, требующих дополнительных понятий вроде двойственности по Фенхелю, мы использовали аналогичную оценку из обзора выше, сохранив все рассуждения автора. Опять же — строгое доказательство оценок на regret для AdaGrad есть в этом обзоре.
Синтаксический сахар
В выкладках очень часто используются суммы, и без сокращенных обозначений читать их невозможно. В литературе про онлайн-обучение приняты вот такие сокращения:
0:t
(w)=
s=0
∑
t
r
s
(w);
Особо отметим обозначение
0:t
(w
t
)=
s=0
), т.е. точка
w
t
w
t
фиксирована и не меняется с индексацией в сумме;
0:t
(w)=f
1:t
(w)+r
0:t
(w) (обычно это будет сумма функции потерь и регуляризатора);
g
t
g
t
— субградиент функции
(w) в точке
w
t
w
t
.
Аддитивные регуляризаторы
В новых обозначениях описанные выше алгоритмы примут вид:
Adaptive FTRL:
min
=arg
w
min
[f
1:t
(w)+R
T
(w)]
Adaptive Linearized FTRL:
min
=arg
w
min
[∇f
1:t
(w
t
)
T
w+R
T
(w)]
Опишем условия, накладываемые нами на алгоритм. В обзоре они называются Setting 1.
Setting 1
От функций
(w) мы потребуем, чтобы они представлялись в виде:
(w)=
t=0
∑
T
r
t
(w)=r
0:T
(w)
Слагаемые должны удовлетворять следующим условиям:
Все
(w) выпуклы (вниз);
(w)≥0;
min
=arg
w
min
r
0
(w).
Также наложим следующие требования на
1:t
=f
1:t
(w)+r
0:t
(w):
Область определения
h
1
:
t
h
1:t
— непустое множество. Это требование может показаться странным, но при желании можно придумать пример
h
1
:
t
h
1:t
с пустой областью определения: достаточно взять несколько регуляризаторов-проекций
(w) на непересекающиеся выпуклые множества (подробнее о таких регуляризаторах мы расскажем в одном из следующих разделов);
Субдифференциал
(w) в точке
w
t
w
t
непуст.
Классы алгоритмов FTRL
Будем рассматривать аддитивные регуляризаторы
(w) из двух семейств в зависимости от того, где у них минимум:
FTRL-Centered:
a
r
g
min
arg
w
min
r
t
(w)=w
0
;
FTRL-Proximal:
a
r
g
min
arg
w
min
r
t
(w)=w
t
;
Composite Objective: смешение первых двух семейств.
Обратите внимание: название Proximal напрямую связано с проксимальным градиентным спуском (ссылка на учебник с проксимальными методами). В обоих случаях мы накладываем регуляризатор в текущей точке
w
t
w
t
.
Обратите внимание: для Proximal регуляризаторов зачастую требуют выполнения более сильного условия:
)=0. Это не такое уж и серьёзное ограничение: все разумные Proximal регуляризаторы (например,
∣∣w−w
t
∣∣
2
) ему удовлетворяют.
Обратите внимание: у обоих семейств есть значимые высокоцитируемые статьи
FTRL-Centered: метод Regularized Dual Averaging. Статья получила премию Test of Time Award на NeurIPS 2021, так как огромное количество последующих громких результатов (тот же AdaGrad) напрямую основывались на этих результатах. В названии Dual Averaging под dual average имеется в виду
1:t
, то есть среднее по градиентам. Кардинально других техник оценок regret там нет, обзор McMahan строго улучшает все доступные там результаты.
FTRL-Proximal: самая известная статья от гугла Ad Click Prediction. Известна она скорее потому, что там выписаны формулы и объяснено, как правильно реализовывать метод для large-scale задач с результатами применения различных дополнительных инженерных идей. Это хороший инженерный обзор, а не математическая статья.
Рассмотрим отдельно каждую из разновидностей алгоритмов
FTRL-Centered
Задача оптимизации имеет вид
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+r
0:t
(w)],
где
(w) таковы, что
a
r
g
min
arg
w
min
r
t
(w)=w
0
Пример: Рассмотрим SGD с фиксированным learning rate и стартом в точке
0
0. Положим
(w)=
2η
1
∣∣w∣∣
(w)=0,t>0
min
t+1
=arg
w
min
[g
1:t
T
w+
2η
1
∣∣w∣∣
2
2
].
Как мы уже знаем, итеративное обновление весов будет иметь вид
t+1
=w
t
−ηg
t
.
FTRL-Proximal
Задача имеет похожий вид
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+r
0:t
(w)],
(w) выбираются так, чтобы
a
r
g
min
arg
w
min
r
t
(w)=w
t
Пример: Рассмотрим SGD с убывающим learning rate:
t−1
1
Подробный вывод связи
мы приведём в одном из следующих разделов, а сейчас просто приведём результат:
(w)=σ
t
∣∣w−w
min
t+1
=arg
w
min
[g
1:t
T
w+
s=1
∑
t
σ
s
∣∣w−w
t+1
Обратите внимание: как правило, на практике Proximal методы работают лучше. Интуитивно, центрирование в недавних точках вместо
Composite-Objective FTRL
Рассмотрим смесь центрированных и проксимальных регуляризаторов:
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+ψ
0:t
(w)+r
0:t
(w)],
где
(w) и
(w) таковы, что
a
r
g
min
arg
w
min
r
t
(w)=w
t
a
r
g
min
arg
w
min
ψ
t
(w)=w
0
Пример: FTRL-Proximal с L1 и L2 регуляризацией
min
t+1
=arg
w
min
[g
1:t
T
w+λ
1,t
∣∣w∣∣
1
+λ
2,t
∣∣w∣∣
w
2
+
s=1
∑
t
σ
s
∣∣w−w
s
∣∣
2
2
]
Обратите внимание: как правило, центрированные регуляризаторы в довесок к проксимальным вводят уже не для «дополнительной стабилизации» алгоритма, а для наложения ограничений на решение
w
w.
Обратите внимание: наиболее правильные и хорошо работающие на практике способы подбора коэффициентов
λ
1
,
t
λ
1,t
2,t
мы приведём в параграфе про учет дополнительной
регуляризации.
Гарантии сходимости для алгоритмов FTRL
В этом разделе мы обсудим теоретические оценки на скорость сходимости алгоритма FTRL или, что то же самое, на скорость убывания maxRegret.
Напомним формулу:
max
maxRegret(T)=
w
∗
max
[
t=1
)−f
1:T
(w
∗
)]
Чтобы делать оценки на maxRegret, нужно пытаться оценить асимптотику ряда, каждое слагаемое которого — это решение сложной оптимизационной задачи
min
min
f
1:t
(w) с произвольными функциями
(w). Работать с такой сущностью крайне сложно. Наша основная цель — сделать верхнюю оценку на regret, в которой не будет этого члена.(???)
Strong FTRL Lemma (Lemma 4)
Пусть
(w) — последовательность произвольных (не обязательно) функций;
Пусть
(w) — последовательность выпуклых неотрицательных регуляризаторов;
Пусть также
min
t+1
=arg
w
min
h
0:t
(w) всегда определен (относительно слабые условия 1-2 требуют от нас это явно проговорить);
Тогда алгоритм, выбирающий
w
t
+
1
w
t+1
по правилу (3), удовлетворяет неравенству
Regret
T
(w
∗
)≤r
0:T
(w
∗
)+
t=1
∑
T
[h
0:t
(w
t
)−h
0:t
(w
t+1
)−r
Из чего состоит эта лемма?
Слагаемое
0:T
(w
∗
) — это суммарная регуляризация в точке
w
∗
w
∗
. Совсем избавиться от вхождения
w
∗
w
∗
не получится, но мы можем выбирать регуляризатор так, чтобы оценить сверху
0:T
(w
∗
) было не очень сложно.
Каждое слагаемое суммы
t=1
∑
T
[h
0:t
(w
t
)−h
0:t
(w
t+1
)
2
1
] отражает, насколько улучшается
t
t-й лосс
h
0
:
t
h
0:t
при замене
min
t+1
=arg
w
min
h
0:t
(w). Поведение разностей
0:t
(w
t
)−h
0:t
(w
t+1
) характеризует стабильность алгоритма. Мы ожидаем, что при больших
t
t у хорошо сходящегося алгоритма на очередном шаге
w
t
w
t
будет достаточно близок к оптимуму
w
t
+
1
w
t+1
, то есть вся сумма будет меняться всё медленнее, и её получится разумно оценить. Пример ситуации, когда это не так, мы уже видели, когда рассматривали FTL без регуляризации для линейной функции потерь (там всё было максимально нестабильно и расходилось). К счастью, введение регуляризации обычно помогает добиться стабильности.
Обе компоненты неразрывно связаны. Добавляя регуляризацию, мы увеличиваем первую компоненту, но улучшает стабильность алгоритма, чем уменьшаем вторую, и наоборот.
Обратите внимание: в условиях леммы допускаются невыпуклые
(w), и это позволяет применять её в весьма общей ситуации. Впрочем, все наши последующие выкладки все-таки будут опираться на выпуклость
(w).
Теоретические оценки на Regret (regret bounds)
Ниже мы представим теоремы 1,2 и 10 из обзора McMahan. Они дают оценки на regret в немного разных исходных предположениях и для разных типов регуляризаторов; асимптотика regret в каждом из случаев
), хотя константы будут различными. О важности констант в сходимости мы поговорим в одной из следующих параграфов, когда будем разбирать метод AdaGrad. В самом конце параграфа мы обсудим, какие оценки получаются для линеаризованного regret. А в следующем параграфе мы займёмся выводом конкретных алгоритмов FTRL для разных видов регуляризаторов.
Мы не будем полностью пересказывать обзор (если вам стало интересно, рекомендуем прочитать его самостоятельно) и докажем в качестве примера теорему 2, а для остальных приведём лишь формулировки.
Напоминание из выпуклого анализа
Определение Выпуклая функция
ψ
(
x
)
ψ(x) называется
σ
σ-сильно выпуклой по отношению к некоторой норме
∣
∣
⋅
∣
∣
∣∣⋅∣∣, если выполнено
∀g∈∂ψ(y)ψ(x)≥ψ(y)+g
T
(x−y)+
2
σ
∣∣x−y∣∣
2
Определение Двойственной нормой
∣∣⋅∣∣
∗
по отношению к норме
∣
∣
⋅
∣
∣
∣∣⋅∣∣ называется
sup
∣∣x∣∣
∗
=
y:∣∣y∣∣≤1
sup
x
T
y
Более подробно о
σ
σ-сильной выпуклости и двойственных нормах вы можете почитать, например, в книге Boyd, 2004, Convex Optimization.
Теорема 1. General FTRL Bound
Пусть
Обновление параметров происходит по правилу
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+r
0:t
(w)];
Выполнены все условия Setting 1;
Регуляризатор
(w) выбирается так, чтобы выражение
0:t
(w)+f
t+1
(w)=r
0:t
(w)+f
1:t+1
(w) было 1-сильно выпукло по отношению к некоторой норме
∣∣⋅∣∣
t
(возможно, своей на каждом шаге).
Тогда
Regret
T
(w
∗
)≤r
0:T−1
(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
(t−1),∗
2
,
где
∣∣⋅∣∣
(t−1),∗
— норма, двойственная к норме
∣∣⋅∣∣
(t−1)
.
Теорема 2. FTRL-Proximal Bound
Пусть
Обновление параметров происходит по правилу
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+r
0:t
(w)];
Выполнены все условия Setting 1;
Все регуляризаторы
(w) лежат в семействе FTRL-Proximal, причём
)=0 для всех
(w) выбирается так, чтобы выражение
0:t
(w)=r
0:t
(w)+f
1:t
(w) было 1-сильно выпукло по отношению к некоторой норме
∣∣⋅∣∣
t
(возможно, своей на каждом шаге).
Тогда
Regret
T
(w
∗
)≤r
0:T
(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
t,∗
2
,
где
∣∣⋅∣∣
t,∗
— норма, двойственная к норме
∣∣⋅∣∣
t
.
Теорема 10. Composite Objective FTRL-Proximal Bound
Пусть
Обновление параметров происходит по правилу
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[g
1:t
T
w+α
1:t
Ψ(w)+r
0:t
(w)];
Выполнены все условия Settning 1;
(w)=f
t
(w)+α
t
Ψ(w)+r
t
(w);
α
t
α
t
— неубывающая последовательность;
Ψ
(
w
)
Ψ(w) — Centered регуляризатор с минимумом в точке
(w) — Proximal регуляризаторы;
(w) выбирается так, чтобы выражение
0:t
^
(w)=r
0:t
(w)+α
1:t
Ψ(w)+f
1:t
(w) было 1-сильно выпукло по отношению к некоторой норме
∣∣⋅∣∣
t
(возможно, своей на каждом шаге).
Тогда
Если мы рассматриваем regret относительно
(w)=f
t
(w)+α
t
Ψ(w), то
Regret
T
(w
∗
)≤r
0:T
(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
t,∗
2
;
Если мы рассматриваем regret относительно
(w), то
Regret
T
(w
∗
)≤r
0:T
(w
∗
)+α
1:t
Ψ(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
t,∗
2
,
где
∣∣⋅∣∣
(t),∗
— норма, двойственная к норме
∣∣⋅∣∣
t
.
Обратите внимание. Оценки Proximal и General отличаются индексацией: до
t
t или до
t
−
1
t−1 соответственно. Это чисто техническое различие, однако именно из-за него с Proximal регуляризаторами удобнее работать как в теоретических выкладках, так и при выведении практических методов.
Обратите внимание. На
(w) мы не хотим накладывать ограничения сильной выпуклости, но сильную выпуклость функции
0:t
(w)=f
1:t
(w)+r
0:t
(w) можно обеспечить за счет выбора сильно выпуклых регуляризаторов. В самом деле, сумма выпуклой и сильно выпуклой функций сильно выпукла. Если
(w)≥f
t
(w
t
)+(w−w
(w)≥r
t
(w
t
)+(w−w
∣∣w−w
(w)+r
t
(w)≥f
t
(w
t
)+r
t
(w
t
)+(w−w
t
)
T
(∇f
t
(w
t
)+∇r
t
(w
t
))+
2
1
∣∣w−w
t
∣∣
2
.
Обратите внимание. Норма
∣∣w∣∣
t,∗
2
является сопряженной к норме, относительно которой 1-сильно выпукла функция
0:t
(w)=f
1:t
(w)+r
0:t
(w). Это значит, что норму мы будем выбирать по сумме регуляризаторов
0:t
(w), а не просто по
(w).
Доказательство на примере теоремы 2
Нам понадобится следующая чисто техническая лемма, доказательство которой мы опустим. Желающие могут прочитать Appendix B в обзоре.
Lemma 7. Пусть
ϕ
1
ϕ
1
— выпуклая функция
→R∪{∞}, для которой существует
min
=arg
x
min
ϕ
1
(x);
ψ
ψ — выпуклая функция;
(x)=ϕ
1
(x)+ψ(x) — выпуклая функция, для которой существует
min
=arg
x
min
ϕ
2
(x) и которая, кроме того, 1-сильно выпукла по норме
∣
∣
⋅
∣
∣
∣∣⋅∣∣.
Тогда, для любого элемента
b
b субдифференциала
ψ имеет место неравенство
∣∣x
1
−x
2
∣∣≤∣∣b∣∣
∗
и для любого
x
′
x
′
имеет место неравенство
)−ϕ
∣∣b∣∣
∗
.
Доказательство теоремы 2
Рассмотрим соседние раунды
t+1
. Имеем
min
min
=arg
w
min
h
0:t−1
=arg
w
min
[f
1:t−1
+r
0:t−1
]
Обозначим
(w)=f
1:t−1
(w)+r
0:t
(w)=h
0:t−1
(w)+r
t
(w)=h
0:t
(w)−f
t
(w). Поскольку
w
t
w
t
одновременно минимизирует и
(w) (т.к. это proximal регуляризатор), и
0:t−1
, имеем
min
min
=arg
w
min
[h
0:t−1
+r
t
(w)]=arg
w
min
ϕ
1
(w).
Далее,
min
min
t+1
=arg
w
min
h
0:t
=arg
w
min
[ϕ
1
(w)+f
t
(w)]
Выпишем оценку из Strong FTRL Lemma и постараемся оценить отмеченные рыжим слагаемые
t=1
)−f
1:T
(w
∗
)≤r
0:T
(w
∗
)+
t=1
∑
T
h
0:t
(w
t
)−h
0:t
(w
t+1
)−r
t
(w
t
)
Так как по условию теоремы
)=0, мы можем убрать это слагаемое:
0:t
(w
t
)−h
0:t
(w
t+1
)−r
t
(w
t
)=h
0:t
(w
t
)−h
0:t
(w
t+1
)+f
t
(w
t
)−(ϕ
1
(w
t+1
)+f
t
(w
t+1
))
Обозначим
(w)=ϕ
1
(w)+f
t
(w). Применив Лемму 7, получаем
)+f
t
(w
t
)−ϕ
1
(w
t+1
)−f
t
(w
t+1
)≤
2
1
∣∣g
t
∣∣
t,∗
2
О связи оценок на regret для обычного и линеаризованного FTRL
Вспомним, что для линеаризованного FTRL имеет место неравенство:
Regret
T
(w)≤LinearizedRegret
T
(w).
Увы, верхняя оценка на левую часть неравенства не помогает оценить правую. Поэтому рассмотрим линеаризованный алгоритм более подробно. Он работает с последовательностью функций
(w)=g
t
T
w, где
. Субдифференциал
состоит из одного вектора (градиента это функции)
(w)
=g
t
Применим приведённые выше оценки на regret для исходного и для линеаризованного алгоритма:
Regret
T
(w
∗
)≤r
0:T
(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
t,∗
LinearizedRegret
T
(w
∗
)≤r
0:T
(w
∗
)+
2
1
t=1
t,∗
2
=r
0:T
(w
∗
)+
2
1
t=1
∑
T
∣∣g
t
∣∣
t,∗
2
Легко убедиться, что оценки regret для обычного и линеаризованного FTRL совпадают и выполнено соотношение
Regret
T
(w
∗
)≤LinearizedRegret
T
(w
∗
)≤TheoremRegret
T
(w
∗
).
Таким образом, для линеаризованного варианта любого алгоритма FTRL не нужно доказывать собственные оценки. А поскольку линеаризованный FTRL намного эффективнее, в дальнейшем мы всегда будем сразу переходить от исходного алгоритма к линеаризованному.
Построение эффективного адаптивного FTRL
Теперь, когда мы получили теоретические оценки на качество работы адаптивного FTRL, настала пора рассмотреть несколько конкретных примеров алгоритмов из этого класса.
Семейство квадратичных регуляризаторов
(w)
Во всех дальнейших выкладках мы сразу ограничим себя семейством квадратичных регуляризаторов:
Для FTRL-Centered алгоритмов:
(w)=w
T
D
t
w=∣∣w∣∣
D
t
2
,
Для FTRL-Proximal алгоритмов:
(w)=(w−w
t
)
T
D
t
(w−w
t
)=∣∣w−w
где
D
t
D
t
— некоторая симметричная положительно определённая матрица (возможно, своя для каждого шага).
Помимо того, что они удобны и привычны, таки регуляризаторы позволяют достаточно просто выписывать оценки на regret. Чтобы в этом убедиться, вспомним, какие нетривиальные сущности возникают в теоремах:
на каждом шаге нам нужно выбрать норму
∣∣⋅∣∣
t
, по отношение к которой выражение
0:t
(w)+f
t+1
(w)=r
0:t
(w)+f
1:t+1
(w) было бы 1-сильно выпуклым;
во всех оценках участвует
0:T
(w
∗
) (или
0:T−1
(w
∗
)), и его хорошо бы уметь оценивать сверху;
также в оценках фигурирует норма, двойственная к
∣∣⋅∣∣
t
, и её нужно уметь выводить.
Давайте разберёмся с каждым из пунктов и поймём, почему для квадратичных регуляризаторов всё довольно хорошо.
Выбор нормы
∣∣⋅∣∣
t
Тут всё просто:
Регуляризатор
∣∣w∣∣
D
является 1-сильно выпуклым относительно нормы
∣∣w∣∣
D
(т.е. относительно себя же);
Регуляризатор
∣∣w−w
t
∣∣
D
является 1-сильно выпуклым относительно той же самой нормы
∣∣w∣∣
D
.
Нам, впрочем, нужна 1-сильная выпуклость всей суммы
0:t
(w), но легко убедиться, что
r
0
:
t
r
0:t
1-сильно выпукло относительно суммарной нормы
∣∣⋅∣∣
D
0:t
2
. Поскольку
D
0
:
t
D
0:t
— тоже симметричная положительно определенная матрица, мы остаёмся в том же классе норм Махаланобиса.
Двойственная норма
0:t
(w)
Оказывается, что
∣∣w∣∣
D,∗
=∣∣w∣∣
D
−1
Ограничение сверху для
0:t
(w
∗
)
Строго говоря, здесь никаких гарантий нет, и, например, очень плохая инициализация может всё сильно испортить. На практике, впрочем, всё работает нормально, но авторы статей не могут себе позволить надеяться на благосклонность судьбы. Поэтому в статьях часто встречается следующий костыль. Для вывода оценок на regret вводится регуляризатор
(w)=I
R
(w), где
(w)={
∞
0
∣∣w∣∣>R
∣∣w∣∣≤R
это проекция на шар. Тогда можно доказать, что
∣∣w
∗
∣∣≤R.
Семейство логарифмических регуляризаторов
Для ряда частных задач вроде expert advice problem и оптимизаций по вероятностным распределениям используется также семейство энтропийных регуляризаторов
log
⁡
w
i
r
t
(w)=
i=1
∑
N
w
i
logw
i
Более подробно о нём можно почитать в обзоре Shai-Shalev Schwartz, пример 2.5.
Constant learning rate FTRL
Простейший пример — это константный регуляризатор
(w)={
2η
1
∣∣w∣∣
2
2
, s=0,
0, s>0
Легко показать, что
∣∣w∣∣
2
2
=∣∣w∣∣
2η
1
I
2
.
Соответствующий итерационный процесс оптимизации имеет вид
min
t+1
=arg
w
min
[g
1:t
T
w+
2η
1
∣∣w∣∣
2
2
]
Как мы уже наблюдали ранее, этот метод эквивалентен методу стохастического градиентного спуска с константным learning rate. А именно, шаг обновления весов можно сформулировать двумя способами:
на языке FTRL:
t+1
=−ηg
1:t
T
;
на языке градиентного спуска:
t+1
=w
t
−ηg
t
.
Оценка на Regret (3.1 Constant Learning Rate Online Gradient Descent). Пусть
∣∣g
t
∣∣≤G;
∣∣w
∗
∣∣≤R.
Тогда, если взять
, то для любого
Regret
T
(w
∗
)≤RG
T
′
В целом, такая стратегия регуляризации не самая оптимальная. Интуитивно, наш регуляризатор фиксирован вне зависимости от того, сколько мы уже сыграли раундов, и со временем может перестать компенсировать член
1:t
T
w, и тогда стабильность алгоритма может падать.
FTRL с learning rate scheduling
Чтобы исправить нестабильность алгоритма, возьмём
L
2
L
2
-регуляризатор, не равный нулю на каждом шаге.
Процесс оптимизации примет вид:
Для FTRL-Proximal:
min
t+1
=arg
w
min
[g
1:t
T
w+
s=0
∑
t
2
σ
s
∣∣w−w
s
∣∣
2
2
];
Для FTRL-Centered:
min
t+1
=arg
w
min
[g
1:t
T
w+
s=0
∑
t
2
σ
s
∣∣w∣∣
2
2
].
Посмотрим, какое обличье примет алгоритм FTRL-Proximal, если его изложить на языке градиентного спуска. Для этого продифференцируем и приравниваем нулю выражение, которое мы минимизируем:
0=g
1:t
+
s=0
∑
t
σ
s
(w−w
s=0
1:t
=σ
0:t
t+1
=
σ
0:t
1
s=0
0:t
1
g
1:t
Попробуем получить рекуррентную формулу для выражения
w
t
+
1
w
t+1
через
t+1
=
σ
0:t
1
(
s=0
1:t
0:t−1
1
(
s=0
∑
t−1
σ
s
w
s
−g
1:t−1
t+1
=
σ
0:t
1
(
s=0
∑
t−1
1:t−1
t+1
=
σ
0:t
1
(σ
0:t−1
0:t
1
(σ
0:t
w
t
−g
t
)=w
t
−
σ
0:t
1
g
t
Если теперь положить
0:t
1
, мы получаем формулу градиентного спуска:
t+1
Таким образом, темп обучения градиентного спуска равен обратной сумме коэффициентов регуляризации ftrl. Точно так же можно выразить
t−1
1
В качестве классической непокоординатной последовательности learning rate обычно берут
t+1
−
t
Оценка на Regret (3.2, Dual Averaging) Пусть
∣∣g
t
∣∣≤G,
∣∣w
∗
∣∣≤R.
Тогда, если выбрать
t+1
R
, то
Regret
Как и в случае с константным learning rate, константа
на практике никому не известна, так что ее подменяют на
α
α и перебирают руками с learning rate, равным
α
t
+
1
t+1
α
.
Data-Adaptive FTRL
До сих пор мы рассматривали в качестве нормы
∣
∣
⋅
∣
∣
∣∣⋅∣∣ стандартное скалярное произведение, в которое различные компоненты вектора весов (которые, грубо говоря, соответствуют различным признакам) вносят равный вклад. Такой подход может быть слишком наивным для «боевых» задач, где геометрия оптимизации имеет форму, например, вытянутого эллипса.
Нетрудно обобщить предыдущие рассуждения на случай произвольного скалярного произведения
min
t+1
=arg
w
min
[g
1:t
T
w+
2
1
s=0
∑
T
∣∣w−w
Коэффициенты
σ
s
σ
s
в этом выражении теперь спрятались в
D
s
D
s
. Найдем точку минимума:
0=g
1:t
+
2
1
s=0
∑
t
(w−w
s
)(D
s
+D
s
T
)=g
1:t
+
s=0
∑
t
(w−w
s=0
∑
t
D
s
)w=
s=0
1:t
Но сразу возникают проблемы:
Нужно хранить
s=0
∑
t
D
s
, в общем случае это квадрат по памяти от числа параметров. Ни в какой реальной задаче мы не сможем себе этого позволить;
На каждой итерации метода нужно решать гигантскую систему линейных уравнений для поиска
w
w. Есть все шансы состариться, так и не успев увидеть решение задачи оптимизации.
Упростим себе жизнь и предположим, что все матрицы
D
s
D
s
диагональны. Тогда
s=1
∑
t
D
s
можно хранить в виде вектора диагональных элементов того же размера, что и
w
w, а система на каждой итерации будет решаться за линию.
AdaGrad: наилучший адаптивный метод
Разрешив себе брать нормы
∣∣⋅∣∣
D
s
с диагональными матрицами
D
s
D
s
, мы сделали алгоритм более гибким, но при этом приобрели дополнительные степени свободы (выбор диагональных элементов). Попробуем ответить на два вопроса:
Можно ли матрицы
D
s
D
s
не угадывать, а настраивать по доступной на очередном шаге информации?
Как выбирать матрицы
D
s
D
s
так, чтобы минимизировать оценки на regret?
В процессе поисков ответов на них мы придём к известному методу оптимизации AdaGrad.
Помня, что
∣∣.∣∣
D,∗
=∣∣.∣∣
D
−1
, выпишем общий вид оценки на regret:
Regret
T
(w
∗
)≤r
0:T−1
(w
∗
)+
2
1
t=1
∑
T
∣∣g∣∣
(t),∗
2
=
t=1
∑
T
∣∣w
t=1
∑
T
∣∣g
t
∣∣
(D
0:T
)
−1
2
Чтобы упростить выкладки, введем новую симметричную положительно определенную матрицу
0:T
−1
и перепишем формулы
Regret
T
(w
∗
)≤r
0:T−1
(w
∗
)+
2
1
t=1
∑
T
∣∣g∣∣
t−1,∗
2
=
t=1
∑
T
∣∣w
t=1
∑
T
∣∣g
t
∣∣
S
T
2
С членом
t=1
∑
T
∣∣w
явно будет очень сложно работать: чтобы им пользоваться, нужно иметь на руках оптимальное решение
для всей предыдущей выборки. Более перспективным выглядит слагаемое
t=1
∑
T
∣∣g
t
∣∣
S
T
2
: вычислять их одно удовольствие. Идея метода AdaGrad как раз в том, чтобы не пытаться работать с первым членом и минимизировать второй, надеясь, что итоговые оценки на regret при этом тоже улучшатся.
Для начала выведем диагональный AdaGrad как более простой случай. Если все
D
t
D
t
диагональны, то матрица
1:T
−1
тоже диагональна и представляется набором диагональных элементов
(уберем индекс
T
T для сокращения выкладок, так как мы рассматриваем фиксированный раунд).
Распишем второе слагаемое в regret
t=1
∑
T
∣∣g
t=1
∑
T
i=1
∑
N
s
i
g
t,i
2
Попробуем минимизировать его
inf
t=1
∑
T
i=1
∑
N
s
i
g
t,i
2
⟶
s
inf
s
i
≥0
Условие
≥0 возникает из неотрицательной определенности матрицы
S
T
S
T
. Решеним такой задачи, очевидно, является
→+∞. Однако в этом случае член
t=1
∑
T
∣∣w
из оценки на regret станет, наоборот, бесконечно большим, и нужен какой-то компромисс. Введем довольно слабое ограничение на положительные коэффициенты
inf
t=1
∑
T
i=1
∑
N
s
i
g
t,i
2
⟶
s
inf
s
i
≥0,
i=1
∑
N
s
i
≤c
и найдём оптимум с помощью метода множителей Лагранжа. Функция Лагранжа имеет вид
L(s,λ,θ)=
t=1
∑
T
i=1
∑
N
s
i
g
t,i
2
+λ
T
s+θ(
i=1
∑
N
s
i
−c)
Отметим, что здесь
λ
λ — это вектор, а
θ
θ — число.
Приравняем к нулю частные производные:
∂L(s,λ,θ)
=−
s
i
1
t=1
∑
T
g
t,i
2
+λ
i
+θ
Вспомним про условия дополняющей нежесткости, требующие, чтобы
=0. Так как
s
i
s
i
мы нулю приравнять здесь не можем, получаем, что
=0:
t=1
∑
T
g
t,i
2
−θ=0
t=1
∑
T
g
t,i
2
Теперь вспомним про условие
i=1
∑
N
s
i
≤c. Можно показать, что оптимум достигается на границе (то есть когда неравенство превращается в равенство). Тогда
i=1
i=1
∑
N
t=1
∑
T
g
t,i
i=1
∑
N
t=1
∑
T
g
t,i
i=1
∑
N
t=1
∑
T
g
t,i
2
c
t=1
∑
T
g
t,i
2
Вернемся к оценке на regret. Чему равно
c
c мы не знаем, поэтому мы просто констатируем, что оптимальные коэффициенты
s
i
s
i
пропорциональны
s=1
∑
t
g
s,i
t=1
∑
T
g
t,i
2
Теперь
S
T
S
T
— диагональная матрица с диагональными элементами
. Следовательно,
0:T
=(S
T
)
−1
- тоже диагональная матрица с диагональными элементами
0:T,i
=
α
1
t=1
∑
T
g
t,i
2
=
t=1
∑
T
α
1
t=1
∑
T
g
t,i
2
−
α
1
t=1
∑
T−1
g
t,i
2
+0,D
0
=0,
и легко убедиться, что
t,i
=
α
1
t=1
∑
T
g
t,i
2
−
α
1
t=1
∑
T
g
t,i
2
Теперь вспомним, что эти формулы в точности повторяют то, что мы получили выше для соотношения
t−1
1
, только вместо общего коэффициента
η
t
η
t
у нас теперь покоординатные коэффициенты
η
t
,
i
η
t,i
t,i
=
s=1
∑
t
g
s,i
2
α
Получаем формулы для метода AdaGrad в градиентной постановке:
t+1,i
=w
t,i
−
s=1
∑
t
g
s,i
2
α
g
t,i
,
где коэффициент
α
α приобретает значение learning rate.
Оценка на Regret (3.4, FTRL-Proximal with Diagonal Matrix Learning Rates)
Если использовать AdaGrad с покоординатными learning rate, то
Regret
T
(w
∗
)≤2
2
R
t=1
∑
T
g
t
2
Отметим, что это оценка отличается от предыдущей тем, что вместо
G
T
G
T
используется
t=1
∑
T
g
t
2
. Таким образом, если у градиента на какой-то из позиций стоит что-то большое, это повлияет лишь на одно из слагаемых под корнем вместо того, чтобы умножиться на
T
T
.
Эффективный размер шага. Предположим, что градиенты ограничены по норме
∣∣g∣∣
2
≤R. Перепишем наши формулы в виде
s=1
∑
T
g
s,i
s=1
∑
T
g
s,i
Из этих формул следует, что в среднем learning rate в AdaGrad убывает как
=O(
T
1
), то есть так же, как в предыдущем методе. Отличие состоит лишь в более правильной покоординатной нормировке, которая улучшает сходимость.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
15.1. Введение в онлайн-обучение
Следующий параграф
15.3. Регуляризация в онлайн-обучении
