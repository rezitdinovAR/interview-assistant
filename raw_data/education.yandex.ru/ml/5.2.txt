---
title: Первое знакомство с полносвязными нейросетями
url: https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami
course: ml
chapter: 5. Глубинное обучение - введение
chapter_id: 5.2
---
Основные понятия глубинного обучения. Базовые слои и функции активации
Основные определения
Искусственная нейронная сеть (далее — нейронная сеть) — это сложная дифференцируемая функция, задающая отображение из исходного признакового пространства в пространство ответов, все параметры которой могут настраиваться одновременно и взаимосвязанно (то есть сеть может обучаться end-to-end).
В частном (и наиболее частом) случае представляет собой последовательность дифференцируемых параметрических преобразований.
Внимательный читатель может заметить, что под указанное выше определение нейронной сети подходят и логистическая, и линейная регрессия. Это верное замечание: и линейная, и логистическая регрессии могут рассматриваться как нейронные сети, задающие отображения в пространство ответов и логитов соответственно.
Сложную функцию удобно представлять в виде суперпозиции простых, и нейронные сети обычно предстают перед программистом в виде конструктора, состоящего из более-менее простых блоков (слоёв, layers). Вот две простейшие их разновидности:
Линейный слой (linear layer, dense layer) — линейное преобразование над входящими данными. Его обучаемые параметры — это матрица
W
W и вектор
x↦xW+b (
W∈R
d×k
,x∈R
d
,b∈R
k
). Такой слой преобразует
d
d-мерные векторы в
k
k-мерные.
Функция активации (activation function) — нелинейное преобразование, поэлементно применяющееся к пришедшим на вход данным. Благодаря функциям активации нейронные сети способны порождать более информативные признаковые описания, преобразуя данные нелинейным образом. Может использоваться, например, ReLU (rectified linear unit)
ReLU
(
x
)
=
max
(
0
,
x
)
ReLU(x)=max(0,x) или уже знакомая вам из логистической регрессии сигмоида
σ(x)=
1+e
−x
1
. К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.
Даже самые сложные нейронные сети обычно собираются из относительно простых блоков, подобных этим. Таким образом, их можно представить в виде вычислительного графа (computational graph), где промежуточным вершинам соответствуют преобразования. На иллюстрации ниже приведён вычислительный граф для логистической регрессии.
16
Не правда ли, похоже на слоёный пирог из преобразований? Отсюда и слои.
Графы могут быть и более сложными, в том числе нелинейными:
16
Давайте разберёмся, что тут происходит.
Input — это вход нейросети, который получает исходные данные. Обычно требуется, чтобы они имели вид матрицы («объекты-признаки») или тензора (многомерной матрицы). Вообще говоря, входов может быть несколько: например, мы можем подавать в нейросеть картинку и какие-нибудь ещё сведения о ней — преобразовывать их мы будем по-разному, поэтому логично предусмотреть два входа в графе.
Дальше к исходным данным
X
0
X
0
применяются два линейных слоя, которые превращают их в промежуточные (внутренние, скрытые) представления
. В литературе они также называются активациями (не путайте с функциями активации).
Каждое из представлений
подвергается нелинейному преобразованию, превращаясь в новые промежуточные представления
соответственно. Переход от
X
0
X
0
к двум новым матрицам (или тензорам)
можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных.
Затем представления
конкатенируются (то есть признаковые описания всех объектов объединяются).
Дальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на выход сети, то есть отдаётся обратно пользователю.
Нейросеть, в которой есть только линейные слои и различные функции активации, называют полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP).
Посмотрим, что происходит с размерностями, если на вход подаётся матрица
N
×
d
N×d:
16
А вот и настоящий пример из реальной жизни. GoogLeNet (она же Inception-v1), показавшая SotA-результат на ILSVRC 2014 (ImageNet challenge), выглядит так:
16
Здесь каждый кирпичик — это некоторое относительно простое преобразование, а белым помечены входы и выходы вычислительного графа.
Современные же сети часто выглядят и ещё сложней, но всё равно они собираются из достаточно простых кирпичиков-слоёв.
Примечание. Впрочем, в общем случае нейронная сеть — это просто некоторая сложная функция (или, что эквивалентно, граф вычислений). Поэтому в некоторых (очень нетривиальных) случаях нет смысла разбивать её на слои.
В качестве иллюстрации ниже приведены структуры агностических нейронных сетей WANN, представленных в работе Weight Agnostic Neural Networks, NeurIPS 2019.
16
Forward & backward propagation
Информация может течь по графу в двух направлениях.
Применение нейронной сети к данным (вычисление выхода по заданному входу) часто называют прямым проходом, или же forward propagation (forward pass). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым.
16
При обратном проходе, или же backward propagation (backward pass), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования.
Механизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагает обратное движение по вычислительному графу сети. С ним вы познакомитесь в следующем параграфе.
16
Архитектуры для простейших задач
Как мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этом параграфе, но с чего-то ведь надо начинать, не так ли?
В тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных:
16
Бинарная классификация
Для решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от
0
0 до
1
1, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв
=σ(f(X
m
)),
где
f
f — некоторая функция, превращающая представление
X
m
X
m
в число (если
X
m
X
m
— матрица, то подойдёт
f(X
m
)=X
m
w+b, где
w
w — вектор-столбец), а
σ
σ — наша любимая сигмоида. При этом
X
m
X
m
может получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения.
В качестве функции потерь удобно брать уже знакомый нам log loss.
Многоклассовая классификация
Работая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно.
Достаточно построить сеть, которая будет выдавать
K
K неотрицательных чисел, суммирующихся в 1 (где
K
K — число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна.
Превратить произвольный набор из
K
K чисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция
softmax
softmax(x
1
,…,x
K
)=(
,…,
Наиболее популярные архитектуры для многоклассовой классификации имеют вид
y
^
=
softmax
=softmax(f(X
m
)),
где
f
f — функция, превращающая
X
m
X
m
в матрицу
B
×
K
B×K (где
B
B — размер батча), а
X
m
X
m
может быть получен любым приятным вам образом.
Но какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяет кросс-энтропия, она же negative log-likelihood — некоторый аналог расстояния между распределениями:
log
,y)=−
B
1
i=1
∑
B
k=1
∑
K
y
ik
log
y
ik
,
где снова
B
B — размер батча, а
K
K — число классов. Легко видеть, что при
K
=
2
K=2 получается та самая функция потерь, которую мы использовали для обучения бинарной классификации.
(Множественная) регрессия
С помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько. Например, координаты ключевых точек лица — кончика носа, уголков рта и так далее.
Достаточно сделать, чтобы последнее представление было матрицей
B
×
M
B×M, где
B
B — размер батча, а
M
M — количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например,
M
S
E
MSE по всей матрице
B
×
M
B×M.
Всё вместе
Если вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация).
Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей.
Популярные функции активации
Для начала поговорим о том, зачем они нужны.
Казалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться.
Рассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации?
out
=(X
Линейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою.
Добавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами.
В качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоида
exp
⁡
(
−
x
)
σ(x)=
1+exp(−x)
1
или ReLU (Rectified linear unit)
ReLU
(
x
)
=
max
(
0
,
x
)
ReLU(x)=max(0,x). К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее.
Примечание. На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий
m
m-мерные векторы в
n
n-мерные, на два, вставив посередине
k
k-мерное представление, где
k
≪
m
,
n
k≪m,n:
16
С точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше
k
k. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться.
Вернёмся к функциям активации. Вот наиболее популярные:
16
Рассмотрим их подробнее.
ReLU, Rectified linear unit
Формула:
ReLU
(
x
)
=
max
ReLU(x)=max(0,x),
ReLU
ReLU:R→[0,+∞).
ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением.
Плюсы:
простота вычисления активации и производной.
Минусы:
область значений является смещённой относительно нуля;
для отрицательных значений производная равна нулю, что может привести к затуханию градиента.
ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды.
Leaky ReLU
Формула:
Leaky ReLU
(
x
)
=
max
const
Leaky ReLU(x)=max(αx,x),α=const,0<α≪1
Leaky ReLU
Leaky ReLU:R→(−∞,+∞).
Гиперпараметр
α
α обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля.
PReLU, Parametric ReLU
Формула:
PReLU
(
x
)
=
max
PReLU(x)=max(αx,x),0<α≪1
PReLU
PReLU:R→(−∞,+∞).
Аналогична Leaky ReLU, но параметр
α
α настраивается градиентными методами.
ELU
ELU – это гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике.
Sigmoid, сигмоида
Формула:
exp
σ(x)=
1+exp(−x)
σ:R→(0,1).
Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона.
Плюсы:
Минусы:
область значений смещена относительно нуля;
сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow question 1, question 2;
на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента;
максимальное значение производной составляет
0.25
0.25, что также приводит к затуханию градиента.
На практике сигмоида редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM).
Tanh, гиперболический тангенс
Формула:
tanh
⁡
(
x
)
=
exp
⁡
(
x
)
−
exp
⁡
(
−
x
)
exp
⁡
(
x
)
+
exp
tanh(x)=
exp(x)+exp(−x)
exp(x)−exp(−x)
,
tanh
tanh:R→(−1,1).
Плюсы:
как и сигмоида, имеет ограниченную область значений;
в отличие от сигмоиды, область значений симметрична.
Минусы:
требует вычисления экспоненты, что является достаточно сложной вычислительной операцией;
на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента.
Вопрос на подумать. А почему симметричность области значений может быть ценным свойством?
Немного о мощи нейросетей
Рассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art.
Теорема Цыбенко (1989). Для любой непрерывной функции
f(x):R
m
→R и для любого
ε
>
0
ε>0 найдётся число
N
N, а также числа
…,w
,…,b
,…,α
N
, для которых
f(x)−
i=1
∑
N
α
i
σ(⟨x,w
i
⟩+b
i
)
<ε
для любых
x
x из единичного куба
[0,1]
В сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаем
x
x в
⟨x,w
i
⟩+b
i
— это можно представить в виде одной матричной операции (линейный слой!):
x↦x
(1)
=x⋅(
)+(
где
w
i
w
i
— вектор-столбцы, а каждое из
b
i
b
i
прибавляется к
i
i-му столбцу, после чего поэлементно берём от
x
(
1
)
x
(1)
сигмоиду (активация)
(2)
=σ(x
(1)
), после чего вычисляем
i=1
(2)
=(α
1
,…,α
N
)⋅x,
и это второй линейный слой (без свободного члена).
Правда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно.
Упражнение. Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательной песочницей. Убедитесь сами, что при использовании одного скрытого слоя из двух нейронов и сигмоиды в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации.
А для получения решения нам необходим метод автоматической настройки всех параметров нейронной сети — метод обратного распространения ошибки, или же error backpropagation. Рассмотрим его в деталях в следующем параграфе.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
5.1. Нейронные сети
Краткий путеводитель по разделу
Следующий параграф
5.3. Метод обратного распространения ошибки
Как эффективно посчитать градиенты по весам нейронной сети
