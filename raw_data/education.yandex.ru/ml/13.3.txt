---
title: PAC-байесовские оценки риска
url: https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska
course: ml
chapter: 13. Теория глубокого обучения
chapter_id: 13.3
---
В предыдущем параграфе рассматривались равномерные оценки разницы истинного и эмпирического рисков. Если в рассматриваемом классе моделей есть «плохие», то равномерные оценки становятся слишком пессимистичными. Часто нельзя гарантировать, что что наш алгоритм обучения их никогда не выбирает, поэтому класс моделей
F
F для равномерной оценки не получится сузить до класса только «хороших» моделей. Но можно надеяться, что плохие выучиваются не слишком часто. Например, известно, что градиентный спуск обычно сходится к хорошим моделям (об этом мы ещё поговорим в параграфе про implicit bias). В этом параграфе мы разберём элегантный способ учесть «предпочтения» алгоритма обучения в оценке разницы рисков.
Вспомним равномерную оценку для конечного
F
F:
P
(
sup
f∈F
sup
(R(f)−
R
^
m
(f))≥ϵ)=P(∃f∈F:(R(f)−
R
^
m
(f))≥ϵ)≤
f∈F
∑
P(R(f)−
R
^
m
(f)≥ϵ)≤∣F∣e
−2mϵ
2
∀ϵ>0,
где
∣
F
∣
∣F∣ – мощность класса
F
F. Эта оценка формально верна и для бесконечного
F
F, но смысл её теряется. Давайте попробуем исправить это.
Пусть
F
F не более, чем счётно. Для каждого
f
∈
F
f∈F возьмём своё
ϵ
(
f
)
ϵ(f). Если взять
ϵ
(
f
)
ϵ(f) таким, чтобы
f∈F
e
−2mϵ
2
(f)
было конечным, то приходим к осмысленной оценке:
P(∃f∈F:(R(f)−
R
^
m
(f))≥ϵ(f))≤
f∈F
∑
P(R(f)−
R
^
m
(f)≥ϵ(f))≤
f∈F
∑
e
−2mϵ
2
(f)
.
Рассмотрим теперь некоторое вероятностное распределение
P
(
f
)
P(f) на
F
F. В качестве
ϵ
(
f
)
ϵ(f) возьмём
−2mϵ
2
(f)
=P(f)e
−2m
ϵ
~
2
,
где
. Из этого уравнения получаем следующее выражение для
ϵ
(
f
)
ϵ(f):
log
ϵ(f)=
log
P(f)
1
.
В итоге, для любого
>0 получаем оценку:
log
P(∃f∈F:(R(f)−
R
^
m
(f))≥
log
P(f)
1
)≤e
−2m
ϵ
~
2
.
Или, что то же самое, с вероятностью
≥
1
−
δ
≥1−δ по
S
m
S
m
для любого
f
∈
F
f∈F:
log
⁡
1
δ
+
log
R(f)−
R
^
m
(f)≤
2m
1
(log
δ
1
+log
P(f)
1
)
Заметим, что если
F
F конечно, а
P
(
f
)
P(f) – равномерное распределение, то оценка выше совпадает с равномерной оценкой. Если же наш алгоритм обучения предпочитает выбирать модели, для которых
P
(
f
)
P(f) велико, то оценка улучшается по сравнению с равномерной. Таким образом, распределение
P
(
f
)
P(f) «кодирует» наши представления о предпочтениях алгоритма. Будем называть
P
(
f
)
P(f) «априорным распределением».
Как обобщить оценку выше на несчётные классы моделей? В первую очередь, предположим, что наш алгоритм обучения
A
A стохастичен, а значит, на выходе даёт не одну модель, а распределение:
=A(S
m
).
Будем называть это распределение «апостериорным». Такое рассуждение осмысленно, например, для стохастического градиентного спуска: очевидно, что результат его работы на невыпуклой функции потерь недетерминирован (он может сходиться в разные локальные минимумы).
Заметим, что главное отличие апостериорного распределения от априорного в том, что первое зависит от данных, а второе – нет. Важно понимать при этом, что, несмотря на названия, эти два распределения не связаны между собой никаким вариантом формулой Байеса. Сходство с байесовским подходом скорее внешнее. Поэтому слова «априорное» и «апостериорное» имеет смысл писать в кавычках, но для экономии места мы их будем в дальнейшем опускать.
Оценки разности рисков, о которых речь пойдёт ниже, называются PAC-байесовскими (PAC-bayesian, где PAC – probably approximately correct).
Сформулируем одну из классических оценок из этого класса:
Теорема Макаллестера. Пусть
F
F – множество моделей и
P
P – распределение на
F
F. Тогда для любого
δ∈(0,1) с вероятностью
≥
1
−
δ
≥1−δ по
S
m
S
m
имеем:
log
2m−1
1
(log
δ
4m
+KL(
Q
^
m
∣∣P))
,
где
R(Q)=E
f∼Q
R(f) и
(Q)=E
f∼Q
R
^
m
(f).
Видим, что оценка тем лучше, чем ближе апостериорное распределение к априорному. Здесь работает следующая интуиция. Если для большинства обучающих наборов данных апостериорное распределение близко к априорному, то оно почти не зависит от данных, а значит, истинный риск и риск на обучающей выборке должны быть близки с высокой вероятностью. Если же апостериорное зависит от данных сильно, то, скорее всего, модель сильно переобучается, а значит, оценка не может быть хорошей; в нашем случае она велика из-за большой KL-дивергенции.
Для доказательства теоремы нам понадобятся две леммы:
Лемма 1. Для любого распределения
P
P на
F
F и для любого
δ∈(0,1) с вероятностью
≥
1
−
δ
≥1−δ по
S
m
S
m
имеем:
f∼P
e
(2m−1)(Δ
m
(f))
2
≤
δ
4m
,
где
(f)=∣R(f)−
R
^
m
(f)∣.
Лемма 2 (лемма Донскера-Вередана, Donsker-Varadhan). Пусть
P
P и
Q
Q – вероятностные распределения на множестве
X
X. Тогда для любого
h
:
X
→
R
h:X→R
log
x∼Q
h(x)≤logE
x∼P
e
h(x)
+KL(Q∣∣P).
Теорема Макаллестера – не единственная из возможных пак-байесовских оценок. Например, несколько улучшенную версию той же оценки можно найти в работе Bounds for averaging classifiers. Другие оценки подобного типа можно найти в монографии PAC-Bayesian supervised classification: the thermodynamics of statistical learning.
Применение пак-байесовских оценок к детерминированным алгоритмам обучения
Выше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделей
F
F всё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит от случайного выбора батчей и от инициализации весов) и теорема Макаллестера выполняется, финальное распределение моделей очень сложно охарактеризовать, и из-за этого непонятно, как считать KL-дивергенцию.
Предположим, что алгоритм обучения всё-таки детерминирован; этого можно добиться, зафиксировав сид генератора случайных чисел при обучении. Как получить осмысленную PAC-байесовскую оценку для детерминированного алгоритма на несчётном множестве моделей?
Мы рассмотрим два способа.
Первый способ – добавить известный шум в финальную модель, выданную детерминированным алгоритмом. Так, для нейронных сетей, результатом работы алгоритма обучения является набор весов. Если добавить в этот набор гауссовский шум, а также в качестве априорного распределения взять гауссовское, то KL-дивергенцию в теореме Макаллестера можно будет посчитать аналитически.
Дисперсию шума в апостериорном распределении тоже можно обучить с помощью градиентного спуска одновременно с весами, тем самым минимизируя правую часть оценки из вышеупомянутой теоремы. Если в найденную модель удастся добавить шум так, чтобы KL-дивергенция значительно уменьшилась, но при этом риск на обучающей выборке не сильно вырос, то оценка на истинный риск получится хорошей.
Это рассуждение связывает PAC-байесовские оценки и гипотезу о том, что «плоские» («широкие») минимумы хорошо обобщают. В самом деле, если минимум «плоский», то в модель из него можно добавить много шума, не испортив качество на обучении. Оценки, основанные на этом принципе, можно найти в работах Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data и A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks.
Второй способ состоит в том, чтобы взять дискретное кодирование
c
c и применить дискретную PAC-байесовскую оценку к закодированной модели вместо оригинальной. Обозначим закодированную модель
f
f через
f
c
f
c
. Следуя работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach, возьмём априорное распределение с массой, убывающей с ростом длины кода:
m(∣f
c
∣)2
−∣f
c
∣
.
Здесь
∣ – длина кода модели
m(k) – некоторое вероятностное распределение на
N
N, а
Z
Z – нормализующая константа. Тогда KL-дивергенция примет следующий вид:
log
log
⁡
2
−
log
KL(δ
f
c
∣∣P
c
)=logZ+∣f
c
∣log2−log(m(∣f
c
∣)).
Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на предположение, что обученные модели в некоторым смысле «простые».
Если модель параметризована весами
θ
θ, типичный алгоритм сжатия выдаст набор
(S,Q,C), где
dim
⁡
θ
]
S=s
1:k
⊂[dimθ] – позиции ненулевых весов;
C=c
1:r
⊂R – «словарь» весов;
Q=q
1:k
∈[r]
∀i∈[k] – квантизованные значения весов.
Выход алгоритма будет выглядеть как
C(θ)
i
=c
q
j
, если
i
=
s
j
i=s
j
, иначе
0
0.
Тогда наивное 32-битное кодирование даст следующую длину:
log
⁡
dim
⁡
θ
+
log
∣C(θ)∣
c
=∣S∣
c
+∣Q∣
c
+∣C∣
c
≤k(logdimθ+logr)+32r.
В работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach описанный выше способ применяется к модели MobileNet (свёрточной сети, сконструированной специально для мобильных устройств), обученной на наборе данных ImageNet, и получают верхнюю оценку на истинный риск, равную
96.5
%
96.5% (риск случайного угадывания –
99.9
%
99.9%). Хотя такой результат и выглядит очень скромным, но это первая осмысленная оценка обобщающей способности реально используемой нейронной сети на реалистичном наборе данных.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
13.2. Обобщающая способность – классическая теория
Следующий параграф
13.4. Сети бесконечной ширины
