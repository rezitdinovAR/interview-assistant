---
title: Энтропия и семейство экспоненциальных распределений
url: https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij
course: ml
chapter: 16. Теормин
chapter_id: 16.7
---
h(x), которое вы при этом получили? Следующие соображения кажутся в этом плане вполне естественными:
чем выше вероятность
P(ξ=x), тем более ожидаемо появление значения
x
x и, соответственно, менее информативно;
и наоборот, наблюдение маловероятного значения
x
x обычно даёт обильную пищу для размышлений и повышает
h
(
x
)
h(x);
при наблюдении двух независимых реализаций
x
x и
y
y случайной величины
ξ
ξ логично складывать полученную информацию:
h((x,y))=h(x)+h(y).
Указанные соображения наводят на мысль, что информацию следует считать убывающей функцией от вероятности:
h(x)=g(P(ξ=x)). Кроме того, функция
g
g должна превращать произведение в сумму, поскольку для независимых случайных величин
ξ
ξ и
η
η
равенство
h((x,y))=h(x)+h(y) влечёт
g(P(ξ=x,η=y))=g(P(ξ=x)P(η=y))=g(P(ξ=x))+g(P(η=y)).
На самом деле выбор тут небогат. Единственная непрерывная функция, обладающая такими свойствами, — это логарифм:
log
⁡
p
g(p)=−logp. Основание логарифма может быть любым числом больше единицы. Поскольку информацию измеряют в битах и байтах, в теории информации обычно предпочитают двоичные логарифмы. Однако для вычислений удобнее использовать натуральный логарифм, и по умолчанию мы будем подразумевать именно его (кстати, соответствующую единицу информации называют «нат»).
Энтропия Шеннона
Среднее количество информации, которое несёт в себе значение дискретной случайной
ξ
ξ с распределением вероятностей
=P(ξ=k), вычисляется по формуле
log
log
⁡
p
k
.
Hξ=E(g(p(ξ)))=−E(logp(ξ))=−
k
∑
p
k
logp
k
.
Это так называемая энтропия (Шэннона).
Пример. Рассмотрим схему Бернулли с вероятностью «успеха»
p
p. Энтропия её результата равна
log
log
Hξ=−(1−p)log(1−p)−plogp,ξ∼Bern(p).
Давайте посмотрим на график этой функции:
entropy
Минимальное значение (нулевое) энтропия принимает при
p
=
0
p=0 или
p
=
1
p=1. Исход такого вырожденного эксперимента заранее известен, и чтобы сообщить кому-то о его результате, достаточно
0
0 бит информации. Иначе говоря, можно вообще ничего не передавать, и так всё предельно ясно.
Максимальное значение энтропии достигается в точке
1
2
2
1
, что вполне соответствует тому, что при
предсказать исход эксперимента сложнее всего.
Упражнение. Найдите энтропию геометрического распределения с вероятностью «успеха»
ξ∼Geom(p),
P(ξ=k)=p(1−p)
k−1
,k∈N,0<p⩽1.
Следующие свойства энтропии дискретной случайной величины
ξ
ξ вытекают прямо из определения:
неотрицательность:
H
ξ
⩾
0
Hξ⩾0;
Hξ=0⟺P(ξ=a)=1 при некотором
a
∈
R
a∈R (нулевую энтропию имеют вырожденные распределения и только они);
H
ξ
⩽
log
⁡
n
Hξ⩽logn, если случайная величина имеет конечный носитель мощности
n
n.
Последнее свойство выводится из неравенства Йенсена. Применяя его к выпуклой вверх логарифмической функции, с учётом нормировки условия
k=1
∑
n
p
k
=1 получаем
log
log
⁡
1
p
k
⩽
log
log
⁡
n
.
−
k=1
∑
n
p
k
logp
k
=
k=1
∑
n
p
k
log
p
1
k
⩽log(
k=1
)=logn.
Вопрос на подумать. Итак, всякое распределение с носителем
{1,2,…,n} имеет энтропию не больше
log
⁡
n
logn. А у какого распределения она в точности равна
log
⁡
n
logn?
Дифференциальная энтропия
Чтобы вычислить энтропию непрерывной случайной величины
ξ
ξ, надо, как водится, сумму заменить на интеграл, и получится формула дифференциальной энтропии:
log
Hξ=−∫p
ξ
(x)logp
ξ
(x)dx.
Замечание. В дальнейшем мы будем использовать одинаковый термин энтропия как для дискретных, так и для непрерывных случайных величин, для краткости опуская слово дифференциальная в последнем случае. Кроме того, энтропию распределения
p
p, заданного через pmf или pdf, будем обозначать
H
[
p
]
H[p]. Такое обозначение позволяет избежать привязки к случайной величине там, где это излишне. Если
ξ∼p(x), то обозначения
H
ξ
Hξ и
H
[
p
]
H[p] эквивалентны. Также отметим, что энтропию можно записать в виде математического ожидания:
log
H[p]=E
ξ∼p(x)
log
p(ξ)
1
.
Пример. Найдём энтропию нормального распределения
N(μ,σ
2
). Его плотность равна
p(x)=
(x−μ)
2
, следовательно,
H[p]=
−∞
∫
+∞
p(x)(
2
1
ln(2πσ
2
)+
2σ
2
(x−μ)
2
)dx=
ln(2πσ
(x−μ)
2
e
−
2σ
2
(x−μ)
2
dx.
Делая в последнем интеграле замену
t=−
2σ
2
(x−μ)
2
, получаем, что
H[p]=
2
1
ln(2πσ
dt=
2
1
ln(2πσ
По свойству гамма-функции
. Таким образом,
H[p]=
2
1
ln(2πσ
2
)+
2
1
.
Как видно, энтропия гауссовского распределения
N(μ,σ
2
) не ограничена ни сверху, ни снизу:
lim
lim
σ→+∞
lim
2
1
ln(2πσ
2
)=+∞,
σ→0+
lim
2
1
ln(2πσ
2
)=−∞.
И да, в отличие от энтропии дискретного распределения дифференциальная энтропия может быть отрицательной. Это связано с тем, что плотность может принимать значения больше единицы, и поэтому математическое ожидание её логарифма с обратным знаком может оказаться меньше нуля. В частности, с нормальным распределением так происходит, если
2πe
1
.
Упражнение. Найдите энтропию показательного распределения
Exp(λ).
KL-дивергенция
В задачах машинного обучения истинное распределение
p
(
x
)
p(x), из которого приходят наблюдения, обычно неизвестно, и его пытаются приблизить распределением
q
(
x
)
q(x) из некоторого класса модельных распределений. Дивергенция Кульбака-Лейблера (KL-дивергенция, относительная энтропия) позволяет оценить расстояние между распределениями
p
p и
log
⁡
p
k
q
k
KL(p∣∣q)=
k
∑
p
k
log
q
k
p
k
в дискретном случае и
log
KL(p∣∣q)=∫p(x)log
q(x)
p(x)
dx
в непрерывном. KL-дивергенцию можно представить в виде разности:
log
log
KL(p∣∣q)=∫p(x)log
q(x)
1
dx−∫p(x)log
p(x)
1
dx=
log
кросс-энтропия
log
энтропия
.
=
кросс-энтропия
E
ξ∼p(x)
logq(ξ)
1
−
энтропия
E
ξ∼p(x)
logp(ξ)
1
.
Здесь вычитаемое – это уже знакомая нам энтропия распределения
p
(
x
)
p(x), которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины
ξ∼p(x). Уменьшаемое носит название кросс-энтропии распределений
p
(
x
)
p(x) и
q
(
x
)
q(x).
Кросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величины
ξ∼p(x) алгоритмом, оптимизированным для кодирования случайной величины
η∼q(x). Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений
p
p, если при настройке алгоритма кодирования вместо
p
p использовать
q
q. Подробнее об этом вы можете почитать, например, в данном посте.
Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности,
KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду (для дискретных и непрерывных распределений это означает, что они просто тождественны). Но при этом она не является симметричной: вообще говоря,
KL(p∣∣q)

=KL(q∣∣p).
Упражнение. Пользуясь неравенством
ln(1+t)⩽t,
t
>
−
1
t>−1, докажите неотрицательность KL-дивергенции.
Пример. С помощью KL-дивергенции измерим расстояние между двумя гауссианами
p(x)=N(x∣μ
1
,σ
1
2
) и
q(x)=N(x∣μ
2
,σ
2
2
).
Подставляя явные выражения для плотностей
p(x)=
(x−μ
1
)
2
и q(x)=
(x−μ
2
)
2
,
находим
q(x)
p(x)
=ln
(x−μ
(x−μ
=ln
)(x−μ
(x−μ
Из свойств нормального распределения вытекает, что
p(x)(x−μ
1
)dx=0,
−∞
∫
+∞
p(x)(x−μ
1
)
2
dx=σ
1
2
.
Таким образом,
KL(p∣∣q)=E
ξ∼p(x)
ln
q(ξ)
p(ξ)
=ln
+(μ
Как и должно быть, полученное выражение равно нулю, если гауссианы совпадают. При равных дисперсиях
получаем, что
KL(p∣∣q)=
. Это выражение остаётся прежним, если поменять местами
, поэтому в этом случае
KL(p∣∣q)=KL(q∣∣p). Если же
, то выражение для
KL(q∣∣p) явно отличается от
KL(p∣∣q), что лишний раз показывает несимметричность KL-дивергенции.
Упражнение. Найдите дивергенцию Кульбака-Лейблера двух показательных распределений
p(x)=Exp(x∣λ) и
q(x)=Exp(x∣μ).
Кросс-энтропия
При определении KL-дивергенции мы уже встречались с кросс-энтропией
log
⁡
q
(
ξ
)
H[p,q]=−E
ξ∼p(x)
logq(ξ)
В зависимости от типа распределений кросс-энтропия вычисляется по формуле
log
⁡
q
k
или
log
H[p,q]=−
k
∑
p
k
logq
k
или H[p,q]=−
−∞
∫
+∞
p(x)logq(x)dx.
Поскольку
KL(p∣∣q)=H[p,q]−H[p],
задача минимизации KL-дивергенции между неизвестным распределением данных
p
(
x
)
p(x) и модельным распределением
q
(
x
)
q(x) эквивалентна задаче минимизации кросс-энтропии. Разница между ними равна энтропии распределения
p
(
x
)
p(x), которая, очевидно, не зависит от
q
(
x
)
q(x).
В машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации на
K
>
1
K>1 классов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным:
y=(y
1
,…,y
K
),y
k
∈{0,1},
k=1
∑
K
y
k
=1.
Классификатор обычно выдаёт вероятности принадлежности каждому из классов,
класс
,…,
y
K
),
y
k
=P(класс k).
Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями:
log
⁡
y
^
k
.
L(y,
y
)=−
k=1
∑
K
y
k
log
y
k
.
И это вполне логично: чем ближе модельное распределение к истинному, тем меньше наши потери. В идеале
L(y,
y
)=0, если
Чтобы вычислить функцию потерь по обучающей выборке из
N
N объектов с метками
y
(
i
)
y
(i)
, обычно берут усреднённную кросс-энтропию
log
i=1
∑
N
L(y
(i)
,
y
(i)
)=−
N
1
i=1
∑
N
k=1
∑
K
y
k
(i)
log
y
k
(i)
.
Принцип максимальной энтропии
В параграфе про оценки параметров были описаны различные свойства параметрических оценок и методы их получения, например, метод моментов или метод максимального правдоподобия. В принципе, если мы уже выбрали для наших данных
,…,X
n
некоторое параметрическое семейство
(x), моделирующее их распределение, восстановить его параметры чаще всего можно по выборочному среднему
k=1
∑
n
X
k
и/или выборочной дисперсии
k=1
А теперь представим, что мы посчитали эти (или какие-то другие) статистики, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию?
Three
Почему-то хочется сказать, что в первом. Почему? Второе не симметрично – но что нас может заставить подозревать, что интересующее нас распределение не симметрично? С третьим проблема в том, что, выбирая его, мы добавляем дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет.
Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Таким образом, искомое распределение должно обладать максимальной неопределённостью при заданных ограничениях, или, говоря более научно, иметь максимально возможную энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше – тем более «‎произвольное» распределение, по крайней мере, в теории. В этом и заключается принцип максимальной энтропии для выбора модели машинного обучения.
Как мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно.
Пример. Покажем, что среди распределений на множестве натуральных чисел
N
N и математическим ожиданием
μ
>
1
μ>1 максимальную энтропию имеет геометрическое распределение.
Для минимизации энтропии
log
⁡
p
n
H[p]=−
n=1
∑
∞
p
n
logp
n
с учётом ограничений
n=1
∑
∞
p
n
=1,
n=1
∑
∞
np
n
=μ
воспользуемся методом множителей Лагранжа, согласно которому требуется минимизировать функцию Лагранжа
log
L(p,a,b)=
n=1
∑
∞
p
n
logp
n
−a(
n=1
∑
∞
p
n
−1)−b(
n=1
∑
∞
np
n
−μ).
Приравняем к нулю частные производные по
log
=1+logp
n
−a−bn=0.
Отсюда следует, что
a−1+bn
=αβ
n
, так что распределение действительно получается геометрическое. Параметры
α
α и
β
β найдём из уравнений
n=1
∑
∞
p
n
=
n=1
∑
∞
αβ
n
=
1−β
n=1
∑
∞
np
n
=αβ
n=1
∑
∞
nβ
n−1
=
(1−β)
2
αβ
.
Деля первое уравнение на второе, получаем
=1−β, или
β=1−
μ
1
. Далее из первого уравнения находим
1−β
=
μ−1
1
. Итак,
μ−1
1
(1−
(1−
μ
1
)
n−1
,
а это и есть геометрическое распределение с параметром
1
μ
μ
1
.
У непрерывных распределений возможны более интересные комбинации из ограничений на носитель и параметры. И конечно же, первую скрипку среди распределений с максимальной энтропией играет гауссовское распределение.
Пример. Докажем, что среди распределений на
R
R c математическим ожиданием
μ
μ и дисперсией
σ
2
σ
2
наибольшую энтропию имеет нормальное распределение
N(μ,σ
2
).
Пусть
p
(
x
)
p(x) – некоторое распределение со средним
μ
μ и дисперсией
q(x)∼N(μ,σ
2
). Как было показано выше,
log
H[q]=
2
1
log(2πσ
2
)+
2
1
. Запишем дивергенцию Кульбака-Лейблера:
log
log
KL(p∣∣q)=
−∞
∫
+∞
p(x)logp(x)dx−
−∞
∫
+∞
p(x)logq(x)dx=
log
=−H[p]−
−∞
∫
+∞
p(x)(−
2
1
log(2πσ
2
)−
2σ
2
1
(x−μ)
2
)dx=
log
=−H[p]+
2
1
log(2πσ
2
)
−∞
∫
+∞
p(x)dx+
2σ
2
1
=V[p]=σ
2
−∞
∫
+∞
(x−μ)
2
p(x)dx
log
=−H[p]+
2
1
log(2πσ
2
)+
2
1
=H[q]−H[p].
Так как KL-дивергенция всегда неотрицательна, получаем, что
H[p]⩽H[q] при любом распределении
p
p, удовлетворяющем заданным ограничениям.
Можно показать, что максимальную энтропию среди многомерных распределений с вектором средних
μ
μ и матрицей ковариаций
Σ
Σ имеет также гауссовское распределение
N(μ,Σ).
Упражнение. Докажите, что среди распределений на отрезке
[
a
,
b
]
[a,b] максимальную энтропию имеет равномерное распределение
U[a,b].
Упражнение. Докажите, что среди распределений на промежутке
[0,+∞) с математическим ожиданием
λ
>
0
λ>0 максимальную энтропию имеет показательное распределение
Exp(
λ
1
).
Как выяснилось, многие классические распределения имеют максимальную энтропию при весьма естественных ограничениях. Но как быть, если даны не эти конкретные, а какие-то другие ограничения? Есть ли какой-нибудь надёжный алгоритм вывода распределения с максимальной энтропией, позволяющий избежать случайных озарений и гаданий на кофейной гуще? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса.
Экспоненциальное семейство распределений
Говорят, что параметрическое семейство распределений относится к экспоненциальному классу, если его pdf (или pmf) может быть представлена в виде
exp
exp
p(x∣θ)=
h(θ)
g(x)
exp(θ
T
u(x))=g(x)exp(θ
T
u(x)−A(θ)),
где
θ
∈
R
m
θ∈R
m
– вектор натуральных параметров распределения;
g
(
x
)
g(x) — неотрицательная функция (base measure), часто равная единице;
h(θ)>0 — нормализатор (partition), обеспечивающий суммируемость pmf или интегрируемость pdf в единицу:
exp
h(θ)=∫g(x)exp(θ
T
u(x))dx;
A(θ)=lnh(θ) — log-partition;
u(x)∈R
m
— вектор достаточных статистик распределения.
Пример. Покажем, что нормальное распределение
N(x∣μ,σ
2
) принадлежит экспоненциальному классу. Оно имеет два параметра, поэтому такую же размерность имеют
θ
θ и вектор-функция
u
u.
Распишем плотность:
1
2
π
σ
exp
exp
exp
exp
exp(−
2σ
2
(x−μ)
2
)=
2π
σ
1
exp(−
σexp(−
2σ
2
μ
2
)
exp(−
Положим
g(x)=
u(x)=(x,x
θ=(θ
1
,θ
2
),θ
Остаётся выразить функцию
exp
h(θ)=σexp(−
2σ
2
μ
2
) через
Упражнение. Выразите partition
h
(
θ
)
h(θ) и log-partition
A
(
θ
)
A(θ) через
θ
θ и запишите плотность нормального распределения в экспоненциальном виде.
Пример. Покажем, что распределение Бернулли
Bern(p) принадлежит экспоненциальному классу. Его pmf
P(ξ=x∣p) можно записать как
exp
⁡
(
x
log
log
exp
⁡
(
x
log
(1−p)
1−x
=exp(xlogp+(1−x)log(1−p))=(1−p)exp(xlog
1−p
p
).
Параметр здесь один, поэтому натуральный параметр
θ
θ тоже один:
θ
=
log
⁡
p
1
−
p
θ=log
1−p
p
. Такая функция от
p
p называется функцией логитов и активно участвует в построении модели логистической регрессии. Остальные функции положим равными
u(x)=x,
g(x)=1,
h(θ)=
1−p
1
. Остаётся выразить partition через
θ
θ:
log
log
log
1−p
p
=log(−1+
1−p
1
)=θ⟺
1−p
1
=1+e
θ
.
Итак,
h(θ)=1+e
θ
, и экспоненциальный вид распределения Бернулли записывается как
1
1
+
e
θ
exp
exp
⁡
(
θ
x
−
log
1+e
θ
1
exp(θx)=exp(θx−log(1+e
θ
)).
Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках
U[a,b]? Казалось бы, да, ведь
exp
⁡
(
0
)
.
p(x)=
b−a
1
I
[a,b]
(x)exp(0).
В чём может быть подвох?
К экспоненциальным семействам относятся многие непрерывные и дискретные распределения из часто встречающихся в теории и на практике, в том числе
нормальное
N(μ,σ
2
);
распределение Пуассона
Pois(λ);
экспоненциальное
Exp(λ);
биномиальное
Bin(n,p);
геометрическое
Geom(p);
бета-распределение;
гамма-распределение;
распределение Дирихле.
Как выглядят натуральные параметры, достаточные статистики и нормализаторы этих и других распределений из экспоненциального класса, можно посмотреть на википедии.
К экспоненциальным семействам не относятся, например, равномерное распределение
U[a,b],
t
t-распределение Стьюдента, распределение Коши, смесь нормальных распределений.
Дифференцирование log-partition
Если распределение
p(x∣θ) принадлежит экспоненциальному классу,
exp
exp
p(x∣θ)=
h(θ)
g(x)
exp(θ
T
u(x))=g(x)exp(θ
T
u(x)−A(θ)),
то моменты его достаточных статистик
u
(
x
)
u(x) могут быть получены дифференцированием функции
A
(
θ
)
=
log
⁡
h
(
θ
)
A(θ)=logh(θ).
Утверждение.
∇
θ
log
logh(θ)=E
ξ∼p(x∣θ)
u(ξ).
Доказательство.
По правилу дифференцирования сложной функции имеем
∇
θ
log
logh(θ)=
h(θ)
∇
θ
h(θ)
.
Нормализатор
h
(
θ
)
h(θ) записывается в виде интеграла
exp
h(θ)=∫g(x)exp(θ
T
u(x))dx,
который мы продифференцируем внесением градиента внутрь под знак интеграла:
exp
h(θ)=∇
θ
∫g(x)exp(θ
T
u(x))dx=
exp
exp
=∫g(x)∇
θ
exp(θ
T
u(x))dx=∫g(x)u(x)exp(θ
T
u(x))dx.
Таким образом,
exp
h(θ)
∇
θ
h(θ)
=∫u(x)
p(x∣θ)
h(θ)
g(x)
exp(θ
T
u(x))
dx=E
ξ∼p(x∣θ)
u(ξ).
Если
(x)=x
i
, то в соответствии с только что доказанным частная производная
∂A(θ)
даёт
i
i-й момент распределения
p(x∣θ).
Упражнение. Вычислите производные по натуральным параметрам от log-partition для распределения Бернулли
Bern(x∣p) и нормального распределения
N(μ,σ
2
) и проверьте, что они совпадают со значениями соответствующих моментов.
Кстати, можно продифференцировать ещё раз и доказать, что
∇
θ
2
log
logh(θ)=cov(u(ξ),u(ξ)).
MLE для семейства из экспоненциального класса
Возможно, вас удивил странный и на первый взгляд не очень естественный вид
p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе.
Запишем функцию правдоподобия i.i.d. выборки
,…,x
exp
p(x
1
,…,x
n
∣θ)=h(θ)
−n
⋅(
i=1
∏
n
g(x
i
))⋅exp(θ
T
i=1
∑
n
u(x
i
)).
Её логарифм равен
log
log
log
logp(x
1
,…,x
n
∣θ)=−nlogh(θ)+
i=1
∑
n
logg(x
i
)+θ
T
i=1
∑
n
u(x
i
).
Дифференцируя по
θ
θ, получаем
∇
θ
log
log
logp(x
1
,…,x
n
∣θ)=−n∇
θ
logh(θ)+
i=1
∑
n
u(x
i
).
Приравнивая
∇
θ
log
logp(x
1
,…,x
n
∣θ) к нулю и пользуясь равенством
∇
θ
log
logh(θ)=E
ξ∼p(x∣θ)
u(ξ), находим
ξ∼p(x∣θ)
u(ξ)=
n
1
i=1
∑
n
u(x
i
).
Таким образом, теоретические матожидания всех компонент
(ξ) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для
(ξ) в качестве моментов. И в следующем пункте выяснится, что распределения из экспоненциальных семейств обладают максимальной энтропией среди тех, что имеют заданные моменты
(ξ).
Теорема Купмана—Питмана—Дармуа
Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса.
В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так.
Теорема. Пусть параметр
θ
∈
R
m
θ∈R
m
распределения
exp
(x)=
h(θ)
1
exp(θ
T
u(x)) выбран так, что
ξ∼p
θ
(x)
u(ξ)=α
для некоторого фиксированного
α
∈
R
m
α∈R
m
. Тогда распределение
(x) обладает наибольшей энтропией среди распределений
q
q с тем же носителем, для которых
ξ∼q(x)
u(ξ)=α.
Выше мы уже находили обладающее максимальной энтропией распределение на множестве натуральных чисел с заданным математическим ожиданием
μ
>
1
μ>1. Таковым оказалось геометрическое распределение
Geom(
μ
1
).
Теорема Купмана—Питмана—Дармуа позволяет сделать это гораздо быстрее.
В данном случае у нас лишь одна функция
(x)=x, которая соответствует фиксации математического ожидания
E
ξ
Eξ. Искомое дискретное распределение имеет вид
exp
=P(ξ=k)=
h(θ)
1
exp(θk)=
h(θ)
Это уже похоже на геометрическое распределение с параметром
p=1−e
θ
. Его математическое ожидание равно
1
p
p
1
, что по условию должно равняться
μ
μ. Итак, наше распределение с максимальной этропией выглядит так:
(1−
μ
1
)
k−1
,k∈N.
Пример. Среди распределений на всей вещественной прямой с заданным математическим ожиданием
μ
μ найдём распределение с максимальной энтропией.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
16.5. Независимость и условные распределения вероятностей
Предыдущий параграф
16.6. Параметрические оценки
