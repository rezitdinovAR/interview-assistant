---
title: Проксимальные методы
url: https://education.yandex.ru/handbook/ml/article/proksimalnye-metody
course: ml
chapter: 14. Оптимизация в ML
chapter_id: 14.2
---
Как оптимизировать функции потерь с $L_1$-регуляризацией
В этом разделе мы поговорим о том, как оптимизировать негладкие функции в ситуациях, когда «плохую» составляющую удаётся локализовать и она сравнительно несложная.
Проксимальная минимизация
Для того, чтобы подступиться к проксимальным методам, посмотрим на градиентный спуск с другой стороны. Для простоты рассмотрим константный размер шага
α
α. Перепишем шаг градиентного спуска следующим образом:
k+1
−x
k
=−∇f(x
k
).
Посмотрим на это уравнение по-другому. Рассмотрим функцию
x
(
t
)
x(t), равную
x
k
x
k
при
(k−1)α<t≤αk (
t
t мы будем воспринимать, как некоторый временной параметр). Тогда при
t
=
α
k
t=αk:
x(t+α)−x(t)
=−∇f(x(t)).
Теперь слева не что иное, как аппроксимация производной! Если мы устремим
α
α к нулю, то получится так называемое уравнение градиентного потока:
=−∇f(x).
Эта динамика в случае выпуклой функции
f
f сходится к точке минимума
x
∗
x
∗
из любой начальной точки при
t
→
+
∞
t→+∞. Сравнение между динамикой градиентного спуска и градиентного потока можно увидеть на следующем изображении:
Proksimalnye
Первый состоит из дискретных шагов, второй же представляет из себя непрерывный процесс.
Нетрудно осознать физический смысл динамики
=−∇f(x): маленькое тело скатывается по склону графика функции так, что в любой момент её скорость совпадает с антиградиентом, то есть оно катится по направлению наискорейшего спуска.
Теперь представим, что мы сейчас занимается не машинным обучением, а численными методами. Перед нами есть обыкновенное дифференциальное уравнение (ОДУ), и его надо решить. Одним из численных методов решения ОДУ (более стабильным, чем обычная схема Эйлера) является обратная схема Эйлера (backward Euler scheme):
k+1
−x
k
=−∇f(x
k+1
).
В обратной схеме Эйлера мы делаем градиентный спуск, только градиент смотрим не в текущей точке (как было бы в обычной схеме Эйлера), а буквально в будущей. Занятная идея, только вот напрямую выразить
x
k
+
1
x
k+1
из этого уравнения не получится. Нужно поступить чуть хитрее. Заметим, что
k+1
(x−x
k
)
i
2
x
k+1
Это позволяет нам сказать, что весь вектор
k+1
−x
k
является градиентом функции
g(u)=
2α
k
1
∥u−x
k
∥
2
, посчитанном в точке
x
k
+
1
x
k+1
. Тогда получаем, что
x
k
+
1
x
k+1
удовлетворяет следующему условию:
∇(g(u)+f(u))(x
k+1
)=0.
Если функция
f
(
x
)
f(x) выпуклая, то
f(x)+g(x) тоже выпуклая, и её стационарная точка будет точкой минимума. Стало быть,
x
k
+
1
x
k+1
можно высчитывать по формуле
x
k
+
1
=
arg
⁡
min
k+1
=arg
u
min
{f(u)+
2α
k
1
∥u−x
k
∥
2
}.
Определим прокс-оператор следующим образом:
arg
⁡
min
prox
f
(x)=argmin{f(u)+
2
1
∥u−x∥
2
}.
Тогда, поскольку умножение на
>0 внутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему:
x
k
+
1
=
arg
⁡
min
k+1
=argmin{α
k
(f(u)+
2α
k
1
∥u−x∥
2
)}=
arg
⁡
min
argmin{α
k
f(u)+
2
1
∥u−x∥
2
}=prox
Итеративный процесс
k+1
=prox
α
k
f
(x
k
) называется методом проксимальной минимизации. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы должны решать задачу оптимизации:
min
min
f(u)+
2α
k
1
∥u−x
k
∥
2
Если
f
f выпуклая, нам есть, что ответить: наличие второго слагаемого гарантирует сильную выпуклость задачи, то есть она решается достаточно эффективно. Но если
f
f не является выпуклой, то мы ничего не достигли этой модификацией.
Композитная оптимизация, проксимальный градиентный метод (PGM)
Чтобы понять, зачем нам понадобилась проксимальная оптимизация, рассмотрим оптимизацию функций вида
min
min
{f(x)=g(x)+h(x)},
где
g
(
x
)
g(x) – это гладкая функция, а
h
(
x
)
h(x) – это функция, для которой прокс-оператор считается аналитически. Воспользуемся следующим трюком: по
g
g мы совершим градиентный шаг, а по
h
h – проксимальный. Получаем следующую итеративную процедуру:
k+1
=prox
∇g(x
k
));
Эта процедура определяет так называемый проксимальный градиентный метод (Proximal Gradient Method, PGM), который может использоваться, например, для решения задачи регрессии с
ℓ
1
ℓ
1
-регуляризацией.
ISTA (Iterative Shrinkage-Thresholding Algorithm)
Теперь решим конкретную задачу
ℓ
1
ℓ
1
-регрессии. Она выглядит следующим образом:
min
⁡
w
.
∥y−Xw∥
2
2
+λ∥w∥
1
→
w
min
.
Мы хотим применить PGM к этой задаче, для этого нужно научиться вычислять прокс-оператор для
ℓ
1
ℓ
1
-нормы. Проделаем эту операцию:
arg
⁡
min
prox
α∥⋅∥
1
(x)=arg
u
min
{∥u∥
1
+
2α
1
∥u−x∥
2
2
}=
=
arg
⁡
min
=arg
u
min
{
i=1
Заметим, что каждое слагаемое зависит только от одной координаты. Это значит, что каждую координату мы можем прооптимизировать отдельно и получить
d
d одномерных задач минимизации вида
arg
⁡
min
arg
u
i
min
{∣u
Решение такой одномерной задачи записывается в виде функции soft thresholding:
prox
α∥⋅∥
1
(x)
∣≤α
x
i
≤−α
Тогда мы получаем следующий алгоритм для
ℓ
1
ℓ
1
-регрессии, которые называются Iterative Shrinkage-Thresholding Algorithm (ISTA):
w = normal(0, 1)                                            # инициализация
repeat S times:                                             # другой вариант: while abs(err) > tolerance
f = X.dot(w)                                            # посчитать предсказание
delta = f - y                                           # посчитать отклонение предсказания
grad = 2 * X.T.dot(delta) / n                           # посчитать градиент
w_prime = w - alpha * grad                              # считаем веса, которые отправим в прокс
for i in range(d):
w[i] = soft_threshold(w_prime[i], alpha * llambda)  # вычисляем прокс
Заметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат.
Конкретно этот метод не применяется на практике, но используются его вариации. Например, статья, которая указана в параграфе про линейные модели о том, как работало предсказание CTR в google в 2012 году, также базируется на вычислении soft threshold как прокс-оператора.
Общие выводы
Подытожим все вышесказанное:
Проксимальные методы – теоретически интересная идея для выпуклой оптимизации, которая должна давать более численно стабильные алгоритмы.
Проксимальные методы позволяют достаточно эффективно решать задачи композитной оптимизации, в частности,
ℓ
1
ℓ
1
-регуляризованную задачу регрессии. Более того, используемые на практике решения задачи
ℓ
1
ℓ
1
-регуляризованной регрессии так или иначе базируются на идее ISTA.
Также есть попытки использовать проксимальные методы для более сложных моделей. Например, статья о применении их в нейросетях.
Кроме того, имеются применения проксимальных методов для построения распределенных алгоритмов. Все подробности можно найти в монографии Neal Parikh и Stephen Boyd, мы же только привели применение этих идей в машинном обучении.
Знак вопроса
Пройдите квиз по параграфу
Чтобы закрепить пройденный материал
Сообщить об ошибке
Предыдущий параграф
14.1. Оптимизация в ML
Как найти оптимум функции потерь: от градиентного спуска до Adam
Следующий параграф
14.3. Методы второго порядка
От метода Ньютона до LBFGS
