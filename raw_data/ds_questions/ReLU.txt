30.11.2025, 20:24

ReLU

Карьерная платформа

ReLU

Вопросы

https://gernar.ru/plus/questions/14bee738-d69b-8182-ae9b-e0333196b8e9

1/2





30.11.2025, 20:24

ReLU

Формула ReLU.

Посмотреть ответ

Ответ

Формула функции активации ReLU (Rectified Linear Unit) выглядит следующим образом: f ( x) = max(0, x) Это означает, что функция ReLU возвращает , x

если

x положительно, и возвращает 0, если

x

отрицательно или равно нулю.

Ссылки для изучения

1. Слои активации в нейронных сетях

© 2023 —2025 Вадим Новосёлов

Ютуб канал

Телеграм канал

https://gernar.ru/plus/questions/14bee738-d69b-8182-ae9b-e0333196b8e9

2/2





