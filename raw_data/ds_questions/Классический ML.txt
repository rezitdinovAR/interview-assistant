Классический ML


Какие основные типы задач машинного обучения существуют?





Ответ


Основные типы задач машинного обучения:


1. Обучение с учителем (Supervised Learning): модели

обучаются на размеченных данных, где каждому

входному примеру соответствует выходной ответ. К

задачам относятся классификация и регрессия.

2. Обучение без учителя (Unsupervised Learning): модели

обучаются на неразмеченных данных без предоставления

выходной информации. К задачам относятся

кластеризация, снижение размерности и ассоциативное

обучение.

3. Полуобученное обучение (Semi-supervised Learning): использует как размеченные, так и неразмеченные

данные для обучения модели.

4. Обучение с подкреплением (Reinforcement Learning): агент обучается взаимодействовать с окружающей

средой, чтобы выполнить определенную задачу,

максимизируя некоторую награду.





Ссылки для изучения


1. Введение в машинное обучение





Что такое алгоритмы без учителя в машинном обучении?



Посмотреть ответ





Ответ


Алгоритмы без учителя в машинном обучении предназначены

для работы с неразмеченными данными, то есть данными, где

нет заранее определенных выходных меток или ответов. Они

позволяют находить в данных закономерности, структуры или

группы, не требуя заранее известных ответов. К основным

задачам алгоритмов без учителя относятся кластеризация, снижение размерности и ассоциативное обучение.





Ссылки для изучения


1. Введение в машинное обучение



Что такое алгоритмы без учителя в машинном обучении?



Посмотреть ответ





Ответ





Алгоритмы без учителя в машинном обучении предназначены

для работы с неразмеченными данными, то есть данными, где

нет заранее определенных выходных меток или ответов. Они

позволяют находить в данных закономерности, структуры или

группы, не требуя заранее известных ответов. К основным

задачам алгоритмов без учителя относятся кластеризация, снижение размерности и ассоциативное обучение.





Ссылки для изучения


1. Введение в машинное обучение



асскажите о принципе работы линейной регрессии, её преимуществах, функции потерь, используемой для линейной регрессии, и проблемах, связанных с большими значениями весовых коэффициентов.



Посмотреть ответ





Ответ


Линейная регрессия - это метод статистического

моделирования, используемый для анализа отношений между

зависимой переменной и одной или несколькими

независимыми переменными. Принцип работы линейной

регрессии заключается в поиске линейной зависимости между

независимыми и зависимой переменными путем подбора

оптимальных весовых коэффициентов.



Преимущества линейной регрессии включают:

1. Простота и интерпретируемость: линейные модели легко

интерпретировать и объяснить.

2. Эффективность: линейная регрессия имеет низкую

вычислительную сложность и может быть быстро обучена

на больших наборах данных.

3. Гибкость: линейная регрессия может быть легко

расширена для учета сложных отношений между

переменными, включая полиномиальные и

взаимодействующие признаки.



Функция потерь, обычно используемая для линейной

регрессии, - это среднеквадратичная ошибка (MSE), которая

измеряет среднеквадратичное отклонение предсказанных

значений от фактических значений.



Одной из проблем, связанных с большими значениями

весовых коэффициентов, является переобучение модели.

Большие веса могут привести к чрезмерной адаптации к

обучающим данным, что ухудшит обобщающую способность

модели на новых данных. Для борьбы с этой проблемой часто

используются методы регуляризации, такие как L1 (Lasso) и L2

(Ridge) регрессии, которые штрафуют за большие значения

весовых коэффициентов, помогая уменьшить их размер и

предотвратить переобучение.





Ссылки для изучения


1. Линейная регрессия в ML





Почему у линейной регрессии функция потерь именно квадратичная, а не

кубическая, с четвертой или пятой степенью?

Посмотреть ответ





Ответ


Линейная регрессия использует квадратичную функцию


потерь, потому что она является удобной математической

формой для оптимизации и имеет несколько важных свойств: 1. Простота: Функция потерь в форме квадрата разности

между предсказанными и фактическими значениями

делает вычисления более простыми и эффективными.

2. Выпуклость: Функция потерь в форме квадрата является

выпуклой функцией, что означает, что у нее есть один

глобальный минимум. Это облегчает процесс

оптимизации.

3. Интерпретируемость: Минимизация квадратичной

функции потерь приводит к оценкам параметров модели, которые можно легко интерпретировать, так как они

имеют прямую связь с наиболее вероятным набором

параметров данных.





4. Математические свойства: Многие методы оптимизации, такие как метод наименьших квадратов (МНК), легко

применяются к квадратичной функции потерь.



Хотя в некоторых случаях могут быть использованы другие

функции потерь, квадратичная функция потерь является

стандартным выбором для линейной регрессии из-за своей

эффективности и удобства.





Ссылки для изучения


1. Линейная регрессия в ML





В чём основная проблема линейной регрессии в машинном обучении?

Посмотреть ответ





Ответ


Основная проблема линейной регрессии заключается в том, что она может плохо справляться с моделированием сложных

нелинейных зависимостей между признаками и целевой

переменной. Линейная регрессия предполагает линейную

связь между признаками и целевой переменной, что может





быть недостаточно для точного предсказания в случае

сложных данных.





Ссылки для изучения


1. Линейная регрессия в ML





Аналитическая регрессия что это такое, как решать?

Посмотреть

ответ





Ответ


Аналитическая регрессия относится к использованию


регрессионного анализа для определения зависимости одной

переменной от другой на основе математических методов.

Часто применяются такие модели как линейная регрессия, нелинейная регрессия, логистическая регрессия.





Ссылки для изучения


1. Линейная регрессия в ML





Что представляет собой Теорема Гаусса-Маркова?





Ответ


Теорема утверждает, что при выполнении определённых

условий (линейности модели, гомоскедастичности, отсутствия

автокорреляции и пр.), линейные оценки методом наименьших

квадратов являются лучшими линейными несмещёнными

оценками (BLUE).





Ссылки для изучения


1. Линейная регрессия в ML



Чем тестовая выборка отличается от валидационной?

Посмотреть

ответ





Ответ


Тестовая выборка и валидационная выборка являются частями

данных, которые используются в машинном обучении для

оценки качества модели. Однако их роли и цели отличаются: Тестовая выборка (Test Set):





● Используется для окончательной оценки качества модели

после ее обучения и настройки.

● Никогда не используется в процессе обучения или

настройки модели, чтобы избежать переобучения.

● Помогает оценить, насколько хорошо модель обобщает

данные, которые она ранее не видела.



Валидационная выборка (Validation Set):

● Используется во время обучения модели для настройки

гиперпараметров и оценки ее качества на данных,

которые не использовались в обучении.

● Помогает выбирать лучшие параметры модели и

предотвращать переобучение.

● Обычно используется для выбора лучшей модели из

нескольких вариантов.





Ссылки для изучения


1. Валидация моделей в ML

Чем логистическая регрессия отличается от линейной регрессии?



Посмотреть ответ





Ответ





Логистическая регрессия используется для задач

классификации, в то время как линейная регрессия - для задач

регрессии. В логистической регрессии используется

логистическая функция для предсказания вероятности

принадлежности к определенному классу, в то время как в

линейной регрессии прогнозируется непрерывное числовое

значение.





Ссылки для изучения


1. Отличие между логистической и линейной регрессией





Чем отличается градиентный бустинг от случайного леса?



Посмотреть ответ





Ответ


Градиентный бустинг строит последовательность деревьев, каждое из которых исправляет ошибки предыдущего, тогда как

случайный лес строит множество независимых деревьев, каждое из которых обучается на случайном подмножестве

данных и признаков.





Ссылки для изучения


1. Ансамбли моделей





Почему некоторые предпочитают использовать линейную регрессию вместо

деревьев решений?

Посмотреть ответ





Ответ


Некоторые предпочитают использовать линейную регрессию


вместо деревьев решений из-за следующих причин:

● Простота интерпретации и понимания результатов

модели.

● Эффективность на больших наборах данных и при

наличии линейных зависимостей между признаками и

целевой переменной.

● Меньшая склонность к переобучению, особенно при

ограниченном количестве данных.





Ссылки для изучения


1. Ансамбли моделей





Каковы различия между алгоритмами k-Nearest Neighbors (kNN) и k-Means?

Посмотреть ответ





Ответ


k-Nearest Neighbors (kNN) - это алгоритм классификации или

регрессии, который основывается на принципе "ближайших

соседей", используя расстояние между точками данных.



k-Means - это алгоритм кластеризации, который группирует

точки данных в заданное количество кластеров, минимизируя

среднеквадратичное расстояние между точками кластера и их

центроидами.





Ссылки для изучения


1. kNN в ML





Что будешь делать, если тебе надо обработать рукописный код (просил

именно какие модели буду использовать).

Посмотреть ответ





Ответ


Для обработки рукописного кода можно использовать

следующие модели машинного обучения:

1. CNN (Convolutional Neural Networks): Эффективны для

распознавания визуальных паттернов в изображениях, включая рукописный текст.

2. RNN (Recurrent Neural Networks): Подходят для работы с

последовательными данными, что делает их хорошим

выбором для интерпретации последовательности

символов в рукописных строках.

3. LSTM (Long Short-Term Memory): Разновидность RNN, особенно эффективная для длинных

последовательностей данных и сохранения контекста в

тексте.

4. CRNN (Convolutional Recurrent Neural Network): Комбинирует CNN для извлечения признаков с RNN для

обработки последовательностей, оптимально для

распознавания рукописного текста.

Transformer и BERT модели: Эти модели, основанные на

механизме внимания, могут быть дообучены для обработки

последовательностей символов, полученных после

предварительной обработки изображений рукописного текста.

5.





Выбор конкретной модели или комбинации моделей зависит от

специфики задачи и доступности обучающих данных.





Ссылки для изучения


1. Распознавание рукописного текста





Какие есть еще линейные модели кроме лин. и лог. регрессий?



Посмотреть ответ





Ответ


Помимо линейной и логистической регрессий, существуют и

другие линейные модели:

1. Ridge регрессия: Регрессионная модель, которая вводит

штраф на коэффициенты модели с целью снижения

переобучения.

2. Lasso регрессия: Также регрессионная модель, которая

добавляет штраф к абсолютным значениям

коэффициентов, что может привести к отбору признаков.

3. ElasticNet регрессия: Комбинация Ridge и Lasso регрессий, которая вводит штрафы на коэффициенты как

по их абсолютным значениям, так и по их квадратам.





4. Метод опорных векторов (SVM): Линейная модель для

задач классификации и регрессии, которая стремится

найти гиперплоскость максимального зазора между

классами.

5. Линейная дискриминантный анализ (LDA): Статистический

метод, который моделирует распределение признаков в

каждом классе и использует его для принятия решений.

6. Обобщенные линейные модели (GLM): Класс моделей, который обобщает линейные модели, позволяя выбирать

различные функции связи и распределения ошибок.





Ссылки для изучения


1. Линейные модели в ML





Чем принципиально отличаются случайный лес и град. бустинг?

Посмотреть ответ





Ответ


Случайный лес и градиентный бустинг - это два разных

подхода к построению ансамблей моделей машинного

обучения:

1. Случайный лес (Random Forest):



● Строит ансамбль деревьев решений, каждое из

которых обучается независимо на случайной

подвыборке обучающих данных.

● Каждое дерево в случайном лесу строится на

основе случайного подмножества признаков.

● Прогнозы получаются путем усреднения

прогнозов всех деревьев (для регрессии) или

путем голосования (для классификации).

● Случайный лес обладает хорошей

устойчивостью к переобучению и хорошей

способностью к обобщению на новые данные.

2. Градиентный бустинг (Gradient Boosting):

● Построение ансамбля деревьев решений, где

каждое последующее дерево исправляет

ошибки предыдущего.

● На каждом шаге строится новое дерево,

которое предсказывает остатки предыдущей

модели.

● Прогнозы получаются путем суммирования

прогнозов всех деревьев.

● Градиентный бустинг имеет меньше

случайности, чем случайный лес, и может

достигать более высоких результатов при

правильной настройке параметров, но более

чувствителен к переобучению.





Ссылки для изучения


1. Градиентный бустинг





В чём разница между случайным лесом и деревом решений в машинном

обучении?

Посмотреть ответ





Ответ


Основное различие между случайным лесом и деревом

решений заключается в том, что случайный лес является

ансамблевым методом, состоящим из множества деревьев

решений, в то время как дерево решений - это одиночный

алгоритм. В случайном лесе каждое дерево строится

независимо на подмножестве данных, а затем результаты

объединяются для получения итогового предсказания. Это

позволяет снизить переобучение и повысить устойчивость

модели.





Ссылки для изучения


1. Ансамбли моделей





Есть три модели (бустинг, логрег и рандомфорест) на какие критерии следует

обратить внимание при выборе модели?

Посмотреть ответ





Ответ


При выборе модели следует обратить внимание на следующие

критерии:

1. Производительность модели: Оцените скорость обучения

и предсказания для каждой модели, особенно если важна

скорость работы в реальном времени.

2. Обобщающая способность: Проверьте, как модели

обобщают данные на отложенной выборке или с

помощью кросс-валидации. Избегайте переобучения,

выбирая модель с наилучшими показателями на тестовых

данных.

3. Интерпретируемость: Некоторые модели, такие как

логистическая регрессия, легче интерпретировать, чем

другие, например, бустинг или случайный лес. Если

важно понимать, какие признаки влияют на прогнозы, это

стоит учитывать.

4. Устойчивость к выбросам: Оцените, насколько модели

устойчивы к выбросам в данных. Некоторые модели могут

быть более чувствительны к выбросам, чем другие.

5. Гибкость: Рассмотрите, насколько легко модель можно

настроить и насколько она гибкая в использовании.

Некоторые модели могут требовать меньше тюнинга

гиперпараметров или быть более подходящими для

конкретных типов данных.





Ссылки для изучения


1. Ансамбли моделей





В каком случае линейная регрессия будет лучше бустинга?



Посмотреть ответ





Ответ


Линейная регрессия может быть лучше бустинга в случае, когда данные действительно линейно зависят от предикторов и

нет необходимости в модели с высокой сложностью. Если

взаимосвязи в данных просты и линейные, то линейная

регрессия может обеспечить достаточно хорошее качество

прогнозов при меньшем числе параметров и менее высокой

вычислительной сложности.





Ссылки для изучения


1. Ансамбли моделей





Что такое градиент?

Посмотреть ответ





Ответ


Градиент представляет собой вектор, который указывает на

направление наибольшего увеличения функции. В контексте

оптимизации и машинного обучения, градиент используется

для определения направления, в котором следует изменить

параметры модели, чтобы уменьшить функцию потерь или

ошибку предсказания.





Ссылки для изучения


1. Градиент в математике





Если у вас есть выбор между градиентным спуском (GD) и стохастическим

градиентным спуском (SGD), что лучше сработает?

Посмотреть ответ





Ответ





Это зависит от размера данных и времени обучения.

Градиентный спуск (GD) обновляет параметры модели по всей

обучающей выборке, что может быть медленным на больших

наборах данных. Стохастический градиентный спуск (SGD) обновляет параметры по одному случайному экземпляру за

раз, что обычно быстрее, но может быть менее стабильным.

Таким образом, выбор зависит от компромисса между

точностью и скоростью обучения.





Ссылки для изучения


1. Стохастический и обычный градиентный спуск





Как сделать, чтобы при каждом запуске кода, модель, обучаемая при помощи

градиентного спуска, сходилась к одной и той же точке?

Посмотреть

ответ





Ответ


Чтобы гарантировать, что модель, обучаемая при помощи

градиентного спуска, сходится к одной и той же точке при

каждом запуске кода, необходимо установить одинаковые

начальные значения параметров модели и использовать





одинаковый порядок обучающих данных. Также важно

убедиться, что все остальные условия, такие как функция

потерь, гиперпараметры и алгоритм оптимизации, остаются

неизменными при каждом запуске.





Ссылки для изучения


1. Градиентный спуск в ML





Как делается один шаг градиентного спуска для обновления весов?



Посмотреть ответ





Ответ


Один шаг градиентного спуска для обновления весов

выполняется следующим образом:

1. Рассчитывается градиент функции потерь по отношению

к каждому параметру модели.

2. Градиенты умножаются на скорость обучения (learning rate), которая определяет размер шага.

3. Полученные значения добавляются к текущим значениям

параметров модели, обновляя их.





Ссылки для изучения


1. Градиентный спуск в ML

В чем смысл обучения на антиградиенте?

Посмотреть ответ





Ответ


Обучение на антиградиенте - это ключевой момент в

градиентном бустинге. Он заключается в том, что на каждом

шаге алгоритм бустинга обучается предсказывать остатки

(направление антиградиента) предыдущей модели. Это

позволяет новой модели сконцентрироваться на тех областях

данных, где предыдущая модель делает большие ошибки.

Таким образом, каждая последующая модель в ансамбле

исправляет ошибки предыдущих моделей, что приводит к

пошаговому улучшению качества прогнозов.





Ссылки для изучения


1. Градиентный спуск в ML





Какие есть преимущества и недостатки у градиентного бустинга?



Посмотреть ответ





Ответ


Преимущества градиентного бустинга:


1. Высокая точность: Градиентный бустинг обычно

обеспечивает высокую точность предсказаний за счет

комбинации нескольких слабых моделей в сильный

ансамбль.

2. Устойчивость к переобучению: Благодаря технике

пошагового улучшения, градиентный бустинг склонен к

низкому уровню переобучения.

3. Адаптивность к данным: Алгоритм способен эффективно

обрабатывать данные с различными типами признаков и

распределениями.



Недостатки градиентного бустинга:

1. Чувствительность к гиперпараметрам: Настройка

гиперпараметров градиентного бустинга может

потребовать значительных усилий и вычислительных

ресурсов.

2. Склонность к переобучению на больших данных: В

случае больших объемов данных и большого числа

деревьев может возникнуть переобучение.





3. Время обучения: Градиентный бустинг может быть более

медленным в обучении, особенно если используются

большие наборы данных или сложные модели.





Ссылки для изучения


1. Градиентный спуск в ML





Как именно получается итоговый ответ при градиентном бустинге?



Посмотреть ответ





Ответ


В градиентном бустинге итоговый ответ получается путем

суммирования предсказаний всех моделей (обычно деревьев

решений), которые составляют ансамбль. Каждая модель

делает свое предсказание на основе обученного алгоритма, и

их предсказания объединяются с помощью весов, которые

могут быть заданы заранее или подобраны в процессе

обучения. В итоге получается сумма предсказаний всех

моделей, которая и является итоговым ответом ансамбля

градиентного бустинга.





Ссылки для изучения


1. Градиентный спуск в ML





Может ли градиентый бустинг переобучиться с увеличением количества

деревьев?

Посмотреть ответ





Ответ


Да, градиентный бустинг может переобучиться с увеличением

количества деревьев. При добавлении большего числа

деревьев к ансамблю, модель может начать "запоминать"

тренировочные данные, что может привести к переобучению.

Чтобы предотвратить это, важно правильно настраивать

гиперпараметры модели, такие как максимальная глубина

деревьев, скорость обучения и количество деревьев.





Ссылки для изучения


1. Градиентный спуск в ML





Какой глубины деревья используются в градиентном бустинге?



Посмотреть ответ





Ответ


В градиентном бустинге глубина деревьев обычно

ограничивается, чтобы предотвратить переобучение и

повысить обобщающую способность модели. Точное значение

глубины деревьев зависит от конкретной задачи, набора

данных и других гиперпараметров модели. Обычно

используются деревья небольшой глубины, например, от 3 до

6 уровней, чтобы каждое дерево было достаточно простым и

недообученным, но при этом способным делать значимые

предсказания.





Ссылки для изучения


1. Градиентный спуск в ML





Как изменится величина метрики ошибки, такой как MAE, если удалить первое

дерево из ансамбля бустинга? А если удалить последнее?



Посмотреть ответ





Ответ


Если удалить первое дерево из ансамбля бустинга, то общее

предсказание модели будет зависеть только от последующих

моделей. Это может привести к изменению предсказаний и, следовательно, к изменению величины метрики ошибки, такой

как MAE. Однако, величина изменения зависит от того, насколько важным оказался вклад первого дерева в общее

предсказание.



Если удалить последнее дерево из ансамбля бустинга, то

также изменится общее предсказание модели и,

соответственно, величина метрики ошибки. Опять же, величина

изменения зависит от того, насколько важным был вклад

последнего дерева в общее предсказание.





Ссылки для изучения


1. Градиентный спуск в ML





Когда бустинг менее эффективен, чем линейная регрессия?



Посмотреть ответ





Ответ


Бустинг может быть менее эффективен, чем линейная

регрессия, в следующих случаях:

1. Линейная зависимость: Если зависимость между

признаками и целевой переменной близка к линейной, то

линейная регрессия может быть более эффективной, так

как она может моделировать эту зависимость

непосредственно.

2. Простота модели: Если данные хорошо

аппроксимируются простой моделью, то бустинг, который

строит сложные ансамбли деревьев, может быть

избыточным и сложным для данной задачи.

3. Большие объемы данных: В случае больших объемов

данных и высоких требований к вычислительным

ресурсам, линейная регрессия может быть более

эффективной, так как обучение модели может быть более

быстрым и менее затратным.

4. Простота интерпретации: Линейная регрессия обладает

простой интерпретируемостью, что делает ее

предпочтительной в ситуациях, где важно понимать вклад

каждого признака в прогноз.





Ссылки для изучения


1. Градиентный спуск в ML





Как происходит разбиение выборки в узле дерева у бустинга для задачи

регрессии?

Посмотреть ответ





Ответ


В бустинге для задачи регрессии обычно используются

решающие деревья. Разбиение выборки в узле дерева

происходит следующим образом:

1. Выбор признака и порога разбиения: Для каждого узла

дерева выбирается оптимальный признак и пороговое

значение, которые минимизируют ошибку на обучающей

выборке.

2. Разбиение выборки: Выборка разбивается на две

подвыборки в соответствии с выбранным признаком и

порогом: одна подвыборка содержит объекты, для

которых значение выбранного признака меньше или

равно порогу, а другая подвыборка содержит объекты, для которых значение признака больше порога.

3. Построение дерева: Для каждого созданного подузла

дерева рекурсивно повторяется процесс выбора признака





и порога разбиения, пока не будет достигнут критерий

остановки, такой как максимальная глубина дерева или

минимальное количество объектов в узле.

4. Оценка весов объектов: В бустинге каждый объект

обучающей выборки имеет вес, который определяет его

вклад в обучение модели. На первом шаге все объекты

имеют одинаковые веса, а затем веса объектов

пересчитываются в соответствии с ошибками

предыдущих моделей.

5. Обучение слабых учеников: На основе оценок весов

объектов и текущего состояния модели обучается слабый

учитель, который старается уменьшить ошибку ансамбля.



Таким образом, разбиение выборки в узле дерева у бустинга

для задачи регрессии осуществляется с учетом выбранного

признака и порогового значения, а также с учетом весов

объектов и текущего состояния ансамбля моделей.





Ссылки для изучения


1. Градиентный спуск в ML





Рассказать про бустинги, что как зачем, умеют ли решать все то же самое, что

и нейронки или нет, принципы работы и различные виды.



Посмотреть ответ





Ответ


Бустинг - это ансамблевый метод машинного обучения, который объединяет несколько слабых моделей (обычно

деревьев решений) в одну сильную модель. Основная идея

заключается в последовательном обучении моделей, каждая

из которых исправляет ошибки предыдущей.

● Принцип работы: Бустинг строит последовательность

моделей, обучая каждую следующую модель таким

образом, чтобы она исправляла ошибки предыдущей

модели.

● Зачем: Бустинг используется для решения различных

задач машинного обучения, включая классификацию и

регрессию. Он позволяет получить высокую точность

предсказаний за счет комбинирования нескольких слабых

моделей.

● Умение решать то же самое, что и нейронные сети: Бустинг и нейронные сети могут использоваться для

решения широкого спектра задач, но у них разные

подходы к обучению и архитектуры. Нейронные сети

обычно используются для задач, требующих сложных

нелинейных зависимостей или обработки больших

объемов данных, в то время как бустинг часто

используется для обучения на небольших и средних

наборах данных с хорошей интерпретируемостью.

● Виды бустинга: Наиболее популярные алгоритмы

бустинга включают AdaBoost, Gradient Boosting Machine (GBM), XGBoost, LightGBM и CatBoost. Каждый из этих

алгоритмов имеет свои особенности, но общая идея





остается той же - последовательное улучшение модели

путем устранения ее ошибок.





Ссылки для изучения


1. Градиентный спуск в ML



Как работает градиентный бустинг?

Посмотреть ответ





Ответ


Градиентный бустинг - это ансамблевый метод машинного

обучения, который строит предсказательную модель в виде

ансамбля слабых моделей, обычно деревьев решений, с

помощью итеративного улучшения. Основная идея

заключается в том, чтобы последовательно добавлять новые

модели к ансамблю, каждая из которых исправляет ошибки

предыдущей модели. Процесс обучения градиентного бустинга

можно описать следующим образом:

1. Инициализация модели: Начинаем с базовой модели, обычно простой модели, которая предсказывает среднее

значение целевой переменной для всех наблюдений.





2. Вычисление остатков: Для каждого наблюдения

вычисляем остатки между фактическим значением

целевой переменной и предсказанием текущей модели.

3. Построение новой модели для остатков: Обучаем новую

модель (например, дерево решений) на остатках

предыдущей модели. Новая модель настраивается таким

образом, чтобы минимизировать остатки.

4. Добавление модели к ансамблю: Предсказания новой

модели добавляются к предыдущим предсказаниям с

учетом некоторого коэффициента, называемого темпом

обучения (learning rate).

5. Итерации: Шаги 2-4 повторяются до тех пор, пока не

будет достигнуто заданное количество моделей или пока

не будет достигнуто определенное значение метрики

качества.



Градиентный бустинг обеспечивает высокую точность и

устойчивость за счет комбинирования нескольких слабых

моделей в сильный ансамбль. Он широко используется в

различных задачах, таких как классификация, регрессия и

ранжирование.





Ссылки для изучения


1. Градиентный спуск в ML

Что такое методы оптимизации в машинном обучении?

Посмотреть

ответ





Ответ


Методы оптимизации в машинном обучении - это алгоритмы, которые используются для настройки параметров моделей с

целью минимизации функции потерь. Они определяют способ

обновления параметров модели на каждом шаге обучения, направляя его к оптимальным значениям. Примеры методов

оптимизации включают градиентный спуск, стохастический

градиентный спуск, методы второго порядка и адаптивные

методы оптимизации, такие как Adam и RMSProp.





Ссылки для изучения


1. Методы оптимизации ML





Как применяют оптимизацию по числам Фибоначчи?

Посмотреть

ответ





Ответ


Оптимизация по числам Фибоначчи - это метод оптимизации, который использует последовательность чисел Фибоначчи для





настройки параметров модели. В этом методе значения

параметров изменяются согласно числам Фибоначчи, что

позволяет находить оптимальные значения с помощью более

эффективного поиска, чем простой градиентный спуск. Обычно

этот метод используется в задачах оптимизации с

ограниченным числом итераций или когда доступ к градиенту

ограничен.





Ссылки для изучения


1. Методы оптимизации ML

Как строится дерево решений?

Посмотреть ответ





Ответ


Дерево решений строится путем разбиения данных на

подгруппы на основе значений признаков с целью

минимизации некоторого критерия неопределенности

(например, энтропии или критерия Джини). Процесс

построения дерева можно описать следующим образом: 1. Выбор признака для разбиения: На каждом узле дерева

выбирается признак, по которому данные будут

разделены на две или более подгруппы. Этот выбор

основывается на критерии разделения, таком как

информационная энтропия или критерий Джини.





2. Выполнение разбиения: Данные разбиваются на

подгруппы на основе выбранного признака. Каждая

подгруппа соответствует разным значениям выбранного

признака.

3. Рекурсивное построение дерева: Данный процесс

повторяется для каждой подгруппы, пока не будет

выполнен некоторый критерий останова, например,

достигнута минимальная глубина дерева или количество

элементов в узле не превышает заданное значение.

4. Определение класса (для задач классификации) или

значения (для задач регрессии): В листовых узлах дерева

определяется класс (для классификации) или значение

(для регрессии), которое будет прогнозироваться для

данных, попавших в данный лист.





Ссылки для изучения


1. Решающие деревья





Может ли дерево решений показывать вероятность?

Посмотреть

ответ





Ответ





Да, дерево решений может показывать вероятность, но это

зависит от того, как оно используется и какая методика

используется для вычисления вероятности.



В некоторых случаях, особенно в задачах классификации, дерево решений может быть модифицировано для выдачи

вероятностных оценок. Например, для бинарной

классификации вероятность может быть рассчитана как доля

положительных (или отрицательных) примеров в листовом

узле, которому принадлежит наблюдение.



Однако стандартные реализации деревьев решений, такие как

CART (Classification and Regression Trees), обычно не

предоставляют прямой способ вычисления вероятности. В

таких случаях вероятности могут быть оценены путем

применения к дереву решений калибровки вероятностей или

других методов, которые адаптируют выходные значения

дерева для предсказания вероятностей.





Ссылки для изучения


1. Решающие деревья





Как получается ответ целевой переменной в дереве решений?



Посмотреть ответ





Ответ


В дереве решений ответ целевой переменной получается

путем прохождения через структуру дерева от корня к листьям.



Каждый узел дерева содержит условие, которое проверяет

значение одного из признаков данных. В зависимости от

результата проверки условия данные направляются по одной

из ветвей дерева к следующему узлу или листу. Этот процесс

повторяется до тех пор, пока не будет достигнут листовой узел.



В листовом узле дерева содержится предсказание для

целевой переменной. В задаче классификации это может быть

конкретный класс, к которому относится наблюдение, а в

задаче регрессии - числовое значение, предсказывающее

целевую переменную для данного наблюдения.





Ссылки для изучения


1. Решающие деревья





Что представляет собой концепция энтропии в контексте деревьев решений, и

стремится ли дерево минимизировать или максимизировать ее значение в

процессе построения?

Посмотреть ответ





Ответ


В контексте деревьев решений, энтропия - это мера

неопределенности в данных. Энтропия показывает, насколько

хорошо данные разделены по целевой переменной: чем

меньше энтропия, тем более чистыми являются группы данных

в узле.



Дерево решений стремится минимизировать энтропию или

другие меры неопределенности (например, критерий Джини) в

процессе построения. Это достигается путем разбиения

данных на подгруппы таким образом, чтобы после разбиения

неопределенность в данных уменьшалась. В результате

построения дерева, узлы с низкой энтропией будут содержать

более однородные данные, что делает прогнозы более

надежными.





Ссылки для изучения





1. Решающие деревья





Какая метрика оптимизируется в регрессионном дереве при выборе разбиения

для нового узла?

Посмотреть ответ





Ответ


При выборе разбиения для нового узла в регрессионном

дереве обычно оптимизируется среднеквадратичная ошибка

(MSE) или средняя абсолютная ошибка (MAE) на основе

значений целевой переменной в подгруппах данных,

полученных в результате разбиения.



Среднеквадратичная ошибка (MSE) оптимизируется, когда

модель стремится минимизировать сумму квадратов

отклонений между прогнозируемыми значениями и

фактическими значениями целевой переменной.



Средняя абсолютная ошибка (MAE) оптимизируется, когда

модель стремится минимизировать среднее абсолютное

отклонение между прогнозируемыми значениями и

фактическими значениями целевой переменной.





В процессе построения дерева решений выбирается

разбиение, которое минимизирует значение выбранной

метрики ошибки для данного узла, таким образом, что ошибка

после разбиения будет наименьшей возможной.





Ссылки для изучения


1. Решающие деревья





За что отвечает L2 регуляризация в дереве?

Посмотреть ответ





Ответ


В дереве решений L2 регуляризация, также известная как

регуляризация Тихонова или регуляризация Ridge, обычно не

применяется напрямую, как в линейной регрессии или в

методах оптимизации с градиентным спуском.



Деревья решений, в отличие от линейных моделей, обычно не

имеют гиперпараметров, которые напрямую соответствуют





регуляризации. Однако, иногда применяются методы обрезки

деревьев, которые могут рассматриваться как форма

регуляризации, и могут влиять на сложность модели и

предотвращать переобучение.





Ссылки для изучения


1. Решающие деревья





На каком условии останавливается построение дерева решений, другими

словами, как определяется критерий завершения построения дерева?

Посмотреть ответ





Ответ


Построение дерева решений останавливается на основе

различных критериев или условий, которые определяются

заранее или в процессе построения модели. Некоторые из

распространенных критериев останова включают:

1. Глубина дерева: Построение дерева останавливается, когда достигнута максимальная глубина дерева. Это

предотвращает построение слишком сложных моделей,

которые могут привести к переобучению.





2. Минимальное количество наблюдений в листе:

Построение дерева останавливается, когда количество

наблюдений в листовом узле становится меньше

заданного порогового значения. Это помогает

предотвратить построение неподходящих узлов с

недостаточным количеством данных для надежных

прогнозов.

3. Минимальное уменьшение неопределенности:

Построение дерева может остановиться, если

дальнейшее разделение узла не приводит к достаточному

уменьшению неопределенности (например, энтропии или

критерия Джини). Это помогает предотвратить лишние

разбиения, которые не улучшают качество модели.

4. Количество узлов/листьев: Можно также задать

максимальное количество узлов или листьев в дереве.

Это также может быть критерием останова для

построения дерева.





Ссылки для изучения


1. Решающие деревья





Что будет с метрикой качества если убрать одно случайное дерево из

случайного леса и бустинга?

Посмотреть ответ





Ответ


Если убрать одно случайное дерево из случайного леса или

бустинга, то общая метрика качества модели (например, ROC-AUC или F1-score) обычно изменится незначительно, поскольку случайный лес и бустинг являются ансамблевыми

методами, то есть результат их работы формируется на основе

большого количества деревьев. Одно дерево в этом случае

вносит незначительный вклад в общую оценку качества

модели. Однако при большом количестве деревьев в ансамбле

эффект от удаления одного дерева будет незначительным.





Ссылки для изучения


1. Решающие деревья





Что лежит в листьях дерева?

Посмотреть ответ





Ответ


В листьях дерева решений содержатся прогнозы или классы

для наблюдений, которые дошли до данного листа. Каждый





лист представляет собой конечный узел дерева, который

определяет конечное решение или классификацию для

соответствующего наблюдения.





Ссылки для изучения


1. Решающие деревья

Что такое случайный лес, как строится?

Посмотреть ответ





Ответ


Случайный лес - это ансамблевый метод машинного обучения, основанный на комбинации нескольких деревьев решений. Он

использует метод бэггинга (bootstrap aggregating), чтобы

построить ансамбль деревьев решений.



Вот основные шаги построения случайного леса:

1. Выбор случайной подвыборки данных: Из обучающего

набора данных случайным образом выбирается

подвыборка данных с возвращением. Это означает, что

одно и то же наблюдение может быть выбрано несколько

раз, а другие наблюдения могут быть пропущены.

2. Построение деревьев решений: Для каждой случайной

подвыборки данных строится дерево решений. При

построении каждого дерева решений на каждом узле

выбирается случайное подмножество признаков из всех





доступных признаков. Это помогает сделать деревья

более разнообразными и уменьшает корреляцию между

деревьями.

3. Обучение деревьев решений: Для каждой случайной

подвыборки данных строится дерево решений с

использованием выбранных признаков. Каждое дерево

строится до тех пор, пока не будет выполнено какое-то

критерий останова (например, достигнута максимальная

глубина дерева или достигнуто минимальное количество

наблюдений в листе).

4. Формирование ансамбля: После построения всех

деревьев решений их результаты комбинируются для

получения окончательного прогноза. В задачах

классификации результаты обычно усредняются или

используется голосование большинства, а в задачах

регрессии результаты усредняются.





Ссылки для изучения


1. Ансамбли в машинном обучении





Каковы плюсы и минусы использования метода случайного леса?



Посмотреть ответ





Ответ





Плюсы использования метода случайного леса:

1. Устойчивость к переобучению: Благодаря случайному

выбору подмножества данных и признаков для

построения каждого дерева, случайный лес склонен к

более устойчивому обобщению на новых данных и

предотвращает переобучение.

2. Хорошая обобщающая способность: Случайный лес

часто демонстрирует хорошую производительность на

различных типах данных и задачах, включая как

классификацию, так и регрессию.

3. Способность к обработке больших объемов данных: Случайный лес способен обрабатывать большие объемы

данных эффективно и параллельно благодаря своей

схеме построения.



Минусы использования метода случайного леса:

1. Склонность к переобучению при большом количестве

деревьев: В некоторых случаях случайный лес может

переобучиться, особенно если используется слишком

большое количество деревьев или недостаточное

ограничение глубины деревьев.

2. Сложность интерпретации: Из-за использования

большого количества деревьев и случайного выбора

признаков для каждого дерева, интерпретация

случайного леса может быть сложной по сравнению с

более простыми моделями.





3. Временные затраты на обучение: Построение большого

количества деревьев в случайном лесу может

потребовать значительных вычислительных ресурсов и

времени для обучения модели.





Ссылки для изучения


1. Ансамбли в машинном обучении





Какой глубины деревья используются в методе случайного леса?



Посмотреть ответ





Ответ


В методе случайного леса обычно используются деревья

ограниченной глубины или неглубокие деревья. Ограничение

глубины деревьев помогает предотвратить переобучение и

способствует устойчивости модели. Обычно устанавливается

максимальная глубина деревьев или минимальное количество

наблюдений в листе для каждого дерева в случайном лесу. Это

позволяет создавать простые и интерпретируемые модели, которые обобщают лучше на новых данных.





Ссылки для изучения


1. Ансамбли в машинном обучении





Какое воздействие на возможное переобучение оказывает добавление еще

одного дерева в случайный лес (Random Forest) или в градиентный бустинг

(Boosting)?

Посмотреть ответ





Ответ


Добавление еще одного дерева в случайный лес или в

градиентный бустинг обычно уменьшает риск переобучения.



Случайный лес (Random Forest): Поскольку случайный лес

строится на основе ансамбля деревьев решений, добавление

нового дерева улучшает стабильность модели и снижает ее

склонность к переобучению. Каждое новое дерево вносит свой

уникальный вклад в ансамбль, усиливая обобщающую

способность модели.





Градиентный бустинг (Boosting): В градиентном бустинге

добавление нового дерева позволяет модели последовательно

улучшать прогнозы, фокусируясь на ошибках предыдущих

деревьев. Это помогает модели лучше адаптироваться к

данным и уменьшает риск переобучения. Однако следует быть

осторожным с количеством деревьев, чтобы избежать

переобучения.





Ссылки для изучения


1. Ансамбли в машинном обучении





Ваши коллеги обратили ваше внимание на то, что модель случайного леса, обученная на строго положительных значениях, теперь выдает отрицательные

результаты. Какие возможные причины этой проблемы и как ее можно

решить?

Посмотреть ответ





Ответ


Возможные причины проблемы с моделью случайного леса, выдающей отрицательные результаты:





1. Проблемы в данных: Возможно, в исходных данных

содержатся ошибки или аномалии, которые приводят к

неправильному обучению модели.

2. Переобучение: Модель случайного леса могла

переобучиться на обучающих данных, что привело к

неправильным прогнозам на новых данных.

3. Неуместное представление данных: Может

потребоваться изменить представление данных или

применить преобразования, чтобы гарантировать

положительные результаты.

4. Проблемы в настройке параметров модели: Некорректно

выбранные гиперпараметры модели могут привести к

нежелательным результатам.

5. Ошибка в коде или реализации модели: Возможно, есть

ошибка в коде или реализации модели, которая приводит

к неправильным выводам.





Ссылки для изучения


1. Ансамбли в машинном обучении





Какие изменения происходят при добавлении дерева в случайный лес?

Посмотреть ответ





Ответ


При добавлении дерева в случайный лес происходят

следующие изменения:

1. Увеличение разнообразия: Добавление нового дерева

увеличивает разнообразие модели, так как каждое дерево

строится на основе случайной подвыборки данных и

случайного подмножества признаков.

2. Усиление стабильности: Ансамбль деревьев становится

более стабильным и устойчивым к переобучению,

поскольку модель усредняет прогнозы множества

деревьев.

3. Улучшение обобщающей способности: Поскольку

случайный лес усредняет прогнозы отдельных деревьев, добавление нового дерева может улучшить обобщающую

способность модели на новых данных.

4. Увеличение сложности модели: Каждое новое дерево

увеличивает сложность модели, что может привести к

более точным прогнозам, но также может повысить риск

переобучения, если не будут применены

соответствующие методы регуляризации.





Ссылки для изучения


1. Ансамбли в машинном обучении





Какие изменения происходят при увеличении глубины деревьев в случайном

лесе?

Посмотреть ответ





Ответ


При увеличении глубины деревьев в случайном лесе

происходят следующие изменения:

1. Увеличение сложности модели: Увеличение глубины

деревьев позволяет модели захватывать более сложные

взаимосвязи между признаками и целевой переменной.

2. Потенциальное улучшение точности: Глубокие деревья

способны делать более точные прогнозы на обучающих

данных за счет лучшего разделения их на классы или

категории.

3. Повышение риска переобучения: Однако увеличение

глубины деревьев может также увеличить риск

переобучения модели, особенно если данных

недостаточно или отсутствуют методы регуляризации.

4. Увеличение вычислительной сложности: Более глубокие

деревья требуют больше вычислительных ресурсов для

обучения и прогнозирования, что может повлиять на

производительность модели.





Ссылки для изучения


1. Ансамбли в машинном обучении





Как происходит подбор подмножества признаков для дерева случайного леса -

один раз перед построением дерева или на каждом разбиении?



Посмотреть ответ





Ответ


При построении дерева в случайном лесу подмножество

признаков подбирается на каждом разбиении. Каждый узел

дерева рассматривает только случайное подмножество

признаков для выбора наилучшего разделения, что

способствует уменьшению коррелированности деревьев в

ансамбле и повышению его разнообразия.





Ссылки для изучения


1. Ансамбли в машинном обучении

Какие существуют методы борьбы с переобучением?

Посмотреть

ответ





Ответ


Существует несколько методов борьбы с переобучением:





1. Кросс-валидация: Разделение данных на обучающий и

тестовый наборы для оценки обобщающей способности

модели.

2. Регуляризация: Использование методов регуляризации, таких как L1 или L2 регуляризация, для контроля

сложности модели путем штрафования больших весовых

коэффициентов.

3. Уменьшение сложности модели: Ограничение глубины

деревьев в случайных лесах или числа параметров в

нейронных сетях, чтобы предотвратить излишнее

обучение.

4. Использование ансамблей моделей: Объединение

нескольких моделей в ансамбль, таких как случайный лес

или градиентный бустинг, для улучшения обобщающей

способности и снижения риска переобучения.

5. Исключение признаков: Удаление избыточных или

неинформативных признаков из набора данных.

6. Увеличение объема данных: Увеличение размера

обучающего набора данных может помочь модели лучше

обобщить на новые данные и снизить риск переобучения.





Ссылки для изучения


1. Переобучение в ML



Уменьшает ли ансамбль стекинга смещение модели?

Посмотреть ответ





Ответ

Да, ансамбль стекинга обычно уменьшает смещение модели.

Поскольку он комбинирует прогнозы нескольких базовых

моделей, а не привязан к предположениям конкретного типа

модели, это позволяет улучшить обобщающую способность и

снизить смещение.

Ссылки для изучения

1. Ансамбли в машинном обучении

Как можно объяснить концепцию разложения на смещение и разброс

(bias-variance decomposition) в методе случайного леса (Random Forest)?

Посмотреть ответ

Ответ

Разложение на смещение и разброс в методе случайного леса

(Random Forest) можно объяснить следующим образом:

1. Смещение (Bias): Отклонение среднего прогноза модели

от истинного значения. В случайном лесе, каждое дерево

обычно недообучается из-за использования случайных

подвыборок данных и случайных подмножеств признаков.

Это может привести к смещению прогнозов каждого





дерева, так как они могут не улавливать все важные

закономерности в данных.

2. Разброс (Variance): Вариация прогнозов модели для

разных наборов данных. В случайном лесе, каждое

дерево обучается на случайной подвыборке данных, что

может привести к большому разбросу между прогнозами

отдельных деревьев. Однако усреднение прогнозов

множества деревьев позволяет уменьшить разброс и

повысить обобщающую способность модели.



Таким образом, в случайном лесе смещение может быть

небольшим из-за среднего значения прогнозов множества

деревьев, в то время как разброс снижается благодаря

усреднению прогнозов.

Ссылки для изучения

1. Ансамбли в машинном обучении

Каковы различия между методами кодирования категориальных переменных,

такими как One-hot-encoder, Label Encoder, Helmert Encoder и Frequency

Encoder?

Посмотреть ответ

Ответ

Вот сравнение этих методов кодирования категориальных

переменных:





1. One-hot Encoder: Создает новый бинарный признак для

каждой уникальной категории исходного признака.

Используется, когда порядок категорий не имеет

значения.

2. Label Encoder: Преобразует каждую категорию в числовое

значение. Используется, когда категории имеют порядок

или можно установить отношения между ними.

3. Helmert Encoder: Создает новые признаки, значения

которых представляют собой разность между средним

значением целевой переменной для текущей категории и

средним значением для предыдущей категории. Часто

используется в регрессионных моделях.

4. Frequency Encoder: Заменяет каждую категорию на

частоту ее встречаемости в исходном признаке. Может

быть полезным, если частота категорий важна для

модели.

Ссылки для изучения

1. Как получить полезную информацию из своих категориальных

признаков





Какие методы вы используете для преобразования категориальных

переменных?

Посмотреть ответ





Ответ

Часто используемые методы для преобразования

категориальных переменных:

1. One-hot Encoding: Для переменных без упорядоченных

категорий, где каждая категория равнозначна.

2. Label Encoding: Для переменных с упорядоченными

категориями или когда порядок имеет значение.

3. Target Encoding: Если частота категорий важна для

моделирования, я использую кодирование по целевой

переменной.

Ссылки для изучения

1. Как получить полезную информацию из своих категориальных

признаков



Приведите методы преобразования ненормального распределения к

нормальному.

Посмотреть ответ





Ответ


Основные методы преобразования ненормального


распределения к нормальному:





1. Преобразование Бокса-Кокса (Box-Cox Transformation): Это семейство преобразований, которое преобразует

данные так, чтобы они приближались к нормальному

распределению. Он зависит от параметра λ, который

можно подобрать для оптимального преобразования.

2. Преобразование Йео-Джонсона (Yeo-Johnson

Transformation): Это обобщение преобразования

Бокса-Кокса, которое также может работать с

отрицательными значениями и нулями.

3. Преобразование Квантилей (Quantile Transformation): Этот метод переводит данные в нормальное

распределение с помощью функции квантилей, делая их

равномерно распределенными.

4. Преобразование Робустное (Robust Transformation): Это

метод, который использует робастные статистики для

преобразования данных, такие как медиана и

интерквартильный диапазон.





Ссылки для изучения


1. Умная нормализация данных





С проблематикой стационарности рядов, зачем она нужна - знакомы?

Опишите своими словами, в чем там есть проблема?

Посмотреть

ответ





Ответ





Конечно, стационарность временного ряда важна, потому что

она предполагает постоянство статистических свойств ряда со

временем. Это означает, что среднее значение, дисперсия и

автокорреляционная структура ряда остаются постоянными во

времени. Если ряд нестационарен, это может означать, что

статистические свойства ряда меняются с течением времени, что делает прогнозирование и анализ более сложными и

менее точными. Нестационарность может проявляться в

трендах, сезонных изменениях, изменяющейся дисперсии или

корреляции.





Ссылки для изучения


1. Анализ временных рядов

Как обучают треплетам?

Посмотреть ответ





Ответ


Обучение троек (треплетов) - это метод обучения нейронных

сетей для задачи сравнения, например, в задачах

ранжирования или распознавания лиц. Каждая тройка состоит

из якорного изображения, положительного примера

(изображения того же класса) и отрицательного примера

(изображения другого класса).





Процесс обучения треплетов включает в себя минимизацию

функции потерь, которая штрафует модель, если расстояние

между якорным и положительным примером меньше, чем

расстояние между якорным и отрицательным примером на

заданное пороговое значение.



Во время обучения модель принимает на вход тройки

изображений и минимизирует функцию потерь, используя

методы оптимизации, такие как стохастический градиентный

спуск или его варианты. Это позволяет модели эффективно

изучать признаки, которые характеризуют сходство или

различие между изображениями.





Ссылки для изучения


1. Triplet loss в ML

Основные методы векторизации текстовых данных.

Посмотреть

ответ





Ответ


Основные методы векторизации текстовых данных:


1. Мешок слов (Bag of Words): Представляет текст как

набор изолированных слов без учета порядка или

структуры предложения. Каждое уникальное слово в





тексте становится признаком, а его частота

встречаемости в документе — значением этого признака.

2. TF-IDF (Term Frequency-Inverse Document Frequency): Учитывает не только частоту встречаемости слова в

документе (TF), но и обратную частоту его встречаемости

во всех документах коллекции (IDF). Это позволяет

выделить наиболее информативные слова, учитывая их

важность в контексте всей коллекции документов.

3. Word Embeddings: Представляют слова в виде векторов

непрерывного пространства, где семантически близкие

слова имеют близкие векторные представления. Примеры

включают Word2Vec, GloVe и FastText.

4. Doc2Vec: Расширение Word2Vec, которое представляет не

только отдельные слова, но и целые документы в виде

векторов, сохраняя их семантическое содержание.

5. N-граммы: Включают в себя комбинации из нескольких

последовательных слов или символов. Например,

биграммы (2-граммы) содержат последовательности из

двух слов, триграммы (3-граммы) — из трех и т.д.





Ссылки для изучения


1. Краткий обзор техник векторизации текста

Каким образом можно описать метод максимального правдоподобия?

Посмотреть ответ





Ответ





Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE) - это метод оценки параметров

статистической модели, который стремится найти такие

значения параметров, которые максимизируют вероятность

получения наблюдаемых данных. Кратко говоря, MLE ищет

параметры модели, которые делают наблюдаемые данные

наиболее вероятными.





Ссылки для изучения


1. Руководство по методу максимального правдоподобия

Что делать, если в данных есть дисбаланс классов?

Посмотреть

ответ





Ответ


Если в данных присутствует дисбаланс классов, можно

применить следующие подходы:

1. Использование весов классов: Некоторые модели, такие

как логистическая регрессия и случайный лес, позволяют

задать веса для классов, учитывая их дисбаланс. Это

позволяет модели уделять больше внимания редким

классам.

2. Undersampling: Уменьшение размера преобладающего

класса путем случайного удаления некоторых его

экземпляров до уровня меньшего класса.





3. Oversampling: Увеличение размера редкого класса путем

добавления дополнительных экземпляров или создания

синтетических данных.

4. Генерация синтетических данных: Использование

алгоритмов генерации синтетических данных, таких как

SMOTE (Synthetic Minority Over-sampling Technique), для

увеличения размера редкого класса.





Ссылки для изучения


1. Как избежать “подводных камней” машинного обучения



Вы знакомы с инструментами контейнеризации?

Посмотреть ответ





Ответ


Да, контейнеризация - это технология, позволяющая

упаковывать приложения и их зависимости в изолированные

контейнеры, которые могут быть запущены и работать на

любой платформе без изменений. Основные инструменты

контейнеризации включают Docker, Kubernetes и Podman. Docker

- это платформа для разработки, доставки и запуска

приложений в контейнерах, Kubernetes - это система для

автоматизации развертывания, масштабирования и

управления контейнеризированными приложениями, а Podman -

альтернатива Docker, которая предоставляет совместимый с

API интерфейс командной строки для управления





контейнерами без необходимости использования демона.

Контейнеризация обеспечивает легкость и надежность

развертывания приложений, а также упрощает управление

зависимостями и конфигурацией.





Ссылки для изучения


1. Docker для Data Science

С точки зрения практики MLOps - какой ваш любимый линтер?



Посмотреть ответ





Ответ


Мой любимый линтер для MLOps - это pylint. Он

обеспечивает широкий спектр проверок для кода на Python, включая стандарты кодирования, правильное использование

синтаксиса и другие аспекты, что помогает поддерживать код в

хорошем состоянии и повышает его читаемость и надежность.





Ссылки для изучения


1. Что такое линтер

Хорошо знакомы с Scikit-Learn?

Посмотреть ответ





Ответ


Да, я хорошо знаком с библиотекой Scikit-learn. Она

предоставляет простой и эффективный инструментарий для

анализа данных и машинного обучения на Python. Scikit-learn включает в себя различные алгоритмы классификации, регрессии, кластеризации, а также инструменты для

предобработки данных, выбора моделей, оценки

производительности и подбора параметров моделей.

Благодаря своей простоте использования и обширной

документации Scikit-learn является популярным выбором для

многих проектов машинного обучения на Python.





Ссылки для изучения


1. Scikit-Learn. Введение





Какие техники с точки зрения проверки качества моделей используете (в

Scikit-Learn)?

Посмотреть ответ





Ответ





В Scikit-Learn я использую различные техники для проверки

качества моделей:

1. Кросс-валидация: Позволяет оценить производительность

модели на разных подмножествах данных, уменьшая

вероятность переобучения и обобщая ее качество.

2. Метрики оценки качества: В зависимости от задачи

(классификация, регрессия и т. д.) использую различные

метрики, такие как accuracy, precision, recall, F1-score, ROC AUC, MSE и другие, чтобы оценить качество

модели.

3. Grid Search и Random Search: Позволяют подобрать

оптимальные гиперпараметры модели, исследуя

пространство параметров и выбирая те, которые

приводят к наилучшей производительности.

4. Learning Curves: Позволяют визуально оценить, как

изменение размера обучающего набора данных влияет

на производительность модели, помогая выявить

проблемы с переобучением или недообучением.

5. Confusion Matrix: Позволяет оценить производительность

классификационных моделей, выявляя количество

верных и ошибочных предсказаний для каждого класса.





Ссылки для изучения


1. Scikit-Learn. Введение





Знакомы с pipeline из Scikit-Learn?

Посмотреть ответ





Ответ


Да, я знаком с понятием Pipeline из Scikit-Learn.



Pipeline представляет собой последовательность шагов

обработки данных и моделирования, которые автоматически

применяются к данным в заданном порядке. Она позволяет

объединить несколько этапов работы с данными, таких как

предобработка, извлечение признаков и построение модели, в

один интегрированный процесс.





Ссылки для изучения


1. Scikit-Learn. Введение

Для чего используется метод взвешенных оценок (WOE), каковы его

преимущества? Какие задачи он помогает решить, и почему нельзя просто

использовать возраст напрямую в логистической регрессии?



Посмотреть ответ





Ответ




Метод взвешенных оценок (WOE) используется в задачах

бинарной классификации для анализа влияния

категориальных переменных на целевую переменную. Он

преобразует категориальные переменные в числовые

значения, которые отражают их влияние на целевую

переменную.



Преимущества метода WOE:

1. Позволяет учитывать монотонную зависимость между

категориальными переменными и целевой переменной.

2. Помогает уловить нелинейные отношения между

переменными.

3. Устойчив к выбросам и пропущенным значениям.



WOE применяется для улучшения производительности модели и

ее интерпретируемости. Нельзя просто использовать возраст

напрямую в логистической регрессии, потому что это не

учитывает нелинейные связи между возрастом и целевой

переменной, а также не обеспечивает монотонной зависимости

между переменными. Использование WOE позволяет корректно

учитывать влияние возраста на целевую переменную,

учитывая его нелинейные и монотонные связи.





Ссылки для изучения





1. Предварительная обработка данных для машинного обучения



Popularity bias, как бороться?

Посмотреть ответ





Ответ


Popularity bias (популярный биас) возникает, когда

рекомендательная система предлагает пользователю только

самые популярные или общеизвестные элементы, игнорируя

индивидуальные предпочтения пользователя. Вот несколько

способов борьбы с ним:

1. Персонализация: Используйте персонализированные

методы ранжирования, которые учитывают предпочтения

и историю взаимодействия конкретного пользователя с

контентом.

2. Разнообразие: Интегрируйте в систему механизмы, которые обеспечивают разнообразие рекомендаций,

чтобы предложения не были ограничены только самыми

популярными элементами.

3. Фильтрация по контексту: Учитывайте контекст, такой как

время, местоположение, текущая активность

пользователя и т. д., для адаптации рекомендаций к

конкретной ситуации.

4. Гибридные модели: Используйте комбинацию различных

методов рекомендаций, таких как коллаборативная

фильтрация, контентная фильтрация, гибридные подходы

и т. д., для более точных и разнообразных рекомендаций.





5. Управление экспозицией: Регулируйте видимость

популярных и непопулярных элементов с помощью

техник, таких как балансировка и динамическая

регулировка рекомендаций в зависимости от интересов

пользователя.





Ссылки для изучения


1. Дропаем ранжирующие метрики в RecSys

Можете ли вы объяснить основную идею метода опорных векторов (SVM)?

Посмотреть ответ





Ответ


Основная идея метода опорных векторов SVM заключается в

поиске оптимальной разделяющей гиперплоскости, которая

максимизирует расстояние (зазор) между двумя классами

данных. SVM стремится найти гиперплоскость, которая

увеличивает расстояние до ближайших точек каждого класса, называемых опорными векторами. Этот подход позволяет

достичь хорошей обобщающей способности и устойчивости к

переобучению. Если данные нелинейно разделимы, SVM может

использовать ядерные функции для перевода данных в

пространство более высокой размерности, где они становятся

линейно разделимыми.





Ссылки для изучения


1. Метод опорных векторов (SVM)

Что такое метод максимального правдоподобия (ММП)?

Посмотреть

ответ





Ответ


Метод максимального правдоподобия (ММП) - это

статистический метод оценки параметров вероятностной

модели путем максимизации функции правдоподобия. Иными

словами, ММП ищет такие значения параметров модели, при

которых вероятность наблюдать имеющиеся данные

максимальна. Этот метод широко используется для оценки

параметров в различных моделях, таких как линейная

регрессия, логистическая регрессия и другие.





Ссылки для изучения


1. Руководство по ММП



Расскажите, как вы будете собирать данные, создавать и выводить в

продакшен.

Посмотреть ответ





Ответ


Для сбора данных я бы использовал различные методы, включая внешние API, веб-скрапинг, базы данных и

собственные источники. Затем данные можно обработать и

очистить, чтобы подготовить их для моделирования. Для

создания модели я бы использовал подходящие методы

машинного обучения или статистического анализа в

зависимости от задачи. После тщательной проверки и оценки

модели, я бы интегрировал ее в продуктовую среду, используя

подходящие инструменты и технологии, чтобы обеспечить

надежную и эффективную работу в реальном времени.





Ссылки для изучения


1. Основные модели машинного обучения

Что включает в себя задача прогнозирования?

Посмотреть ответ





Ответ


Задача прогнозирования включает в себя использование

данных о прошлом для предсказания будущих событий, значений или трендов. Она включает в себя построение

моделей, которые могут прогнозировать значения целевой



переменной на основе доступных признаков или данных, что

позволяет принимать более осознанные решения или действия

в будущем. Такие модели могут быть использованы в

различных областях, включая финансы, экономику, медицину, маркетинг и другие.





Ссылки для изучения


1. Прогнозирование в ML





