30.11.2025, 20:20                                                       Трансформеры


                    Карьерная платформа




                    Трансформеры
                         Вопросы

                           Что такое Multi-Head Attention?
                                  Посмотреть ответ
                                     Ответ
                                   Multi-Head Attention - это механизм внимания в
                                   архитектуре трансформера, который позволяет
                                   модели сосредотачиваться на различных частях
                                   входных данных сразу. Он достигается путем
                                   одновременного применения нескольких
                                   "голов" (head) внимания к входным данным,
                                   каждая из которых обрабатывает данные с
                                   различными весами. Это позволяет модели
                                   обрабатывать различные аспекты контекста
                                   независимо друг от друга, что может привести к
                                   более эффективному изучению долгосрочных
                                   зависимостей в данных.
https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  1/11
30.11.2025, 20:20                                                       Трансформеры


                                     Ссылки для изучения
                                 1. Трансформеры




                           Чем Encoder отличается от Decoder-а ?
                                  Посмотреть ответ
                                     Ответ
                                    Encoder и Decoder - это две основные
                                   компоненты в архитектуре seq2seq , которая
                                   часто используется в задачах машинного
                                   перевода.
                                          Encoder принимает входные данные и
                                         кодирует их в скрытый вектор,
                                         представляющий собой сжатое
                                         представление исходной
                                         последовательности. Это позволяет модели
                                         понимать контекст входных данных.

                                          Decoder использует скрытый вектор,
                                         полученный от Encoder, чтобы генерировать
                                         выходную последовательность. Он
                                         декодирует скрытый вектор в
                                         последовательность символов или токенов
                                         на выходе.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Что такое Positional Encoding и зачем он нужен?
                                  Посмотреть ответ
                                     Ответ
                                   Positional Encoding - это механизм,
                                   используемый в моделях трансформера для
                                   добавления информации о позиции слов в

https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  2/11
30.11.2025, 20:20                                                       Трансформеры

                                   последовательности. Это необходимо потому,
                                   что в стандартной архитектуре трансформера
                                   нет явного представления о порядке слов во
                                   входных данных. Positional Encoding добавляет
                                   эту информацию, позволяя модели учитывать
                                   порядок слов при обработке входных данных.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Какие примеры трансформерных моделей существуют?
                           (например, BERT как Encoder , GPT как Decoder )?
                                  Посмотреть ответ
                                     Ответ
                                   Примеры трансформенных моделей:
                                     1. BERT (Bidirectional Encoder Representations
                                         from Transformers) - используется для
                                         решения задач обработки естественного
                                         языка (Natural Language Processing, NLP).

                                     2. GPT (Generative Pre-trained Transformer) -
                                         также используется в задачах NLP, но в
                                         отличие от BERT, GPT обычно используется
                                         для генерации текста.

                                     3. T5 (Text-To-Text Transfer Transformer) -
                                         обобщенная трансформерная модель,
                                         которая может быть применена к различным
                                         задачам NLP, представленная в работе
                                         "Exploring the Limits of Transfer Learning with a
                                         Unified Text-to-Text Transformer".

                                     Ссылки для изучения
                                 1. Трансформеры




https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                        3/11
30.11.2025, 20:20                                                       Трансформеры



                           Расскажите про оптимизаторы. ( AdamW , LAMB , LANS , большой
                           батч)
                                  Посмотреть ответ
                                     Ответ
                                   Кратко о некоторых оптимизаторах:
                                     1. AdamW (Adam with Weight Decay) - это
                                       вариант оптимизатора Adam, который
                                         добавляет весовую декэй-регуляризацию для
                                         улучшения стабильности и обобщающей
                                         способности модели.

                                     2. LAMB (Layer-wise Adaptive Moments) - это
                                         оптимизатор, который адаптивно
                                         масштабирует градиенты для каждого слоя
                                         нейронной сети, что позволяет эффективнее
                                         обучать модели с различными масштабами
                                         градиентов.

                                     3. LANS (Lookahead with Noisy Student) - это
                                       метод, который сочетает в себе технику
                                         Lookahead для ускорения сходимости с
                                         оптимизацией Noisy Student для улучшения
                                         обобщения модели.

                                     4. Оптимизация с использованием больших
                                        батчей - это подход к обучению нейронных
                                         сетей, при котором используются более
                                         крупные батчи данных для ускорения
                                         процесса обучения и повышения
                                         параллелизма, что может привести к более
                                         эффективному использованию аппаратного
                                         обеспечения.

                                     Ссылки для изучения
                                 1. Трансформеры




https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                     4/11
30.11.2025, 20:20                                                       Трансформеры



                           Почему лучше использовать нормализацию по слоям (layer
                           norm) вместо нормализации по батчу (batch norm) ?

                                  Посмотреть ответ
                                     Ответ
                                   Использование нормализации по слоям (layer
                                   norm) обычно предпочтительнее нормализации
                                   по батчу (batch norm) в задачах, где размер
                                   батча может быть небольшим или изменчивым,
                                   так как:
                                     1. Более стабильное обучение: Нормализация
                                         по слоям не зависит от размера батча,
                                         поэтому обеспечивает более стабильное
                                         обучение в различных условиях.

                                     2. Независимость от размера батча:
                                         Нормализация по слоям независима от
                                         размера батча и может быть эффективно
                                         применена даже при работе с одним
                                         образцом данных.

                                     3. Большая параллелизация: Нормализация по
                                         слоям позволяет большей степени
                                         параллелизма при обучении на устройствах с
                                         разными характеристиками, такими как
                                         распределенные системы или GPU с
                                         ограниченной памятью.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Чем отличается модель BERT от модели GPT ?
                                  Посмотреть ответ
                                     Ответ
                                    BERT (Bidirectional Encoder Representations from
                                   Transformers) и GPT (Generative Pre-trained
https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  5/11
30.11.2025, 20:20                                                       Трансформеры

                                   Transformer) - это обе модели на основе
                                   трансформеров, но с разными архитектурами и
                                   целями:
                                     1. Направленность: BERT - это модель на
                                         основе энкодера, который обучается на
                                         двунаправленном контексте. Он пытается
                                         предсказать следующее слово в
                                         предложении, используя контекст как слева,
                                         так и справа от текущего слова. GPT , с
                                         другой стороны, - это модель на основе
                                         декодера, которая генерирует текст
                                         последовательно, одно слово за другим.

                                     2. Цель обучения: BERT обучается на задачах
                                        предварительной тренировки, таких как
                                         предсказание маскированных слов и
                                         предсказание следующего предложения. Он
                                         используется для создания представлений
                                         слов и контекста, которые можно
                                         использовать в различных задачах NLP. GPT
                                         обучается на задаче предсказания
                                         следующего слова в тексте и используется в
                                         основном для генерации текста и
                                         выполнения других задач, связанных с
                                         пониманием текста.

                                     3. Генерация и понимание: GPT хорошо
                                        подходит для генерации текста, такого как
                                         продолжение предложений или создание
                                         новых текстовых данных. BERT , с другой
                                         стороны, лучше подходит для задач
                                         понимания текста, таких как классификация,
                                         извлечение информации или вопросно-
                                         ответные системы.

                                     Ссылки для изучения
                                 1. Трансформеры

https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  6/11
30.11.2025, 20:20                                                       Трансформеры




                           Есть ли отдельный слой-токенизатор у трансформера?
                                  Посмотреть ответ
                                     Ответ
                                   Нет, у архитектуры трансформера нет отдельного
                                   слоя токенизатора. Токенизация выполняется до
                                   подачи данных в модель и осуществляется
                                   токенизатором, который преобразует текстовые
                                   данные в числовые токены. Затем эти числовые
                                   токены передаются в эмбеддинговый слой
                                   трансформера, который преобразует их в
                                   векторные представления и добавляет
                                   позиционные эмбеддинги для дальнейшей
                                   обработки моделью.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Чем эмбеддинг w2v отличается от эмбеддинга трансформера?
                                  Посмотреть ответ
                                     Ответ
                                   Эмбеддинг word2vec (w2v) - это статические
                                   векторы, представляющие слова в пространстве
                                   непрерывных векторов, обученные на больших
                                   текстовых корпусах. Они представляют собой
                                   фиксированные представления слов, которые не
                                   изменяются во время обработки моделью.


                                   Эмбеддинги трансформера - это динамические
                                   векторы, которые вычисляются во время
                                   работы модели и зависят от контекста. Они
                                   создаются на основе входных данных и
                                   внутренних матриц весов модели
                                   трансформера. Эти эмбеддинги учитывают
https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  7/11
30.11.2025, 20:20                                                       Трансформеры

                                   контекст и семантику слова в конкретном
                                   контексте.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Расскажите полностью архитектуру трансформера.
                                  Посмотреть ответ
                                     Ответ
                                   Архитектура трансформера - это модель
                                   глубокого обучения, предназначенная для
                                   обработки последовательностей данных, таких
                                   как тексты или временные ряды. Она была
                                   представлена в статье "Attention is All You
                                   Need" в 2017 году. Вот ее основные компоненты:

                                     1. Входной вектор: На вход трансформеру
                                         поступает последовательность токенов (слов,
                                         символов или эмбеддингов) размерности
                                         T × d, где T - длина последовательности, а
                                         d - размерность эмбеддинга для каждого
                                         токена.

                                     2. Позиционные эмбеддинги: Для кодирования
                                        информации о позиции каждого токена в
                                         последовательности добавляются
                                         позиционные эмбеддинги.

                                     3. Энкодер: Энкодер состоит из нескольких
                                         одинаковых слоев. Каждый слой включает в
                                         себя два основных подмодуля: много-
                                         головые внимательные механизмы (Multi-
                                         Head Attention) и полносвязные слои с
                                         применением пакетной нормализации и
                                         функций активации.



https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  8/11
30.11.2025, 20:20                                                       Трансформеры


                                     4. Много-головые внимательные механизмы
                                         (Multi-Head Attention): Этот механизм
                                         позволяет модели фокусироваться на разных
                                         частях входной последовательности
                                         одновременно, используя несколько "голов"
                                         внимания. Каждая голова вычисляет
                                         внимательность между входными токенами,
                                         учитывая их взаимосвязи и важность для
                                         предсказания.

                                     5. Полносвязные слои: После много-головых
                                         внимательных механизмов выходные данные
                                         подвергаются обработке полносвязными
                                         слоями для обеспечения нелинейности и
                                         извлечения высокоуровневых признаков.

                                     6. Декодер: В задачах генерации текста или
                                         машинного перевода используется декодер,
                                         который также состоит из нескольких слоев с
                                         много-головыми внимательными
                                         механизмами и полносвязными слоями.
                                         Декодер генерирует выходную
                                         последовательность, принимая на вход
                                         выход энкодера и внимание на входной
                                         последовательности.

                                     7. Финальный полносвязный слой: Финальный
                                         полносвязный слой преобразует выходы
                                         декодера в вероятности или векторы
                                         предсказания.

                                   Трансформеры показали выдающиеся
                                   результаты в ряде задач обработки
                                   естественного языка, включая машинный
                                   перевод, генерацию текста, суммаризацию и
                                   другие. Их архитектура позволяет моделировать
                                   долгосрочные зависимости и эффективно


https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                  9/11
30.11.2025, 20:20                                                       Трансформеры

                                   работать с различными типами данных и
                                   задачами.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Батч нормализация происходит во время backward или forward
                           pass . Зачем вообще нужна?

                                  Посмотреть ответ
                                     Ответ
                                   Батч нормализация происходит во время forward
                                   pass. Ее основная цель - ускорить обучение и
                                   улучшить обобщающую способность нейронной
                                   сети. Путем нормализации входных данных к
                                   батчу (входных активаций) по их среднему
                                   значению и дисперсии внутри батча, батч
                                   нормализация помогает справиться с проблемой
                                   внутреннего сдвига и способствует более
                                   стабильному обучению модели. Она также
                                   может уменьшить зависимость от выбора
                                   начальных весов и улучшить работу
                                   оптимизатора, позволяя использовать более
                                   высокие скорости обучения.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Чем отличается RNN от Трансформера?
                                  Посмотреть ответ
                                     Ответ
                                   Между RNN и Трансформером есть основное
                                   отличие:
                                          RNN обрабатывает данные последовательно
                                         и хорошо подходит для временных рядов, но
https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                   10/11
30.11.2025, 20:20                                                       Трансформеры

                                         страдает от проблемы долгосрочной
                                         зависимости.

                                         Трансформеры параллельно обрабатывают
                                         последовательности и используют механизм
                                         внимания ( attention ), что позволяет
                                         эффективно работать с длинными
                                         последовательностями.

                                     Ссылки для изучения
                                 1. Трансформеры




                           Какая вычислительная сложность у RNN и Трансформера?
                                  Посмотреть ответ
                                     Ответ
                                   У RNN и Трансформера следующая
                                   вычислительная сложность:
                                         У RNN сложность O(n), где n — длина
                                         последовательности.

                                         У Трансформера сложность O(n²) из-за
                                         механизма внимания, но он лучше
                                         масштабируется благодаря параллелизму.

                                     Ссылки для изучения
                                 1. Трансформеры




                                                  © 2023 —2025 Вадим Новосёлов
                                                     Ютуб канал         Телеграм канал


https://gernar.ru/plus/questions/14bee738-d69b-81a6-bcbb-d17de9c35c1b                    11/11
