30.11.2025, 17:45

Карьерная платформа - Catalyst

Карьерная платформа

Лайфкодинг ML

Задачи

Задача №1

Ручные признаки для анализа изображений

Помимо использования глубокого обучения, часто для решения задач приход

На вход Вам дана черно-белая картинка в виде матрицы, где 1 — черный пи

кластером группу горизонтально или вертикально связанных черных пикселе

и вертикально окружены белым.

Входные данные: Матрица где каждый элемент “1” или “0”

Задача: Для заданной матрицы найти количество черных кластеров.

Пример:

[["1","1","1","1","0"],["1","1","0","1","0"], ["1","1","0","0","0"],["0

Ответ: 1





Посмотреть ответ

Ответ

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 1/19

30.11.2025, 17:45

Карьерная платформа - Catalyst

Для решения этой задачи можно использовать

алгоритм поиска в глубину (DFS), который позволит

нам найти все связанные черные пиксели в матрице.

Мы будем проходить по каждому пикселю в матрице

и, если встречаем черный пиксель, начнем обходить

все смежные черные пиксели, чтобы образовать

один кластер.

def count_clusters(matrix):

def dfs(matrix, i, j):

# Проверяем граничные условия и наличие черн

if i < 0 or i >= len(matrix) or j < 0 or j > return

# Отмечаем текущий пиксель как посещенный

matrix[i][j] = "0"

# Рекурсивно вызываем DFS для всех смежных п

dfs(matrix, i+1, j)

dfs(matrix, i-1, j)

dfs(matrix, i, j+1)

dfs(matrix, i, j-1)

count = 0

# Проходим по каждому пикселю в матрице

for i in range(len(matrix)):

for j in range(len(matrix[0])):

# Если встречаем черный пиксель, увеличи

if matrix[i][j] == "1": count += 1

dfs(matrix, i, j)

return count

# Пример использования

matrix = [["1", "1", "1", "1", "0"],

["1", "1", "0", "1", "0"],

["1", "1", "0", "0", "0"],

["0", "0", "0", "0", "0"]]

print(count_clusters(matrix)) # 1





https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 2/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

Задача №2

Представлено 100 наблюдений, из которых 60 дефолтных.

Наивная модель предполагает, что вероятность случайного выбора дефолтно

Для такой модели, каково среднее значение precision и recall?





Посмотреть ответ

Ответ

Для оценки средних значений precision и recall для

наивной модели, предполагающей, что вероятность

случайного выбора дефолтного наблюдения

составляет 60%, мы можем использовать следующие

формулы:

P recision =

TP



( TP + FP )

Recall =

TP



( TP + FN)

Где:

TP (True Positives) - количество дефолтных

наблюдений, которые были правильно

классифицированы как дефолтные.

FP (False Positives) - количество не-дефолтных

наблюдений, которые были ошибочно

классифицированы как дефолтные.

FN (False Negatives) - количество дефолтных

наблюдений, которые были ошибочно

классифицированы как не-дефолтные.



Для наивной модели precisio n будет равен 60%, так как она всегда предсказывает, что наблюдение

дефолтное, и поэтому FP = 4

0 и TP = 6 .

0

Точно так же, для recall также будет 60%, так как

она не учитывает FN и всегда предсказывает, что

наблюдение дефолтное, и поэтому FN =

0 и

TP = 60.

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 3/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

Таким образом, средние значения precisio n и

recall для наивной модели будут равны 60%.

Задача №3

Дана функция rnd2, возвращающая равновероятно 0 или 1.

Используя rnd2, написать функцию rnd3, которая равновероятно возвращает





Посмотреть ответ

Ответ

Решение:

import random

def rnd2():

return random.randint(0, 1)

def rnd3():

while True:

# Генерируем два случайных бита с помощью rn bit1 = rnd2()

bit2 = rnd2()

# Преобразуем два бита в число от 0 до 3

num = bit1 * 2 + bit2

# Если число меньше 3, возвращаем его

if num < 3:

return num

print(rnd3())





Задача №4

Дано: датасет N нулей, M единиц.

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 4/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

Нужно минимизировать логлосс: - sum_i(t_i * log(p) + (1-t_i) * log(1-p) Посмотреть ответ

Ответ





Решение:

from scipy.optimize import minimize_scalar import numpy as np

def log_loss(p, t):

loss = -np.sum(t * np.log(p) + (1 - t) * np.log(

return loss

def minimize_log_loss(N, M):

t = np.concatenate([np.zeros(N), np.ones(M)]) #

result = minimize_scalar(log_loss, args=(t,), bo return result.x

# Пример использования

N = 50 # Количество нулей

M = 30 # Количество единиц

optimal_p = minimize_log_loss(N, M) print("Optimal p:", optimal_p)





Задача №5

Предположим, что у нас есть разметчик, который имеет навык 90, то есть

что независимо от задачи, он отвечает правильно с вероятностью 90 %. Пу

причем наш набор данных сильно несбалансированный: котиков в нем всего

На случайно выбранную картинку разметчик отвечает, что это котик. Каков





Посмотреть ответ

Ответ

Решение:

Для решения этой задачи мы можем

воспользоваться формулой Байеса. Пусть событие C

- это выбор картинки с котиком, а событие

A - это

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 5/19

30.11.2025, 17:45

Карьерная платформа - Catalyst

ответ разметчика о том, что на картинке изображен

котик.

Тогда мы хотим найти вероятность того, что на

картинке действительно изображен котик при

условии того, что разметчик ответил, что это котик: P ( C∣ A) = P( A∣ C)⋅ P( C) P ( A)

Где:

P ( C∣ A

) - вероятность того, что выбрана картика

с котиком при условии, что разметчик ответил, что это котик (искомая нами вероятность).

P ( A∣ C

) - вероятность того, что разметчик

ответил, что на картинке котик, при условии, что

на самом деле на картинке котик. По условию

задачи, это равно навыку разметчика, то есть

P ( A∣ C) = 0. .

9

P ( C

) - вероятность того, что на случайно

выбранной картинке изображен котик. По

условию задачи, котиков в наборе данных всего

10%, то есть P ( C) = 0. .

1

P ( A

) - общая вероятность того, что разметчик

ответил, что на картинке котик. Это сумма

вероятности того, что разметчик ответил, что на

картинке котик, при условии того, что на самом

деле там котик, и вероятности того, что

разметчик ответил, что на картинке котик, при

условии того, что на самом деле там собачка, то

есть

P ( A) = P ( A∣ C) ⋅ P ( C) + P ( A∣ C) ⋅ P ( C ,

)

где C обозначает событие "на картинке собачка".

Теперь мы можем вычислить вероятность P ( C∣ A) : https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 6/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

P ( A) = 0.9 ⋅ 0.1 + 0.1 ⋅ 0.9 = 0.09 + 0.09 =

0.18

P ( C ∣ A) = P( A∣ C)⋅ P( C) P ( A)

P ( C ∣ A) = 0.09 = 0.5



0.18



Ответ: 0.5.

Задача №6

Написать реализацию метода ближайших соседей (KNN) с использованием тол

без использования библиотек типа NumPy, scikit-learn и тому подобных.





Посмотреть ответ

Ответ

Решение:

import math

class KNN:

def __init__(self, k):

# Конструктор класса KNN, принимает параметр

self.k = k

def fit(self, X_train, y_train):

# Метод для обучения модели, принимает обуча

self.X_train = X_train # Обучающие данные

self.y_train = y_train # Метки классов для

def predict(self, X_test):

# Метод для предсказания меток классов для т

predictions = [] # Инициализация списка пре

for x in X_test:

# Для каждого тестового примера

neighbors = self.get_neighbors(x) # Пол

predicted_label = self.vote(neighbors) predictions.append(predicted_label) # Д

return predictions # Вернуть список предска

def euclidean_distance(self, x1, x2):

# Метод для вычисления евклидова расстояния

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 7/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

distance = 0 # Начальное значение расстояни

for i in range(len(x1)):

# Для каждой компоненты точек

distance += (x1[i] - x2[i]) ** 2 # Доба

return math.sqrt(distance) # Вернуть корень

def get_neighbors(self, x):

# Метод для получения k ближайших соседей дл

distances = [] # Инициализация списка расст

for i in range(len(self.X_train)):

# Для каждой точки в обучающих данных

dist = self.euclidean_distance(x, self.X

distances.append((self.X_train[i], self.

distances.sort(key=lambda x: x[2]) # Отсорт

neighbors = [distances[i] for i in range(sel return neighbors # Вернуть список ближайших

def vote(self, neighbors):

# Метод для проголосовать за класс среди бли

votes = {} # Словарь для подсчета голосов з

for neighbor in neighbors:

# Для каждого соседа

label = neighbor[1] # Метка класса сосе

if label in votes:

votes[label] += 1 # Увеличить счетч

else:

votes[label] = 1 # Инициализировать

return max(votes, key=votes.get) # Вернуть

Задача №7

Как обучить сетку так, чтобы на батче из 32 объектов она обучалась так





Посмотреть ответ

Ответ

Решение:

Чтобы обучить сетку так, чтобы она обучалась так

же на батче из 32 объектов, как на батче из 2000, https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 8/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

можно воспользоваться методом под названием

Batch Normalization (нормализация по батчу). Этот

метод обычно применяется после каждого слоя

активации в нейронной сети и помогает

стабилизировать процесс обучения.

Batch Normalization вычисляет среднее и

дисперсию для каждого признака по всем объектам

в батче. Затем производится нормализация

признаков путем вычитания среднего и деления на

стандартное отклонение. Это позволяет сети

обучаться более стабильно и быстро.

Таким образом, применение Batch Normalization позволит сети эффективно использовать как батчи

из 32 объектов, так и батчи из 2000 объектов для

обучения.

Задача №8

Als, как считается? Как заполнить таблицы в случае ютуба, когда мы хоти

Что делать, если пользователь просмотрит 99% ролика - он ему понравился

Как учесть различные факторы - комментарий под видео, лайк, просмотр. (

Как можно восстановить матрицу с помощью автокодировщиков?





Посмотреть ответ

Ответ

Решение:

ALS (Alternative Least Squares) - это метод

коллаборативной фильтрации, используемый для

рекомендаций. Он основан на разложении матрицы

пользовательских предпочтений на две более

низкоранговые матрицы.

Чтобы заполнить таблицы для рекомендаций на

YouTube, вам нужно собрать данные о поведении

пользователей, такие как просмотры видео, лайки, https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 9/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

комментарии и т. д. Если вы хотите увеличить

среднее время просмотра, можете использовать

различные методы стимулирования пользователей

смотреть видео дольше, например, рекомендуя им

похожие или продолжающие тему видео, создавая

увлекательный контент и т. д.

Когда пользователь просматривает 99% ролика, это

может означать, что ему ролик понравился, но это

не всегда так. Вам нужно анализировать

дополнительные факторы, такие как время

просмотра, взаимодействие с другими видео на

канале, отзывы и т. д., чтобы сделать более точные

предположения о пользовательском опыте.

Автокодировщики (Autoencoders) могут быть

использованы для восстановления матрицы

пользовательских предпочтений, преобразовав ее в

пространство более низкой размерности и затем

обратно в исходное пространство. Это позволяет

сжать информацию о предпочтениях пользователей, сохраняя при этом их суть.

Задача №9

Есть два вектора в записи вида RLE. Нужно найти скалярное произведение





Посмотреть ответ

Ответ

Решение:

Для вычисления скалярного произведения двух

векторов в записи вида RLE (Run-Length Encoding) нужно учитывать только элементы с одинаковыми

индексами и их частоты в каждом из векторов.

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 10/19



30.11.2025, 17:45

Карьерная платформа - Catalyst

1. Преобразуйте векторы из RLE в обычный

список, заменяя повторяющиеся элементы и их

частоты на соответствующие числа.

2. Выполните попарное умножение элементов

полученных списков.

3. Произведите суммирование результатов пункта 2.

Пример:

Даны два вектора в записи RLE : [(2, 3), (5, 1), (7, 2)] и [(2, 2), (5, 2), (7, 1)] .

1. Преобразуем векторы в обычный список: [2, 2, 5, 7, 7] и [2, 2, 5, 5, 7] .

2. Выполним попарное умножение: [4, 4, 25, 35, 49] .

3. Суммируем результаты:

4 + 4 + 25 + 35 + 49 = 11 .

7

Таким образом, скалярное произведение данных

двух векторов равно 117.

Задача №10

сlient_id sum day

1 30 2020-04-01

1 12 2020-04-03

1 6 2020-04-04

2 30 2020-04-02

2 15 2020-04-03

2 120 2020-04-04

3 0 2020-04-03

3 3 2020-04-04

3 12 2020-04-05

Сделать в таблице колонку со средней суммой продаж для клиента за три д

день в записи, предыдущий и следующий.





Посмотреть ответ

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 11/19



30.11.2025, 17:45

Карьерная платформа - Catalyst

Ответ

Решение:

import pandas as pd

# Входные данные

data = {

'client_id': [1, 1, 1, 2, 2, 2, 3, 3, 3],

'sum': [30, 12, 6, 30, 15, 120, 0, 3, 12],

'day': ['2020-04-01', '2020-04-03', '2020-04-04'

'2020-04-02', '2020-04-03', '2020-04-04'

'2020-04-03', '2020-04-04', '2020-04-05'

}

# Преобразование в DataFrame

df = pd.DataFrame(data)

# Преобразование столбца 'day' в тип datetime df['day'] = pd.to_datetime(df['day'])

# Сортировка данных по 'client_id' и 'day'

df = df.sort_values(by=['client_id', 'day'])

# Вычисление средней суммы продаж за три дня для каж

df['rolling_avg'] = df.groupby('client_id')['sum'].r

# Вывод результата

print(df)





Задача №11

Мы продаем пиццу и компания хочет организовать акцию, продавать пиццы с

Нам надо рассчитать экономику акции и собрать все варианты начинок с це

Вывод надо отсортировать по цене (по убыванию) и начинкам в алфавитном

+--------------+-----------------+

| topping_name | ingredient_cost |

+--------------+-----------------+

| Pepperoni | 0.50 |

+--------------+-----------------+

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 12/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

| Mushrooms | 0.70 |

+--------------+-----------------+

| Chicken | 0.55 |

+--------------+-----------------+

| Extra Cheese | 0.40 |

Формат вывода

+--------------------------------+------------+

| pizza | total_cost |

+--------------------------------+------------+

| Chicken,Pepperoni,Sausage | 1.75 |

+--------------------------------+------------+

| Chicken,Extra Cheese,Sausage | 1.65 |

+--------------------------------+------------+

| Extra Cheese,Pepperoni,Sausage | 1.60 |

+--------------------------------+------------+

| Chicken,Extra Cheese,Pepperoni | 1.45 |

Посмотреть ответ

Ответ

Решение:

import itertools

import pandas as pd





# Входные данные

pizza_toppings = {

'topping_name': ['Pepperoni', 'Mushrooms', 'Chic

'ingredient_cost': [0.50, 0.70, 0.55, 0.40]

}

# Преобразование в DataFrame

df_toppings = pd.DataFrame(pizza_toppings)

# Создание всех возможных комбинаций начинок

combinations = list(itertools.combinations(df_toppin

# Расчет общей стоимости для каждой комбинации

pizza_prices = []

for combo in combinations:

total_cost = sum(df_toppings[df_toppings['toppin pizza_prices.append((sorted(combo), total_cost)) https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 13/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

# Сортировка по убыванию общей стоимости и начинкам

pizza_prices.sort(key=lambda x: (-x[1], x[0]))

# Вывод результатов

print("+--------------------------------+-----------

print("| pizza | total_cost print("+--------------------------------+-----------

for pizza, total_cost in pizza_prices: print(f"| {','.join(pizza):<30} | {total_cost:<1

print("+--------------------------------+-----------

Задача №12

Даны для LLM, одна на 100к токенов, вторая на 200к

На вход подается один и тот же текст, предобработанный одинаково

Железо для моделей одинаковое

Какая модель даст лучшее качество?

В каком случае вторая модель отработает быстрее первой и почему?

Посмотреть ответ

Ответ

Решение:

Для оценки того, какая модель LLM даст лучшее

качество, следует провести сравнительный

анализ, возможно, на тестовых данных, чтобы

оценить их производительность и эффективность.

Важно учитывать не только размер словаря, но и

различные гиперпараметры моделей, такие как

количество слоев, размерность векторного

представления, архитектура сети и т. д.





Что касается времени выполнения, вторая

модель (LLM на 200k токенов) скорее всего будет

работать быстрее, если использовать тот же

объем железа. Это связано с тем, что вторая

модель имеет более крупный словарь, что

позволяет ей лучше улавливать многие аспекты

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 14/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

языка и предсказывать более точные

вероятности. Однако, это также зависит от

реализации и конкретной архитектуры моделей.

В целом, больший размер словаря требует

больше вычислительных ресурсов для обучения

и выполнения предсказаний, но может

обеспечить более высокое качество результатов.

Задача №13

Есть кубик Рубик разобранный. Кубики разного цвета , размещены в сцене

Какими методами мл можно решить задачу . Что делать если есть еще посто





Посмотреть ответ

Ответ

Решение:

Для решения задачи определения цветов кубика

Рубика можно использовать методы машинного

обучения и компьютерного зрения. Вот несколько

подходов:

1. Цветовая сегментация: Можно использовать

алгоритмы цветовой сегментации, такие как k-средних или методы на основе порогового

значения, чтобы выделить области

определенного цвета на изображении. Затем

можно использовать классификацию для

определения цвета каждой области.

2. Нейронные сети: Можно обучить сверточную

нейронную сеть (CNN) на большом наборе

данных изображений кубика Рубика, чтобы

классифицировать цвета. CNN может

обнаруживать и классифицировать цвета на

изображениях с высокой точностью.

3. Цветовая модель: Можно использовать

стандартные цветовые модели, такие как RGB

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 15/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

или HSV , чтобы преобразовать цвета пикселей

изображения в числовые значения. Затем можно

использовать алгоритмы кластеризации, такие

как k-средних , для кластеризации пикселей и

определения цветов кубика.

4. Использование дополнительных признаков: Помимо цветовых характеристик, можно

использовать другие признаки, такие как форма

или текстура кубика Рубика, чтобы более точно

определить его цвета.

Что касается посторонних предметов такого же

цвета, можно использовать дополнительные

признаки или алгоритмы для разделения кубика

Рубика и посторонних предметов на изображении.

Например, можно использовать алгоритмы

сегментации для выделения только кубика Рубика на

изображении перед определением его цветов.

Задача №14

Есть КПП в Армению. В Армении N городов, в которых [k1, k2, k3, ..., kN

Написать функцию, которая вызывается каждый раз, когда айтишник из Росс

задача функции отправлять айтишников в города так, чтобы изначальное ра

Число айтишников наперед неизвестно.





Посмотреть ответ

Ответ

Решение:

def distribute_programmers(population, programmers): total_population = sum(population) city_ratios = [p / total_population for p in pop allocated_programmers = [int(p * programmers) fo remaining_programmers = programmers - sum(alloca https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 16/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

# Жадно распределяем оставшихся программистов в

while remaining_programmers > 0: max_shortage_index = city_ratios.index(max(c allocated_programmers[max_shortage_index] +=

city_ratios[max_shortage_index] = population remaining_programmers -= 1

return allocated_programmers

Задача №15

Найти пользователей, которые купили за последний месяц больше 10 кг тов

user_id | sku (id товара) | dt (дата) | cnt (количество едниц)





Таблица B:

sku (id товара) | weight (масса, кг)





Посмотреть ответ

Ответ

Решение:

SELECT user_id

FROM A

INNER JOIN B ON A.sku = B.sku

WHERE dt >= CURRENT_DATE - INTERVAL '1 month'

GROUP BY user_id

HAVING SUM(cnt * weight) > 10; Задача №16

Дан текст

"Мама мыла раму"

"Мама любит Анну"

"Анна играет гамму"

Посчитать tf-idf второго предложения.

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 17/19





30.11.2025, 17:45

Карьерная платформа - Catalyst

Посмотреть ответ

Ответ

Решение:

Чтобы посчитать TF-IDF для второго

предложения, нужно сначала разделить текст на

отдельные слова, затем вычислить TF (term frequency) и IDF (inverse document frequency) для каждого слова, а затем перемножить их.

Например, TF можно посчитать как количество

вхождений слова "Мама" во втором

предложении, деленное на общее количество

слов во втором предложении. IDF вычисляется

по формуле, которая учитывает количество

предложений и количество предложений, в

которых встречается данное слово. После этого

TF и IDF перемножаются.

Таким образом, для второго предложения

"Мама любит Анну":

TF для слова "Мама" равен 1/3

TF для слова "любит" равен 1/3

TF для слова "Анну" равен 1/3

Для вычисления IDF нужно знать общее

количество предложений (3) и количество

предложений, в которых встречается каждое

слово:

IDF для слова "Мама" равен log(3/2) IDF для слова "любит" равен log(3/1) IDF для слова "Анну" равен log(3/1) Затем TF и IDF перемножаются для каждого

слова, и результаты суммируются: TF-IDF для слова "Мама" = (1/3) * log(3/2) https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 18/19

30.11.2025, 17:45

Карьерная платформа - Catalyst

TF-IDF для слова "любит" = (1/3) * log(3/1) TF-IDF для слова "Анну" = (1/3) * log(3/1) После чего можно сложить эти значения, чтобы

получить итоговый TF-IDF для всего

предложения.

© 2023 —2025 Вадим Новосёлов

Ютуб канал

Телеграм канал

https://gernar.ru/plus/questions/14bee738-d69b-8190-ba7c-dc822909923d 19/19





