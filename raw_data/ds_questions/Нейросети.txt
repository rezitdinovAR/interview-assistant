Что такое перцептрон?

Посмотреть ответ

Ответ

Перцептрон - это простая форма искусственной нейронной

сети, представляющая собой один нейрон с несколькими

входами, каждый из которых имеет весовое значение. Он

принимает входные данные, взвешивает их с помощью весов и

применяет функцию активации к суммированному

взвешенному входу, чтобы сгенерировать выход. Перцептрон

может использоваться для бинарной классификации, когда

выход является бинарным (например, 0 или 1), и может быть

обучен методом градиентного спуска для настройки весовых

коэффициентов.

Ссылки для изучения

1. Нейронные сети. Перцептрон

Как работает Back Propagation?

Посмотреть ответ

Ответ

Алгоритм обратного распространения ошибки (Back propagation) - это метод обучения нейронных сетей, который





используется для определения и корректировки весовых

коэффициентов сети, чтобы минимизировать ошибку между

прогнозируемым и фактическим выходом. Он работает в два

этапа:

1. Прямое распространение: Входные данные проходят

через нейронную сеть, и каждый нейрон вычисляет свой

выход на основе текущих весовых коэффициентов и

функции активации.

2. Обратное распространение: Ошибка между

прогнозируемым и фактическим выходом вычисляется и

обратно распространяется через сеть, чтобы определить, какие веса нужно скорректировать для уменьшения этой

ошибки. Это делается путем вычисления градиента

функции потерь по отношению к каждому весу с

использованием правила цепочки.



После этого веса обновляются с использованием градиентного

спуска или его вариантов, чтобы минимизировать ошибку

предсказания. Этот процесс повторяется на протяжении

нескольких эпох обучения до тех пор, пока модель не

достигнет удовлетворительного уровня производительности.

Ссылки для изучения

1. Как работает алгоритм Back Propagation

Какие методы регуляризации существуют для нейросетей?



Посмотреть ответ





Ответ

Для регуляризации нейронных сетей существуют следующие

методы:

1. L1 и L2 регуляризация: Добавление штрафа на веса в

функцию потерь для сокращения их значений и

предотвращения переобучения.

2. Dropout: Случайное исключение некоторых нейронов во

время обучения для уменьшения зависимости между

нейронами и предотвращения переобучения.

3. Early stopping: Остановка обучения, когда

производительность на валидационном наборе данных

перестает улучшаться, чтобы избежать переобучения.

4. Batch Normalization: Нормализация активаций между

слоями для ускорения обучения и уменьшения

переобучения.

Эти методы помогают сделать нейронные сети более

устойчивыми к переобучению и улучшить их обобщающую

способность.

Ссылки для изучения

1. Тонкости обучения нейросети





Каким образом применение метода дропаут способствует регуляризации

модели?

Посмотреть ответ

Ответ

Метод дропаут случайным образом исключает некоторые

нейроны во время обучения с определенной вероятностью.

Это заставляет нейроны работать независимо друг от друга и

предотвращает слишком сильную адаптацию модели к

обучающим данным, что в свою очередь уменьшает риск

переобучения и улучшает обобщающую способность модели.

Ссылки для изучения

1. Тонкости обучения нейросети

Разделите нейронные сети на классы в соответствии с методом обучения и

типом решаемых задач.

Посмотреть ответ

Ответ

Нейронные сети можно классифицировать по нескольким

критериям:

1. По типу обучения:





● Надзорное обучение (Supervised learning): сети, обучаемые на размеченных данных для

задач классификации, регрессии и др.

● Без надзора (Unsupervised learning): сети, работающие с неразмеченными данными, например, для кластеризации или снижения

размерности.

● Полу-надзорное обучение (Semi-supervised learning): комбинация методов надзорного и

безнадзорного обучения.

2. По типу задачи:

● Классификация: нейронные сети,

используемые для определения категории или

класса для входных данных.

● Регрессия: сети, предсказывающие

непрерывные значения на основе входных

данных.

● Кластеризация: сети, анализирующие

структуру неразмеченных данных и

группирующие их в кластеры или сегменты.

Ссылки для изучения

1. Виды нейронных сетей

Расскажите об отличиях BERT, GPT, Т5 и ELECTRA.

Посмотреть

ответ



Ответ

Каждая из этих моделей - BERT, GPT, T5 и ELECTRA - является

мощным представителем глубокого обучения для обработки

текста, но они имеют свои уникальные особенности: 1. BERT (Bidirectional Encoder Representations from Transformers):

● BERT основан на архитектуре трансформера и

предназначен для обучения двунаправленных

контекстуализированных эмбеддингов слов.

● Он способен моделировать контекст в обе

стороны при обработке текста, что помогает в

решении различных задач, таких как

классификация, разметка и вопросно-ответные

системы.

2. GPT (Generative Pre-trained Transformer):

● GPT также основан на архитектуре

трансформера, но он предназначен для

генерации текста.

● GPT использует однонаправленное

моделирование контекста и обучается на

больших корпусах текста для генерации текста

по заданному контексту.

3. T5 (Text-To-Text Transfer Transformer):

● T5 является универсальной моделью, которая

преобразует все задачи обработки текста в

единый формат "текст-в-текст", где вход и

выход текстовой информации.





● Это позволяет использовать одну и ту же

модель для различных задач, таких как

машинный перевод, суммаризация,

вопросно-ответные системы и другие.

4. ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):

● ELECTRA - это модель, предложенная в 2020

году, которая использует подход adversarial training для обучения.

В отличие от BERT, который обучается маскированию слов, ELECTRA обучает модель определять, какие слова заменены

случайно сгенерированными словами. Это позволяет

эффективнее использовать обучающие данные и улучшить

качество модели.

●

В целом, каждая из этих моделей имеет свои преимущества и

подходы к обработке текста, и выбор модели зависит от

конкретной задачи и требований к качеству и

производительности.

Ссылки для изучения

1. Большое сравнение нейросетей

Как ещё называют полносвязные нейронные сети?

Посмотреть

ответ





Ответ

Полносвязные нейронные сети также называются

многослойными перцептронами (MLP - Multilayer Perceptrons).

Ссылки для изучения

1. Полносвязные нейронные сети в ML

Что такое Word2vec?

Посмотреть ответ

Ответ

Word2Vec - это метод для создания векторных представлений

слов на основе контекста, в котором они встречаются в тексте.

Этот метод обычно используется в обработке естественного

языка и машинном обучении. Он позволяет представить слова

в виде векторов в многомерном пространстве таким образом, что близкие слова в этом пространстве имеют семантическую

схожесть. Векторы слов можно использовать для различных

задач, таких как поиск схожих слов, классификация текста и

многие другие.





Ссылки для изучения

1. Эмбеддинги для начинающих





Чем модели эмбеддингов отличают от самих LLM?

Посмотреть ответ

Ответ

Модели эмбеддингов, такие как Word2Vec или GloVe, создают

векторы для слов, представляя их в непрерывном

пространстве. LLM (Large Language Models), такие как BERT или

GPT, используют эмбеддинги в качестве одного из слоёв, но

дополнительно обучаются на больших корпусах текста для

выполнения более сложных задач, таких как генерация текста

или ответ на вопросы.

Ссылки для изучения

1. Эмбеддинги для начинающих





Нужны ли другие эмбеддинги кроме CLS токена для классификации?



Посмотреть ответ

Ответ

CLS токен обычно используется в моделях вроде BERT для

получения финальной репрезентации текста для задач

классификации. Однако для некоторых задач могут

использоваться другие эмбеддинги, например, агрегированные

или средние значения по всем токенам.

Ссылки для изучения

1. Эмбеддинги для начинающих

Какие есть оптимизаторы в нейронных сетях?

Посмотреть ответ

Ответ

В нейронных сетях оптимизаторы используются для

обновления весов сети на основе данных и функции потерь.

Вот несколько наиболее распространенных оптимизаторов:



1. SGD (Stochastic Gradient Descent): Классический метод, который обновляет параметры в направлении

антиградиента текущей оценки градиента функции

потерь.

2. Momentum: Вариант SGD, который добавляет понятие

инерции в обновления параметров, тем самым ускоряя

сходимость.

3. Nesterov Accelerated Gradient (NAG): Усовершенствование Momentum, где корректировка

параметров происходит более предсказуемо.

4. Adagrad: Адаптирует скорость обучения к параметрам, увеличивая эффективность для разреженных данных.

5. RMSprop: Изменяет метод Adagrad для борьбы с его

агрессивным, монотонно уменьшающимся скоростям

обучения.

6. Adam (Adaptive Moment Estimation): Сочетает идеи

Momentum и RMSprop, поддерживая оценки первого и

второго моментов градиента.

7. Adamax: Вариант Adam, основанный на норме L-infinity.

Nadam (Nesterov-accelerated Adaptive Moment Estimation): Интегрирует Nesterov Momentum в Adam.

8.

Эти оптимизаторы позволяют настроить процесс обучения, улучшить скорость сходимости и повысить точность модели на

разнообразных задачах.

Ссылки для изучения





1. Оптимизаторы в нейронных сетях

Что такое автоэнкодеры?

Посмотреть ответ

Ответ

Автоэнкодеры - это класс нейронных сетей, которые

используются для обучения эффективного представления

данных путем обучения восстанавливать входные данные на

выходе. Они состоят из двух основных частей: кодировщика

(encoder), который преобразует входные данные в скрытое

представление (код), и декодировщика (decoder), который

восстанавливает входные данные из скрытого представления.

Автоэнкодеры могут использоваться для снижения

размерности данных, извлечения признаков, удаления шума и

генерации новых данных.

Ссылки для изучения

1. Автокодировщик





Есть ли модели основанные на Encoder и Decoder?

Посмотреть ответ





Ответ

Да, архитектуры типа "Encoder-Decoder" используются в

моделях для машинного перевода (например, seq2seq, Transformer). Энкодер обрабатывает входные данные, а

декодер генерирует выход на основе закодированной

информации.

Ссылки для изучения

1. Автокодировщик





Какие Positional Encoders ты знаешь?

Посмотреть ответ

Ответ

Есть два позиционных энкодера:



● Sinusoidal Positional Encoding — добавляет

синусоидальные функции к вектору для кодирования

позиций токенов.





● Learned Positional Encoding — обучаемые векторы, которые добавляются к эмбеддингам для кодирования

позиций.

Ссылки для изучения

1. Автокодировщик

Как посчитать attention?

Посмотреть ответ

Ответ

Attention в нейронных сетях используется для выделения

важных элементов входных данных при выполнении задачи.

Применительно к моделям seq2seq и трансформерам, attention считается следующим образом:

1. Вычисляются веса (scores) для каждой пары элементов

входной и выходной последовательностей. Эти веса

обычно вычисляются с помощью функции скалярного

произведения, например, dot-product, или с

использованием нейронной сети.

2. Веса нормализуются с помощью функции Softmax, чтобы

получить вероятностное распределение.

3. Вычисляется взвешенная сумма элементов входной

последовательности с использованием вычисленных

весов, чтобы получить контекстное представление.





Ссылки для изучения

1. ML Attention

Как определить размер пакета (batch_size)?

Посмотреть ответ

Ответ

Размер пакета (batch_size) определяется выбором между

компромиссом между скоростью обучения и использованием

памяти. Большие размеры пакета могут ускорить обучение, так

как модель обрабатывает несколько примеров данных

одновременно, что позволяет эффективнее использовать

аппаратное обеспечение. Однако слишком большие пакеты

могут привести к ухудшению обобщающей способности модели

и замедлению обучения из-за ухудшения обновления весов.

Маленькие пакеты обеспечивают более стабильное обучение и

лучшую обобщающую способность, но могут потреблять

больше времени на обработку. Оптимальный размер пакета

зависит от размера данных, доступной памяти и архитектуры

модели.

Ссылки для изучения

1. Бачти в нейросетях





Как изменяются свойства пакета в зависимости от размера пакета

(batch_size)?

Посмотреть ответ

Ответ

Размер пакета (batch_size) влияет на скорость обучения и

устойчивость процесса обучения. Большие размеры пакетов

обычно ускоряют обучение, но могут уменьшить стабильность

процесса и ухудшить обобщающую способность модели.

Маленькие пакеты обеспечивают более стабильное обучение и

лучшую обобщающую способность, но могут требовать

больше времени на обработку и могут привести к более

медленному обучению.

Ссылки для изучения

1. Бачти в нейросетях





