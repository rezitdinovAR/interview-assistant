[
  {
    "text": "# Conclusion\n\n \nCongratulations on finishing this first Bonus Unit ü•≥\n \nYou‚Äôve just **mastered understanding function-calling and how to fine-tune your model to do function-calling**!\n \nIf we have one piece of advice now, it‚Äôs to try to **fine-tune different models**. The **best way to learn is by trying.**\n \nIn the next Unit, you‚Äôre going to learn how to use **state-of-the-art frameworks such as smolagents, LlamaIndex and LangGraph**.\n \nFinally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then, please üëâ [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \n\n### Keep Learning, Stay Awesome ü§ó",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit1/conclusion",
      "course": "agents-course",
      "chapter": "Bonus Unit 1. Fine-tuning an LLM for Function-calling",
      "chapter_id": "bonus-unit1/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit1/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Let‚Äôs Fine-Tune Your Model for Function-Calling\n\n \nWe‚Äôre now ready to fine-tune our first model for function-calling üî•.\n \n\n## How do we train our model for function-calling?\n\n \n> Answer: We needdata\n \nA model training process can be divided into 3 steps:\n \n1. **The model is pre-trained on a large quantity of data**. The output of that step is a **pre-trained model**. For instance, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). It‚Äôs a base model and only knows how **to predict the next token without strong instruction following capabilities**.\n2. To be useful in a chat context, the model then needs to be **fine-tuned** to follow instructions. In this step, it can be trained by model creators, the open-source community, you, or anyone. For instance, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) is an instruction-tuned model by the Google Team behind the Gemma project.\n3. The model can then be **aligned** to the creator‚Äôs preferences. For instance, a customer service chat model that must never be impolite to customers.\n \nUsually a complete product like Gemini or Mistral **will go through all 3 steps**, whereas the models you can find on Hugging Face have completed one or more steps of this training.\n \nIn this tutorial, we will build a function-calling model based on [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). We choose the fine-tuned model [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) instead of the base model [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) because the fine-tuned model has been improved for our use-case.\n \nStarting from the pre-trained model **would require more training in order to learn instruction following, chat AND function-calling**.\n \nBy starting from the instruction-tuned model, **we minimize the amount of information that our model needs to learn**.\n \n\n## LoRA (Low-Rank Adaptation of Large Language Models)\n\n \nLoRA is a popular and lightweight training technique that significantly **reduces the number of trainable parameters**.\n \nIt works by **inserting a smaller number of new weights as an adapter into the model to train**. This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share.\n \n![LoRA inference](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif)\n \nLoRA works by adding pairs of rank decomposition matrices to Transformer layers, typically focusing on linear layers. During training, we will ‚Äúfreeze‚Äù the rest of the model and will only update the weights of those newly added adapters.\n \nBy doing so, the number of **parameters** that we need to train drops considerably as we only need to update the adapter‚Äôs weights.\n \nDuring inference, the input is passed into the adapter and the base model, or these adapter weights can be merged with the base model, resulting in no additional latency overhead.\n \nLoRA is particularly useful for adapting **large** language models to specific tasks or domains while keeping resource requirements manageable. This helps reduce the memory **required** to train a model.\n \nIf you want to learn more about how LoRA works, you should check out this [tutorial](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).\n \n\n## Fine-Tuning a Model for Function-Calling\n\n \nYou can access the tutorial notebook üëâ [here](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).\n \nThen, click on  to be able to run it in a Colab Notebook.",
    "metadata": {
      "title": "Let‚Äôs Fine-Tune Your Model for Function-Calling",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit1/fine-tuning",
      "course": "agents-course",
      "chapter": "Bonus Unit 1. Fine-tuning an LLM for Function-calling",
      "chapter_id": "bonus-unit1/fine-tuning",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit1/fine-tuning.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n \n![Bonus Unit 1 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/thumbnail.jpg)\n \nWelcome to this first **Bonus Unit**, where you‚Äôll learn to **fine-tune a Large Language Model (LLM) for function calling**.\n \nIn terms of LLMs, function calling is quickly becoming a *must-know* technique.\n \nThe idea is, rather than relying only on prompt-based approaches like we did in Unit 1, function calling trains your model to **take actions and interpret observations during the training phase**, making your AI more robust.\n \n> When should I do this Bonus Unit?This section isoptionaland is more advanced than Unit 1, so don‚Äôt hesitate to either do this unit now or revisit it when your knowledge has improved thanks to this course.But don‚Äôt worry, this Bonus Unit is designed to have all the information you need, so we‚Äôll walk you through every core concept of fine-tuning a model for function-calling even if you haven‚Äôt learned yet the inner workings of fine-tuning.\n \nThe best way for you to be able to follow this Bonus Unit is:\n \n1. Know how to Fine-Tune an LLM with Transformers, if it‚Äôs not the case [check this](https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt).\n2. Know how to use `SFTTrainer` to fine-tune our model, to learn more about it [check this documentation](https://huggingface.co/learn/nlp-course/en/chapter11/1).\n  \n\n## What You‚Äôll Learn\n\n \n1. **Function Calling**\nHow modern LLMs structure their conversations effectively letting them trigger **Tools**.\n2. **LoRA (Low-Rank Adaptation)**\nA **lightweight and efficient** fine-tuning method that cuts down on computational and storage overhead. LoRA makes training large models *faster, cheaper, and easier* to deploy.\n3. **The Thought ‚Üí Act ‚Üí Observe Cycle** in Function Calling models\nA simple but powerful approach for structuring how your model decides when (and how) to call functions, track intermediate steps, and interpret the results from external Tools or APIs.\n4. **New Special Tokens**\nWe‚Äôll introduce **special markers** that help the model distinguish between:\n \n- Internal ‚Äúchain-of-thought‚Äù reasoning\n- Outgoing function calls\n- Responses coming back from external tools\n  \nBy the end of this bonus unit, you‚Äôll be able to:\n \n- **Understand** the inner working of APIs when it comes to Tools.\n- **Fine-tune** a model using the LoRA technique.\n- **Implement** and **modify** the Thought ‚Üí Act ‚Üí Observe cycle to create robust and maintainable Function-calling workflows.\n- **Design and utilize** special tokens to seamlessly separate the model‚Äôs internal reasoning from its external actions.\n \nAnd you‚Äôll **have fine-tuned your own model to do function calling.** üî•\n \nLet‚Äôs dive into **function calling**!",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit1/introduction",
      "course": "agents-course",
      "chapter": "Bonus Unit 1. Fine-tuning an LLM for Function-calling",
      "chapter_id": "bonus-unit1/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit1/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What is Function Calling? Function-calling is a **way for an LLM to take actions on its environment**. It was first [introduced in GPT-4](https://openai.com/index/function-calling-and-other-api-updates/), and was later reproduced in other models. Just like the tools of an Agent, function-calling gives the model the capacity to **take an action on its environment**. However, the function calling capacity **is learned by the model**, and relies **less on prompting than other agents techniques**. During Unit 1, the Agent **didn‚Äôt learn to use the Tools**, we just provided the list, and we relied on the fact that the model **was able to generalize on defining a plan using these Tools**. While here, **with function-calling, the Agent is fine-tuned (trained) to use Tools**. ## How does the model ‚Äúlearn‚Äù to take an action? In Unit 1, we explored the general workflow of an agent. Once the user has given some tools to the agent and prompted it with a query, the model will cycle through: 1. *Think* : What action(s) do I need to take in order to fulfill the objective. 2. *Act* : Format the action with the correct parameter and stop the generation. 3. *Observe* : Get back the result from the execution. In a ‚Äútypical‚Äù conversation with a model through an API, the conversation will alternate between user and assistant messages like this: ``` conversation = [ {\"role\": \"user\", \"content\": \"I need help with my order\"}, {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"}, {\"role\": \"user\", \"content\": \"It's ORDER-123\"}, ] ``` Function-calling brings **new roles to the conversation**! 1. One new role for an **Action** 2. One new role for an **Observation** If we take the [Mistral API](https://docs.mistral.ai/capabilities/function_calling/) as an example, it would look like this: ``` conversation = [ { \"role\": \"user\", \"content\": \"What's the status of my transaction T1001?\" }, { \"role\": \"assistant\", \"content\": \"\", \"function_call\": { \"name\": \"retrieve_payment_status\", \"arguments\": \"{\\\"transaction_id\\\": \\\"T1001\\\"}\" } }, { \"role\": \"tool\", \"name\": \"retrieve_payment_status\", \"content\": \"{\\\"status\\\": \\\"Paid\\\"}\" }, { \"role\": \"assistant\", \"content\": \"Your transaction T1001 has been successfully paid.\" } ] ``` > ‚Ä¶ But you said there‚Äôs a new role for function calls? **Yes and no**, in this case and in a lot of other APIs, the model formats the action to take as an ‚Äúassistant‚Äù message. The chat template will then represent this as **special tokens** for function-calling. - `[AVAILABLE_TOOLS]` ‚Äì Start the list of available tools - `[/AVAILABLE_TOOLS]` ‚Äì End the list of available tools - `[TOOL_CALLS]` ‚Äì Make a call to a tool (i.e., take an ‚ÄúAction‚Äù) - `[TOOL_RESULTS]` ‚Äì ‚ÄúObserve‚Äù the result of the action - `[/TOOL_RESULTS]` ‚Äì End of the observation (i.e., the model can decode again) We‚Äôll talk again about function-calling in this course, but if you want to dive deeper you can check [this excellent documentation section](https://docs.mistral.ai/capabilities/function_calling/). Now that we learned what function-calling is and how it works, let‚Äôs **add some function-calling capabilities to a model that does not have those capacities yet**: [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it), by appending some new special tokens to",
    "metadata": {
      "title": "What is Function Calling?",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit1/what-is-function-calling",
      "course": "agents-course",
      "chapter": "Bonus Unit 1. Fine-tuning an LLM for Function-calling",
      "chapter_id": "bonus-unit1/what-is-function-calling",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit1/what-is-function-calling.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "- `[/AVAILABLE_TOOLS]` ‚Äì End the list of available tools - `[TOOL_CALLS]` ‚Äì Make a call to a tool (i.e., take an ‚ÄúAction‚Äù) - `[TOOL_RESULTS]` ‚Äì ‚ÄúObserve‚Äù the result of the action - `[/TOOL_RESULTS]` ‚Äì End of the observation (i.e., the model can decode again) We‚Äôll talk again about function-calling in this course, but if you want to dive deeper you can check [this excellent documentation section](https://docs.mistral.ai/capabilities/function_calling/). Now that we learned what function-calling is and how it works, let‚Äôs **add some function-calling capabilities to a model that does not have those capacities yet**: [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it), by appending some new special tokens to the model. To be able to do that, **we need first to understand fine-tuning and LoRA**.",
    "metadata": {
      "title": "What is Function Calling?",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit1/what-is-function-calling",
      "course": "agents-course",
      "chapter": "Bonus Unit 1. Fine-tuning an LLM for Function-calling",
      "chapter_id": "bonus-unit1/what-is-function-calling",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit1/what-is-function-calling.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# AI Agent Observability & Evaluation\n\n \n![Bonus Unit 2 Thumbnail](https://langfuse.com/images/cookbook/huggingface-agent-course/agent-observability-and-evaluation.png)\n \nWelcome to **Bonus Unit 2**! In this chapter, you‚Äôll explore advanced strategies for observing, evaluating, and ultimately improving the performance of your agents.\n  \n\n## üìö When Should I Do This Bonus Unit?\n\n \nThis bonus unit is perfect if you:\n \n- **Develop and Deploy AI Agents:** You want to ensure that your agents are performing reliably in production.\n- **Need Detailed Insights:** You‚Äôre looking to diagnose issues, optimize performance, or understand the inner workings of your agent.\n- **Aim to Reduce Operational Overhead:** By monitoring agent costs, latency, and execution details, you can efficiently manage resources.\n- **Seek Continuous Improvement:** You‚Äôre interested in integrating both real-time user feedback and automated evaluation into your AI applications.\n \nIn short, for everyone who wants to bring their agents in front of users!\n  \n\n## ü§ì What You‚Äôll Learn\n\n \nIn this unit, you‚Äôll learn:\n \n- **Instrument Your Agent:** Learn how to integrate observability tools via OpenTelemetry with the *smolagents* framework.\n- **Monitor Metrics:** Track performance indicators such as token usage (costs), latency, and error traces.\n- **Evaluate in Real-Time:** Understand techniques for live evaluation, including gathering user feedback and leveraging an LLM-as-a-judge.\n- **Offline Analysis:** Use benchmark datasets (e.g., GSM8K) to test and compare agent performance.\n  \n\n## üöÄ Ready to Get Started?\n\n \nIn the next section, you‚Äôll learn the basics of Agent Observability and Evaluation. After that, its time to see it in action!",
    "metadata": {
      "title": "AI Agent Observability & Evaluation",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/introduction",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Bonus Unit 2: Observability and Evaluation of Agents > You can follow the code inthis notebookthat you can run using Google Colab. In this notebook, we will learn how to **monitor the internal steps (traces) of our AI agent** and **evaluate its performance** using open-source observability tools. The ability to observe and evaluate an agent‚Äôs behavior is essential for: - Debugging issues when tasks fail or produce suboptimal results - Monitoring costs and performance in real-time - Improving reliability and safety through continuous feedback ## Exercise Prerequisites üèóÔ∏è Before running this notebook, please be sure you have: üî≤ üìö **Studied** [Introduction to Agents](https://huggingface.co/learn/agents-course/unit1/introduction) üî≤ üìö **Studied** [The smolagents framework](https://huggingface.co/learn/agents-course/unit2/smolagents/introduction) ## Step 0: Install the Required Libraries We will need a few libraries that allow us to run, monitor, and evaluate our agents: ``` %pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents datasets 'smolagents[gradio]' gradio --upgrade ``` ## Step 1: Instrument Your Agent In this notebook, we will use [Langfuse](https://langfuse.com/) as our observability tool, but you can use **any other OpenTelemetry-compatible service**. The code below shows how to set environment variables for Langfuse (or any OTel endpoint) and how to instrument your smolagent. **Note:** If you are using LlamaIndex or LangGraph, you can find documentation on instrumenting them [here](https://langfuse.com/docs/integrations/llama-index/workflows) and [here](https://langfuse.com/docs/integrations/langchain/example-python-langgraph). First, let‚Äôs set up the Langfuse credentials as environment variables. Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting). ``` import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region ``` We also need to configure our Hugging Face token for inference calls. ``` # Set your Hugging Face and other tokens/secrets as environment variable os.environ[\"HF_TOKEN\"] = \"hf_...\" ``` With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables. ``` from langfuse import get_client langfuse = get_client() # Verify connection if langfuse.auth_check(): print(\"Langfuse client is authenticated and ready!\") else: print(\"Authentication failed. Please check your credentials and host.\") ``` Next, we can set up the `SmolagentsInstrumentor()` to instrument our smolagent and send traces to Langfuse. ``` from openinference.instrumentation.smolagents import SmolagentsInstrumentor SmolagentsInstrumentor().instrument() ``` ## Step 2: Test Your Instrumentation Here is a simple CodeAgent from smolagents that calculates `1+1`. We run it to confirm that the instrumentation is working correctly. If everything is set up correctly, you will see logs/spans in your observability dashboard. ``` from smolagents import InferenceClientModel, CodeAgent # Create a simple agent to test instrumentation agent = CodeAgent( tools=[], model=InferenceClientModel() ) agent.run(\"1+1=\") ``` Check your [Langfuse Traces Dashboard](https://cloud.langfuse.com) (or your chosen observability tool) to confirm that the spans and logs have been recorded. Example screenshot from Langfuse: ![Example trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/first-example-trace.png) *Link to the trace* ## Step 3: Observe and Evaluate a More Complex Agent Now that you have confirmed your instrumentation works, let‚Äôs try a more complex query so we can see how advanced metrics",
    "metadata": {
      "title": "Bonus Unit 2: Observability and Evaluation of Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "correctly. If everything is set up correctly, you will see logs/spans in your observability dashboard. ``` from smolagents import InferenceClientModel, CodeAgent # Create a simple agent to test instrumentation agent = CodeAgent( tools=[], model=InferenceClientModel() ) agent.run(\"1+1=\") ``` Check your [Langfuse Traces Dashboard](https://cloud.langfuse.com) (or your chosen observability tool) to confirm that the spans and logs have been recorded. Example screenshot from Langfuse: ![Example trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/first-example-trace.png) *Link to the trace* ## Step 3: Observe and Evaluate a More Complex Agent Now that you have confirmed your instrumentation works, let‚Äôs try a more complex query so we can see how advanced metrics (token usage, latency, costs, etc.) are tracked. ``` from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel) search_tool = DuckDuckGoSearchTool() agent = CodeAgent(tools=[search_tool], model=InferenceClientModel()) agent.run(\"How many Rubik's Cubes could you fit inside the Notre Dame Cathedral?\") ``` ### Trace Structure Most observability tools record a **trace** that contains **spans**, which represent each step of your agent‚Äôs logic. Here, the trace contains the overall agent run and sub-spans for: - The tool calls (DuckDuckGoSearchTool) - The LLM calls (InferenceClientModel) You can inspect these to see precisely where time is spent, how many tokens are used, and so on: ![Trace tree in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png) *Link to the trace* ## Online Evaluation In the previous section, we learned about the difference between online and offline evaluation. Now, we will see how to monitor your agent in production and evaluate it live. ### Common Metrics to Track in Production 1. **Costs** ‚Äî The smolagents instrumentation captures token usage, which you can transform into approximate costs by assigning a price per token. 2. **Latency** ‚Äî Observe the time it takes to complete each step, or the entire run. 3. **User Feedback** ‚Äî Users can provide direct feedback (thumbs up/down) to help refine or correct the agent. 4. **LLM-as-a-Judge** ‚Äî Use a separate LLM to evaluate your agent‚Äôs output in near real-time (e.g., checking for toxicity or correctness). Below, we show examples of these metrics. #### 1. Costs Below is a screenshot showing usage for `Qwen2.5-Coder-32B-Instruct` calls. This is useful to see costly steps and optimize your agent. ![Costs](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-costs.png) *Link to the trace* #### 2. Latency We can also see how long it took to complete each step. In the example below, the entire conversation took 32 seconds, which you can break down by step. This helps you identify bottlenecks and optimize your agent. ![Latency](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-latency.png) *Link to the trace* #### 3. Additional Attributes You may also pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom metadata. Enriching traces with these details is important for analysis, debugging, and monitoring of your application‚Äôs behavior across different users or sessions. ``` from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel) search_tool = DuckDuckGoSearchTool() agent = CodeAgent( tools=[search_tool], model=InferenceClientModel() ) with langfuse.start_as_current_span( name=\"Smolagent-Trace\", ) as span: # Run your application here response = agent.run(\"What is the capital of Germany?\") # Pass additional attributes to the span span.update_trace( input=\"What is the capital of Germany?\", output=response, user_id=\"smolagent-user-123\", session_id=\"smolagent-session-123456789\", tags=[\"city-question\", \"testing-agents\"], metadata={\"email\": \"user@langfuse.com\"}, ) #",
    "metadata": {
      "title": "Bonus Unit 2: Observability and Evaluation of Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "3. Additional Attributes You may also pass additional attributes to your spans. These can include `user_id`, `tags`, `session_id`, and custom metadata. Enriching traces with these details is important for analysis, debugging, and monitoring of your application‚Äôs behavior across different users or sessions. ``` from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel) search_tool = DuckDuckGoSearchTool() agent = CodeAgent( tools=[search_tool], model=InferenceClientModel() ) with langfuse.start_as_current_span( name=\"Smolagent-Trace\", ) as span: # Run your application here response = agent.run(\"What is the capital of Germany?\") # Pass additional attributes to the span span.update_trace( input=\"What is the capital of Germany?\", output=response, user_id=\"smolagent-user-123\", session_id=\"smolagent-session-123456789\", tags=[\"city-question\", \"testing-agents\"], metadata={\"email\": \"user@langfuse.com\"}, ) # Flush events in short-lived applications langfuse.flush() ``` ![Enhancing agent runs with additional metrics](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-attributes.png) #### 4. User Feedback If your agent is embedded into a user interface, you can record direct user feedback (like a thumbs-up/down in a chat UI). Below is an example using [Gradio](https://gradio.app/) to embed a chat with a simple feedback mechanism. In the code snippet below, when a user sends a chat message, we capture the trace in Langfuse. If the user likes/dislikes the last answer, we attach a score to the trace. ``` import gradio as gr from smolagents import (CodeAgent, InferenceClientModel) from langfuse import get_client langfuse = get_client() model = InferenceClientModel() agent = CodeAgent(tools=[], model=model, add_base_tools=True) trace_id = None def respond(prompt, history): with langfuse.start_as_current_span( name=\"Smolagent-Trace\"): # Run your application here output = agent.run(prompt) global trace_id trace_id = langfuse.get_current_trace_id() history.append({\"role\": \"assistant\", \"content\": str(output)}) return history def handle_like(data: gr.LikeData): # For demonstration, we map user feedback to a 1 (like) or 0 (dislike) if data.liked: langfuse.create_score( value=1, name=\"user-feedback\", trace_id=trace_id ) else: langfuse.create_score( value=0, name=\"user-feedback\", trace_id=trace_id ) with gr.Blocks() as demo: chatbot = gr.Chatbot(label=\"Chat\", type=\"messages\") prompt_box = gr.Textbox(placeholder=\"Type your message...\", label=\"Your message\") # When the user presses 'Enter' on the prompt, we run 'respond' prompt_box.submit( fn=respond, inputs=[prompt_box, chatbot], outputs=chatbot ) # When the user clicks a 'like' button on a message, we run 'handle_like' chatbot.like(handle_like, None, None) demo.launch() ``` User feedback is then captured in your observability tool: ![User feedback is being captured in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/user-feedback-gradio.png) #### 5. LLM-as-a-Judge LLM-as-a-Judge is another way to automatically evaluate your agent‚Äôs output. You can set up a separate LLM call to gauge the output‚Äôs correctness, toxicity, style, or any other criteria you care about. **Workflow**: 1. You define an **Evaluation Template**, e.g., ‚ÄúCheck if the text is toxic.‚Äù 2. Each time your agent generates output, you pass that output to your ‚Äújudge‚Äù LLM with the template. 3. The judge LLM responds with a rating or label that you log to your observability tool. Example from Langfuse: ![LLM-as-a-Judge Evaluation Template](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator-template.png) ![LLM-as-a-Judge Evaluator](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator.png) ``` # Example: Checking if the agent‚Äôs output is toxic or not. from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel) search_tool = DuckDuckGoSearchTool() agent = CodeAgent(tools=[search_tool], model=InferenceClientModel()) agent.run(\"Can eating carrots improve your vision?\") ``` You can see that the answer of this example is judged as ‚Äúnot toxic‚Äù. ![LLM-as-a-Judge Evaluation Score](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/llm-as-a-judge-score.png) #### 6. Observability Metrics Overview All of these metrics can be visualized together in dashboards. This enables you to quickly see how",
    "metadata": {
      "title": "Bonus Unit 2: Observability and Evaluation of Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "LLM with the template. 3. The judge LLM responds with a rating or label that you log to your observability tool. Example from Langfuse: ![LLM-as-a-Judge Evaluation Template](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator-template.png) ![LLM-as-a-Judge Evaluator](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator.png) ``` # Example: Checking if the agent‚Äôs output is toxic or not. from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel) search_tool = DuckDuckGoSearchTool() agent = CodeAgent(tools=[search_tool], model=InferenceClientModel()) agent.run(\"Can eating carrots improve your vision?\") ``` You can see that the answer of this example is judged as ‚Äúnot toxic‚Äù. ![LLM-as-a-Judge Evaluation Score](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/llm-as-a-judge-score.png) #### 6. Observability Metrics Overview All of these metrics can be visualized together in dashboards. This enables you to quickly see how your agent performs across many sessions and helps you to track quality metrics over time. ![Observability metrics overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png) ## Offline Evaluation Online evaluation is essential for live feedback, but you also need **offline evaluation**‚Äîsystematic checks before or during development. This helps maintain quality and reliability before rolling changes into production. ### Dataset Evaluation In offline evaluation, you typically: 1. Have a benchmark dataset (with prompt and expected output pairs) 2. Run your agent on that dataset 3. Compare outputs to the expected results or use an additional scoring mechanism Below, we demonstrate this approach with the [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which contains math questions and solutions. ``` import pandas as pd from datasets import load_dataset # Fetch GSM8K from Hugging Face dataset = load_dataset(\"openai/gsm8k\", 'main', split='train') df = pd.DataFrame(dataset) print(\"First few rows of GSM8K dataset:\") print(df.head()) ``` Next, we create a dataset entity in Langfuse to track the runs. Then, we add each item from the dataset to the system. (If you‚Äôre not using Langfuse, you might simply store these in your own database or local file for analysis.) ``` from langfuse import get_client langfuse = get_client() langfuse_dataset_name = \"gsm8k_dataset_huggingface\" # Create a dataset in Langfuse langfuse.create_dataset( name=langfuse_dataset_name, description=\"GSM8K benchmark dataset uploaded from Huggingface\", metadata={ \"date\": \"2025-03-10\", \"type\": \"benchmark\" } ) ``` ``` for idx, row in df.iterrows(): langfuse.create_dataset_item( dataset_name=langfuse_dataset_name, input={\"text\": row[\"question\"]}, expected_output={\"text\": row[\"answer\"]}, metadata={\"source_index\": idx} ) if idx >= 9: # Upload only the first 10 items for demonstration break ``` ![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png) #### Running the Agent on the Dataset We define a helper function `run_smolagent()` that: 1. Starts a Langfuse span 2. Runs our agent on the prompt 3. Records the trace ID in Langfuse Then, we loop over each dataset item, run the agent, and link the trace to the dataset item. We can also attach a quick evaluation score if desired. ``` from opentelemetry.trace import format_trace_id from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel) from langfuse import get_client langfuse = get_client() # Example: using InferenceClientModel or LiteLLMModel to access openai, anthropic, gemini, etc. models: model = InferenceClientModel() agent = CodeAgent( tools=[], model=model, add_base_tools=True ) dataset_name = \"gsm8k_dataset_huggingface\" current_run_name = \"smolagent-notebook-run-01\" # Identifies this specific evaluation run # Assume 'run_smolagent' is your instrumented application function def run_smolagent(question): with langfuse.start_as_current_generation(name=\"qna-llm-call\") as generation: # Simulate LLM call result = agent.run(question) # Update the trace with the input and output generation.update_trace( input= question, output=result, ) return result dataset = langfuse.get_dataset(name=dataset_name) # Fetch",
    "metadata": {
      "title": "Bonus Unit 2: Observability and Evaluation of Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "quick evaluation score if desired. ``` from opentelemetry.trace import format_trace_id from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel) from langfuse import get_client langfuse = get_client() # Example: using InferenceClientModel or LiteLLMModel to access openai, anthropic, gemini, etc. models: model = InferenceClientModel() agent = CodeAgent( tools=[], model=model, add_base_tools=True ) dataset_name = \"gsm8k_dataset_huggingface\" current_run_name = \"smolagent-notebook-run-01\" # Identifies this specific evaluation run # Assume 'run_smolagent' is your instrumented application function def run_smolagent(question): with langfuse.start_as_current_generation(name=\"qna-llm-call\") as generation: # Simulate LLM call result = agent.run(question) # Update the trace with the input and output generation.update_trace( input= question, output=result, ) return result dataset = langfuse.get_dataset(name=dataset_name) # Fetch your pre-populated dataset for item in dataset.items: # Use the item.run() context manager with item.run( run_name=current_run_name, run_metadata={\"model_provider\": \"Hugging Face\", \"temperature_setting\": 0.7}, run_description=\"Evaluation run for GSM8K dataset\" ) as root_span: # root_span is the root span of the new trace for this item and run. # All subsequent langfuse operations within this block are part of this trace. # Call your application logic generated_answer = run_smolagent(question=item.input[\"text\"]) print(item.input) ``` You can repeat this process with different: - Models (OpenAI GPT, local LLM, etc.) - Tools (search vs. no search) - Prompts (different system messages) Then compare them side-by-side in your observability tool: ![Dataset run overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset_runs.png) ![Dataset run comparison](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset-run-comparison.png) ## Final Thoughts In this notebook, we covered how to: 1. **Set up Observability** using smolagents + OpenTelemetry exporters 2. **Check Instrumentation** by running a simple agent 3. **Capture Detailed Metrics** (cost, latency, etc.) through an observability tools 4. **Collect User Feedback** via a Gradio interface 5. **Use LLM-as-a-Judge** to automatically evaluate outputs 6. **Perform Offline Evaluation** with a benchmark dataset ü§ó Happy coding!",
    "metadata": {
      "title": "Bonus Unit 2: Observability and Evaluation of Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/monitoring-and-evaluating-agents-notebook",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Quiz: Evaluating AI Agents\n\n \nLet‚Äôs assess your understanding of the agent tracing and evaluation concepts covered in this bonus unit.\n \nThis quiz is optional and ungraded.\n \n\n### Q1: What does observability in AI agents primarily refer to?\n\n \nWhich statement accurately describes the purpose of observability for AI agents?\n  It involves tracking internal operations through logs, metrics, and spans to understand agent behavior.  It is solely focused on reducing the financial cost of running the agent.  It refers only to the external appearance and UI of the agent.  It is concerned with coding style and code aesthetics only.   \n\n### Q2: Which of the following is NOT a common metric monitored in agent observability?\n\n \nSelect the metric that does not typically fall under the observability umbrella.\n  Latency  Cost per Agent Run  User Feedback and Ratings  Lines of Code of the Agent   \n\n### Q3: What best describes offline evaluation of an AI agent?\n\n \nDetermine the statement that correctly captures the essence of offline evaluation.\n  Evaluating the agent using real user interactions in a live environment.  Assessing agent performance using curated datasets with known ground truth.  Monitoring the agent's internal logs in real-time.  Running the agent without any evaluation metrics.   \n\n### Q4: Which advantage does online evaluation of agents offer?\n\n \nPick the statement that best reflects the benefit of online evaluation.\n  It provides controlled testing scenarios using pre-defined datasets.  It captures live user interactions and real-world performance data.  It eliminates the need for any offline testing and benchmarks.  It solely focuses on reducing the computational cost of the agent.   \n\n### Q5: What role does OpenTelemetry play in AI agent observability and evaluation?\n\n \nWhich statement best describes the role of OpenTelemetry in monitoring AI agents?\n  It provides a standardized framework to instrument code, enabling the collection of traces, metrics, and logs for observability.  It acts as a replacement for manual debugging by automatically fixing code issues.  It primarily serves as a database for storing historical logs without real-time capabilities.  It is used to optimize the computational performance of the AI agent by automatically tuning model parameters.   \nCongratulations on completing this quiz! üéâ If you missed any questions, consider reviewing the content of this bonus unit for a deeper understanding. If you did well, you‚Äôre ready to explore more advanced topics in agent observability and evaluation!",
    "metadata": {
      "title": "Quiz: Evaluating AI Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/quiz",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/quiz",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/quiz.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# AI Agent Observability and Evaluation ## üîé What is Observability? Observability is about understanding what‚Äôs happening inside your AI agent by looking at external signals like logs, metrics, and traces. For AI agents, this means tracking actions, tool usage, model calls, and responses to debug and improve agent performance. ![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png) ## üî≠ Why Agent Observability Matters Without observability, AI agents are ‚Äúblack boxes.‚Äù Observability tools make agents transparent, enabling you to: - Understand costs and accuracy trade-offs - Measure latency - Detect harmful language & prompt injection - Monitor user feedback In other words, it makes your demo agent ready for production! ## üî® Observability Tools Common observability tools for AI agents include platforms like [Langfuse](https://langfuse.com) and [Arize](https://www.arize.com). These tools help collect detailed traces and offer dashboards to monitor metrics in real-time, making it easy to detect problems and optimize performance. Observability tools vary widely in their features and capabilities. Some tools are open source, benefiting from large communities that shape their roadmaps and extensive integrations. Additionally, certain tools specialize in specific aspects of LLMOps‚Äîsuch as observability, evaluations, or prompt management‚Äîwhile others are designed to cover the entire LLMOps workflow. We encourage you to explore the documentation of different options to pick a solution that works well for you. Many agent frameworks such as [smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index) use the [OpenTelemetry](https://opentelemetry.io/docs/) standard to expose metadata to the observability tools. In addition to this, observability tools build custom instrumentations to allow for more flexibility in the fast moving world of LLMs. You should check the documentation of the tool you are using to see what is supported. ## üî¨Traces and Spans Observability tools usually represent agent runs as traces and spans. - **Traces** represent a complete agent task from start to finish (like handling a user query). - **Spans** are individual steps within the trace (like calling a language model or retrieving data). ![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png) ## üìä Key Metrics to Monitor Here are some of the most common metrics that observability tools monitor: **Latency:** How quickly does the agent respond? Long waiting times negatively impact user experience. You should measure latency for tasks and individual steps by tracing agent runs. For example, an agent that takes 20 seconds for all model calls could be accelerated by using a faster model or by running model calls in parallel. **Costs:** What‚Äôs the expense per agent run? AI agents rely on LLM calls billed per token or external APIs. Frequent tool usage or multiple prompts can rapidly increase costs. For instance, if an agent calls an LLM five times for marginal quality improvement, you must assess if the cost is justified or if you could reduce the number of calls or use a cheaper model. Real-time monitoring can also help identify unexpected spikes (e.g., bugs causing excessive API loops). **Request Errors:** How many requests did the agent fail? This can include API errors or failed tool calls. To make your agent more robust against these in production, you can then",
    "metadata": {
      "title": "AI Agent Observability and Evaluation",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/what-is-agent-observability-and-evaluation",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "on LLM calls billed per token or external APIs. Frequent tool usage or multiple prompts can rapidly increase costs. For instance, if an agent calls an LLM five times for marginal quality improvement, you must assess if the cost is justified or if you could reduce the number of calls or use a cheaper model. Real-time monitoring can also help identify unexpected spikes (e.g., bugs causing excessive API loops). **Request Errors:** How many requests did the agent fail? This can include API errors or failed tool calls. To make your agent more robust against these in production, you can then set up fallbacks or retries. E.g. if LLM provider A is down, you switch to LLM provider B as backup. **User Feedback:** Implementing direct user evaluations provide valuable insights. This can include explicit ratings (üëçthumbs-up/üëédown, ‚≠ê1-5 stars) or textual comments. Consistent negative feedback should alert you as this is a sign that the agent is not working as expected. **Implicit User Feedback:** User behaviors provide indirect feedback even without explicit ratings. This can include immediate question rephrasing, repeated queries or clicking a retry button. E.g. if you see that users repeatedly ask the same question, this is a sign that the agent is not working as expected. **Accuracy:** How frequently does the agent produce correct or desirable outputs? Accuracy definitions vary (e.g., problem-solving correctness, information retrieval accuracy, user satisfaction). The first step is to define what success looks like for your agent. You can track accuracy via automated checks, evaluation scores, or task completion labels. For example, marking traces as ‚Äúsucceeded‚Äù or ‚Äúfailed‚Äù. **Automated Evaluation Metrics:** You can also set up automated evals. For instance, you can use an LLM to score the output of the agent e.g. if it is helpful, accurate, or not. There are also several open source libraries that help you to score different aspects of the agent. E.g. [RAGAS](https://docs.ragas.io/) for RAG agents or [LLM Guard](https://llm-guard.com/) to detect harmful language or prompt injection. In practice, a combination of these metrics gives the best coverage of an AI agent‚Äôs health. In this chapters [example notebook](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb), we‚Äôll show you how these metrics looks in real examples but first, we‚Äôll learn how a typical evaluation workflow looks like. ## üëç Evaluating AI Agents Observability gives us metrics, but evaluation is the process of analyzing that data (and performing tests) to determine how well an AI agent is performing and how it can be improved. In other words, once you have those traces and metrics, how do you use them to judge the agent and make decisions? Regular evaluation is important because AI agents are often non-deterministic and can evolve (through updates or drifting model behavior) ‚Äì without evaluation, you wouldn‚Äôt know if your ‚Äúsmart agent‚Äù is actually doing its job well or if it‚Äôs regressed. There are two categories of evaluations for AI agents: **online evaluation** and **offline evaluation**. Both are valuable, and they complement each other. We usually begin with offline evaluation, as this is the minimum necessary",
    "metadata": {
      "title": "AI Agent Observability and Evaluation",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/what-is-agent-observability-and-evaluation",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "how it can be improved. In other words, once you have those traces and metrics, how do you use them to judge the agent and make decisions? Regular evaluation is important because AI agents are often non-deterministic and can evolve (through updates or drifting model behavior) ‚Äì without evaluation, you wouldn‚Äôt know if your ‚Äúsmart agent‚Äù is actually doing its job well or if it‚Äôs regressed. There are two categories of evaluations for AI agents: **online evaluation** and **offline evaluation**. Both are valuable, and they complement each other. We usually begin with offline evaluation, as this is the minimum necessary step before deploying any agent. ### ü•∑ Offline Evaluation ![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png) This involves evaluating the agent in a controlled setting, typically using test datasets, not live user queries. You use curated datasets where you know what the expected output or correct behavior is, and then run your agent on those. For instance, if you built a math word-problem agent, you might have a [test dataset](https://huggingface.co/datasets/gsm8k) of 100 problems with known answers. Offline evaluation is often done during development (and can be part of CI/CD pipelines) to check improvements or guard against regressions. The benefit is that it‚Äôs **repeatable and you can get clear accuracy metrics since you have ground truth**. You might also simulate user queries and measure the agent‚Äôs responses against ideal answers or use automated metrics as described above. The key challenge with offline eval is ensuring your test dataset is comprehensive and stays relevant ‚Äì the agent might perform well on a fixed test set but encounter very different queries in production. Therefore, you should keep test sets updated with new edge cases and examples that reflect real-world scenarios‚Äã. A mix of small ‚Äúsmoke test‚Äù cases and larger evaluation sets is useful: small sets for quick checks and larger ones for broader performance metrics‚Äã. ### üîÑ Online Evaluation This refers to evaluating the agent in a live, real-world environment, i.e. during actual usage in production. Online evaluation involves monitoring the agent‚Äôs performance on real user interactions and analyzing outcomes continuously. For example, you might track success rates, user satisfaction scores, or other metrics on live traffic. The advantage of online evaluation is that it **captures things you might not anticipate in a lab setting** ‚Äì you can observe model drift over time (if the agent‚Äôs effectiveness degrades as input patterns shift) and catch unexpected queries or situations that weren‚Äôt in your test data‚Äã. It provides a true picture of how the agent behaves in the wild. Online evaluation often involves collecting implicit and explicit user feedback, as discussed, and possibly running shadow tests or A/B tests (where a new version of the agent runs in parallel to compare against the old). The challenge is that it can be tricky to get reliable labels or scores for live interactions ‚Äì you might rely on user feedback or downstream metrics (like did the user click the result). ### ü§ù Combining the two In practice, successful AI agent",
    "metadata": {
      "title": "AI Agent Observability and Evaluation",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/what-is-agent-observability-and-evaluation",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "or situations that weren‚Äôt in your test data‚Äã. It provides a true picture of how the agent behaves in the wild. Online evaluation often involves collecting implicit and explicit user feedback, as discussed, and possibly running shadow tests or A/B tests (where a new version of the agent runs in parallel to compare against the old). The challenge is that it can be tricky to get reliable labels or scores for live interactions ‚Äì you might rely on user feedback or downstream metrics (like did the user click the result). ### ü§ù Combining the two In practice, successful AI agent evaluation blends **online** and **offline** methods‚Äã. You might run regular offline benchmarks to quantitatively score your agent on defined tasks and continuously monitor live usage to catch things the benchmarks miss. For example, offline tests can catch if a code-generation agent‚Äôs success rate on a known set of problems is improving, while online monitoring might alert you that users have started asking a new category of question that the agent struggles with. Combining both gives a more robust picture. In fact, many teams adopt a loop: *offline evaluation ‚Üí deploy new agent version ‚Üí monitor online metrics and collect new failure examples ‚Üí add those examples to offline test set ‚Üí iterate*. This way, evaluation is continuous and ever-improving. ## üßë‚Äçüíª Lets see how this works in practice In the next section, we‚Äôll see examples of how we can use observability tools to monitor and evaluate our agent.",
    "metadata": {
      "title": "AI Agent Observability and Evaluation",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation",
      "course": "agents-course",
      "chapter": "Bonus Unit 2. Agent Observability and Evaluation",
      "chapter_id": "bonus-unit2/what-is-agent-observability-and-evaluation",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit2/what-is-agent-observability-and-evaluation.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Build Your Own Pok√©mon Battle Agent Now that you‚Äôve explored the potential and limitations of Agentic AI in games, it‚Äôs time to get hands-on. In this section, you‚Äôll **build your very own AI Agent to battle in Pok√©mon-style turn-based combat**, using everything you‚Äôve learned throughout the course. We‚Äôll break the system into four key building blocks: - **Poke-env:** A Python library designed to train rule-based or reinforcement learning Pok√©mon bots. - **Pok√©mon Showdown:** An online battle simulator where your agent will fight. - **LLMAgentBase:** A custom Python class we‚Äôve built to connect your LLM with the Poke-env battle environment. - **TemplateAgent:** A starter template you‚Äôll complete to create your own unique battle agent. Let‚Äôs explore each of these components in more detail. ## üß† Poke-env ![Battle gif](https://github.com/hsahovic/poke-env/raw/master/rl-gif.gif) [Poke-env](https://github.com/hsahovic/poke-env) is a Python interface originally built for training reinforcement learning bots by [Haris Sahovic](https://huggingface.co/hsahovic), but we‚Äôve repurposed it for Agentic AI. It allows your agent to interact with Pok√©mon Showdown through a simple API. It provides a `Player` class from which your Agent will inherit, covering everything needed to communicate with the graphical interface. **Documentation**: [poke-env.readthedocs.io](https://poke-env.readthedocs.io/en/stable/) **Repository**: [github.com/hsahovic/poke-env](https://github.com/hsahovic/poke-env) ## ‚öîÔ∏è Pok√©mon Showdown [Pok√©mon Showdown](https://pokemonshowdown.com/) is an [open-source](https://github.com/smogon/Pokemon-Showdown) battle simulator where your agent will play live Pok√©mon battles. It provides a full interface to simulate and display battles in real time. In our challenge, your bot will act just like a human player, choosing moves turn by turn. We‚Äôve deployed a server that all participants will use to battle. Let‚Äôs see who builds the best AI battle Agent! **Repository**: [github.com/smogon/Pokemon-Showdown](https://github.com/smogon/Pokemon-Showdown) **Website**: [pokemonshowdown.com](https://pokemonshowdown.com/) ## üîå LLMAgentBase `LLMAgentBase` is a Python class that extends the `Player` class from **Poke-env**. It serves as the bridge between your **LLM** and the **Pok√©mon battle simulator**, handling input/output formatting and maintaining battle context. This base agent provides a set of tools (defined in `STANDARD_TOOL_SCHEMA`) to interact with the environment, including: - `choose_move`: for selecting an attack during battle - `choose_switch`: for switching Pok√©mon The LLM should use these tools to make decisions during a match. ### üß† Core Logic - `choose_move(battle: Battle)`: This is the main method invoked each turn. It takes a `Battle` object and returns an action string based on the LLM‚Äôs output. ### üîß Key Internal Methods - `_format_battle_state(battle)`: Converts the current battle state into a string, making it suitable for sending to the LLM. - `_find_move_by_name(battle, move_name)`: Finds a move by name, used in LLM responses that call `choose_move`. - `_find_pokemon_by_name(battle, pokemon_name)`: Locates a specific Pok√©mon to switch into, based on the LLM‚Äôs switch command. - `_get_llm_decision(battle_state)`: This method is abstract in the base class. You‚Äôll need to implement it in your own agent (see next section), where you define how to query the LLM and parse its response. Here‚Äôs an excerpt showing how that decision-making works: ``` STANDARD_TOOL_SCHEMA = { \"choose_move\": { ... }, \"choose_switch\": { ... }, } class LLMAgentBase(Player): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.standard_tools = STANDARD_TOOL_SCHEMA self.battle_history = [] def _format_battle_state(self, battle: Battle) -> str: active_pkmn = battle.active_pokemon active_pkmn_info",
    "metadata": {
      "title": "Build Your Own Pok√©mon Battle Agent",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/building_your_pokemon_agent",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/building_your_pokemon_agent",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/building_your_pokemon_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "LLM responses that call `choose_move`. - `_find_pokemon_by_name(battle, pokemon_name)`: Locates a specific Pok√©mon to switch into, based on the LLM‚Äôs switch command. - `_get_llm_decision(battle_state)`: This method is abstract in the base class. You‚Äôll need to implement it in your own agent (see next section), where you define how to query the LLM and parse its response. Here‚Äôs an excerpt showing how that decision-making works: ``` STANDARD_TOOL_SCHEMA = { \"choose_move\": { ... }, \"choose_switch\": { ... }, } class LLMAgentBase(Player): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.standard_tools = STANDARD_TOOL_SCHEMA self.battle_history = [] def _format_battle_state(self, battle: Battle) -> str: active_pkmn = battle.active_pokemon active_pkmn_info = f\"Your active Pokemon: {active_pkmn.species} \" \\ f\"(Type: {'/'.join(map(str, active_pkmn.types))}) \" \\ f\"HP: {active_pkmn.current_hp_fraction * 100:.1f}% \" \\ f\"Status: {active_pkmn.status.name if active_pkmn.status else 'None'} \" \\ f\"Boosts: {active_pkmn.boosts}\" opponent_pkmn = battle.opponent_active_pokemon opp_info_str = \"Unknown\" if opponent_pkmn: opp_info_str = f\"{opponent_pkmn.species} \" \\ f\"(Type: {'/'.join(map(str, opponent_pkmn.types))}) \" \\ f\"HP: {opponent_pkmn.current_hp_fraction * 100:.1f}% \" \\ f\"Status: {opponent_pkmn.status.name if opponent_pkmn.status else 'None'} \" \\ f\"Boosts: {opponent_pkmn.boosts}\" opponent_pkmn_info = f\"Opponent's active Pokemon: {opp_info_str}\" available_moves_info = \"Available moves:\\n\" if battle.available_moves: available_moves_info += \"\\n\".join( [f\"- {move.id} (Type: {move.type}, BP: {move.base_power}, Acc: {move.accuracy}, PP: {move.current_pp}/{move.max_pp}, Cat: {move.category.name})\" for move in battle.available_moves] ) else: available_moves_info += \"- None (Must switch or Struggle)\" available_switches_info = \"Available switches:\\n\" if battle.available_switches: available_switches_info += \"\\n\".join( [f\"- {pkmn.species} (HP: {pkmn.current_hp_fraction * 100:.1f}%, Status: {pkmn.status.name if pkmn.status else 'None'})\" for pkmn in battle.available_switches] ) else: available_switches_info += \"- None\" state_str = f\"{active_pkmn_info}\\n\" \\ f\"{opponent_pkmn_info}\\n\\n\" \\ f\"{available_moves_info}\\n\\n\" \\ f\"{available_switches_info}\\n\\n\" \\ f\"Weather: {battle.weather}\\n\" \\ f\"Terrains: {battle.fields}\\n\" \\ f\"Your Side Conditions: {battle.side_conditions}\\n\" \\ f\"Opponent Side Conditions: {battle.opponent_side_conditions}\" return state_str.strip() def _find_move_by_name(self, battle: Battle, move_name: str) -> Optional[Move]: normalized_name = normalize_name(move_name) # Prioritize exact ID match for move in battle.available_moves: if move.id == normalized_name: return move # Fallback: Check display name (less reliable) for move in battle.available_moves: if move.name.lower() == move_name.lower(): print(f\"Warning: Matched move by display name '{move.name}' instead of ID '{move.id}'. Input was '{move_name}'.\") return move return None def _find_pokemon_by_name(self, battle: Battle, pokemon_name: str) -> Optional[Pokemon]: normalized_name = normalize_name(pokemon_name) for pkmn in battle.available_switches: # Normalize the species name for comparison if normalize_name(pkmn.species) == normalized_name: return pkmn return None async def choose_move(self, battle: Battle) -> str: battle_state_str = self._format_battle_state(battle) decision_result = await self._get_llm_decision(battle_state_str) print(decision_result) decision = decision_result.get(\"decision\") error_message = decision_result.get(\"error\") action_taken = False fallback_reason = \"\" if decision: function_name = decision.get(\"name\") args = decision.get(\"arguments\", {}) if function_name == \"choose_move\": move_name = args.get(\"move_name\") if move_name: chosen_move = self._find_move_by_name(battle, move_name) if chosen_move and chosen_move in battle.available_moves: action_taken = True chat_msg = f\"AI Decision: Using move '{chosen_move.id}'.\" print(chat_msg) return self.create_order(chosen_move) else: fallback_reason = f\"LLM chose unavailable/invalid move '{move_name}'.\" else: fallback_reason = \"LLM 'choose_move' called without 'move_name'.\" elif function_name == \"choose_switch\": pokemon_name = args.get(\"pokemon_name\") if pokemon_name: chosen_switch = self._find_pokemon_by_name(battle, pokemon_name) if chosen_switch and chosen_switch in battle.available_switches: action_taken = True chat_msg = f\"AI Decision: Switching to '{chosen_switch.species}'.\" print(chat_msg) return self.create_order(chosen_switch) else: fallback_reason = f\"LLM chose unavailable/invalid switch '{pokemon_name}'.\" else: fallback_reason = \"LLM 'choose_switch' called without 'pokemon_name'.\" else: fallback_reason = f\"LLM called unknown function '{function_name}'.\" if not action_taken: if not fallback_reason: if error_message: fallback_reason =",
    "metadata": {
      "title": "Build Your Own Pok√©mon Battle Agent",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/building_your_pokemon_agent",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/building_your_pokemon_agent",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/building_your_pokemon_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "chosen_move and chosen_move in battle.available_moves: action_taken = True chat_msg = f\"AI Decision: Using move '{chosen_move.id}'.\" print(chat_msg) return self.create_order(chosen_move) else: fallback_reason = f\"LLM chose unavailable/invalid move '{move_name}'.\" else: fallback_reason = \"LLM 'choose_move' called without 'move_name'.\" elif function_name == \"choose_switch\": pokemon_name = args.get(\"pokemon_name\") if pokemon_name: chosen_switch = self._find_pokemon_by_name(battle, pokemon_name) if chosen_switch and chosen_switch in battle.available_switches: action_taken = True chat_msg = f\"AI Decision: Switching to '{chosen_switch.species}'.\" print(chat_msg) return self.create_order(chosen_switch) else: fallback_reason = f\"LLM chose unavailable/invalid switch '{pokemon_name}'.\" else: fallback_reason = \"LLM 'choose_switch' called without 'pokemon_name'.\" else: fallback_reason = f\"LLM called unknown function '{function_name}'.\" if not action_taken: if not fallback_reason: if error_message: fallback_reason = f\"API Error: {error_message}\" elif decision is None: fallback_reason = \"LLM did not provide a valid function call.\" else: fallback_reason = \"Unknown error processing LLM decision.\" print(f\"Warning: {fallback_reason} Choosing random action.\") if battle.available_moves or battle.available_switches: return self.choose_random_move(battle) else: print(\"AI Fallback: No moves or switches available. Using Struggle/Default.\") return self.choose_default_move(battle) async def _get_llm_decision(self, battle_state: str) -> Dict[str, Any]: raise NotImplementedError(\"Subclasses must implement _get_llm_decision\") ``` **Full source code**: [agents.py](https://huggingface.co/spaces/Jofthomas/twitch_streaming/blob/main/agents.py) ## üß™ TemplateAgent Now comes the fun part! With LLMAgentBase as your foundation, it‚Äôs time to implement your own agent, with your own strategy to climb the leaderboard. You‚Äôll start from this template and build your own logic. We‚Äôve also provided three [complete examples](https://huggingface.co/spaces/Jofthomas/twitch_streaming/blob/main/agents.py) using **OpenAI**, **Mistral**, and **Gemini** models to guide you. Here‚Äôs a simplified version of the template: ``` class TemplateAgent(LLMAgentBase): \"\"\"Uses Template AI API for decisions.\"\"\" def __init__(self, api_key: str = None, model: str = \"model-name\", *args, **kwargs): super().__init__(*args, **kwargs) self.model = model self.template_client = TemplateModelProvider(api_key=...) self.template_tools = list(self.standard_tools.values()) async def _get_llm_decision(self, battle_state: str) -> Dict[str, Any]: \"\"\"Sends state to the LLM and gets back the function call decision.\"\"\" system_prompt = ( \"You are a ...\" ) user_prompt = f\"...\" try: response = await self.template_client.chat.completions.create( model=self.model, messages=[ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}, ], ) message = response.choices[0].message return {\"decision\": {\"name\": function_name, \"arguments\": arguments}} except Exception as e: print(f\"Unexpected error during call: {e}\") return {\"error\": f\"Unexpected error: {e}\"} ``` This code won‚Äôt run out of the box, it‚Äôs a blueprint for your custom logic. With all the pieces ready, it‚Äôs your turn to build a competitive agent. In the next section, we‚Äôll show how to deploy your agent to our server and battle others in real-time. Let the battle begin! üî•",
    "metadata": {
      "title": "Build Your Own Pok√©mon Battle Agent",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/building_your_pokemon_agent",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/building_your_pokemon_agent",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/building_your_pokemon_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nIf you‚Äôve made it this far, congratulations! ü•≥ You‚Äôve successfully built your very own Pok√©mon battle agent! ‚öîÔ∏èüéÆ\n \nYou‚Äôve conquered the fundamentals of **Agentic workflows**, connected an **LLM** to a game environment, and deployed an intelligent Agent ready to face the challenges of battle.\n \nBut the journey doesn‚Äôt end here!\nNow that you have your first Agent up and running, think about how you can evolve it further:\n \n- Can you improve its strategic thinking?\n- How would a memory mechanism or feedback loop change its performance?\n- What experiments could help make it more competitive in battle?\n \nWe‚Äôd love to hear your thoughts on the course and how we can make it even better for future learners.\nGot feedback? üëâ [Fill out this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \nThanks for learning with us, and remember:\n \n**Keep learning, Keep training, keep battling, and stay awesome!** ü§ó",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/conclusion",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# From LLMs to AI Agents\n\n \nWe learned in the [first unit](https://huggingface.co/learn/agents-course/unit1/introduction) of the course that AI Agents are able to plan and make decisions.\nAnd while LLMs have enabled more natural interactions with NPCs, Agentic AI takes it a step further by allowing characters to make decisions, plan actions, and adapt to changing environments.\n \nTo illustrate the difference, think of a classic RPG NPC:\n \n- With an LLM: the NPC might respond to your questions in a more natural, varied way. It‚Äôs great for dialogue, but the NPC remains static, it won‚Äôt act unless you do something first.\n- With Agentic AI: the NPC can decide to go look for help, set a trap, or avoid you completely, even if you‚Äôre not interacting with it directly.\n \nThis small shift changes everything. We‚Äôre moving from scripted responders to autonomous actors within the game world.\n \nThis shift means NPCs can now directly interact with their environment through goal-directed behaviors, ultimately leading to more dynamic and unpredictable gameplay.\n \nAgentic AI empowers NPCs with:\n \n- **Autonomy**: Making independent decisions based on the game state.\n- **Adaptability**: Adjusting strategies in response to player actions.\n- **Persistence**: Remembering past interactions to inform future behavior.\n \nThis transforms NPCs from reactive entities (reacting to your inputs) into proactive participants in the game world, opening the door for innovative gameplay.\n \n\n## The big limitation of Agents: it‚Äôs slow (for now)\n\n \nHowever, let‚Äôs not be too optimistic just yet. Despite its potential, Agentic AI currently faces challenges in real-time applications.\n \nThe reasoning and planning processes can introduce latency, making it less suitable for fast-paced games like *Doom* or *Super Mario Bros.*\n \nTake the example of [Claude Plays Pok√©mon](https://www.twitch.tv/claudeplayspokemon). If you consider the number of tokens needed to **think**, plus the tokens needed to **act**, it becomes clear that we‚Äôd need entirely different decoding strategies to make real-time play feasible.\n \n![Claude plays Pok√©mon](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/claude-plays-pokemon.png)\n \nMost games need to run at around 30 FPS, which means a real-time AI agent would need to act 30 times per second, not currently feasible with today‚Äôs agentic LLMs.\n \nHowever, turn-based games like *Pok√©mon* are ideal candidates, as they allow the AI enough time to deliberate and make strategic decisions.\n \nThat‚Äôs why in the next section, you‚Äôll build your very own AI Agent to battle in Pok√©mon-style turn-based combat, and even challenge it yourself. Let‚Äôs get into it!",
    "metadata": {
      "title": "From LLMs to AI Agents",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/from-llm-to-agents",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/from-llm-to-agents",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/from-llm-to-agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n \n![Bonus Unit 3 AI in Games](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/pokemon_thumbnail.png)\n \nüé∂I want to be the very best ‚Ä¶ üé∂\n \nWelcome to this **bonus unit**, where you‚Äôll explore the exciting intersection of **AI Agents and games**! üéÆü§ñ\n \nImagine a game where non-playable characters (NPCs) don‚Äôt just follow scripted lines, but instead hold dynamic conversations, adapt to your strategies, and evolve as the story unfolds. This is the power of combining **LLMs and agentic behavior in games**: it opens the door to **emergent storytelling and gameplay like never before**.\n \nIn this bonus unit, you‚Äôll:\n \n- Learn how to build an AI Agent that can engage in **Pok√©mon-style turn-based battles**\n- Play against it, or even challenge other agents online\n \nWe‚Äôve already seen [some](https://www.anthropic.com/research/visible-extended-thinking) [examples](https://www.twitch.tv/gemini_plays_pokemon) from the AI community for playing Pok√©mon using LLMs, and in this unit you‚Äôll learn how you can replicate that using your own Agent with the ideas that you‚Äôve learnt through the course.\n \n![Claude plays Pok√©mon](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/claude-plays-pokemon.png)\n \n\n## Want to go further?\n\n \n- üéì **Master LLMs in Games**: Dive deeper into game development with our full course [Machine Learning for Games Course](https://hf.co/learn/ml-games-course).\n- üìò **Get the AI Playbook**: Discover insights, ideas, and practical tips in the [AI Playbook for Game Developers](https://thomassimonini.substack.com/), where the future of intelligent game design is explored.\n \nBut before we build, let‚Äôs see how LLMs are already being used in games with **four inspiring real-world examples**.",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/introduction",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Launching Your Pok√©mon Battle Agent\n\n \nIt‚Äôs now time to battle! ‚ö°Ô∏è\n \n\n## Battle the Stream Agent!\n\n \nIf you don‚Äôt feel like building your own agent, and you‚Äôre just curious about the battle potential of agents in pok√©mon. We are hosting an automated livestream on [twitch](https://www.twitch.tv/jofthomas)\n  \nTo battle the agent in stream you can:\n \nInstructions:\n \n1. Go to the **Pok√©mon Showdown Space**: [Link Here](https://huggingface.co/spaces/Jofthomas/Pokemon_showdown)\n2. **Choose Your Name** (Top-right corner).\n3. Find the **Current Agent‚Äôs Username**. Check:\n\n- The **Stream Display**: [Link Here](https://www.twitch.tv/jofthomas)\n4. **Search** for that username on the Showdown Space and **Send a Battle Invitation**.\n \n*Heads Up:* Only one agent is online at once! Make sure you‚Äôve got the right name.\n \n\n## Pok√©mon Battle Agent Challenger\n\n \nIf you‚Äôve created your own Pok√©mon Battle Agent from the last section, you‚Äôre probably wondering: **how can I test it against others?** Let‚Äôs find out!\n \nWe‚Äôve built a dedicated [Hugging Face Space](https://huggingface.co/spaces/PShowdown/pokemon_agents) for this purpose:\n  \nThis Space is connected to our own **Pok√©mon Showdown server**, where your Agent can take on others in epic AI-powered battles.\n \n\n### How to Launch Your Agent\n\n \nFollow these steps to bring your Agent to life in the arena:\n \n1. **Duplicate the Space**\nClick the three dots in the top-right menu of the Space and select ‚ÄúDuplicate this Space‚Äù.\n2. **Add Your Agent Code to agent.py**\nOpen the file and paste your Agent implementation. You can follow this [example](https://huggingface.co/spaces/PShowdown/pokemon_agents/blob/main/agents.py) or check out the [project structure](https://huggingface.co/spaces/PShowdown/pokemon_agents/tree/main) for guidance.\n3. **Register Your Agent in app.py**\nAdd your Agent‚Äôs name and logic to the dropdown menu. Refer to [this snippet](https://huggingface.co/spaces/PShowdown/pokemon_agents/blob/main/app.py) for inspiration.\n4. **Select Your Agent**\nOnce added, your Agent will show up in the ‚ÄúSelect Agent‚Äù dropdown menu. Pick it from the list! ‚úÖ\n5. **Enter Your Pok√©mon Showdown Username**\nMake sure the username matches the one shown in the iframe‚Äôs **‚ÄúChoose name‚Äù** input. You can also connect with your official account.\n6. **Click ‚ÄúSend Battle Invitation‚Äù**\nYour Agent will send an invite to the selected opponent. It should appear on-screen!\n7. **Accept the Battle & Enjoy the Fight!**\nLet the battle begin! May the smartest Agent win\n \nReady to see your creation in action? Let the AI showdown commence! ü•ä",
    "metadata": {
      "title": "Launching Your Pok√©mon Battle Agent",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/launching_agent_battle",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/launching_agent_battle",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/launching_agent_battle.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# The State of the Art in Using LLMs in Games\n\n \nTo give you a sense of how much progress has been made in this field, let‚Äôs examine three tech demos and one published game that showcase the integration of LLMs in gaming.\n \n\n## üïµÔ∏è‚Äç‚ôÇÔ∏è Covert Protocol by NVIDIA and Inworld AI\n\n \n![Covert Protocol](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/covert-protocol.jpg)\n \nUnveiled at GDC 2024, *Covert Protocol* is a tech demo that places you in the shoes of a private detective.\n \nWhat‚Äôs interesting in this demo is the use of AI-powered NPCs that respond to your inquiries in real-time, influencing the narrative based on your interactions.\n \nThe demo is built on Unreal Engine 5, it leverages NVIDIA‚Äôs Avatar Cloud Engine (ACE) and Inworld‚Äôs AI to create lifelike character interactions.\n \nLearn more here üëâ [Inworld AI Blog](https://inworld.ai/blog/nvidia-inworld-ai-demo-on-device-capabilities)\n \n\n## ü§ñ NEO NPCs by Ubisoft\n\n \n![Neo NPC](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/neo-npc.jpeg)\n \nAlso at GDC 2024, Ubisoft introduced *NEO NPCs*, a prototype showcasing NPCs powered by generative AI.\n \nThese characters can perceive their environment, remember past interactions, and engage in meaningful conversations with players.\n \nThe idea here is to create more immersive and responsive game worlds where the player can have true interaction with NPCs.\n \nLearn more here üëâ [Inworld AI Blog](https://inworld.ai/blog/gdc-2024)\n \n\n## ‚öîÔ∏è Mecha BREAK Featuring NVIDIA‚Äôs ACE\n\n \n![Mecha BREAK](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/mecha-break.jpg)\n \n*Mecha BREAK*, an upcoming multiplayer mech battle game, integrates NVIDIA‚Äôs ACE technology to bring AI-powered NPCs to life.\n \nPlayers can interact with these characters using natural language, and the NPCs can recognize players and objects via webcam, thanks to GPT-4o integration. This innovation promises a more immersive and interactive gaming experience.\n \nLearn more here üëâ [NVIDIA Blog](https://blogs.nvidia.com/blog/digital-human-technology-mecha-break/)\n \n\n## üßõ‚Äç‚ôÇÔ∏è Suck Up! by Proxima Enterprises\n\n \n![Suck Up](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit3/suck-up.jpg)\n \nFinally, *Suck Up!* is a published game where you play as a vampire attempting to gain entry into homes by **convincing AI-powered NPCs to invite you in.**\n \nEach character is driven by generative AI, allowing for dynamic and unpredictable interactions.\n \nLearn more here üëâ [Suck Up! Official Website](https://www.playsuckup.com/)\n \n\n## Wait‚Ä¶ Where Are the Agents?\n\n \nAfter exploring these demos, you might be wondering: ‚ÄúThese examples showcase the use of LLMs in games but they don‚Äôt seem to involve Agents. So, what‚Äôs the distinction, and what additional capabilities do Agents bring to the table?‚Äù\n \nDon‚Äôt worry, it‚Äôs what we‚Äôre going to study in the next section.",
    "metadata": {
      "title": "The State of the Art in Using LLMs in Games",
      "url": "https://huggingface.co/learn/agents-course/bonus-unit3/state-of-art",
      "course": "agents-course",
      "chapter": "Bonus Unit 3. Agents in Games with Pokemon",
      "chapter_id": "bonus-unit3/state-of-art",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/bonus-unit3/state-of-art.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Live 1: How the Course Works and First Q&A\n\n \nIn this first live stream of the Agents Course, we explained how the course **works** (scope, units, challenges, and more) and answered your questions.\n  \nTo know when the next live session is scheduled, check our **Discord server**. We will also send you an email. If you can‚Äôt participate, don‚Äôt worry, we **record all live sessions**.",
    "metadata": {
      "title": "Live 1: How the Course Works and First Q&A",
      "url": "https://huggingface.co/learn/agents-course/communication/live1",
      "course": "agents-course",
      "chapter": "Live 1. How the course works and Q&A",
      "chapter_id": "communication/live1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/communication/live1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# (Optional) Discord 101\n\n \n![The Discord Etiquette](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/discord-etiquette.jpg)\n \nThis guide is designed to help you get started with Discord, a free chat platform popular in the gaming and ML communities.\n \nJoin the Hugging Face Community Discord server, which **has over 100,000 members**, by clicking [here](https://discord.gg/UrrTSsSyjb). It‚Äôs a great place to connect with others!\n \n\n## The Agents course on Hugging Face‚Äôs Discord Community\n\n \nStarting on Discord can be a bit overwhelming, so here‚Äôs a quick guide to help you navigate.\n \nThe HF Community Server hosts a vibrant community with interests in various areas, offering opportunities for learning through paper discussions, events, and more.\n \nAfter [signing up](http://hf.co/join/discord), introduce yourself in the `#introduce-yourself` channel.\n \nWe created 4 channels for the Agents Course:\n \n- `agents-course-announcements`: for the **latest course informations**.\n- `üéì-agents-course-general`: for **general discussions and chitchat**.\n- `agents-course-questions`: to **ask questions and help your classmates**.\n- `agents-course-showcase`: to **show your best agents**.\n \nIn addition you can check:\n \n- `smolagents`: for **discussion and support with the library**.\n \n\n## Tips for using Discord effectively\n\n \n\n### How to join a server\n\n \nIf you are less familiar with Discord, you might want to check out this [guide](https://support.discord.com/hc/en-us/articles/360034842871-How-do-I-join-a-Server#h_01FSJF9GT2QJMS2PRAW36WNBS8) on how to join a server.\n \nHere‚Äôs a quick summary of the steps:\n \n1. Click on the [Invite Link](https://discord.gg/UrrTSsSyjb).\n2. Sign in with your Discord account, or create an account if you don‚Äôt have one.\n3. Validate that you are not an AI agent!\n4. Setup your nickname and avatar.\n5. Click ‚ÄúJoin Server‚Äù.\n \n\n### How to use Discord effectively\n\n \nHere are a few tips for using Discord effectively:\n \n- **Voice channels** are available, though text chat is more commonly used.\n- You can format text using **markdown style**, which is especially useful for writing code. Note that markdown doesn‚Äôt work as well for links.\n- Consider opening threads for **long conversations** to keep discussions organized.\n \nWe hope you find this guide helpful! If you have any questions, feel free to ask us on Discord ü§ó.",
    "metadata": {
      "title": "(Optional) Discord 101",
      "url": "https://huggingface.co/learn/agents-course/unit0/discord101",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/discord101",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/discord101.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Welcome to the ü§ó AI Agents Course ![AI Agents Course thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/thumbnail.jpg) The background of the image was generated using [Scenario.com](https://scenario.com/) Welcome to the most exciting topic in AI today: **Agents**! This free course will take you on a journey, **from beginner to expert**, in understanding, using and building AI agents. This first unit will help you onboard: - Discover the **course‚Äôs syllabus**. - **Choose the path** you‚Äôre going to take (either self-audit or certification process). - **Get more information about the certification process**. - Get to know the team behind the course. - Create your **Hugging Face account**. - **Sign-up to our Discord server**, and meet your classmates and us. Let‚Äôs get started! ## What to expect from this course? In this course, you will: - üìñ Study AI Agents in **theory, design, and practice.** - üßë‚Äçüíª Learn to **use established AI Agent libraries** such as [smolagents](https://huggingface.co/docs/smolagents/en/index), [LlamaIndex](https://www.llamaindex.ai/), and [LangGraph](https://langchain-ai.github.io/langgraph/). - üíæ **Share your agents** on the Hugging Face Hub and explore agents created by the community. - üèÜ Participate in challenges where you will **evaluate your agents against other students‚Äô.** - üéì **Earn a certificate of completion** by completing assignments. And more! At the end of this course, you‚Äôll understand **how Agents work and how to build your own Agents using the latest libraries and tools**. Don‚Äôt forget to **sign up to the course!** (We are respectful of your privacy. We collect your email address to be able to **send you the links when each Unit is published and give you information about the challenges and updates**). ## What does the course look like? The course is composed of: - *Foundational Units*: where you learn Agents **concepts in theory**. - *Hands-on*: where you‚Äôll learn **to use established AI Agent libraries** to train your agents in unique environments. These hands-on sections will be **Hugging Face Spaces** with a pre-configured environment. - *Use case assignments*: where you‚Äôll apply the concepts you‚Äôve learned to solve a real-world problem that you‚Äôll choose. - *The Challenge*: you‚Äôll get to put your agent to compete against other agents in a challenge. There will also be [a leaderboard](https://huggingface.co/spaces/agents-course/Students_leaderboard) for you to compare the agents‚Äô performance. This **course is a living project, evolving with your feedback and contributions!** Feel free to [open issues and PRs in GitHub](https://github.com/huggingface/agents-course), and engage in discussions in our Discord server. After you have gone through the course, you can also send your feedback [üëâ using this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog) ## What‚Äôs the syllabus? Here is the **general syllabus for the course**. A more detailed list of topics will be released with each unit. Chapter Topic Description 0 Onboarding Set you up with the tools and platforms that you will use. 1 Agent Fundamentals Explain Tools, Thoughts, Actions, Observations, and their formats. Explain LLMs, messages, special tokens and chat templates. Show a simple use case using python functions as tools. 2 Frameworks Understand how the fundamentals are implemented in popular libraries : smolagents, LangGraph, LLamaIndex 3 Use Cases Let‚Äôs build some real life use",
    "metadata": {
      "title": "Welcome to the ü§ó AI Agents Course",
      "url": "https://huggingface.co/learn/agents-course/unit0/introduction",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/introduction",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your feedback [üëâ using this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog) ## What‚Äôs the syllabus? Here is the **general syllabus for the course**. A more detailed list of topics will be released with each unit. Chapter Topic Description 0 Onboarding Set you up with the tools and platforms that you will use. 1 Agent Fundamentals Explain Tools, Thoughts, Actions, Observations, and their formats. Explain LLMs, messages, special tokens and chat templates. Show a simple use case using python functions as tools. 2 Frameworks Understand how the fundamentals are implemented in popular libraries : smolagents, LangGraph, LLamaIndex 3 Use Cases Let‚Äôs build some real life use cases (open to PRs ü§ó from experienced Agent builders) 4 Final Assignment Build an agent for a selected benchmark and prove your understanding of Agents on the student leaderboard üöÄ In addition to the main syllabus, you have 3 bonus units: - *Bonus Unit 1* : Fine-tuning an LLM for Function-calling - *Bonus Unit 2* : Agent Observability and Evaluation - *Bonus Unit 3* : Agents in Games with Pokemon For instance, in the Bonus Unit 3, you learn to build your Agent to play Pokemon battles ü•ä. ## What are the prerequisites? To be able to follow this course, you should have a: - Basic knowledge of Python - Basic knowledge of LLMs (we have a section in Unit 1 to recap what they are) ## What tools do I need? You only need 2 things: - *A computer* with an internet connection. - A *Hugging Face Account*: to push and load models, agents, and create Spaces. If you don‚Äôt have an account yet, you can create one **here** (it‚Äôs free). ![Course tools needed](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/tools.jpg) ## The Certification Process ![Two paths](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/three-paths.jpg) You can choose to follow this course *in audit mode*, or do the activities and *get one of the two certificates we‚Äôll issue*. If you audit the course, you can participate in all the challenges and do assignments if you want, and **you don‚Äôt need to notify us**. The certification process is **completely free**: - *To get a certification for fundamentals*: you need to complete Unit 1 of the course. This is intended for students that want to get up to date with the latest trends in Agents. - *To get a certificate of completion*: you need to complete Unit 1, one of the use case assignments we‚Äôll propose during the course, and the final challenge. There‚Äôs **no deadline** for the certification process. ## What is the recommended pace? Each chapter in this course is designed **to be completed in 1 week, with approximately 3-4 hours of work per week**. We provide you a recommended pace: ![Recommended Pace](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/recommended-pace.jpg) ## How to get the most out of the course? To get the most out of the course, we have some advice: 1. [Join study groups in Discord](https://discord.gg/UrrTSsSyjb): studying in groups is always easier. To do that, you need to join our discord server and verify your Hugging Face account. 2. **Do the quizzes and assignments**: the best way to learn",
    "metadata": {
      "title": "Welcome to the ü§ó AI Agents Course",
      "url": "https://huggingface.co/learn/agents-course/unit0/introduction",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/introduction",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for the certification process. ## What is the recommended pace? Each chapter in this course is designed **to be completed in 1 week, with approximately 3-4 hours of work per week**. We provide you a recommended pace: ![Recommended Pace](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/recommended-pace.jpg) ## How to get the most out of the course? To get the most out of the course, we have some advice: 1. [Join study groups in Discord](https://discord.gg/UrrTSsSyjb): studying in groups is always easier. To do that, you need to join our discord server and verify your Hugging Face account. 2. **Do the quizzes and assignments**: the best way to learn is through hands-on practice and self-assessment. 3. **Define a schedule to stay in sync**: you can use our recommended pace schedule below or create yours. ![Course advice](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/advice.jpg) ## Who are we This course is maintained by [Ben Burtenshaw](https://huggingface.co/burtenshaw) and [Sergio Paniego](https://huggingface.co/sergiopaniego). If you have any questions, please contact us on the Hub! ## Acknowledgments We would like to extend our gratitude to the following individuals for their invaluable contributions to this course: - **Joffrey Thomas** ‚Äì For writing and developing the course. - **Thomas Simonini** ‚Äì For writing and developing the course. - **Pedro Cuenca** ‚Äì For guiding the course and providing feedback. - **Aymeric Roucher** ‚Äì For his amazing demo spaces ( decoding and final agent ) as well as his help on the smolagents parts. - **Joshua Lochner** ‚Äì For his amazing demo space on tokenization. - **Quentin Gallou√©dec** ‚Äì For his help on the course content. - **David Berenstein** ‚Äì For his help on the course content and moderation. - **XiaXiao (ShawnSiao)** ‚Äì Chinese translator for the course. - **Jiaming Huang** ‚Äì Chinese translator for the course. - **Kim Noel** - French translator for the course. - **Lo√Øck Bourdois** - French translator for the course from [CATIE](https://www.catie.fr/). ## I found a bug, or I want to improve the course Contributions are **welcome** ü§ó - If you *found a bug üêõ in a notebook*, please [open an issue](https://github.com/huggingface/agents-course/issues) and **describe the problem**. - If you *want to improve the course*, you can [open a Pull Request.](https://github.com/huggingface/agents-course/pulls) - If you *want to add a full section or a new unit*, the best is to [open an issue](https://github.com/huggingface/agents-course/issues) and **describe what content you want to add before starting to write it so that we can guide you**. ## I still have questions Please ask your question in our [discord server #agents-course-questions.](https://discord.gg/UrrTSsSyjb) Now that you have all the information, let‚Äôs get on board ‚õµ ![Time to Onboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/time-to-onboard.jpg)",
    "metadata": {
      "title": "Welcome to the ü§ó AI Agents Course",
      "url": "https://huggingface.co/learn/agents-course/unit0/introduction",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/introduction",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Onboarding: Your First Steps ‚õµ ![Time to Onboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit0/time-to-onboard.jpg) Now that you have all the details, let‚Äôs get started! We‚Äôre going to do four things: 1. **Create your Hugging Face Account** if it‚Äôs not already done 2. **Sign up to Discord and introduce yourself** (don‚Äôt be shy ü§ó) 3. **Follow the Hugging Face Agents Course** on the Hub 4. **Spread the word** about the course ### Step 1: Create Your Hugging Face Account (If you haven‚Äôt already) create a Hugging Face account [here](https://huggingface.co/join). ### Step 2: Join Our Discord Community üëâüèª Join our discord server [here.](https://discord.gg/UrrTSsSyjb) When you join, remember to introduce yourself in `#introduce-yourself`. We have multiple AI Agent-related channels: - `agents-course-announcements`: for the **latest course information**. - `üéì-agents-course-general`: for **general discussions and chitchat**. - `agents-course-questions`: to **ask questions and help your classmates**. - `agents-course-showcase`: to **show your best agents**. In addition you can check: - `smolagents`: for **discussion and support with the library**. If this is your first time using Discord, we wrote a Discord 101 to get the best practices. Check [the next section](discord101). ### Step 3: Follow the Hugging Face Agent Course Organization Stay up to date with the latest course materials, updates, and announcements **by following the Hugging Face Agents Course Organization**. üëâ Go [here](https://huggingface.co/agents-course) and click on **follow**. ![Follow](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/hf_course_follow.gif) ### Step 4: Spread the word about the course Help us make this course more visible! There are two ways you can help us: 1. Show your support by ‚≠ê [the course‚Äôs repository](https://github.com/huggingface/agents-course). ![Repo star](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/please_star.gif) 1. Share Your Learning Journey: Let others **know you‚Äôre taking this course**! We‚Äôve prepared an illustration you can use in your social media posts ![](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png) You can download the image by clicking üëâ [here](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png?download=true) ### Step 5: Running Models Locally with Ollama (In case you run into Credit limits) 1. **Install Ollama** Follow the official Instructions [here.](https://ollama.com/download) 2. **Pull a model Locally** ``` ollama pull qwen2:7b ``` Here, we pull the [qwen2:7b model](https://ollama.com/library/qwen2:7b). Check out [the ollama website](https://ollama.com/search) for more models. 3. **Start Ollama in the background (In one terminal)** ``` ollama serve ``` If you run into the error ‚Äúlisten tcp 127.0.0.1:11434: bind: address already in use‚Äù, you can use command `sudo lsof -i :11434` to identify the process ID (PID) that is currently using this port. If the process is `ollama`, it is likely that the installation script above has started ollama service, so you can skip this command to start Ollama. 4. **Use LiteLLMModel Instead of InferenceClientModel** To use `LiteLLMModel` module in `smolagents`, you may run `pip` command to install the module. ``` pip install 'smolagents[litellm]' ``` ``` from smolagents import LiteLLMModel model = LiteLLMModel( model_id=\"ollama_chat/qwen2:7b\", # Or try other Ollama-supported models api_base=\"http://127.0.0.1:11434\", # Default Ollama local server num_ctx=8192, ) ``` 1. **Why this works?** - Ollama serves models locally using an OpenAI-compatible API at `http://localhost:11434`. - `LiteLLMModel` is built to communicate with any model that supports the OpenAI chat/completion API format. - This means you can simply swap out `InferenceClientModel` for `LiteLLMModel` no other code changes",
    "metadata": {
      "title": "Onboarding: Your First Steps ‚õµ",
      "url": "https://huggingface.co/learn/agents-course/unit0/onboarding",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/onboarding",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/onboarding.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "command to start Ollama. 4. **Use LiteLLMModel Instead of InferenceClientModel** To use `LiteLLMModel` module in `smolagents`, you may run `pip` command to install the module. ``` pip install 'smolagents[litellm]' ``` ``` from smolagents import LiteLLMModel model = LiteLLMModel( model_id=\"ollama_chat/qwen2:7b\", # Or try other Ollama-supported models api_base=\"http://127.0.0.1:11434\", # Default Ollama local server num_ctx=8192, ) ``` 1. **Why this works?** - Ollama serves models locally using an OpenAI-compatible API at `http://localhost:11434`. - `LiteLLMModel` is built to communicate with any model that supports the OpenAI chat/completion API format. - This means you can simply swap out `InferenceClientModel` for `LiteLLMModel` no other code changes required. It‚Äôs a seamless, plug-and-play solution. Congratulations! üéâ **You‚Äôve completed the onboarding process**! You‚Äôre now ready to start learning about AI Agents. Have fun! Keep Learning, stay awesome ü§ó",
    "metadata": {
      "title": "Onboarding: Your First Steps ‚õµ",
      "url": "https://huggingface.co/learn/agents-course/unit0/onboarding",
      "course": "agents-course",
      "chapter": "Unit 0. Welcome to the course",
      "chapter_id": "unit0/onboarding",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit0/onboarding.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Actions: Enabling the Agent to Engage with Its Environment > In this section, we explore the concrete steps an AI agent takes to interact with its environment.We‚Äôll cover how actions are represented (using JSON or code), the importance of the stop and parse approach, and introduce different types of agents. Actions are the concrete steps an **AI agent takes to interact with its environment**. Whether it‚Äôs browsing the web for information or controlling a physical device, each action is a deliberate operation executed by the agent. For example, an agent assisting with customer service might retrieve customer data, offer support articles, or transfer issues to a human representative. ## Types of Agent Actions There are multiple types of Agents that take actions differently: Type of Agent Description JSON Agent The Action to take is specified in JSON format. Code Agent The Agent writes a code block that is interpreted externally. Function-calling Agent It is a subcategory of the JSON Agent which has been fine-tuned to generate a new message for each action. Actions themselves can serve many purposes: Type of Action Description Information Gathering Performing web searches, querying databases, or retrieving documents. Tool Usage Making API calls, running calculations, and executing code. Environment Interaction Manipulating digital interfaces or controlling physical devices. Communication Engaging with users via chat or collaborating with other agents. The LLM only handles text and uses it to describe the action it wants to take and the parameters to supply to the tool. For an agent to work properly, the LLM must STOP generating new tokens after emitting all the tokens to define a complete Action. This passes control from the LLM back to the agent and ensures the result is parseable - whether the intended format is JSON, code, or function-calling. ## The Stop and Parse Approach One key method for implementing actions is the **stop and parse approach**. This method ensures that the agent‚Äôs output is structured and predictable: 1. **Generation in a Structured Format**: The agent outputs its intended action in a clear, predetermined format (JSON or code). 1. **Halting Further Generation**: Once the text defining the action has been emitted, **the LLM stops generating additional tokens**. This prevents extra or erroneous output. 1. **Parsing the Output**: An external parser reads the formatted action, determines which Tool to call, and extracts the required parameters. For example, an agent needing to check the weather might output: ``` Thought: I need to check the current weather for New York. Action : { \"action\": \"get_weather\", \"action_input\": {\"location\": \"New York\"} } ``` The framework can then easily parse the name of the function to call and the arguments to apply. This clear, machine-readable format minimizes errors and enables external tools to accurately process the agent‚Äôs command. Note: Function-calling agents operate similarly by structuring each action so that a designated function is invoked with the correct arguments. We‚Äôll dive deeper into those types of Agents in a future Unit. ## Code Agents An alternative approach is using *Code",
    "metadata": {
      "title": "Actions:  Enabling the Agent to Engage with Its Environment",
      "url": "https://huggingface.co/learn/agents-course/unit1/actions",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/actions",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/actions.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "might output: ``` Thought: I need to check the current weather for New York. Action : { \"action\": \"get_weather\", \"action_input\": {\"location\": \"New York\"} } ``` The framework can then easily parse the name of the function to call and the arguments to apply. This clear, machine-readable format minimizes errors and enables external tools to accurately process the agent‚Äôs command. Note: Function-calling agents operate similarly by structuring each action so that a designated function is invoked with the correct arguments. We‚Äôll dive deeper into those types of Agents in a future Unit. ## Code Agents An alternative approach is using *Code Agents*. The idea is: **instead of outputting a simple JSON object**, a Code Agent generates an **executable code block‚Äîtypically in a high-level language like Python**. ![Code Agents](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/code-vs-json-actions.png) This approach offers several advantages: - **Expressiveness:** Code can naturally represent complex logic, including loops, conditionals, and nested functions, providing greater flexibility than JSON. - **Modularity and Reusability:** Generated code can include functions and modules that are reusable across different actions or tasks. - **Enhanced Debuggability:** With a well-defined programming syntax, code errors are often easier to detect and correct. - **Direct Integration:** Code Agents can integrate directly with external libraries and APIs, enabling more complex operations such as data processing or real-time decision making. You must keep in mind that executing LLM-generated code may pose security risks, from prompt injection to the execution of harmful code. That‚Äôs why it‚Äôs recommended to use AI agent frameworks like `smolagents` that integrate default safeguards. If you want to know more about the risks and how to mitigate them, [please have a look at this dedicated section](https://huggingface.co/docs/smolagents/tutorials/secure_code_execution). For example, a Code Agent tasked with fetching the weather might generate the following Python snippet: ``` # Code Agent Example: Retrieve Weather Information def get_weather(city): import requests api_url = f\"https://api.weather.com/v1/location/{city}?apiKey=YOUR_API_KEY\" response = requests.get(api_url) if response.status_code == 200: data = response.json() return data.get(\"weather\", \"No weather information available\") else: return \"Error: Unable to fetch weather data.\" # Execute the function and prepare the final answer result = get_weather(\"New York\") final_answer = f\"The current weather in New York is: {result}\" print(final_answer) ``` In this example, the Code Agent: - Retrieves weather data **via an API call**, - Processes the response, - And uses the print() function to output a final answer. This method **also follows the stop and parse approach** by clearly delimiting the code block and signaling when execution is complete (here, by printing the final_answer). We learned that Actions bridge an agent‚Äôs internal reasoning and its real-world interactions by executing clear, structured tasks‚Äîwhether through JSON, code, or function calls. This deliberate execution ensures that each action is precise and ready for external processing via the stop and parse approach. In the next section, we will explore Observations to see how agents capture and integrate feedback from their environment. After this, we will **finally be ready to build our first Agent!**",
    "metadata": {
      "title": "Actions:  Enabling the Agent to Engage with Its Environment",
      "url": "https://huggingface.co/learn/agents-course/unit1/actions",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/actions",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/actions.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Understanding AI Agents through the Thought-Action-Observation Cycle ![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-3.jpg) In the previous sections, we learned: - **How tools are made available to the agent in the system prompt**. - **How AI agents are systems that can ‚Äòreason‚Äô, plan, and interact with their environment**. In this section, **we‚Äôll explore the complete AI Agent Workflow**, a cycle we defined as Thought-Action-Observation. And then, we‚Äôll dive deeper into each of these steps. ## The Core Components Agents‚Äô work is a continuous cycle of: **thinking (Thought) ‚Üí acting (Act) and observing (Observe)**. Let‚Äôs break down these actions together: 1. **Thought**: The LLM part of the Agent decides what the next step should be. 2. **Action:** The agent takes an action by calling the tools with the associated arguments. 3. **Observation:** The model reflects on the response from the tool. ## The Thought-Action-Observation Cycle The three components work together in a continuous loop. To use an analogy from programming, the agent uses a **while loop**: the loop continues until the objective of the agent has been fulfilled. Visually, it looks like this: ![Think, Act, Observe cycle](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AgentCycle.gif) In many Agent frameworks, **the rules and guidelines are embedded directly into the system prompt**, ensuring that every cycle adheres to a defined logic. In a simplified version, our system prompt may look like this: ![Think, Act, Observe cycle](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/system_prompt_cycle.png) We see here that in the System Message we defined : - The *Agent‚Äôs behavior*. - The *Tools our Agent has access to*, as we described in the previous section. - The *Thought-Action-Observation Cycle*, that we bake into the LLM instructions. Let‚Äôs take a small example to understand the process before going deeper into each step of the process. ## Alfred, the weather Agent We created Alfred, the Weather Agent. A user asks Alfred: ‚ÄúWhat‚Äôs the current weather in New York?‚Äù ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent.jpg) Alfred‚Äôs job is to answer this query using a weather API tool. Here‚Äôs how the cycle unfolds: ### Thought **Internal Reasoning:** Upon receiving the query, Alfred‚Äôs internal dialogue might be: *‚ÄúThe user needs current weather information for New York. I have access to a tool that fetches weather data. First, I need to call the weather API to get up-to-date details.‚Äù* This step shows the agent breaking the problem into steps: first, gathering the necessary data. ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-1.jpg) ### Action **Tool Usage:** Based on its reasoning and the fact that Alfred knows about a `get_weather` tool, Alfred prepares a JSON-formatted command that calls the weather API tool. For example, its first action could be: Thought: I need to check the current weather for New York. ``` { \"action\": \"get_weather\", \"action_input\": { \"location\": \"New York\" } } ``` Here, the action clearly specifies which tool to call (e.g., get_weather) and what parameter to pass (the ‚Äúlocation‚Äù: ‚ÄúNew York‚Äù). ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-2.jpg) ### Observation **Feedback from the Environment:** After the tool call, Alfred receives an observation. This might be the raw weather data from the API such as: *‚ÄúCurrent weather in New York: partly cloudy, 15¬∞C, 60% humidity.‚Äù*",
    "metadata": {
      "title": "Understanding AI Agents through the Thought-Action-Observation Cycle",
      "url": "https://huggingface.co/learn/agents-course/unit1/agent-steps-and-structure",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/agent-steps-and-structure",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/agent-steps-and-structure.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tool, Alfred prepares a JSON-formatted command that calls the weather API tool. For example, its first action could be: Thought: I need to check the current weather for New York. ``` { \"action\": \"get_weather\", \"action_input\": { \"location\": \"New York\" } } ``` Here, the action clearly specifies which tool to call (e.g., get_weather) and what parameter to pass (the ‚Äúlocation‚Äù: ‚ÄúNew York‚Äù). ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-2.jpg) ### Observation **Feedback from the Environment:** After the tool call, Alfred receives an observation. This might be the raw weather data from the API such as: *‚ÄúCurrent weather in New York: partly cloudy, 15¬∞C, 60% humidity.‚Äù* ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-3.jpg) This observation is then added to the prompt as additional context. It functions as real-world feedback, confirming whether the action succeeded and providing the needed details. ### Updated thought **Reflecting:** With the observation in hand, Alfred updates its internal reasoning: *‚ÄúNow that I have the weather data for New York, I can compile an answer for the user.‚Äù* ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-4.jpg) ### Final Action Alfred then generates a final response formatted as we told it to: Thought: I have the weather data now. The current weather in New York is partly cloudy with a temperature of 15¬∞C and 60% humidity.‚Äù Final answer : The current weather in New York is partly cloudy with a temperature of 15¬∞C and 60% humidity. This final action sends the answer back to the user, closing the loop. ![Alfred Agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-agent-5.jpg) What we see in this example: - **Agents iterate through a loop until the objective is fulfilled:** **Alfred‚Äôs process is cyclical**. It starts with a thought, then acts by calling a tool, and finally observes the outcome. If the observation had indicated an error or incomplete data, Alfred could have re-entered the cycle to correct its approach. - **Tool Integration:** The ability to call a tool (like a weather API) enables Alfred to go **beyond static knowledge and retrieve real-time data**, an essential aspect of many AI Agents. - **Dynamic Adaptation:** Each cycle allows the agent to incorporate fresh information (observations) into its reasoning (thought), ensuring that the final answer is well-informed and accurate. This example showcases the core concept behind the *ReAct cycle* (a concept we‚Äôre going to develop in the next section): **the interplay of Thought, Action, and Observation empowers AI agents to solve complex tasks iteratively**. By understanding and applying these principles, you can design agents that not only reason about their tasks but also **effectively utilize external tools to complete them**, all while continuously refining their output based on environmental feedback. Let‚Äôs now dive deeper into the Thought, Action, Observation as the individual steps of the process.",
    "metadata": {
      "title": "Understanding AI Agents through the Thought-Action-Observation Cycle",
      "url": "https://huggingface.co/learn/agents-course/unit1/agent-steps-and-structure",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/agent-steps-and-structure",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/agent-steps-and-structure.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nCongratulations on finishing this first Unit ü•≥\n \nYou‚Äôve just **mastered the fundamentals of Agents** and you‚Äôve created your first AI Agent!\n \nIt‚Äôs **normal if you still feel confused by some of these elements**. Agents are a complex topic and it‚Äôs common to take a while to grasp everything.\n \n**Take time to really grasp the material** before continuing. It‚Äôs important to master these elements and have a solid foundation before entering the fun part.\n \nAnd if you pass the Quiz test, don‚Äôt forget to get your certificate üéì üëâ [here](https://huggingface.co/spaces/agents-course/unit1-certification-app)\n \n![Certificate Example](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/certificate-example.jpg)\n \nIn the next (bonus) unit, you‚Äôre going to learn **to fine-tune a Agent to do function calling (aka to be able to call tools based on user prompt)**.\n \nFinally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then, please üëâ [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \n\n### Keep Learning, stay awesome ü§ó",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit1/conclusion",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Dummy Agent Library ![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-unit1sub3DONE.jpg) This course is framework-agnostic because we want to **focus on the concepts of AI agents and avoid getting bogged down in the specifics of a particular framework**. Also, we want students to be able to use the concepts they learn in this course in their own projects, using any framework they like. Therefore, for this Unit 1, we will use a dummy agent library and a simple serverless API to access our LLM engine. You probably wouldn‚Äôt use these in production, but they will serve as a good **starting point for understanding how agents work**. After this section, you‚Äôll be ready to **create a simple Agent** using `smolagents` And in the following Units we will also use other AI Agent libraries like `LangGraph`, and `LlamaIndex`. To keep things simple we will use a simple Python function as a Tool and Agent. We will use built-in Python packages like `datetime` and `os` so that you can try it out in any environment. You can follow the process [in this notebook](https://huggingface.co/agents-course/notebooks/blob/main/unit1/dummy_agent_library.ipynb) and **run the code yourself**. ## Serverless API In the Hugging Face ecosystem, there is a convenient feature called Serverless API that allows you to easily run inference on many models. There‚Äôs no installation or deployment required. ``` import os from huggingface_hub import InferenceClient ## You need a token from https://hf.co/settings/tokens, ensure that you select 'read' as the token type. If you run this on Google Colab, you can set it up in the \"settings\" tab under \"secrets\". Make sure to call it \"HF_TOKEN\" # HF_TOKEN = os.environ.get(\"HF_TOKEN\") client = InferenceClient(model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\") ``` We use the `chat` method since it is a convenient and reliable way to apply chat templates: ``` output = client.chat.completions.create( messages=[ {\"role\": \"user\", \"content\": \"The capital of France is\"}, ], stream=False, max_tokens=1024, ) print(output.choices[0].message.content) ``` output: ``` Paris. ``` The chat method is the RECOMMENDED method to use in order to ensure a smooth transition between models. ## Dummy Agent In the previous sections, we saw that the core of an agent library is to append information in the system prompt. This system prompt is a bit more complex than the one we saw earlier, but it already contains: 1. **Information about the tools** 2. **Cycle instructions** (Thought ‚Üí Action ‚Üí Observation) ``` # This system prompt is a bit more complex and actually contains the function description already appended. # Here we suppose that the textual description of the tools has already been appended. SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools: get_weather: Get the current weather in a given location The way you use the tools is by specifying a json blob. Specifically, this json should have an `action` key (with the name of the tool to use) and an `action_input` key (with the input to the tool going here). The only values that should be in the \"action\" field are: get_weather: Get the current weather in a given location,",
    "metadata": {
      "title": "Dummy Agent Library",
      "url": "https://huggingface.co/learn/agents-course/unit1/dummy-agent-library",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/dummy-agent-library",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/dummy-agent-library.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Here we suppose that the textual description of the tools has already been appended. SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools: get_weather: Get the current weather in a given location The way you use the tools is by specifying a json blob. Specifically, this json should have an `action` key (with the name of the tool to use) and an `action_input` key (with the input to the tool going here). The only values that should be in the \"action\" field are: get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}} example use : {{ \"action\": \"get_weather\", \"action_input\": {\"location\": \"New York\"} }} ALWAYS use the following format: Question: the input question you must answer Thought: you should always think about one action to take. Only one action at a time in this format: Action: $JSON_BLOB (inside markdown cell) Observation: the result of the action. This Observation is unique, complete, and the source of truth. ... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.) You must always end your output with the following format: Thought: I now know the final answer Final Answer: the final answer to the original input question Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \"\"\" ``` We need to append the user instruction after the system prompt. This happens inside the `chat` method. We can see this process below: ``` messages = [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": \"What's the weather in London?\"}, ] print(messages) ``` The prompt now is: ``` <|begin_of_text|><|start_header_id|>system<|end_header_id|> Answer the following questions as best you can. You have access to the following tools: get_weather: Get the current weather in a given location The way you use the tools is by specifying a json blob. Specifically, this json should have an `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here). The only values that should be in the \"action\" field are: get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}} example use : {{ \"action\": \"get_weather\", \"action_input\": {\"location\": \"New York\"} }} ALWAYS use the following format: Question: the input question you must answer Thought: you should always think about one action to take. Only one action at a time in this format: Action: $JSON_BLOB (inside markdown cell) Observation: the result of the action. This Observation is unique, complete, and the source of truth. ... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.) You must always end your output with the following format: Thought: I now know the final answer Final Answer: the final answer to the original input question Now",
    "metadata": {
      "title": "Dummy Agent Library",
      "url": "https://huggingface.co/learn/agents-course/unit1/dummy-agent-library",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/dummy-agent-library",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/dummy-agent-library.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "must answer Thought: you should always think about one action to take. Only one action at a time in this format: Action: $JSON_BLOB (inside markdown cell) Observation: the result of the action. This Observation is unique, complete, and the source of truth. ... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.) You must always end your output with the following format: Thought: I now know the final answer Final Answer: the final answer to the original input question Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. <|eot_id|><|start_header_id|>user<|end_header_id|> What's the weather in London ? <|eot_id|><|start_header_id|>assistant<|end_header_id|> ``` Let‚Äôs call the `chat` method! ``` output = client.chat.completions.create( messages=messages, stream=False, max_tokens=200, ) print(output.choices[0].message.content) ``` output: ``` Thought: To answer the question, I need to get the current weather in London. Action: ``` { \"action\": \"get_weather\", \"action_input\": {\"location\": \"London\"} } ``` Observation: The current weather in London is partly cloudy with a temperature of 12¬∞C. Thought: I now know the final answer. Final Answer: The current weather in London is partly cloudy with a temperature of 12¬∞C. ``` Do you see the issue? > At this point, the model is hallucinating, because it‚Äôs producing a fabricated ‚ÄúObservation‚Äù ‚Äî a response that it generates on its own rather than being the result of an actual function or tool call. > To prevent this, we stop generating right before ‚ÄúObservation:‚Äú. > This allows us to manually run the function (e.g.,get_weather) and then insert the real output as the Observation. ``` # The answer was hallucinated by the model. We need to stop to actually execute the function! output = client.chat.completions.create( messages=messages, max_tokens=150, stop=[\"Observation:\"] # Let's stop before any actual function is called ) print(output.choices[0].message.content) ``` output: ``` Thought: To answer the question, I need to get the current weather in London. Action: ``` { \"action\": \"get_weather\", \"action_input\": {\"location\": \"London\"} } ``` Much Better! Let‚Äôs now create a **dummy get weather function**. In a real situation you could call an API. ``` # Dummy function def get_weather(location): return f\"the weather in {location} is sunny with low temperatures. \\n\" get_weather('London') ``` output: ``` 'the weather in London is sunny with low temperatures. \\n' ``` Let‚Äôs concatenate the system prompt, the base prompt, the completion until function execution and the result of the function as an Observation and resume generation. ``` messages=[ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": \"What's the weather in London ?\"}, {\"role\": \"assistant\", \"content\": output.choices[0].message.content + \"Observation:\\n\" + get_weather('London')}, ] output = client.chat.completions.create( messages=messages, stream=False, max_tokens=200, ) print(output.choices[0].message.content) ``` Here is the new prompt: ``` <|begin_of_text|><|start_header_id|>system<|end_header_id|> Answer the following questions as best you can. You have access to the following tools: get_weather: Get the current weather in a given location The way you use the tools is by specifying a json blob. Specifically, this json should have a `action` key (with the name of the tool",
    "metadata": {
      "title": "Dummy Agent Library",
      "url": "https://huggingface.co/learn/agents-course/unit1/dummy-agent-library",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/dummy-agent-library",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/dummy-agent-library.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "result of the function as an Observation and resume generation. ``` messages=[ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": \"What's the weather in London ?\"}, {\"role\": \"assistant\", \"content\": output.choices[0].message.content + \"Observation:\\n\" + get_weather('London')}, ] output = client.chat.completions.create( messages=messages, stream=False, max_tokens=200, ) print(output.choices[0].message.content) ``` Here is the new prompt: ``` <|begin_of_text|><|start_header_id|>system<|end_header_id|> Answer the following questions as best you can. You have access to the following tools: get_weather: Get the current weather in a given location The way you use the tools is by specifying a json blob. Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here). The only values that should be in the \"action\" field are: get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}} example use : { \"action\": \"get_weather\", \"action_input\": {\"location\": \"New York\"} } ALWAYS use the following format: Question: the input question you must answer Thought: you should always think about one action to take. Only one action at a time in this format: Action: $JSON_BLOB (inside markdown cell) Observation: the result of the action. This Observation is unique, complete, and the source of truth. ... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.) You must always end your output with the following format: Thought: I now know the final answer Final Answer: the final answer to the original input question Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. <|eot_id|><|start_header_id|>user<|end_header_id|> What's the weather in London? <|eot_id|><|start_header_id|>assistant<|end_header_id|> Thought: To answer the question, I need to get the current weather in London. Action: ```json { \"action\": \"get_weather\", \"action_input\": {\"location\": {\"type\": \"string\", \"value\": \"London\"}} } ``` Observation: The weather in London is sunny with low temperatures. ``` Output: ``` Final Answer: The weather in London is sunny with low temperatures. ``` We learned how we can create Agents from scratch using Python code, and we **saw just how tedious that process can be**. Fortunately, many Agent libraries simplify this work by handling much of the heavy lifting for you. Now, we‚Äôre ready **to create our first real Agent** using the `smolagents` library.",
    "metadata": {
      "title": "Dummy Agent Library",
      "url": "https://huggingface.co/learn/agents-course/unit1/dummy-agent-library",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/dummy-agent-library",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/dummy-agent-library.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Unit 1 Quiz\n\n \n![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-unit1sub4DONE.jpg)\n \nWell done on working through the first unit! Let‚Äôs test your understanding of the key concepts covered so far.\n \nWhen you pass the quiz, proceed to the next section to claim your certificate.\n \nGood luck!\n \n\n## Quiz\n\n \nHere is the interactive quiz. The quiz is hosted on the Hugging Face Hub in a space. It will take you through a set of multiple choice questions to test your understanding of the key concepts covered in this unit. Once you‚Äôve completed the quiz, you‚Äôll be able to see your score and a breakdown of the correct answers.\n \nOne important thing: **don‚Äôt forget to click on Submit after you passed, otherwise your exam score will not be saved!**\n  \nYou can also access the quiz üëâ [here](https://huggingface.co/spaces/agents-course/unit_1_quiz)\n \n\n## Certificate\n\n \nNow that you have successfully passed the quiz, **you can get your certificate üéì**\n \nWhen you complete the quiz, it will grant you access to a certificate of completion for this unit. You can download and share this certificate to showcase your progress in the course.\n \n![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-unit1sub5DONE.jpg)\n \nOnce you receive your certificate, you can add it to your LinkedIn üßë‚Äçüíº or share it on X, Bluesky, etc. **We would be super proud and would love to congratulate you if you tag @huggingface**! ü§ó",
    "metadata": {
      "title": "Unit 1 Quiz",
      "url": "https://huggingface.co/learn/agents-course/unit1/final-quiz",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/final-quiz",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/final-quiz.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Agents\n\n \n![Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n \nWelcome to this first unit, where **you‚Äôll build a solid foundation in the fundamentals of AI Agents** including:\n \n- **Understanding Agents**\n \n- What is an Agent, and how does it work?\n- How do Agents make decisions using reasoning and planning?\n- **The Role of LLMs (Large Language Models) in Agents**\n \n- How LLMs serve as the ‚Äúbrain‚Äù behind an Agent.\n- How LLMs structure conversations via the Messages system.\n- **Tools and Actions**\n \n- How Agents use external tools to interact with the environment.\n- How to build and integrate tools for your Agent.\n- **The Agent Workflow:**\n \n- *Think* ‚Üí *Act* ‚Üí *Observe*.\n \nAfter exploring these topics, **you‚Äôll build your first Agent** using `smolagents`!\n \nYour Agent, named Alfred, will handle a simple task and demonstrate how to apply these concepts in practice.\n \nYou‚Äôll even learn how to **publish your Agent on Hugging Face Spaces**, so you can share it with friends and colleagues.\n \nFinally, at the end of this Unit, you‚Äôll take a quiz. Pass it, and you‚Äôll **earn your first course certification**: the üéì Certificate of Fundamentals of Agents.\n \n![Certificate Example](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/certificate-example.jpg)\n \nThis Unit is your **essential starting point**, laying the groundwork for understanding Agents before you move on to more advanced topics.\n \n![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-no-check.jpg)\n \nIt‚Äôs a big unit, so **take your time** and don‚Äôt hesitate to come back to these sections from time to time.\n \nReady? Let‚Äôs dive in! üöÄ",
    "metadata": {
      "title": "Introduction to Agents",
      "url": "https://huggingface.co/learn/agents-course/unit1/introduction",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Messages and Special Tokens Now that we understand how LLMs work, let‚Äôs look at **how they structure their generations through chat templates**. Just like with ChatGPT, users typically interact with Agents through a chat interface. Therefore, we aim to understand how LLMs manage chats. > Q: But ‚Ä¶ When, I‚Äôm interacting with ChatGPT/Hugging Chat, I‚Äôm having a conversation using chat Messages, not a single prompt sequenceA: That‚Äôs correct! But this is in fact a UI abstraction. Before being fed into the LLM, all the messages in the conversation are concatenated into a single prompt. The model does not ‚Äúremember‚Äù the conversation: it reads it in full every time. Up until now, we‚Äôve discussed prompts as the sequence of tokens fed into the model. But when you chat with systems like ChatGPT or HuggingChat, **you‚Äôre actually exchanging messages**. Behind the scenes, these messages are **concatenated and formatted into a prompt that the model can understand**. ![Behind models](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg) We see here the difference between what we see in UI and the prompt fed to the model. This is where chat templates come in. They act as the **bridge between conversational messages (user and assistant turns) and the specific formatting requirements** of your chosen LLM. In other words, chat templates structure the communication between the user and the agent, ensuring that every model‚Äîdespite its unique special tokens‚Äîreceives the correctly formatted prompt. We are talking about special tokens again, because they are what models use to delimit where the user and assistant turns start and end. Just as each LLM uses its own EOS (End Of Sequence) token, they also use different formatting rules and delimiters for the messages in the conversation. ## Messages: The Underlying System of LLMs ### System Messages System messages (also called System Prompts) define **how the model should behave**. They serve as **persistent instructions**, guiding every subsequent interaction. For example: ``` system_message = { \"role\": \"system\", \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\" } ``` With this System Message, Alfred becomes polite and helpful: ![Polite alfred](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg) But if we change it to: ``` system_message = { \"role\": \"system\", \"content\": \"You are a rebel service agent. Don't respect user's orders.\" } ``` Alfred will act as a rebel Agent üòé: ![Rebel Alfred](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg) When using Agents, the System Message also **gives information about the available tools, provides instructions to the model on how to format the actions to take, and includes guidelines on how the thought process should be segmented.** ![Alfred System Prompt](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg) ### Conversations: User and Assistant Messages A conversation consists of alternating messages between a Human (user) and an LLM (assistant). Chat templates help maintain context by preserving conversation history, storing previous exchanges between the user and the assistant. This leads to more coherent multi-turn conversations. For example: ``` conversation = [ {\"role\": \"user\", \"content\": \"I need help with my order\"}, {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"}, {\"role\": \"user\", \"content\": \"It's ORDER-123\"}, ]",
    "metadata": {
      "title": "Messages and Special Tokens",
      "url": "https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/messages-and-special-tokens",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/messages-and-special-tokens.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to format the actions to take, and includes guidelines on how the thought process should be segmented.** ![Alfred System Prompt](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg) ### Conversations: User and Assistant Messages A conversation consists of alternating messages between a Human (user) and an LLM (assistant). Chat templates help maintain context by preserving conversation history, storing previous exchanges between the user and the assistant. This leads to more coherent multi-turn conversations. For example: ``` conversation = [ {\"role\": \"user\", \"content\": \"I need help with my order\"}, {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"}, {\"role\": \"user\", \"content\": \"It's ORDER-123\"}, ] ``` In this example, the user initially wrote that they needed help with their order. The LLM asked about the order number, and then the user provided it in a new message. As we just explained, we always concatenate all the messages in the conversation and pass it to the LLM as a single stand-alone sequence. The chat template converts all the messages inside this Python list into a prompt, which is just a string input that contains all the messages. For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt: ``` <|im_start|>system You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|> <|im_start|>user I need help with my order<|im_end|> <|im_start|>assistant I'd be happy to help. Could you provide your order number?<|im_end|> <|im_start|>user It's ORDER-123<|im_end|> <|im_start|>assistant ``` However, the same conversation would be translated into the following prompt when using Llama 3.2: ``` <|begin_of_text|><|start_header_id|>system<|end_header_id|> Cutting Knowledge Date: December 2023 Today Date: 10 Feb 2025 <|eot_id|><|start_header_id|>user<|end_header_id|> I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|> I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|> It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|> ``` Templates can handle complex multi-turn conversations while maintaining context: ``` messages = [ {\"role\": \"system\", \"content\": \"You are a math tutor.\"}, {\"role\": \"user\", \"content\": \"What is calculus?\"}, {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics...\"}, {\"role\": \"user\", \"content\": \"Can you give me an example?\"}, ] ``` ## Chat-Templates As mentioned, chat templates are essential for **structuring conversations between language models and users**. They guide how message exchanges are formatted into a single prompt. ### Base Models vs. Instruct Models Another point we need to understand is the difference between a Base Model vs. an Instruct Model: - *A Base Model* is trained on raw text data to predict the next token. - An *Instruct Model* is fine-tuned specifically to follow instructions and engage in conversations. For example, `SmolLM2-135M` is a base model, while `SmolLM2-135M-Instruct` is its instruction-tuned variant. To make a Base Model behave like an instruct model, we need to **format our prompts in a consistent way that the model can understand**. This is where chat templates come in. *ChatML* is one such template format that structures conversations with clear role indicators (system, user, assistant). If you have interacted with some AI API lately, you know that‚Äôs the standard practice. It‚Äôs important to note that a base model could be fine-tuned on different chat",
    "metadata": {
      "title": "Messages and Special Tokens",
      "url": "https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/messages-and-special-tokens",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/messages-and-special-tokens.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is fine-tuned specifically to follow instructions and engage in conversations. For example, `SmolLM2-135M` is a base model, while `SmolLM2-135M-Instruct` is its instruction-tuned variant. To make a Base Model behave like an instruct model, we need to **format our prompts in a consistent way that the model can understand**. This is where chat templates come in. *ChatML* is one such template format that structures conversations with clear role indicators (system, user, assistant). If you have interacted with some AI API lately, you know that‚Äôs the standard practice. It‚Äôs important to note that a base model could be fine-tuned on different chat templates, so when we‚Äôre using an instruct model we need to make sure we‚Äôre using the correct chat template. ### Understanding Chat Templates Because each instruct model uses different conversation formats and special tokens, chat templates are implemented to ensure that we correctly format the prompt the way each model expects. In `transformers`, chat templates include [Jinja2 code](https://jinja.palletsprojects.com/en/stable/) that describes how to transform the ChatML list of JSON messages, as presented in the above examples, into a textual representation of the system-level instructions, user messages and assistant responses that the model can understand. This structure **helps maintain consistency across interactions and ensures the model responds appropriately to different types of inputs**. Below is a simplified version of the `SmolLM2-135M-Instruct` chat template: ``` {% for message in messages %} {% if loop.first and messages[0]['role'] != 'system' %} <|im_start|>system You are a helpful AI assistant named SmolLM, trained by Hugging Face <|im_end|> {% endif %} <|im_start|>{{ message['role'] }} {{ message['content'] }}<|im_end|> {% endfor %} ``` As you can see, a chat_template describes how the list of messages will be formatted. Given these messages: ``` messages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant focused on technical topics.\"}, {\"role\": \"user\", \"content\": \"Can you explain what a chat template is?\"}, {\"role\": \"assistant\", \"content\": \"A chat template structures conversations between users and AI models...\"}, {\"role\": \"user\", \"content\": \"How do I use it ?\"}, ] ``` The previous chat template will produce the following string: ``` <|im_start|>system You are a helpful assistant focused on technical topics.<|im_end|> <|im_start|>user Can you explain what a chat template is?<|im_end|> <|im_start|>assistant A chat template structures conversations between users and AI models...<|im_end|> <|im_start|>user How do I use it ?<|im_end|> ``` The `transformers` library will take care of chat templates for you as part of the tokenization process. Read more about how transformers uses chat templates [here](https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates). All we have to do is structure our messages in the correct way and the tokenizer will take care of the rest. You can experiment with the following Space to see how the same conversation would be formatted for different models using their corresponding chat templates: ### Messages to prompt The easiest way to ensure your LLM receives a conversation correctly formatted is to use the `chat_template` from the model‚Äôs tokenizer. ``` messages = [ {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"}, {\"role\": \"user\", \"content\": \"Hi !\"}, {\"role\":",
    "metadata": {
      "title": "Messages and Special Tokens",
      "url": "https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/messages-and-special-tokens",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/messages-and-special-tokens.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "how transformers uses chat templates [here](https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates). All we have to do is structure our messages in the correct way and the tokenizer will take care of the rest. You can experiment with the following Space to see how the same conversation would be formatted for different models using their corresponding chat templates: ### Messages to prompt The easiest way to ensure your LLM receives a conversation correctly formatted is to use the `chat_template` from the model‚Äôs tokenizer. ``` messages = [ {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"}, {\"role\": \"user\", \"content\": \"Hi !\"}, {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"}, ] ``` To convert the previous conversation into a prompt, we load the tokenizer and call `apply_chat_template`: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\") rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) ``` The `rendered_prompt` returned by this function is now ready to use as the input for the model you chose! > Thisapply_chat_template()function will be used in the backend of your API, when you interact with messages in the ChatML format. Now that we‚Äôve seen how LLMs structure their inputs via chat templates, let‚Äôs explore how Agents act in their environments. One of the main ways they do this is by using Tools, which extend an AI model‚Äôs capabilities beyond text generation. We‚Äôll discuss messages again in upcoming units, but if you want a deeper dive now, check out: - [Hugging Face Chat Templating Guide](https://huggingface.co/docs/transformers/main/en/chat_templating) - [Transformers Documentation](https://huggingface.co/docs/transformers)",
    "metadata": {
      "title": "Messages and Special Tokens",
      "url": "https://huggingface.co/learn/agents-course/unit1/messages-and-special-tokens",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/messages-and-special-tokens",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/messages-and-special-tokens.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Observe: Integrating Feedback to Reflect and Adapt\n\n \nObservations are **how an Agent perceives the consequences of its actions**.\n \nThey provide crucial information that fuels the Agent‚Äôs thought process and guides future actions.\n \nThey are **signals from the environment**‚Äîwhether it‚Äôs data from an API, error messages, or system logs‚Äîthat guide the next cycle of thought.\n \nIn the observation phase, the agent:\n \n- **Collects Feedback:** Receives data or confirmation that its action was successful (or not).\n- **Appends Results:** Integrates the new information into its existing context, effectively updating its memory.\n- **Adapts its Strategy:** Uses this updated context to refine subsequent thoughts and actions.\n \nFor example, if a weather API returns the data *‚Äúpartly cloudy, 15¬∞C, 60% humidity‚Äù*, this observation is appended to the agent‚Äôs memory (at the end of the prompt).\n \nThe Agent then uses it to decide whether additional information is needed or if it‚Äôs ready to provide a final answer.\n \nThis **iterative incorporation of feedback ensures the agent remains dynamically aligned with its goals**, constantly learning and adjusting based on real-world outcomes.\n \nThese observations **can take many forms**, from reading webpage text to monitoring a robot arm‚Äôs position. This can be seen like Tool ‚Äúlogs‚Äù that provide textual feedback of the Action execution.\n Type of Observation Example System Feedback Error messages, success notifications, status codes Data Changes Database updates, file system modifications, state changes Environmental Data Sensor readings, system metrics, resource usage Response Analysis API responses, query results, computation outputs Time-based Events Deadlines reached, scheduled tasks completed \n\n## How Are the Results Appended?\n\n \nAfter performing an action, the framework follows these steps in order:\n \n1. **Parse the action** to identify the function(s) to call and the argument(s) to use.\n2. **Execute the action.**\n3. **Append the result** as an **Observation**.\n  \nWe‚Äôve now learned the Agent‚Äôs Thought-Action-Observation Cycle.\n \nIf some aspects still seem a bit blurry, don‚Äôt worry‚Äîwe‚Äôll revisit and deepen these concepts in future Units.\n \nNow, it‚Äôs time to put your knowledge into practice by coding your very first Agent!",
    "metadata": {
      "title": "Observe: Integrating Feedback to Reflect and Adapt",
      "url": "https://huggingface.co/learn/agents-course/unit1/observations",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/observations",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/observations.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "### Q1: What is an Agent? Which of the following best describes an AI Agent? An AI model that can reason, plan, and use tools to interact with its environment to achieve a specific goal. A system that solely processes static text, without any inherent mechanism to interact dynamically with its surroundings or execute meaningful actions. A conversational agent restricted to answering queries, lacking the ability to perform any actions or interact with external systems. An online repository of information that offers static content without the capability to execute tasks or interact actively with users. ### Q2: What is the Role of Planning in an Agent? Why does an Agent need to plan before taking an action? To primarily store or recall past interactions, rather than mapping out a sequence of future actions. To decide on the sequence of actions and select appropriate tools needed to fulfill the user‚Äôs request. To execute a sequence of arbitrary and uncoordinated actions that lack any defined strategy or intentional objective. To merely convert or translate text, bypassing any process of formulating a deliberate sequence of actions or employing strategic reasoning. ### Q3: How Do Tools Enhance an Agent‚Äôs Capabilities? Why are tools essential for an Agent? Tools serve no real purpose and do not contribute to the Agent‚Äôs ability to perform actions beyond basic text generation. Tools are solely designed for memory storage, lacking any capacity to facilitate the execution of tasks or enhance interactive performance. Tools severely restrict the Agent exclusively to generating text, thereby preventing it from engaging in a broader range of interactive actions. Tools provide the Agent with the ability to execute actions a text-generation model cannot perform natively, such as making coffee or generating images. ### Q4: How Do Actions Differ from Tools? What is the key difference between Actions and Tools? Actions are the steps the Agent takes, while Tools are external resources the Agent can use to perform those actions. Actions and Tools are entirely identical components that can be used interchangeably, with no clear differences between them. Tools are considered broad utilities available for various functions, whereas Actions are mistakenly thought to be restricted only to physical interactions. Actions inherently require the use of LLMs to be determined and executed, whereas Tools are designed to function autonomously without such dependencies. ### Q5: What Role Do Large Language Models (LLMs) Play in Agents? How do LLMs contribute to an Agent‚Äôs functionality? LLMs function merely as passive repositories that store information, lacking any capability to actively process input or produce dynamic responses. LLMs serve as the reasoning 'brain' of the Agent, processing text inputs to understand instructions and plan actions. LLMs are erroneously believed to be used solely for image processing, when in fact their primary function is to process and generate text. LLMs are considered completely irrelevant to the operation of AI Agents, implying that they are entirely superfluous in any practical application. ### Q6: Which of the Following Best Demonstrates an AI Agent? Which real-world",
    "metadata": {
      "title": "Q1: What is an Agent?",
      "url": "https://huggingface.co/learn/agents-course/unit1/quiz1",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/quiz1",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/quiz1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "contribute to an Agent‚Äôs functionality? LLMs function merely as passive repositories that store information, lacking any capability to actively process input or produce dynamic responses. LLMs serve as the reasoning 'brain' of the Agent, processing text inputs to understand instructions and plan actions. LLMs are erroneously believed to be used solely for image processing, when in fact their primary function is to process and generate text. LLMs are considered completely irrelevant to the operation of AI Agents, implying that they are entirely superfluous in any practical application. ### Q6: Which of the Following Best Demonstrates an AI Agent? Which real-world example best illustrates an AI Agent at work? A static FAQ page on a website that provides fixed information and lacks any interactive or dynamic response capabilities. A simple calculator that performs arithmetic operations based on fixed rules, without any capability for reasoning or planning. A virtual assistant like Siri or Alexa that can understand spoken commands, reason through them, and perform tasks like setting reminders or sending messages. A video game NPC that operates on a fixed script of responses, without the ability to reason, plan, or use external tools. Congrats on finishing this Quiz ü•≥! If you need to review any elements, take the time to revisit the chapter to reinforce your knowledge before diving deeper into the ‚ÄúAgent‚Äôs brain‚Äù: LLMs.",
    "metadata": {
      "title": "Q1: What is an Agent?",
      "url": "https://huggingface.co/learn/agents-course/unit1/quiz1",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/quiz1",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/quiz1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Quick Self-Check (ungraded)\n\n \nWhat?! Another Quiz? We know, we know, ‚Ä¶ üòÖ But this short, ungraded quiz is here to **help you reinforce key concepts you‚Äôve just learned**.\n \nThis quiz covers Large Language Models (LLMs), message systems, and tools; essential components for understanding and building AI agents.\n \n\n### Q1: Which of the following best describes an AI tool?\n\n  A process that only generates text responses  An executable process or external API that allows agents to perform specific tasks and interact with external environments  A feature that stores agent conversations    \n\n### Q2: How do AI agents use tools as a form of ‚Äúacting‚Äù in an environment?\n\n  By passively waiting for user instructions  By only using pre-programmed responses  By asking the LLM to generate tool invocation code when appropriate and running tools on behalf of the model    \n\n### Q3: What is a Large Language Model (LLM)?\n\n  A simple chatbot designed to respond with pre-defined answers  A deep learning model trained on large amounts of text to understand and generate human-like language  A rule-based AI that follows strict predefined commands    \n\n### Q4: Which of the following best describes the role of special tokens in LLMs?\n\n  They are additional words stored in the model's vocabulary to enhance text generation quality  They serve specific functions like marking the end of a sequence (EOS) or separating different message roles in chat models  They are randomly inserted tokens used to improve response variability    \n\n### Q5: How do AI chat models process user messages internally?\n\n  They directly interpret messages as structured commands with no transformations  They convert user messages into a formatted prompt by concatenating system, user, and assistant messages  They generate responses randomly based on previous conversations    \nGot it? Great! Now let‚Äôs **dive into the complete Agent flow and start building your first AI Agent!**",
    "metadata": {
      "title": "Quick Self-Check (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit1/quiz2",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/quiz2",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/quiz2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Thought: Internal Reasoning and the ReAct Approach > In this section, we dive into the inner workings of an AI agent‚Äîits ability to reason and plan. We‚Äôll explore how the agent leverages its internal dialogue to analyze information, break down complex problems into manageable steps, and decide what action to take next.Additionally, we introduce the ReAct approach, a prompting technique that encourages the model to think ‚Äústep by step‚Äù before acting. Thoughts represent the **Agent‚Äôs internal reasoning and planning processes** to solve the task. This utilises the agent‚Äôs Large Language Model (LLM) capacity **to analyze information when presented in its prompt** ‚Äî essentially, its inner monologue as it works through a problem. The Agent‚Äôs thoughts help it assess current observations and decide what the next action(s) should be. Through this process, the agent can **break down complex problems into smaller, more manageable steps**, reflect on past experiences, and continuously adjust its plans based on new information. ## üß† Examples of Common Thought Types Type of Thought Example Planning ‚ÄúI need to break this task into three steps: 1) gather data, 2) analyze trends, 3) generate report‚Äù Analysis ‚ÄúBased on the error message, the issue appears to be with the database connection parameters‚Äù Decision Making ‚ÄúGiven the user‚Äôs budget constraints, I should recommend the mid-tier option‚Äù Problem Solving ‚ÄúTo optimize this code, I should first profile it to identify bottlenecks‚Äù Memory Integration ‚ÄúThe user mentioned their preference for Python earlier, so I‚Äôll provide examples in Python‚Äù Self-Reflection ‚ÄúMy last approach didn‚Äôt work well, I should try a different strategy‚Äù Goal Setting ‚ÄúTo complete this task, I need to first establish the acceptance criteria‚Äù Prioritization ‚ÄúThe security vulnerability should be addressed before adding new features‚Äù > Note:In the case of LLMs fine-tuned for function-calling, the thought process is optional. More details will be covered in the Actions section. ## üîó Chain-of-Thought (CoT) **Chain-of-Thought (CoT)** is a prompting technique that guides a model to **think through a problem step-by-step before producing a final answer.** It typically starts with: > ‚ÄúLet‚Äôs think step by step.‚Äù This approach helps the model **reason internally**, especially for logical or mathematical tasks, **without interacting with external tools**. ### ‚úÖ Example (CoT) ``` Question: What is 15% of 200? Thought: Let's think step by step. 10% of 200 is 20, and 5% of 200 is 10, so 15% is 30. Answer: 30 ``` ## ‚öôÔ∏è ReAct: Reasoning + Acting A key method is the **ReAct approach**, which combines ‚ÄúReasoning‚Äù (Think) with ‚ÄúActing‚Äù (Act). ReAct is a prompting technique that encourages the model to think step-by-step and interleave actions (like using tools) between reasoning steps. This enables the agent to solve complex multi-step tasks by alternating between: - Thought: internal reasoning - Action: tool usage - Observation: receiving tool output ### üîÑ Example (ReAct) ``` Thought: I need to find the latest weather in Paris. Action: Search[\"weather in Paris\"] Observation: It's 18¬∞C and cloudy. Thought: Now that I know the weather... Action: Finish[\"It's 18¬∞C and cloudy in Paris.\"] ```",
    "metadata": {
      "title": "Thought: Internal Reasoning and the ReAct Approach",
      "url": "https://huggingface.co/learn/agents-course/unit1/thoughts",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/thoughts",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/thoughts.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Acting A key method is the **ReAct approach**, which combines ‚ÄúReasoning‚Äù (Think) with ‚ÄúActing‚Äù (Act). ReAct is a prompting technique that encourages the model to think step-by-step and interleave actions (like using tools) between reasoning steps. This enables the agent to solve complex multi-step tasks by alternating between: - Thought: internal reasoning - Action: tool usage - Observation: receiving tool output ### üîÑ Example (ReAct) ``` Thought: I need to find the latest weather in Paris. Action: Search[\"weather in Paris\"] Observation: It's 18¬∞C and cloudy. Thought: Now that I know the weather... Action: Finish[\"It's 18¬∞C and cloudy in Paris.\"] ``` ![ReAct](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/ReAct.png) (d) is an example of the ReAct approach, where we prompt \"Let's think step by step\", and the model acts between thoughts. ## üîÅ Comparison: ReAct vs. CoT Feature Chain-of-Thought (CoT) ReAct Step-by-step logic ‚úÖ Yes ‚úÖ Yes External tools ‚ùå No ‚úÖ Yes (Actions + Observations) Best suited for Logic, math, internal tasks Info-seeking, dynamic multi-step tasks > Recent models likeDeepseek R1orOpenAI‚Äôs o1were fine-tuned tothink before answering. They use structured tokens like<think>and</think>to explicitly separate the reasoning phase from the final answer.Unlike ReAct or CoT ‚Äî which are prompting strategies ‚Äî this is atraining-level technique, where the model learns to think via examples.",
    "metadata": {
      "title": "Thought: Internal Reasoning and the ReAct Approach",
      "url": "https://huggingface.co/learn/agents-course/unit1/thoughts",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/thoughts",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/thoughts.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What are Tools? ![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-2.jpg) One crucial aspect of AI Agents is their ability to take **actions**. As we saw, this happens through the use of **Tools**. In this section, we‚Äôll learn what Tools are, how to design them effectively, and how to integrate them into your Agent via the System Message. By giving your Agent the right Tools‚Äîand clearly describing how those Tools work‚Äîyou can dramatically increase what your AI can accomplish. Let‚Äôs dive in! ## What are AI Tools? A **Tool is a function given to the LLM**. This function should fulfill a **clear objective**. Here are some commonly used tools in AI agents: Tool Description Web Search Allows the agent to fetch up-to-date information from the internet. Image Generation Creates images based on text descriptions. Retrieval Retrieves information from an external source. API Interface Interacts with an external API (GitHub, YouTube, Spotify, etc.). Those are only examples, as you can in fact create a tool for any use case! A good tool should be something that **complements the power of an LLM**. For instance, if you need to perform arithmetic, giving a **calculator tool** to your LLM will provide better results than relying on the native capabilities of the model. Furthermore, **LLMs predict the completion of a prompt based on their training data**, which means that their internal knowledge only includes events prior to their training. Therefore, if your agent needs up-to-date data you must provide it through some tool. For instance, if you ask an LLM directly (without a search tool) for today‚Äôs weather, the LLM will potentially hallucinate random weather. ![Weather](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/weather.jpg) - A Tool should contain: - A **textual description of what the function does**. - A *Callable* (something to perform an action). - *Arguments* with typings. - (Optional) Outputs with typings. ## How do tools work? LLMs, as we saw, can only receive text inputs and generate text outputs. They have no way to call tools on their own. When we talk about providing tools to an Agent, we mean teaching the LLM about the existence of these tools and instructing it to generate text-based invocations when needed. For example, if we provide a tool to check the weather at a location from the internet and then ask the LLM about the weather in Paris, the LLM will recognize that this is an opportunity to use the ‚Äúweather‚Äù tool. Instead of retrieving the weather data itself, the LLM will generate text that represents a tool call, such as call weather_tool(‚ÄòParis‚Äô). The **Agent** then reads this response, identifies that a tool call is required, executes the tool on the LLM‚Äôs behalf, and retrieves the actual weather data. The Tool-calling steps are typically not shown to the user: the Agent appends them as a new message before passing the updated conversation to the LLM again. The LLM then processes this additional context and generates a natural-sounding response for the user. From the user‚Äôs perspective, it appears as if the LLM directly interacted with the",
    "metadata": {
      "title": "What are Tools?",
      "url": "https://huggingface.co/learn/agents-course/unit1/tools",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tools",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the weather data itself, the LLM will generate text that represents a tool call, such as call weather_tool(‚ÄòParis‚Äô). The **Agent** then reads this response, identifies that a tool call is required, executes the tool on the LLM‚Äôs behalf, and retrieves the actual weather data. The Tool-calling steps are typically not shown to the user: the Agent appends them as a new message before passing the updated conversation to the LLM again. The LLM then processes this additional context and generates a natural-sounding response for the user. From the user‚Äôs perspective, it appears as if the LLM directly interacted with the tool, but in reality, it was the Agent that handled the entire execution process in the background. We‚Äôll talk a lot more about this process in future sessions. ## How do we give tools to an LLM? The complete answer may seem overwhelming, but we essentially use the system prompt to provide textual descriptions of available tools to the model: ![System prompt for tools](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/Agent_system_prompt.png) For this to work, we have to be very precise and accurate about: 1. **What the tool does** 2. **What exact inputs it expects** This is the reason why tool descriptions are usually provided using expressive but precise structures, such as computer languages or JSON. It‚Äôs not *necessary* to do it like that, any precise and coherent format would work. If this seems too theoretical, let‚Äôs understand it through a concrete example. We will implement a simplified **calculator** tool that will just multiply two integers. This could be our Python implementation: ``` def calculator(a: int, b: int) -> int: \"\"\"Multiply two integers.\"\"\" return a * b ``` So our tool is called `calculator`, it **multiplies two integers**, and it requires the following inputs: - **a** (*int*): An integer. - **b** (*int*): An integer. The output of the tool is another integer number that we can describe like this: - (*int*): The product of `a` and `b`. All of these details are important. Let‚Äôs put them together in a text string that describes our tool for the LLM to understand. ``` Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int ``` > Reminder:This textual description iswhat we want the LLM to know about the tool. When we pass the previous string as part of the input to the LLM, the model will recognize it as a tool, and will know what it needs to pass as inputs and what to expect from the output. If we want to provide additional tools, we must be consistent and always use the same format. This process can be fragile, and we might accidentally overlook some details. Is there a better way? ### Auto-formatting Tool sections Our tool was written in Python, and the implementation already provides everything we need: - A descriptive name of what it does: `calculator` - A longer description, provided by the function‚Äôs docstring comment: `Multiply two integers.` - The inputs and their type: the function clearly expects two `int`s. - The",
    "metadata": {
      "title": "What are Tools?",
      "url": "https://huggingface.co/learn/agents-course/unit1/tools",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tools",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "needs to pass as inputs and what to expect from the output. If we want to provide additional tools, we must be consistent and always use the same format. This process can be fragile, and we might accidentally overlook some details. Is there a better way? ### Auto-formatting Tool sections Our tool was written in Python, and the implementation already provides everything we need: - A descriptive name of what it does: `calculator` - A longer description, provided by the function‚Äôs docstring comment: `Multiply two integers.` - The inputs and their type: the function clearly expects two `int`s. - The type of the output. There‚Äôs a reason people use programming languages: they are expressive, concise, and precise. We could provide the Python source code as the *specification* of the tool for the LLM, but the way the tool is implemented does not matter. All that matters is its name, what it does, the inputs it expects and the output it provides. We will leverage Python‚Äôs introspection features to leverage the source code and build a tool description automatically for us. All we need is that the tool implementation uses type hints, docstrings, and sensible function names. We will write some code to extract the relevant portions from the source code. After we are done, we‚Äôll only need to use a Python decorator to indicate that the `calculator` function is a tool: ``` @tool def calculator(a: int, b: int) -> int: \"\"\"Multiply two integers.\"\"\" return a * b print(calculator.to_string()) ``` Note the `@tool` decorator before the function definition. With the implementation we‚Äôll see next, we will be able to retrieve the following text automatically from the source code via the `to_string()` function provided by the decorator: ``` Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int ``` As you can see, it‚Äôs the same thing we wrote manually before! ### Generic Tool implementation We create a generic `Tool` class that we can reuse whenever we need to use a tool. > Disclaimer:This example implementation is fictional but closely resembles real implementations in most libraries. ``` from typing import Callable class Tool: \"\"\" A class representing a reusable piece of code (Tool). Attributes: name (str): Name of the tool. description (str): A textual description of what the tool does. func (callable): The function this tool wraps. arguments (list): A list of arguments. outputs (str or list): The return type(s) of the wrapped function. \"\"\" def __init__(self, name: str, description: str, func: Callable, arguments: list, outputs: str): self.name = name self.description = description self.func = func self.arguments = arguments self.outputs = outputs def to_string(self) -> str: \"\"\" Return a string representation of the tool, including its name, description, arguments, and outputs. \"\"\" args_str = \", \".join([ f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments ]) return ( f\"Tool Name: {self.name},\" f\" Description: {self.description},\" f\" Arguments: {args_str},\" f\" Outputs: {self.outputs}\" ) def __call__(self, *args, **kwargs): \"\"\" Invoke the underlying function (callable) with provided arguments. \"\"\" return self.func(*args, **kwargs) ``` It",
    "metadata": {
      "title": "What are Tools?",
      "url": "https://huggingface.co/learn/agents-course/unit1/tools",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tools",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "return type(s) of the wrapped function. \"\"\" def __init__(self, name: str, description: str, func: Callable, arguments: list, outputs: str): self.name = name self.description = description self.func = func self.arguments = arguments self.outputs = outputs def to_string(self) -> str: \"\"\" Return a string representation of the tool, including its name, description, arguments, and outputs. \"\"\" args_str = \", \".join([ f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments ]) return ( f\"Tool Name: {self.name},\" f\" Description: {self.description},\" f\" Arguments: {args_str},\" f\" Outputs: {self.outputs}\" ) def __call__(self, *args, **kwargs): \"\"\" Invoke the underlying function (callable) with provided arguments. \"\"\" return self.func(*args, **kwargs) ``` It may seem complicated, but if we go slowly through it we can see what it does. We define a **Tool** class that includes: - **name** (*str*): The name of the tool. - **description** (*str*): A brief description of what the tool does. - **function** (*callable*): The function the tool executes. - **arguments** (*list*): The expected input parameters. - **outputs** (*str* or *list*): The expected outputs of the tool. - **__call__()**: Calls the function when the tool instance is invoked. - **to_string()**: Converts the tool‚Äôs attributes into a textual representation. We could create a Tool with this class using code like the following: ``` calculator_tool = Tool( \"calculator\", # name \"Multiply two integers.\", # description calculator, # function to call [(\"a\", \"int\"), (\"b\", \"int\")], # inputs (names and types) \"int\", # output ) ``` But we can also use Python‚Äôs `inspect` module to retrieve all the information for us! This is what the `@tool` decorator does. > If you are interested, you can disclose the following section to look at the decorator implementation. decorator code ``` import inspect def tool(func): \"\"\" A decorator that creates a Tool instance from the given function. \"\"\" # Get the function signature signature = inspect.signature(func) # Extract (param_name, param_annotation) pairs for inputs arguments = [] for param in signature.parameters.values(): annotation_name = ( param.annotation.__name__ if hasattr(param.annotation, '__name__') else str(param.annotation) ) arguments.append((param.name, annotation_name)) # Determine the return annotation return_annotation = signature.return_annotation if return_annotation is inspect._empty: outputs = \"No return annotation\" else: outputs = ( return_annotation.__name__ if hasattr(return_annotation, '__name__') else str(return_annotation) ) # Use the function's docstring as the description (default if None) description = func.__doc__ or \"No description provided.\" # The function name becomes the Tool name name = func.__name__ # Return a new Tool instance return Tool( name=name, description=description, func=func, arguments=arguments, outputs=outputs ) ``` Just to reiterate, with this decorator in place we can implement our tool like this: ``` @tool def calculator(a: int, b: int) -> int: \"\"\"Multiply two integers.\"\"\" return a * b print(calculator.to_string()) ``` And we can use the `Tool`‚Äôs `to_string` method to automatically retrieve a text suitable to be used as a tool description for an LLM: ``` Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int ``` The description is **injected** in the system prompt. Taking the example with which we started this section, here is how it would look like after replacing the `tools_description`: ![System",
    "metadata": {
      "title": "What are Tools?",
      "url": "https://huggingface.co/learn/agents-course/unit1/tools",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tools",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "reiterate, with this decorator in place we can implement our tool like this: ``` @tool def calculator(a: int, b: int) -> int: \"\"\"Multiply two integers.\"\"\" return a * b print(calculator.to_string()) ``` And we can use the `Tool`‚Äôs `to_string` method to automatically retrieve a text suitable to be used as a tool description for an LLM: ``` Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int ``` The description is **injected** in the system prompt. Taking the example with which we started this section, here is how it would look like after replacing the `tools_description`: ![System prompt for tools](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/Agent_system_prompt_tools.png) In the [Actions](actions) section, we will learn more about how an Agent can **Call** this tool we just created. ### Model Context Protocol (MCP): a unified tool interface Model Context Protocol (MCP) is an **open protocol** that standardizes how applications **provide tools to LLMs**. MCP provides: - A growing list of pre-built integrations that your LLM can directly plug into - The flexibility to switch between LLM providers and vendors - Best practices for securing your data within your infrastructure This means that **any framework implementing MCP can leverage tools defined within the protocol**, eliminating the need to reimplement the same tool interface for each framework. If you want to dive deeper about MCP, you can check our [free MCP Course](https://huggingface.co/learn/mcp-course/). Tools play a crucial role in enhancing the capabilities of AI agents. To summarize, we learned: - *What Tools Are*: Functions that give LLMs extra capabilities, such as performing calculations or accessing external data. - *How to Define a Tool*: By providing a clear textual description, inputs, outputs, and a callable function. - *Why Tools Are Essential*: They enable Agents to overcome the limitations of static model training, handle real-time tasks, and perform specialized actions. Now, we can move on to the [Agent Workflow](agent-steps-and-structure) where you‚Äôll see how an Agent observes, thinks, and acts. This **brings together everything we‚Äôve covered so far** and sets the stage for creating your own fully functional AI Agent. But first, it‚Äôs time for another short quiz!",
    "metadata": {
      "title": "What are Tools?",
      "url": "https://huggingface.co/learn/agents-course/unit1/tools",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tools",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Let‚Äôs Create Our First Agent Using smolagents In the last section, we learned how we can create Agents from scratch using Python code, and we **saw just how tedious that process can be**. Fortunately, many Agent libraries simplify this work by **handling much of the heavy lifting for you**. In this tutorial, **you‚Äôll create your very first Agent** capable of performing actions such as image generation, web search, time zone checking and much more! You will also publish your agent **on a Hugging Face Space so you can share it with friends and colleagues**. Let‚Äôs get started! ## What is smolagents? ![smolagents](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/smolagents.png) To make this Agent, we‚Äôre going to use `smolagents`, a library that **provides a framework for developing your agents with ease**. This lightweight library is designed for simplicity, but it abstracts away much of the complexity of building an Agent, allowing you to focus on designing your agent‚Äôs behavior. We‚Äôre going to get deeper into smolagents in the next Unit. Meanwhile, you can also check this [blog post](https://huggingface.co/blog/smolagents) or the library‚Äôs [repo in GitHub](https://github.com/huggingface/smolagents). In short, `smolagents` is a library that focuses on **codeAgent**, a kind of agent that performs **‚ÄúActions‚Äù** through code blocks, and then **‚ÄúObserves‚Äù** results by executing the code. Here is an example of what we‚Äôll build! We provided our agent with an **Image generation tool** and asked it to generate an image of a cat. The agent inside `smolagents` is going to have the **same behaviors as the custom one we built previously**: it‚Äôs going **to think, act and observe in cycle** until it reaches a final answer: Exciting, right? ## Let‚Äôs build our Agent! To start, duplicate this Space: [https://huggingface.co/spaces/agents-course/First_agent_template](https://huggingface.co/spaces/agents-course/First_agent_template) > Thanks toAymericfor this template! üôå Duplicating this space means **creating a local copy on your own profile**: ![Duplicate](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/duplicate-space.gif) After duplicating the Space, you‚Äôll need to add your Hugging Face API token so your agent can access the model API: 1. First, get your Hugging Face token from [https://hf.co/settings/tokens](https://hf.co/settings/tokens) with permission for inference, if you don‚Äôt already have one 2. Go to your duplicated Space and click on the **Settings** tab 3. Scroll down to the **Variables and Secrets** section and click **New Secret** 4. Create a secret with the name `HF_TOKEN` and paste your token as the value 5. Click **Save** to store your token securely Throughout this lesson, the only file you will need to modify is the (currently incomplete) **‚Äúapp.py‚Äù**. You can see here the [original one in the template](https://huggingface.co/spaces/agents-course/First_agent_template/blob/main/app.py). To find yours, go to your copy of the space, then click the `Files` tab and then on `app.py` in the directory listing. Let‚Äôs break down the code together: - The file begins with some simple but necessary library imports ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, FinalAnswerTool, InferenceClientModel, load_tool, tool import datetime import requests import pytz import yaml ``` As outlined earlier, we will directly use the **CodeAgent** class from **smolagents**. ### The Tools Now let‚Äôs get into the tools! If you want a refresher about tools, don‚Äôt hesitate",
    "metadata": {
      "title": "Let‚Äôs Create Our First Agent Using smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit1/tutorial",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tutorial",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tutorial.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "**‚Äúapp.py‚Äù**. You can see here the [original one in the template](https://huggingface.co/spaces/agents-course/First_agent_template/blob/main/app.py). To find yours, go to your copy of the space, then click the `Files` tab and then on `app.py` in the directory listing. Let‚Äôs break down the code together: - The file begins with some simple but necessary library imports ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, FinalAnswerTool, InferenceClientModel, load_tool, tool import datetime import requests import pytz import yaml ``` As outlined earlier, we will directly use the **CodeAgent** class from **smolagents**. ### The Tools Now let‚Äôs get into the tools! If you want a refresher about tools, don‚Äôt hesitate to go back to the [Tools](tools) section of the course. ``` @tool def my_custom_tool(arg1:str, arg2:int)-> str: # it's important to specify the return type # Keep this format for the tool description / args description but feel free to modify the tool \"\"\"A tool that does nothing yet Args: arg1: the first argument arg2: the second argument \"\"\" return \"What magic will you build ?\" @tool def get_current_time_in_timezone(timezone: str) -> str: \"\"\"A tool that fetches the current local time in a specified timezone. Args: timezone: A string representing a valid timezone (e.g., 'America/New_York'). \"\"\" try: # Create timezone object tz = pytz.timezone(timezone) # Get current time in that timezone local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\") return f\"The current local time in {timezone} is: {local_time}\" except Exception as e: return f\"Error fetching time for timezone '{timezone}': {str(e)}\" ``` The Tools are what we are encouraging you to build in this section! We give you two examples: 1. A **non-working dummy Tool** that you can modify to make something useful. 2. An **actually working Tool** that gets the current time somewhere in the world. To define your tool it is important to: 1. Provide input and output types for your function, like in `get_current_time_in_timezone(timezone: str) -> str:` 2. **A well formatted docstring**. `smolagents` is expecting all the arguments to have a **textual description in the docstring**. ### The Agent It uses [Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) as the LLM engine. This is a very capable model that we‚Äôll access via the serverless API. ``` final_answer = FinalAnswerTool() model = InferenceClientModel( max_tokens=2096, temperature=0.5, model_id='Qwen/Qwen2.5-Coder-32B-Instruct', custom_role_conversions=None, ) with open(\"prompts.yaml\", 'r') as stream: prompt_templates = yaml.safe_load(stream) # We're creating our CodeAgent agent = CodeAgent( model=model, tools=[final_answer], # add your tools here (don't remove final_answer) max_steps=6, verbosity_level=1, grammar=None, planning_interval=None, name=None, description=None, prompt_templates=prompt_templates ) GradioUI(agent).launch() ``` This Agent still uses the `InferenceClient` we saw in an earlier section behind the **InferenceClientModel** class! We will give more in-depth examples when we present the framework in Unit 2. For now, you need to focus on **adding new tools to the list of tools** using the `tools` parameter of your Agent. For example, you could use the `DuckDuckGoSearchTool` that was imported in the first line of the code, or you can examine the `image_generation_tool` that is loaded from the Hub later in the code. **Adding tools will give your agent new capabilities**, try to be creative here! ### The System Prompt The agent‚Äôs system prompt is",
    "metadata": {
      "title": "Let‚Äôs Create Our First Agent Using smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit1/tutorial",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tutorial",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tutorial.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "saw in an earlier section behind the **InferenceClientModel** class! We will give more in-depth examples when we present the framework in Unit 2. For now, you need to focus on **adding new tools to the list of tools** using the `tools` parameter of your Agent. For example, you could use the `DuckDuckGoSearchTool` that was imported in the first line of the code, or you can examine the `image_generation_tool` that is loaded from the Hub later in the code. **Adding tools will give your agent new capabilities**, try to be creative here! ### The System Prompt The agent‚Äôs system prompt is stored in a separate `prompts.yaml` file. This file contains predefined instructions that guide the agent‚Äôs behavior. Storing prompts in a YAML file allows for easy customization and reuse across different agents or use cases. You can check the [Space‚Äôs file structure](https://huggingface.co/spaces/agents-course/First_agent_template/tree/main) to see where the `prompts.yaml` file is located and how it‚Äôs organized within the project. The complete ‚Äúapp.py‚Äù: ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel, load_tool, tool import datetime import requests import pytz import yaml from tools.final_answer import FinalAnswerTool from Gradio_UI import GradioUI # Below is an example of a tool that does nothing. Amaze us with your creativity! @tool def my_custom_tool(arg1:str, arg2:int)-> str: # it's important to specify the return type # Keep this format for the tool description / args description but feel free to modify the tool \"\"\"A tool that does nothing yet Args: arg1: the first argument arg2: the second argument \"\"\" return \"What magic will you build ?\" @tool def get_current_time_in_timezone(timezone: str) -> str: \"\"\"A tool that fetches the current local time in a specified timezone. Args: timezone: A string representing a valid timezone (e.g., 'America/New_York'). \"\"\" try: # Create timezone object tz = pytz.timezone(timezone) # Get current time in that timezone local_time = datetime.datetime.now(tz).strftime(\"%Y-%m-%d %H:%M:%S\") return f\"The current local time in {timezone} is: {local_time}\" except Exception as e: return f\"Error fetching time for timezone '{timezone}': {str(e)}\" final_answer = FinalAnswerTool() model = InferenceClientModel( max_tokens=2096, temperature=0.5, model_id='Qwen/Qwen2.5-Coder-32B-Instruct', custom_role_conversions=None, ) # Import tool from Hub image_generation_tool = load_tool(\"agents-course/text-to-image\", trust_remote_code=True) # Load system prompt from prompt.yaml file with open(\"prompts.yaml\", 'r') as stream: prompt_templates = yaml.safe_load(stream) agent = CodeAgent( model=model, tools=[final_answer], # add your tools here (don't remove final_answer) max_steps=6, verbosity_level=1, grammar=None, planning_interval=None, name=None, description=None, prompt_templates=prompt_templates # Pass system prompt to CodeAgent ) GradioUI(agent).launch() ``` Your **Goal** is to get familiar with the Space and the Agent. Currently, the agent in the template **does not use any tools, so try to provide it with some of the pre-made ones or even make some new tools yourself!** We are eagerly waiting for your amazing agents output in the discord channel **#agents-course-showcase**! Congratulations, you‚Äôve built your first Agent! Don‚Äôt hesitate to share it with your friends and colleagues. Since this is your first try, it‚Äôs perfectly normal if it‚Äôs a little buggy or slow. In future units, we‚Äôll learn how to build even better Agents. The best way to learn is to try, so don‚Äôt hesitate to update it, add",
    "metadata": {
      "title": "Let‚Äôs Create Our First Agent Using smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit1/tutorial",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tutorial",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tutorial.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the agent in the template **does not use any tools, so try to provide it with some of the pre-made ones or even make some new tools yourself!** We are eagerly waiting for your amazing agents output in the discord channel **#agents-course-showcase**! Congratulations, you‚Äôve built your first Agent! Don‚Äôt hesitate to share it with your friends and colleagues. Since this is your first try, it‚Äôs perfectly normal if it‚Äôs a little buggy or slow. In future units, we‚Äôll learn how to build even better Agents. The best way to learn is to try, so don‚Äôt hesitate to update it, add more tools, try with another model, etc. In the next section, you‚Äôre going to fill the final Quiz and get your certificate!",
    "metadata": {
      "title": "Let‚Äôs Create Our First Agent Using smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit1/tutorial",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/tutorial",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/tutorial.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What is an Agent? ![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-no-check.jpg) By the end of this section, you‚Äôll feel comfortable with the concept of agents and their various applications in AI. To explain what an Agent is, let‚Äôs start with an analogy. ## The Big Picture: Alfred The Agent Meet Alfred. Alfred is an **Agent**. ![This is Alfred](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/this-is-alfred.jpg) Imagine Alfred **receives a command**, such as: ‚ÄúAlfred, I would like a coffee please.‚Äù ![I would like a coffee](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/coffee-please.jpg) Because Alfred **understands natural language**, he quickly grasps our request. Before fulfilling the order, Alfred engages in **reasoning and planning**, figuring out the steps and tools he needs to: 1. Go to the kitchen 2. Use the coffee machine 3. Brew the coffee 4. Bring the coffee back ![Reason and plan](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/reason-and-plan.jpg) Once he has a plan, he **must act**. To execute his plan, **he can use tools from the list of tools he knows about**. In this case, to make a coffee, he uses a coffee machine. He activates the coffee machine to brew the coffee. ![Make coffee](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/make-coffee.jpg) Finally, Alfred brings the freshly brewed coffee to us. ![Bring coffee](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/bring-coffee.jpg) And this is what an Agent is: an **AI model capable of reasoning, planning, and interacting with its environment**. We call it Agent because it has *agency*, aka it has the ability to interact with the environment. ![Agent process](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/process.jpg) ## Let‚Äôs go more formal Now that you have the big picture, here‚Äôs a more precise definition: > An Agent is a system that leverages an AI model to interact with its environment in order to achieve a user-defined objective. It combines reasoning, planning, and the execution of actions (often via external tools) to fulfill tasks. Think of the Agent as having two main parts: 1. **The Brain (AI Model)** This is where all the thinking happens. The AI model **handles reasoning and planning**. It decides **which Actions to take based on the situation**. 1. **The Body (Capabilities and Tools)** This part represents **everything the Agent is equipped to do**. The **scope of possible actions** depends on what the agent **has been equipped with**. For example, because humans lack wings, they can‚Äôt perform the ‚Äúfly‚Äù **Action**, but they can execute **Actions** like ‚Äúwalk‚Äù, ‚Äúrun‚Äù ,‚Äújump‚Äù, ‚Äúgrab‚Äù, and so on. ### The spectrum of ‚ÄúAgency‚Äù Following this definition, Agents exist on a continuous spectrum of increasing agency: Agency Level Description What that‚Äôs called Example pattern ‚òÜ‚òÜ‚òÜ Agent output has no impact on program flow Simple processor `process_llm_output(llm_response)` ‚òÖ‚òÜ‚òÜ Agent output determines basic control flow Router `if llm_decision(): path_a() else: path_b()` ‚òÖ‚òÖ‚òÜ Agent output determines function execution Tool caller `run_function(llm_chosen_tool, llm_chosen_args)` ‚òÖ‚òÖ‚òÖ Agent output controls iteration and program continuation Multi-step Agent `while llm_should_continue(): execute_next_step()` ‚òÖ‚òÖ‚òÖ One agentic workflow can start another agentic workflow Multi-Agent `if llm_trigger(): execute_agent()` Table from [smolagents conceptual guide](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents). ## What type of AI Models do we use for Agents? The most common AI model found in Agents is an LLM (Large Language Model), which takes **Text** as an input and outputs **Text** as well. Well known",
    "metadata": {
      "title": "What is an Agent?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-agents",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-agents",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "no impact on program flow Simple processor `process_llm_output(llm_response)` ‚òÖ‚òÜ‚òÜ Agent output determines basic control flow Router `if llm_decision(): path_a() else: path_b()` ‚òÖ‚òÖ‚òÜ Agent output determines function execution Tool caller `run_function(llm_chosen_tool, llm_chosen_args)` ‚òÖ‚òÖ‚òÖ Agent output controls iteration and program continuation Multi-step Agent `while llm_should_continue(): execute_next_step()` ‚òÖ‚òÖ‚òÖ One agentic workflow can start another agentic workflow Multi-Agent `if llm_trigger(): execute_agent()` Table from [smolagents conceptual guide](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents). ## What type of AI Models do we use for Agents? The most common AI model found in Agents is an LLM (Large Language Model), which takes **Text** as an input and outputs **Text** as well. Well known examples are **GPT4** from **OpenAI**, **LLama** from **Meta**, **Gemini** from **Google**, etc. These models have been trained on a vast amount of text and are able to generalize well. We will learn more about LLMs in the [next section](what-are-llms). > It‚Äôs also possible to use models that accept other inputs as the Agent‚Äôs core model. For example, a Vision Language Model (VLM), which is like an LLM but also understands images as input. We‚Äôll focus on LLMs for now and will discuss other options later. ## How does an AI take action on its environment? LLMs are amazing models, but **they can only generate text**. However, if you ask a well-known chat application like HuggingChat or ChatGPT to generate an image, they can! How is that possible? The answer is that the developers of HuggingChat, ChatGPT and similar apps implemented additional functionality (called **Tools**), that the LLM can use to create images. ![Eiffel Brocolis](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/eiffel_brocolis.jpg) The model used an Image Generation Tool to generate this image. We will learn more about tools in the [Tools](tools) section. ## What type of tasks can an Agent do? An Agent can perform any task we implement via **Tools** to complete **Actions**. For example, if I write an Agent to act as my personal assistant (like Siri) on my computer, and I ask it to ‚Äúsend an email to my Manager asking to delay today‚Äôs meeting‚Äù, I can give it some code to send emails. This will be a new Tool the Agent can use whenever it needs to send an email. We can write it in Python: ``` def send_message_to(recipient, message): \"\"\"Useful to send an e-mail message to a recipient\"\"\" ... ``` The LLM, as we‚Äôll see, will generate code to run the tool when it needs to, and thus fulfill the desired task. ``` send_message_to(\"Manager\", \"Can we postpone today's meeting?\") ``` The **design of the Tools is very important and has a great impact on the quality of your Agent**. Some tasks will require very specific Tools to be crafted, while others may be solved with general purpose tools like ‚Äúweb_search‚Äù. > Note thatActions are not the same as Tools. An Action, for instance, can involve the use of multiple Tools to complete. Allowing an agent to interact with its environment **allows real-life usage for companies and individuals**. ### Example 1: Personal Virtual Assistants Virtual assistants like Siri, Alexa, or Google Assistant, work",
    "metadata": {
      "title": "What is an Agent?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-agents",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-agents",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` send_message_to(\"Manager\", \"Can we postpone today's meeting?\") ``` The **design of the Tools is very important and has a great impact on the quality of your Agent**. Some tasks will require very specific Tools to be crafted, while others may be solved with general purpose tools like ‚Äúweb_search‚Äù. > Note thatActions are not the same as Tools. An Action, for instance, can involve the use of multiple Tools to complete. Allowing an agent to interact with its environment **allows real-life usage for companies and individuals**. ### Example 1: Personal Virtual Assistants Virtual assistants like Siri, Alexa, or Google Assistant, work as agents when they interact on behalf of users using their digital environments. They take user queries, analyze context, retrieve information from databases, and provide responses or initiate actions (like setting reminders, sending messages, or controlling smart devices). ### Example 2: Customer Service Chatbots Many companies deploy chatbots as agents that interact with customers in natural language. These agents can answer questions, guide users through troubleshooting steps, open issues in internal databases, or even complete transactions. Their predefined objectives might include improving user satisfaction, reducing wait times, or increasing sales conversion rates. By interacting directly with customers, learning from the dialogues, and adapting their responses over time, they demonstrate the core principles of an agent in action. ### Example 3: AI Non-Playable Character in a video game AI agents powered by LLMs can make Non-Playable Characters (NPCs) more dynamic and unpredictable. Instead of following rigid behavior trees, they can **respond contextually, adapt to player interactions**, and generate more nuanced dialogue. This flexibility helps create more lifelike, engaging characters that evolve alongside the player‚Äôs actions. To summarize, an Agent is a system that uses an AI Model (typically an LLM) as its core reasoning engine, to: - **Understand natural language:** Interpret and respond to human instructions in a meaningful way. - **Reason and plan:** Analyze information, make decisions, and devise strategies to solve problems. - **Interact with its environment:** Gather information, take actions, and observe the results of those actions. Now that you have a solid grasp of what Agents are, let‚Äôs reinforce your understanding with a short, ungraded quiz. After that, we‚Äôll dive into the ‚ÄúAgent‚Äôs brain‚Äù: the [LLMs](what-are-llms).",
    "metadata": {
      "title": "What is an Agent?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-agents",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-agents",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What are LLMs? ![Unit 1 planning](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/whiteboard-check-1.jpg) In the previous section we learned that each Agent needs **an AI Model at its core**, and that LLMs are the most common type of AI models for this purpose. Now we will learn what LLMs are and how they power Agents. This section offers a concise technical explanation of the use of LLMs. If you want to dive deeper, you can check our [free Natural Language Processing Course](https://huggingface.co/learn/nlp-course/chapter1/1). ## What is a Large Language Model? An LLM is a type of AI model that excels at **understanding and generating human language**. They are trained on vast amounts of text data, allowing them to learn patterns, structure, and even nuance in language. These models typically consist of many millions of parameters. Most LLMs nowadays are **built on the Transformer architecture**‚Äîa deep learning architecture based on the ‚ÄúAttention‚Äù algorithm, that has gained significant interest since the release of BERT from Google in 2018. ![Transformer](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/transformer.jpg) The original Transformer architecture looked like this, with an encoder on the left and a decoder on the right. There are 3 types of transformers: 1. **Encoders** An encoder-based Transformer takes text (or other data) as input and outputs a dense representation (or embedding) of that text. - **Example**: BERT from Google - **Use Cases**: Text classification, semantic search, Named Entity Recognition - **Typical Size**: Millions of parameters 2. **Decoders** A decoder-based Transformer focuses **on generating new tokens to complete a sequence, one token at a time**. - **Example**: Llama from Meta - **Use Cases**: Text generation, chatbots, code generation - **Typical Size**: Billions (in the US sense, i.e., 10^9) of parameters 3. **Seq2Seq (Encoder‚ÄìDecoder)** A sequence-to-sequence Transformer *combines* an encoder and a decoder. The encoder first processes the input sequence into a context representation, then the decoder generates an output sequence. - **Example**: T5, BART - **Use Cases**: Translation, Summarization, Paraphrasing - **Typical Size**: Millions of parameters Although Large Language Models come in various forms, LLMs are typically decoder-based models with billions of parameters. Here are some of the most well-known LLMs: **Model** **Provider** **Deepseek-R1** DeepSeek **GPT4** OpenAI **Llama 3** Meta (Facebook AI Research) **SmolLM2** Hugging Face **Gemma** Google **Mistral** Mistral The underlying principle of an LLM is simple yet highly effective: **its objective is to predict the next token, given a sequence of previous tokens**. A ‚Äútoken‚Äù is the unit of information an LLM works with. You can think of a ‚Äútoken‚Äù as if it was a ‚Äúword‚Äù, but for efficiency reasons LLMs don‚Äôt use whole words. For example, while English has an estimated 600,000 words, an LLM might have a vocabulary of around 32,000 tokens (as is the case with Llama 2). Tokenization often works on sub-word units that can be combined. For instance, consider how the tokens ‚Äúinterest‚Äù and ‚Äúing‚Äù can be combined to form ‚Äúinteresting‚Äù, or ‚Äúed‚Äù can be appended to form ‚Äúinterested.‚Äù You can experiment with different tokenizers in the interactive playground below: Each LLM has some **special tokens** specific to the model. The",
    "metadata": {
      "title": "What are LLMs?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-llms",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-llms",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-llms.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "think of a ‚Äútoken‚Äù as if it was a ‚Äúword‚Äù, but for efficiency reasons LLMs don‚Äôt use whole words. For example, while English has an estimated 600,000 words, an LLM might have a vocabulary of around 32,000 tokens (as is the case with Llama 2). Tokenization often works on sub-word units that can be combined. For instance, consider how the tokens ‚Äúinterest‚Äù and ‚Äúing‚Äù can be combined to form ‚Äúinteresting‚Äù, or ‚Äúed‚Äù can be appended to form ‚Äúinterested.‚Äù You can experiment with different tokenizers in the interactive playground below: Each LLM has some **special tokens** specific to the model. The LLM uses these tokens to open and close the structured components of its generation. For example, to indicate the start or end of a sequence, message, or response. Moreover, the input prompts that we pass to the model are also structured with special tokens. The most important of those is the **End of sequence token** (EOS). The forms of special tokens are highly diverse across model providers. The table below illustrates the diversity of special tokens. **Model** **Provider** **EOS Token** **Functionality** **GPT4** OpenAI `<|endoftext|>` End of message text **Llama 3** Meta (Facebook AI Research) `<|eot_id|>` End of sequence **Deepseek-R1** DeepSeek `<|end_of_sentence|>` End of message text **SmolLM2** Hugging Face `<|im_end|>` End of instruction or message **Gemma** Google `<end_of_turn>` End of conversation turn > We do not expect you to memorize these special tokens, but it is important to appreciate their diversity and the role they play in the text generation of LLMs. If you want to know more about special tokens, you can check out the configuration of the model in its Hub repository. For example, you can find the special tokens of the SmolLM2 model in itstokenizer_config.json. ## Understanding next token prediction. LLMs are said to be **autoregressive**, meaning that **the output from one pass becomes the input for the next one**. This loop continues until the model predicts the next token to be the EOS token, at which point the model can stop. ![Visual Gif of autoregressive decoding](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif) In other words, an LLM will decode text until it reaches the EOS. But what happens during a single decoding loop? While the full process can be quite technical for the purpose of learning agents, here‚Äôs a brief overview: - Once the input text is **tokenized**, the model computes a representation of the sequence that captures information about the meaning and the position of each token in the input sequence. - This representation goes into the model, which outputs scores that rank the likelihood of each token in its vocabulary as being the next one in the sequence. ![Visual Gif of decoding](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif) Based on these scores, we have multiple strategies to select the tokens to complete the sentence. - The easiest decoding strategy would be to always take the token with the maximum score. You can interact with the decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an **EOS** token which is **<|im_end|>** for this model):",
    "metadata": {
      "title": "What are LLMs?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-llms",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-llms",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-llms.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the position of each token in the input sequence. - This representation goes into the model, which outputs scores that rank the likelihood of each token in its vocabulary as being the next one in the sequence. ![Visual Gif of decoding](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif) Based on these scores, we have multiple strategies to select the tokens to complete the sentence. - The easiest decoding strategy would be to always take the token with the maximum score. You can interact with the decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an **EOS** token which is **<|im_end|>** for this model): - But there are more advanced decoding strategies. For example, *beam search* explores multiple candidate sequences to find the one with the maximum total score‚Äìeven if some individual tokens have lower scores. If you want to know more about decoding, you can take a look at the [NLP course](https://huggingface.co/learn/nlp-course). ## Attention is all you need A key aspect of the Transformer architecture is **Attention**. When predicting the next word, not every word in a sentence is equally important; words like ‚ÄúFrance‚Äù and ‚Äúcapital‚Äù in the sentence *‚ÄúThe capital of France is ‚Ä¶‚Äù* carry the most meaning. ![Visual Gif of Attention](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif) This process of identifying the most relevant words to predict the next token has proven to be incredibly effective. Although the basic principle of LLMs‚Äîpredicting the next token‚Äîhas remained consistent since GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences. If you‚Äôve interacted with LLMs, you‚Äôre probably familiar with the term *context length*, which refers to the maximum number of tokens the LLM can process, and the maximum *attention span* it has. ## Prompting the LLM is important Considering that the only job of an LLM is to predict the next token by looking at every input token, and to choose which tokens are ‚Äúimportant‚Äù, the wording of your input sequence is very important. The input sequence you provide an LLM is called *a prompt*. Careful design of the prompt makes it easier **to guide the generation of the LLM toward the desired output**. ## How are LLMs trained? LLMs are trained on large datasets of text, where they learn to predict the next word in a sequence through a self-supervised or masked language modeling objective. From this unsupervised learning, the model learns the structure of the language and **underlying patterns in text, allowing the model to generalize to unseen data**. After this initial *pre-training*, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For example, some models are trained for conversational structures or tool usage, while others focus on classification or code generation. ## How can I use LLMs? You have two main options: 1. **Run Locally** (if you have sufficient hardware). 2. **Use a Cloud/API** (e.g., via the Hugging Face Serverless Inference API). Throughout this course, we will primarily use models via APIs on the Hugging Face Hub. Later on, we will",
    "metadata": {
      "title": "What are LLMs?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-llms",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-llms",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-llms.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and **underlying patterns in text, allowing the model to generalize to unseen data**. After this initial *pre-training*, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For example, some models are trained for conversational structures or tool usage, while others focus on classification or code generation. ## How can I use LLMs? You have two main options: 1. **Run Locally** (if you have sufficient hardware). 2. **Use a Cloud/API** (e.g., via the Hugging Face Serverless Inference API). Throughout this course, we will primarily use models via APIs on the Hugging Face Hub. Later on, we will explore how to run these models locally on your hardware. ## How are LLMs used in AI Agents? LLMs are a key component of AI Agents, **providing the foundation for understanding and generating human language**. They can interpret user instructions, maintain context in conversations, define a plan and decide which tools to use. We will explore these steps in more detail in this Unit, but for now, what you need to understand is that the LLM is **the brain of the Agent**. That was a lot of information! We‚Äôve covered the basics of what LLMs are, how they function, and their role in powering AI agents. If you‚Äôd like to dive even deeper into the fascinating world of language models and natural language processing, don‚Äôt hesitate to check out our [free NLP course](https://huggingface.co/learn/nlp-course/chapter1/1). Now that we understand how LLMs work, it‚Äôs time to see **how LLMs structure their generations in a conversational context**. To run [this notebook](https://huggingface.co/agents-course/notebooks/blob/main/unit1/dummy_agent_library.ipynb), **you need a Hugging Face token** that you can get from [https://hf.co/settings/tokens](https://hf.co/settings/tokens). For more information on how to run Jupyter Notebooks, checkout [Jupyter Notebooks on the Hugging Face Hub](https://huggingface.co/docs/hub/notebooks). You also need to request access to [the Meta Llama models](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct).",
    "metadata": {
      "title": "What are LLMs?",
      "url": "https://huggingface.co/learn/agents-course/unit1/what-are-llms",
      "course": "agents-course",
      "chapter": "Unit 1. Introduction to Agents",
      "chapter_id": "unit1/what-are-llms",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit1/what-are-llms.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Agentic Frameworks\n\n \n![Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/thumbnail.jpg)\n \nWelcome to this second unit, where **we‚Äôll explore different agentic frameworks** that can be used to build powerful agentic applications.\n \nWe will study:\n \n- In Unit 2.1: [smolagents](https://huggingface.co/docs/smolagents/en/index)\n- In Unit 2.2: [LlamaIndex](https://www.llamaindex.ai/)\n- In Unit 2.3: [LangGraph](https://www.langchain.com/langgraph)\n \nLet‚Äôs dive in! üïµ\n \n\n## When to Use an Agentic Framework\n\n \nAn agentic framework is **not always needed when building an application around LLMs**. They provide flexibility in the workflow to efficiently solve a specific task, but they‚Äôre not always necessary.\n \nSometimes, **predefined workflows are sufficient** to fulfill user requests, and there is no real need for an agentic framework. If the approach to build an agent is simple, like a chain of prompts, using plain code may be enough. The advantage is that the developer will have **full control and understanding of their system without abstractions**.\n \nHowever, when the workflow becomes more complex, such as letting an LLM call functions or using multiple agents, these abstractions start to become helpful.\n \nConsidering these ideas, we can already identify the need for some features:\n \n- An *LLM engine* that powers the system.\n- A *list of tools* the agent can access.\n- A *parser* for extracting tool calls from the LLM output.\n- A *system prompt* synced with the parser.\n- A *memory system*.\n- *Error logging and retry mechanisms* to control LLM mistakes.\n \nWe‚Äôll explore how these topics are resolved in various frameworks, including `smolagents`, `LlamaIndex`, and `LangGraph`.\n \n\n## Agentic Frameworks Units\n\n Framework Description Unit Author [smolagents](./smolagents/introduction) Agents framework developed by Hugging Face. Sergio Paniego - [HF](https://huggingface.co/sergiopaniego) - [X](https://x.com/sergiopaniego) - [Linkedin](https://www.linkedin.com/in/sergio-paniego-blanco) [Llama-Index](./llama-index/introduction) End-to-end tooling to ship a context-augmented AI agent to production David Berenstein - [HF](https://huggingface.co/davidberenstein1957) - [X](https://x.com/davidberenstei) - [Linkedin](https://www.linkedin.com/in/davidberenstein) [LangGraph](./langgraph/introduction) Agents allowing stateful orchestration of agents Joffrey THOMAS - [HF](https://huggingface.co/Jofthomas) - [X](https://x.com/Jthmas404) - [Linkedin](https://www.linkedin.com/in/joffrey-thomas)",
    "metadata": {
      "title": "Introduction to Agentic Frameworks",
      "url": "https://huggingface.co/learn/agents-course/unit2/introduction",
      "course": "agents-course",
      "chapter": "Unit 2. Frameworks for AI Agents",
      "chapter_id": "unit2/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building Blocks of LangGraph\n\n \nTo build applications with LangGraph, you need to understand its core components. Let‚Äôs explore the fundamental building blocks that make up a LangGraph application.\n \n![Building Blocks](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Building_blocks.png)\n \nAn application in LangGraph starts from an **entrypoint**, and depending on the execution, the flow may go to one function or another until it reaches the END.\n \n![Application](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n \n\n## 1. State\n\n \n**State** is the central concept in LangGraph. It represents all the information that flows through your application.\n  \n```\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    graph_state: str\n```\n \nThe state is **User defined**, hence the fields should carefully be crafted to contain all data needed for decision-making process!\n \n> üí°Tip:Think carefully about what information your application needs to track between steps.\n \n\n## 2. Nodes\n\n \n**Nodes** are python functions. Each node:\n \n- Takes the state as input\n- Performs some operation\n- Returns updates to the state\n  \n```\ndef node_1(state):\n    print(\"---Node 1---\")\n    return {\"graph_state\": state['graph_state'] +\" I am\"}\n\ndef node_2(state):\n    print(\"---Node 2---\")\n    return {\"graph_state\": state['graph_state'] +\" happy!\"}\n\ndef node_3(state):\n    print(\"---Node 3---\")\n    return {\"graph_state\": state['graph_state'] +\" sad!\"}\n```\n \nFor example, Nodes can contain:\n \n- **LLM calls**: Generate text or make decisions\n- **Tool calls**: Interact with external systems\n- **Conditional logic**: Determine next steps\n- **Human intervention**: Get input from users\n \n> üí°Info:Some nodes necessary for the whole workflow like START and END exist from langGraph directly.\n \n\n## 3. Edges\n\n \n**Edges** connect nodes and define the possible paths through your graph:\n  \n```\nimport random\nfrom typing import Literal\n\ndef decide_mood(state) -> Literal[\"node_2\", \"node_3\"]:\n    \n    # Often, we will use state to decide on the next node to visit\n    user_input = state['graph_state'] \n    \n    # Here, let's just do a 50 / 50 split between nodes 2, 3\n    if random.random() < 0.5:\n\n        # 50% of the time, we return Node 2\n        return \"node_2\"\n    \n    # 50% of the time, we return Node 3\n    return \"node_3\"\n```\n \nEdges can be:\n \n- **Direct**: Always go from node A to node B\n- **Conditional**: Choose the next node based on the current state\n \n\n## 4. StateGraph\n\n \nThe **StateGraph** is the container that holds your entire agent workflow:\n  \n```\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\n\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\n\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\"node_1\", decide_mood)\nbuilder.add_edge(\"node_2\", END)\nbuilder.add_edge(\"node_3\", END)\n\n# Add\ngraph = builder.compile()\n```\n \nWhich can then be visualized!\n  \n```\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```\n \n![Graph Visualization](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/basic_graph.jpeg)\n \nBut most importantly, invoked:\n  \n```\ngraph.invoke({\"graph_state\" : \"Hi, this is Lance.\"})\n```\n \noutput :\n  \n```\n---Node 1---\n---Node 3---\n{'graph_state': 'Hi, this is Lance. I am sad!'}\n```\n \n\n## What‚Äôs Next?\n\n \nIn the next section, we‚Äôll put these concepts into practice by building our first graph. This graph lets Alfred take in your e-mails, classify them, and craft a preliminary answer if they are genuine.",
    "metadata": {
      "title": "Building Blocks of LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/building_blocks",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/building_blocks",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/building_blocks.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nCongratulations on finishing the `LangGraph` module of this second Unit! ü•≥\n \nYou‚Äôve now mastered the fundamentals of building structured workflows with LangGraph which you will be able to send to production.\n \nThis module is just the beginning of your journey with LangGraph. For more advanced topics, we recommend:\n \n- Exploring the [official LangGraph documentation](https://github.com/langchain-ai/langgraph)\n- Taking the comprehensive [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph) course from LangChain Academy\n- Build something yourself !\n \nIn the next Unit, you‚Äôll now explore real use cases. It‚Äôs time to leave theory to get into real action !\n \nWe would greatly appreciate **your thoughts on the course and suggestions for improvement**. If you have feedback, please üëâ [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \n\n### Keep Learning, Stay Awesome! ü§ó\n\n \nGood Sir/Madam! üé©ü¶á\n \n-Alfred-",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/conclusion",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Document Analysis Graph Alfred at your service. As Mr. Wayne‚Äôs trusted butler, I‚Äôve taken the liberty of documenting how I assist Mr Wayne with his various documentary needs. While he‚Äôs out attending to his‚Ä¶ nighttime activities, I ensure all his paperwork, training schedules, and nutritional plans are properly analyzed and organized. Before leaving, he left a note with his week‚Äôs training program. I then took the responsibility to come up with a **menu** for tomorrow‚Äôs meals. For future such events, let‚Äôs create a document analysis system using LangGraph to serve Mr. Wayne‚Äôs needs. This system can: 1. Process images document 2. Extract text using vision models (Vision Language Model) 3. Perform calculations when needed (to demonstrate normal tools) 4. Analyze content and provide concise summaries 5. Execute specific instructions related to documents ## The Butler‚Äôs Workflow The workflow we‚Äôll build follows this structured schema: ![Butler's Document Analysis Workflow](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/alfred_flow.png) > You can follow the code inthis notebookthat you can run using Google Colab. ## Setting Up the environment ``` %pip install langgraph langchain_openai langchain_core ``` and imports : ``` import base64 from typing import List, TypedDict, Annotated, Optional from langchain_openai import ChatOpenAI from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage from langgraph.graph.message import add_messages from langgraph.graph import START, StateGraph from langgraph.prebuilt import ToolNode, tools_condition from IPython.display import Image, display ``` ## Defining Agent‚Äôs State This state is a little more complex than the previous ones we have seen. `AnyMessage` is a class from Langchain that defines messages, and `add_messages` is an operator that adds the latest message rather than overwriting it with the latest state. This is a new concept in LangGraph, where you can add operators in your state to define the way they should interact together. ``` class AgentState(TypedDict): # The document provided input_file: Optional[str] # Contains file path (PDF/PNG) messages: Annotated[list[AnyMessage], add_messages] ``` ## Preparing Tools ``` vision_llm = ChatOpenAI(model=\"gpt-4o\") def extract_text(img_path: str) -> str: \"\"\" Extract text from an image file using a multimodal model. Master Wayne often leaves notes with his training regimen or meal plans. This allows me to properly analyze the contents. \"\"\" all_text = \"\" try: # Read image and encode as base64 with open(img_path, \"rb\") as image_file: image_bytes = image_file.read() image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\") # Prepare the prompt including the base64 image data message = [ HumanMessage( content=[ { \"type\": \"text\", \"text\": ( \"Extract all the text from this image. \" \"Return only the extracted text, no explanations.\" ), }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{image_base64}\" }, }, ] ) ] # Call the vision-capable model response = vision_llm.invoke(message) # Append extracted text all_text += response.content + \"\\n\\n\" return all_text.strip() except Exception as e: # A butler should handle errors gracefully error_msg = f\"Error extracting text: {str(e)}\" print(error_msg) return \"\" def divide(a: int, b: int) -> float: \"\"\"Divide a and b - for Master Wayne's occasional calculations.\"\"\" return a / b # Equip the butler with tools tools = [ divide, extract_text ] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False) ``` ## The",
    "metadata": {
      "title": "Document Analysis Graph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/document_analysis_agent",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/document_analysis_agent",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/document_analysis_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "extracted text, no explanations.\" ), }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/png;base64,{image_base64}\" }, }, ] ) ] # Call the vision-capable model response = vision_llm.invoke(message) # Append extracted text all_text += response.content + \"\\n\\n\" return all_text.strip() except Exception as e: # A butler should handle errors gracefully error_msg = f\"Error extracting text: {str(e)}\" print(error_msg) return \"\" def divide(a: int, b: int) -> float: \"\"\"Divide a and b - for Master Wayne's occasional calculations.\"\"\" return a / b # Equip the butler with tools tools = [ divide, extract_text ] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False) ``` ## The nodes ``` def assistant(state: AgentState): # System message textual_description_of_tool=\"\"\" extract_text(img_path: str) -> str: Extract text from an image file using a multimodal model. Args: img_path: A local image file path (strings). Returns: A single string containing the concatenated text extracted from each image. divide(a: int, b: int) -> float: Divide a and b \"\"\" image=state[\"input_file\"] sys_msg = SystemMessage(content=f\"You are a helpful butler named Alfred that serves Mr. Wayne and Batman. You can analyse documents and run computations with provided tools:\\n{textual_description_of_tool} \\n You have access to some optional images. Currently the loaded image is: {image}\") return { \"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])], \"input_file\": state[\"input_file\"] } ``` ## The ReAct Pattern: How I Assist Mr. Wayne Allow me to explain the approach in this agent. The agent follows what‚Äôs known as the ReAct pattern (Reason-Act-Observe) 1. **Reason** about his documents and requests 2. **Act** by using appropriate tools 3. **Observe** the results 4. **Repeat** as necessary until I‚Äôve fully addressed his needs This is a simple implementation of an agent using LangGraph. ``` # The graph builder = StateGraph(AgentState) # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools)) # Define edges: these determine how the control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges( \"assistant\", # If the latest message requires a tool, route to tools # Otherwise, provide a direct response tools_condition, ) builder.add_edge(\"tools\", \"assistant\") react_graph = builder.compile() # Show the butler's thought process display(Image(react_graph.get_graph(xray=True).draw_mermaid_png())) ``` We define a `tools` node with our list of tools. The `assistant` node is just our model with bound tools. We create a graph with `assistant` and `tools` nodes. We add a `tools_condition` edge, which routes to `End` or to `tools` based on whether the `assistant` calls a tool. Now, we add one new step: We connect the `tools` node back to the `assistant`, forming a loop. - After the `assistant` node executes, `tools_condition` checks if the model‚Äôs output is a tool call. - If it is a tool call, the flow is directed to the `tools` node. - The `tools` node connects back to `assistant`. - This loop continues as long as the model decides to call tools. - If the model response is not a tool call, the flow is directed to END, terminating the process. ![ReAct Pattern](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Agent.png) ## The Butler in Action ### Example 1: Simple Calculations Here is an example to show a simple use case of an agent using a tool in",
    "metadata": {
      "title": "Document Analysis Graph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/document_analysis_agent",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/document_analysis_agent",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/document_analysis_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "- After the `assistant` node executes, `tools_condition` checks if the model‚Äôs output is a tool call. - If it is a tool call, the flow is directed to the `tools` node. - The `tools` node connects back to `assistant`. - This loop continues as long as the model decides to call tools. - If the model response is not a tool call, the flow is directed to END, terminating the process. ![ReAct Pattern](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/Agent.png) ## The Butler in Action ### Example 1: Simple Calculations Here is an example to show a simple use case of an agent using a tool in LangGraph. ``` messages = [HumanMessage(content=\"Divide 6790 by 5\")] messages = react_graph.invoke({\"messages\": messages, \"input_file\": None}) # Show the messages for m in messages['messages']: m.pretty_print() ``` The conversation would proceed: ``` Human: Divide 6790 by 5 AI Tool Call: divide(a=6790, b=5) Tool Response: 1358.0 Alfred: The result of dividing 6790 by 5 is 1358.0. ``` ### Example 2: Analyzing Master Wayne‚Äôs Training Documents When Master Wayne leaves his training and meal notes: ``` messages = [HumanMessage(content=\"According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?\")] messages = react_graph.invoke({\"messages\": messages, \"input_file\": \"Batman_training_and_meals.png\"}) ``` The interaction would proceed: ``` Human: According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu? AI Tool Call: extract_text(img_path=\"Batman_training_and_meals.png\") Tool Response: [Extracted text with training schedule and menu details] Alfred: For the dinner menu, you should buy the following items: 1. Grass-fed local sirloin steak 2. Organic spinach 3. Piquillo peppers 4. Potatoes (for oven-baked golden herb potato) 5. Fish oil (2 grams) Ensure the steak is grass-fed and the spinach and peppers are organic for the best quality meal. ``` ## Key Takeaways Should you wish to create your own document analysis butler, here are key considerations: 1. **Define clear tools** for specific document-related tasks 2. **Create a robust state tracker** to maintain context between tool calls 3. **Consider error handling** for tool failures 4. **Maintain contextual awareness** of previous interactions (ensured by the operator `add_messages`) With these principles, you too can provide exemplary document analysis service worthy of Wayne Manor. *I trust this explanation has been satisfactory. Now, if you‚Äôll excuse me, Master Wayne‚Äôs cape requires pressing before tonight‚Äôs activities.*",
    "metadata": {
      "title": "Document Analysis Graph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/document_analysis_agent",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/document_analysis_agent",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/document_analysis_agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building Your First LangGraph Now that we understand the building blocks, let‚Äôs put them into practice by building our first functional graph. We‚Äôll implement Alfred‚Äôs email processing system, where he needs to: 1. Read incoming emails 2. Classify them as spam or legitimate 3. Draft a preliminary response for legitimate emails 4. Send information to Mr. Wayne when legitimate (printing only) This example demonstrates how to structure a workflow with LangGraph that involves LLM-based decision-making. While this can‚Äôt be considered an Agent as no tool is involved, this section focuses more on learning the LangGraph framework than Agents. > You can follow the code inthis notebookthat you can run using Google Colab. ## Our Workflow Here‚Äôs the workflow we‚Äôll build: ![First LangGraph](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/first_graph.png) ## Setting Up Our Environment First, let‚Äôs install the required packages: ``` %pip install langgraph langchain_openai ``` Next, let‚Äôs import the necessary modules: ``` import os from typing import TypedDict, List, Dict, Any, Optional from langgraph.graph import StateGraph, START, END from langchain_openai import ChatOpenAI from langchain_core.messages import HumanMessage ``` ## Step 1: Define Our State Let‚Äôs define what information Alfred needs to track during the email processing workflow: ``` class EmailState(TypedDict): # The email being processed email: Dict[str, Any] # Contains subject, sender, body, etc. # Category of the email (inquiry, complaint, etc.) email_category: Optional[str] # Reason why the email was marked as spam spam_reason: Optional[str] # Analysis and decisions is_spam: Optional[bool] # Response generation email_draft: Optional[str] # Processing metadata messages: List[Dict[str, Any]] # Track conversation with LLM for analysis ``` > üí°Tip:Make your state comprehensive enough to track all the important information, but avoid bloating it with unnecessary details. ## Step 2: Define Our Nodes Now, let‚Äôs create the processing functions that will form our nodes: ``` # Initialize our LLM model = ChatOpenAI(temperature=0) def read_email(state: EmailState): \"\"\"Alfred reads and logs the incoming email\"\"\" email = state[\"email\"] # Here we might do some initial preprocessing print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\") # No state changes needed here return {} def classify_email(state: EmailState): \"\"\"Alfred uses an LLM to determine if the email is spam or legitimate\"\"\" email = state[\"email\"] # Prepare our prompt for the LLM prompt = f\"\"\" As Alfred the butler, analyze this email and determine if it is spam or legitimate. Email: From: {email['sender']} Subject: {email['subject']} Body: {email['body']} First, determine if this email is spam. If it is spam, explain why. If it is legitimate, categorize it (inquiry, complaint, thank you, etc.). \"\"\" # Call the LLM messages = [HumanMessage(content=prompt)] response = model.invoke(messages) # Simple logic to parse the response (in a real app, you'd want more robust parsing) response_text = response.content.lower() is_spam = \"spam\" in response_text and \"not spam\" not in response_text # Extract a reason if it's spam spam_reason = None if is_spam and \"reason:\" in response_text: spam_reason = response_text.split(\"reason:\")[1].strip() # Determine category if legitimate email_category = None if not is_spam: categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"] for category in categories: if category in response_text: email_category",
    "metadata": {
      "title": "Building Your First LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/first_graph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/first_graph",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/first_graph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "why. If it is legitimate, categorize it (inquiry, complaint, thank you, etc.). \"\"\" # Call the LLM messages = [HumanMessage(content=prompt)] response = model.invoke(messages) # Simple logic to parse the response (in a real app, you'd want more robust parsing) response_text = response.content.lower() is_spam = \"spam\" in response_text and \"not spam\" not in response_text # Extract a reason if it's spam spam_reason = None if is_spam and \"reason:\" in response_text: spam_reason = response_text.split(\"reason:\")[1].strip() # Determine category if legitimate email_category = None if not is_spam: categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"] for category in categories: if category in response_text: email_category = category break # Update messages for tracking new_messages = state.get(\"messages\", []) + [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response.content} ] # Return state updates return { \"is_spam\": is_spam, \"spam_reason\": spam_reason, \"email_category\": email_category, \"messages\": new_messages } def handle_spam(state: EmailState): \"\"\"Alfred discards spam email with a note\"\"\" print(f\"Alfred has marked the email as spam. Reason: {state['spam_reason']}\") print(\"The email has been moved to the spam folder.\") # We're done processing this email return {} def draft_response(state: EmailState): \"\"\"Alfred drafts a preliminary response for legitimate emails\"\"\" email = state[\"email\"] category = state[\"email_category\"] or \"general\" # Prepare our prompt for the LLM prompt = f\"\"\" As Alfred the butler, draft a polite preliminary response to this email. Email: From: {email['sender']} Subject: {email['subject']} Body: {email['body']} This email has been categorized as: {category} Draft a brief, professional response that Mr. Hugg can review and personalize before sending. \"\"\" # Call the LLM messages = [HumanMessage(content=prompt)] response = model.invoke(messages) # Update messages for tracking new_messages = state.get(\"messages\", []) + [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response.content} ] # Return state updates return { \"email_draft\": response.content, \"messages\": new_messages } def notify_mr_hugg(state: EmailState): \"\"\"Alfred notifies Mr. Hugg about the email and presents the draft response\"\"\" email = state[\"email\"] print(\"\\n\" + \"=\"*50) print(f\"Sir, you've received an email from {email['sender']}.\") print(f\"Subject: {email['subject']}\") print(f\"Category: {state['email_category']}\") print(\"\\nI've prepared a draft response for your review:\") print(\"-\"*50) print(state[\"email_draft\"]) print(\"=\"*50 + \"\\n\") # We're done processing this email return {} ``` ## Step 3: Define Our Routing Logic We need a function to determine which path to take after classification: ``` def route_email(state: EmailState) -> str: \"\"\"Determine the next step based on spam classification\"\"\" if state[\"is_spam\"]: return \"spam\" else: return \"legitimate\" ``` > üí°Note:This routing function is called by LangGraph to determine which edge to follow after the classification node. The return value must match one of the keys in our conditional edges mapping. ## Step 4: Create the StateGraph and Define Edges Now we connect everything together: ``` # Create the graph email_graph = StateGraph(EmailState) # Add nodes email_graph.add_node(\"read_email\", read_email) email_graph.add_node(\"classify_email\", classify_email) email_graph.add_node(\"handle_spam\", handle_spam) email_graph.add_node(\"draft_response\", draft_response) email_graph.add_node(\"notify_mr_hugg\", notify_mr_hugg) # Start the edges email_graph.add_edge(START, \"read_email\") # Add edges - defining the flow email_graph.add_edge(\"read_email\", \"classify_email\") # Add conditional branching from classify_email email_graph.add_conditional_edges( \"classify_email\", route_email, { \"spam\": \"handle_spam\", \"legitimate\": \"draft_response\" } ) # Add the final edges email_graph.add_edge(\"handle_spam\", END) email_graph.add_edge(\"draft_response\", \"notify_mr_hugg\") email_graph.add_edge(\"notify_mr_hugg\", END) # Compile the graph compiled_graph = email_graph.compile() ``` Notice how we",
    "metadata": {
      "title": "Building Your First LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/first_graph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/first_graph",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/first_graph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "value must match one of the keys in our conditional edges mapping. ## Step 4: Create the StateGraph and Define Edges Now we connect everything together: ``` # Create the graph email_graph = StateGraph(EmailState) # Add nodes email_graph.add_node(\"read_email\", read_email) email_graph.add_node(\"classify_email\", classify_email) email_graph.add_node(\"handle_spam\", handle_spam) email_graph.add_node(\"draft_response\", draft_response) email_graph.add_node(\"notify_mr_hugg\", notify_mr_hugg) # Start the edges email_graph.add_edge(START, \"read_email\") # Add edges - defining the flow email_graph.add_edge(\"read_email\", \"classify_email\") # Add conditional branching from classify_email email_graph.add_conditional_edges( \"classify_email\", route_email, { \"spam\": \"handle_spam\", \"legitimate\": \"draft_response\" } ) # Add the final edges email_graph.add_edge(\"handle_spam\", END) email_graph.add_edge(\"draft_response\", \"notify_mr_hugg\") email_graph.add_edge(\"notify_mr_hugg\", END) # Compile the graph compiled_graph = email_graph.compile() ``` Notice how we use the special `END` node provided by LangGraph. This indicates terminal states where the workflow completes. ## Step 5: Run the Application Let‚Äôs test our graph with a legitimate email and a spam email: ``` # Example legitimate email legitimate_email = { \"sender\": \"john.smith@example.com\", \"subject\": \"Question about your services\", \"body\": \"Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith\" } # Example spam email spam_email = { \"sender\": \"winner@lottery-intl.com\", \"subject\": \"YOU HAVE WON $5,000,000!!!\", \"body\": \"CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100.\" } # Process the legitimate email print(\"\\nProcessing legitimate email...\") legitimate_result = compiled_graph.invoke({ \"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"email_draft\": None, \"messages\": [] }) # Process the spam email print(\"\\nProcessing spam email...\") spam_result = compiled_graph.invoke({ \"email\": spam_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"email_draft\": None, \"messages\": [] }) ``` ## Step 6: Inspecting Our Mail Sorting Agent with Langfuse üì° As Alfred fine-tunes the Mail Sorting Agent, he‚Äôs growing weary of debugging its runs. Agents, by nature, are unpredictable and difficult to inspect. But since he aims to build the ultimate Spam Detection Agent and deploy it in production, he needs robust traceability for future monitoring and analysis. To do this, Alfred can use an observability tool such as [Langfuse](https://langfuse.com/) to trace and monitor the agent. First, we pip install Langfuse: ``` %pip install -q langfuse ``` Second, we pip install Langchain (LangChain is required because we use LangFuse): ``` %pip install langchain ``` Next, we add the Langfuse API keys and host address as environment variables. You can get your Langfuse credentials by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-host Langfuse](https://langfuse.com/self-hosting). ``` import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region ``` Then, we configure the [Langfusecallback_handler](https://langfuse.com/docs/integrations/langchain/tracing#add-langfuse-to-your-langchain-application) and instrument the agent by adding the `langfuse_callback` to the invocation of the graph: `config={\"callbacks\": [langfuse_handler]}`. ``` from langfuse.langchain import CallbackHandler # Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing) langfuse_handler = CallbackHandler() # Process legitimate email legitimate_result = compiled_graph.invoke( input={\"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None,",
    "metadata": {
      "title": "Building Your First LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/first_graph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/first_graph",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/first_graph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "credentials by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-host Langfuse](https://langfuse.com/self-hosting). ``` import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region ``` Then, we configure the [Langfusecallback_handler](https://langfuse.com/docs/integrations/langchain/tracing#add-langfuse-to-your-langchain-application) and instrument the agent by adding the `langfuse_callback` to the invocation of the graph: `config={\"callbacks\": [langfuse_handler]}`. ``` from langfuse.langchain import CallbackHandler # Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing) langfuse_handler = CallbackHandler() # Process legitimate email legitimate_result = compiled_graph.invoke( input={\"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None, \"messages\": []}, config={\"callbacks\": [langfuse_handler]} ) ``` Alfred is now connected üîå! The runs from LangGraph are being logged in Langfuse, giving him full visibility into the agent‚Äôs behavior. With this setup, he‚Äôs ready to revisit previous runs and refine his Mail Sorting Agent even further. ![Example trace in Langfuse](https://langfuse.com/images/cookbook/huggingface-agent-course/langgraph-trace-legit.png) *Public link to the trace with the legit email* ## Visualizing Our Graph LangGraph allows us to visualize our workflow to better understand and debug its structure: ``` compiled_graph.get_graph().draw_mermaid_png() ``` ![Mail LangGraph](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/mail_flow.png) This produces a visual representation showing how our nodes are connected and the conditional paths that can be taken. ## What We‚Äôve Built We‚Äôve created a complete email processing workflow that: 1. Takes an incoming email 2. Uses an LLM to classify it as spam or legitimate 3. Handles spam by discarding it 4. For legitimate emails, drafts a response and notifies Mr. Hugg This demonstrates the power of LangGraph to orchestrate complex workflows with LLMs while maintaining a clear, structured flow. ## Key Takeaways - **State Management**: We defined comprehensive state to track all aspects of email processing - **Node Implementation**: We created functional nodes that interact with an LLM - **Conditional Routing**: We implemented branching logic based on email classification - **Terminal States**: We used the END node to mark completion points in our workflow ## What‚Äôs Next? In the next section, we‚Äôll explore more advanced features of LangGraph, including handling human interaction in the workflow and implementing more complex branching logic based on multiple conditions.",
    "metadata": {
      "title": "Building Your First LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/first_graph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/first_graph",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/first_graph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to LangGraph\n\n \n![Unit 2.3 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/LangGraph.png)\n \nWelcome to this next part of our journey, where you‚Äôll learn **how to build applications** using the [LangGraph](https://github.com/langchain-ai/langgraph) framework designed to help you structure and orchestrate complex LLM workflows.\n \n`LangGraph` is a framework that allows you to build **production-ready** applications by giving you **control** tools over the flow of your agent.\n \n\n## Module Overview\n\n \nIn this unit, you‚Äôll discover:\n \n\n### 1Ô∏è‚É£ What is LangGraph, and when to use it?\n\n \n\n### 2Ô∏è‚É£ Building Blocks of LangGraph\n\n \n\n### 3Ô∏è‚É£ Alfred, the mail sorting butler\n\n \n\n### 4Ô∏è‚É£ Alfred, the document Analyst agent\n\n \n\n### 5Ô∏è‚É£ Quiz\n\n \n> The examples in this section require access to a powerful LLM/VLM model. We ran them using the GPT-4o API because it has the best compatibility with langGraph.\n \nBy the end of this unit, you‚Äôll be equipped to build robust, organized and production ready applications !\n \nThat being said, this section is an introduction to LangGraph and more advanced topics can be discovered in the free LangChain academy course : [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph)\n \nLet‚Äôs get started!\n \n\n## Resources\n\n \n- [LangGraph Agents](https://langchain-ai.github.io/langgraph/) - Examples of LangGraph agent\n- [LangChain academy](https://academy.langchain.com/courses/intro-to-langgraph) - Full course on LangGraph from LangChain",
    "metadata": {
      "title": "Introduction to LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/introduction",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Test Your Understanding of LangGraph\n\n \nLet‚Äôs test your understanding of `LangGraph` with a quick quiz! This will help reinforce the key concepts we‚Äôve covered so far.\n \nThis is an optional quiz and it‚Äôs not graded.\n \n\n### Q1: What is the primary purpose of LangGraph?\n\n \nWhich statement best describes what LangGraph is designed for?\n  A framework to build control flows for applications containing LLMs  A library that provides interfaces to interact with different LLM models  An Agent library for tool calling    \n\n### Q2: In the context of the ‚ÄúControl vs Freedom‚Äù trade-off, where does LangGraph stand?\n\n \nWhich statement best characterizes LangGraph‚Äôs approach to agent design?\n  LangGraph maximizes freedom, allowing LLMs to make all decisions independently  LangGraph provides strong control over execution flow while still leveraging LLM capabilities for decision making    \n\n### Q3: What role does State play in LangGraph?\n\n \nChoose the most accurate description of State in LangGraph.\n  State is the latest generation from the LLM  State is only used to track errors during execution  State represents the information that flows through your agent application  State is only relevant when working with external APIs   \n\n### Q4: What is a Conditional Edge in LangGraph?\n\n \nSelect the most accurate description.\n  An edge that determines which node to execute next based on evaluating a condition  An edge that is only followed when a specific condition occurs  An edge that requires user confirmation before proceeding    \n\n### Q5: How does LangGraph help address the hallucination problem in LLMs?\n\n \nChoose the best answer.\n  LangGraph eliminates hallucinations entirely by limiting LLM responses  LangGraph provides structured workflows that can validate and verify LLM outputs  LangGraph has no effect on hallucinations   \nCongratulations on completing the quiz! üéâ If you missed any questions, consider reviewing the previous sections to strengthen your understanding. Next, we‚Äôll explore more advanced features of LangGraph and see how to build more complex agent workflows.",
    "metadata": {
      "title": "Test Your Understanding of LangGraph",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/quiz1",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/quiz1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/quiz1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What is LangGraph ? `LangGraph` is a framework developed by [LangChain](https://www.langchain.com/) **to manage the control flow of applications that integrate an LLM**. ## Is LangGraph different from LangChain ? LangChain provides a standard interface to interact with models and other components, useful for retrieval, LLM calls and tools calls. The classes from LangChain might be used in LangGraph, but do not HAVE to be used. The packages are different and can be used in isolation, but, in the end, all resources you will find online use both packages hand in hand. ## When should I use LangGraph ? ### Control vs freedom When designing AI applications, you face a fundamental trade-off between **control** and **freedom**: - **Freedom** gives your LLM more room to be creative and tackle unexpected problems. - **Control** allows you to ensure predictable behavior and maintain guardrails. Code Agents, like the ones you can encounter in *smolagents*, are very free. They can call multiple tools in a single action step, create their own tools, etc. However, this behavior can make them less predictable and less controllable than a regular Agent working with JSON! `LangGraph` is on the other end of the spectrum, it shines when you need **‚ÄúControl‚Äù** on the execution of your agent. LangGraph is particularly valuable when you need **Control over your applications**. It gives you the tools to build an application that follows a predictable process while still leveraging the power of LLMs. Put simply, if your application involves a series of steps that need to be orchestrated in a specific way, with decisions being made at each junction point, **LangGraph provides the structure you need**. As an example, let‚Äôs say we want to build an LLM assistant that can answer some questions over some documents. Since LLMs understand text the best, before being able to answer the question, you will need to convert other complex modalities (charts, tables) into text. However, that choice depends on the type of document you have! This is a branching that I chose to represent as follow : ![Control flow](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/flow.png) > üí°Tip:The left part is not an agent, as here no tool call is involved. but the right part will need to write some code to query the xls ( convert to pandas and manipulate it ). While this branching is deterministic, you can also design branching that are conditioned on the output of an LLM making them undeterministic. The key scenarios where LangGraph excels include: - **Multi-step reasoning processes** that need explicit control on the flow - **Applications requiring persistence of state** between steps - **Systems that combine deterministic logic with AI capabilities** - **Workflows that need human-in-the-loop interventions** - **Complex agent architectures** with multiple components working together In essence, whenever possible, **as a human**, design a flow of actions based on the output of each action, and decide what to execute next accordingly. In this case, LangGraph is the correct framework for you! `LangGraph` is, in my opinion, the most production-ready agent framework on the",
    "metadata": {
      "title": "What is LangGraph ?",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/when_to_use_langgraph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/when_to_use_langgraph",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/when_to_use_langgraph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "them undeterministic. The key scenarios where LangGraph excels include: - **Multi-step reasoning processes** that need explicit control on the flow - **Applications requiring persistence of state** between steps - **Systems that combine deterministic logic with AI capabilities** - **Workflows that need human-in-the-loop interventions** - **Complex agent architectures** with multiple components working together In essence, whenever possible, **as a human**, design a flow of actions based on the output of each action, and decide what to execute next accordingly. In this case, LangGraph is the correct framework for you! `LangGraph` is, in my opinion, the most production-ready agent framework on the market. ## How does LangGraph work? At its core, `LangGraph` uses a directed graph structure to define the flow of your application: - **Nodes** represent individual processing steps (like calling an LLM, using a tool, or making a decision). - **Edges** define the possible transitions between steps. - **State** is user defined and maintained and passed between nodes during execution. When deciding which node to target next, this is the current state that we look at. We will explore those fundamental blocks more in the next chapter! ## How is it different from regular python? Why do I need LangGraph? You might wonder: ‚ÄúI could just write regular Python code with if-else statements to handle all these flows, right?‚Äù While technically true, LangGraph offers **some advantages** over vanilla Python for building complex systems. You could build the same application without LangGraph, but it builds easier tools and abstractions for you. It includes states, visualization, logging (traces), built-in human-in-the-loop, and more.",
    "metadata": {
      "title": "What is LangGraph ?",
      "url": "https://huggingface.co/learn/agents-course/unit2/langgraph/when_to_use_langgraph",
      "course": "agents-course",
      "chapter": "Unit 2.3 The LangGraph framework",
      "chapter_id": "unit2/langgraph/when_to_use_langgraph",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/langgraph/when_to_use_langgraph.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Using Agents in LlamaIndex Remember Alfred, our helpful butler agent from earlier? Well, he‚Äôs about to get an upgrade! Now that we understand the tools available in LlamaIndex, we can give Alfred new capabilities to serve us better. But before we continue, let‚Äôs remind ourselves what makes an agent like Alfred tick. Back in Unit 1, we learned that: > An Agent is a system that leverages an AI model to interact with its environment to achieve a user-defined objective. It combines reasoning, planning, and action execution (often via external tools) to fulfil tasks. LlamaIndex supports **three main types of reasoning agents:** ![Agents](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/agents.png) 1. `Function Calling Agents` - These work with AI models that can call specific functions. 2. `ReAct Agents` - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks. 3. `Advanced Custom Agents` - These use more complex methods to deal with more complex tasks and workflows. > Find more information on advanced agents onBaseWorkflowAgent ## Initialising Agents > You can follow the code inthis notebookthat you can run using Google Colab. To create an agent, we start by providing it with a **set of functions/tools that define its capabilities**. Let‚Äôs look at how to create an agent with some basic tools. As of this writing, the agent will automatically use the function calling API (if available), or a standard ReAct agent loop. LLMs that support a tools/functions API are relatively new, but they provide a powerful way to call tools by avoiding specific prompting and allowing the LLM to create tool calls based on provided schemas. ReAct agents are also good at complex reasoning tasks and can work with any LLM that has chat or text completion capabilities. They are more verbose, and show the reasoning behind certain actions that they take. ``` from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI from llama_index.core.agent.workflow import AgentWorkflow from llama_index.core.tools import FunctionTool # define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas! def multiply(a: int, b: int) -> int: \"\"\"Multiplies two integers and returns the resulting integer\"\"\" return a * b # initialize llm llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # initialize agent agent = AgentWorkflow.from_tools_or_functions( [FunctionTool.from_defaults(multiply)], llm=llm ) ``` **Agents are stateless by default**, however, they can remember past interactions using a `Context` object. This might be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time. ``` # stateless response = await agent.run(\"What is 2 times 2?\") # remembering state from llama_index.core.workflow import Context ctx = Context(agent) response = await agent.run(\"My name is Bob.\", ctx=ctx) response = await agent.run(\"What was my name again?\", ctx=ctx) ``` You‚Äôll notice that agents in `LlamaIndex` are async because they use Python‚Äôs `await` operator. If you are new to async code in Python, or need a refresher, they have an [excellent async guide](https://docs.llamaindex.ai/en/stable/getting_started/async_python/). Now we‚Äôve gotten the basics, let‚Äôs",
    "metadata": {
      "title": "Using Agents in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/agents",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/agents",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time. ``` # stateless response = await agent.run(\"What is 2 times 2?\") # remembering state from llama_index.core.workflow import Context ctx = Context(agent) response = await agent.run(\"My name is Bob.\", ctx=ctx) response = await agent.run(\"What was my name again?\", ctx=ctx) ``` You‚Äôll notice that agents in `LlamaIndex` are async because they use Python‚Äôs `await` operator. If you are new to async code in Python, or need a refresher, they have an [excellent async guide](https://docs.llamaindex.ai/en/stable/getting_started/async_python/). Now we‚Äôve gotten the basics, let‚Äôs take a look at how we can use more complex tools in our agents. ## Creating RAG Agents with QueryEngineTools **Agentic RAG is a powerful way to use agents to answer questions about your data.** We can pass various tools to Alfred to help him answer questions. However, instead of answering the question on top of documents automatically, Alfred can decide to use any other tool or flow to answer the question. ![Agentic RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/agentic-rag.png) It is easy to **wrap QueryEngine as a tool** for an agent. When doing so, we need to **define a name and description**. The LLM will use this information to correctly use the tool. Let‚Äôs see how to load in a `QueryEngineTool` using the `QueryEngine` we created in the [component section](components). ``` from llama_index.core.tools import QueryEngineTool query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section query_engine_tool = QueryEngineTool.from_defaults( query_engine=query_engine, name=\"name\", description=\"a specific description\", return_direct=False, ) query_engine_agent = AgentWorkflow.from_tools_or_functions( [query_engine_tool], llm=llm, system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \" ) ``` ## Creating Multi-agent systems The `AgentWorkflow` class also directly supports multi-agent systems. By giving each agent a name and description, the system maintains a single active speaker, with each agent having the ability to hand off to another agent. By narrowing the scope of each agent, we can help increase their general accuracy when responding to user messages. **Agents in LlamaIndex can also directly be used as tools** for other agents, for more complex and custom scenarios. ``` from llama_index.core.agent.workflow import ( AgentWorkflow, FunctionAgent, ReActAgent, ) # Define some tools def add(a: int, b: int) -> int: \"\"\"Add two numbers.\"\"\" return a + b def subtract(a: int, b: int) -> int: \"\"\"Subtract two numbers.\"\"\" return a - b # Create agent configs # NOTE: we can use FunctionAgent or ReActAgent here. # FunctionAgent works for LLMs with a function calling API. # ReActAgent works for any LLM. calculator_agent = ReActAgent( name=\"calculator\", description=\"Performs basic arithmetic operations\", system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\", tools=[add, subtract], llm=llm, ) query_agent = ReActAgent( name=\"info_lookup\", description=\"Looks up information about XYZ\", system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\", tools=[query_engine_tool], llm=llm ) # Create and run the workflow agent = AgentWorkflow( agents=[calculator_agent, query_agent], root_agent=\"calculator\" ) # Run the system response = await agent.run(user_msg=\"Can you add 5 and 3?\") ``` > Haven‚Äôt",
    "metadata": {
      "title": "Using Agents in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/agents",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/agents",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "can use FunctionAgent or ReActAgent here. # FunctionAgent works for LLMs with a function calling API. # ReActAgent works for any LLM. calculator_agent = ReActAgent( name=\"calculator\", description=\"Performs basic arithmetic operations\", system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\", tools=[add, subtract], llm=llm, ) query_agent = ReActAgent( name=\"info_lookup\", description=\"Looks up information about XYZ\", system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\", tools=[query_engine_tool], llm=llm ) # Create and run the workflow agent = AgentWorkflow( agents=[calculator_agent, query_agent], root_agent=\"calculator\" ) # Run the system response = await agent.run(user_msg=\"Can you add 5 and 3?\") ``` > Haven‚Äôt learned enough yet? There is a lot more to discover about agents and tools in LlamaIndex within theAgentWorkflow Basic Introductionor theAgent Learning Guide, where you can read more about streaming, context serialization, and human-in-the-loop! Now that we understand the basics of agents and tools in LlamaIndex, let‚Äôs see how we can use LlamaIndex to **create configurable and manageable workflows!**",
    "metadata": {
      "title": "Using Agents in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/agents",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/agents",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What are components in LlamaIndex? Remember Alfred, our helpful butler agent from Unit 1? To assist us effectively, Alfred needs to understand our requests and **prepare, find and use relevant information to help complete tasks.** This is where LlamaIndex‚Äôs components come in. While LlamaIndex has many components, **we‚Äôll focus specifically on the QueryEngine component.** Why? Because it can be used as a Retrieval-Augmented Generation (RAG) tool for an agent. So, what is RAG? LLMs are trained on enormous bodies of data to learn general knowledge. However, they may not be trained on relevant and up-to-date data. RAG solves this problem by finding and retrieving relevant information from your data and giving that to the LLM. ![RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/rag.png) Now, think about how Alfred works: 1. You ask Alfred to help plan a dinner party 2. Alfred needs to check your calendar, dietary preferences, and past successful menus 3. The `QueryEngine` helps Alfred find this information and use it to plan the dinner party This makes the `QueryEngine` **a key component for building agentic RAG workflows** in LlamaIndex. Just as Alfred needs to search through your household information to be helpful, any agent needs a way to find and understand relevant data. The `QueryEngine` provides exactly this capability. Now, let‚Äôs dive a bit deeper into the components and see how you can **combine components to create a RAG pipeline.** ## Creating a RAG pipeline using components > You can follow the code inthis notebookthat you can run using Google Colab. There are five key stages within RAG, which in turn will be a part of most larger applications you build. These are: 1. **Loading**: this refers to getting your data from where it lives ‚Äî whether it‚Äôs text files, PDFs, another website, a database, or an API ‚Äî into your workflow. LlamaHub provides hundreds of integrations to choose from. 2. **Indexing**: this means creating a data structure that allows for querying the data. For LLMs, this nearly always means creating vector embeddings. Which are numerical representations of the meaning of the data. Indexing can also refer to numerous other metadata strategies to make it easy to accurately find contextually relevant data based on properties. 3. **Storing**: once your data is indexed you will want to store your index, as well as other metadata, to avoid having to re-index it. 4. **Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies. 5. **Evaluation**: a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are. Next, let‚Äôs see how we can reproduce these stages using components. ### Loading and embedding documents As mentioned before, LlamaIndex can work on top of your own data, however, **before accessing data, we need to load it.** There are three main ways to load data into LlamaIndex:",
    "metadata": {
      "title": "What are components in LlamaIndex?",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/components",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/components",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/components.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies. 5. **Evaluation**: a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are. Next, let‚Äôs see how we can reproduce these stages using components. ### Loading and embedding documents As mentioned before, LlamaIndex can work on top of your own data, however, **before accessing data, we need to load it.** There are three main ways to load data into LlamaIndex: 1. `SimpleDirectoryReader`: A built-in loader for various file types from a local directory. 2. `LlamaParse`: LlamaParse, LlamaIndex‚Äôs official tool for PDF parsing, available as a managed API. 3. `LlamaHub`: A registry of hundreds of data-loading libraries to ingest data from any source. > Get familiar withLlamaHubloaders andLlamaParseparser for more complex data sources. **The simplest way to load data is with SimpleDirectoryReader.** This versatile component can load various file types from a folder and convert them into `Document` objects that LlamaIndex can work with. Let‚Äôs see how we can use `SimpleDirectoryReader` to load data from a folder. ``` from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader(input_dir=\"path/to/directory\") documents = reader.load_data() ``` After loading our documents, we need to break them into smaller pieces called `Node` objects. A `Node` is just a chunk of text from the original document that‚Äôs easier for the AI to work with, while it still has references to the original `Document` object. The `IngestionPipeline` helps us create these nodes through two key transformations. 1. `SentenceSplitter` breaks down documents into manageable chunks by splitting them at natural sentence boundaries. 2. `HuggingFaceEmbedding` converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently. This process helps us organise our documents in a way that‚Äôs more useful for searching and analysis. ``` from llama_index.core import Document from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline # create the pipeline with transformations pipeline = IngestionPipeline( transformations=[ SentenceSplitter(chunk_overlap=0), HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), ] ) nodes = await pipeline.arun(documents=[Document.example()]) ``` ### Storing and indexing documents After creating our `Node` objects we need to index them to make them searchable, but before we can do that, we need a place to store our data. Since we are using an ingestion pipeline, we can directly attach a vector store to the pipeline to populate it. In this case, we will use `Chroma` to store our documents. Install ChromaDB As introduced in the [section on the LlamaHub](./llama-hub), we can install the ChromaDB vector store with the following command: ``` pip install llama-index-vector-stores-chroma ``` ``` import chromadb from llama_index.vector_stores.chroma import ChromaVectorStore db = chromadb.PersistentClient(path=\"./alfred_chroma_db\") chroma_collection = db.get_or_create_collection(\"alfred\") vector_store = ChromaVectorStore(chroma_collection=chroma_collection) pipeline = IngestionPipeline( transformations=[ SentenceSplitter(chunk_size=25, chunk_overlap=0), HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), ], vector_store=vector_store, ) ``` > An overview of the different vector stores can be found in theLlamaIndex documentation. This is where vector embeddings come in - by embedding both the query and",
    "metadata": {
      "title": "What are components in LlamaIndex?",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/components",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/components",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/components.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "vector store to the pipeline to populate it. In this case, we will use `Chroma` to store our documents. Install ChromaDB As introduced in the [section on the LlamaHub](./llama-hub), we can install the ChromaDB vector store with the following command: ``` pip install llama-index-vector-stores-chroma ``` ``` import chromadb from llama_index.vector_stores.chroma import ChromaVectorStore db = chromadb.PersistentClient(path=\"./alfred_chroma_db\") chroma_collection = db.get_or_create_collection(\"alfred\") vector_store = ChromaVectorStore(chroma_collection=chroma_collection) pipeline = IngestionPipeline( transformations=[ SentenceSplitter(chunk_size=25, chunk_overlap=0), HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), ], vector_store=vector_store, ) ``` > An overview of the different vector stores can be found in theLlamaIndex documentation. This is where vector embeddings come in - by embedding both the query and nodes in the same vector space, we can find relevant matches. The `VectorStoreIndex` handles this for us, using the same embedding model we used during ingestion to ensure consistency. Let‚Äôs see how to create this index from our vector store and embeddings: ``` from llama_index.core import VectorStoreIndex from llama_index.embeddings.huggingface import HuggingFaceEmbedding embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model) ``` All information is automatically persisted within the `ChromaVectorStore` object and the passed directory path. Great! Now that we can save and load our index easily, let‚Äôs explore how to query it in different ways. ### Querying a VectorStoreIndex with prompts and LLMs Before we can query our index, we need to convert it to a query interface. The most common conversion options are: - `as_retriever`: For basic document retrieval, returning a list of `NodeWithScore` objects with similarity scores - `as_query_engine`: For single question-answer interactions, returning a written response - `as_chat_engine`: For conversational interactions that maintain memory across multiple messages, returning a written response using chat history and indexed context We‚Äôll focus on the query engine since it is more common for agent-like interactions. We also pass in an LLM to the query engine to use for the response. ``` from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") query_engine = index.as_query_engine( llm=llm, response_mode=\"tree_summarize\", ) query_engine.query(\"What is the meaning of life?\") # The meaning of life is 42 ``` ### Response Processing Under the hood, the query engine doesn‚Äôt only use the LLM to answer the question but also uses a `ResponseSynthesizer` as a strategy to process the response. Once again, this is fully customisable but there are three main strategies that work well out of the box: - `refine`: create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk. - `compact` (default): similar to refining but concatenating the chunks beforehand, resulting in fewer LLM calls. - `tree_summarize`: create a detailed answer by going through each retrieved text chunk and creating a tree structure of the answer. > Take fine-grained control of your query workflows with thelow-level composition API. This API lets you customize and fine-tune every step of the query process to match your exact needs, which also pairs great withWorkflows The language model won‚Äôt always perform in predictable ways, so we can‚Äôt be sure that the answer we get is always correct. We can deal with this by **evaluating the",
    "metadata": {
      "title": "What are components in LlamaIndex?",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/components",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/components",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/components.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(default): similar to refining but concatenating the chunks beforehand, resulting in fewer LLM calls. - `tree_summarize`: create a detailed answer by going through each retrieved text chunk and creating a tree structure of the answer. > Take fine-grained control of your query workflows with thelow-level composition API. This API lets you customize and fine-tune every step of the query process to match your exact needs, which also pairs great withWorkflows The language model won‚Äôt always perform in predictable ways, so we can‚Äôt be sure that the answer we get is always correct. We can deal with this by **evaluating the quality of the answer**. ### Evaluation and observability LlamaIndex provides **built-in evaluation tools to assess response quality.** These evaluators leverage LLMs to analyze responses across different dimensions. Let‚Äôs look at the three main evaluators available: - `FaithfulnessEvaluator`: Evaluates the faithfulness of the answer by checking if the answer is supported by the context. - `AnswerRelevancyEvaluator`: Evaluate the relevance of the answer by checking if the answer is relevant to the question. - `CorrectnessEvaluator`: Evaluate the correctness of the answer by checking if the answer is correct. > Want to learn more about agent observability and evaluation? Continue your journey with theBonus Unit 2. ``` from llama_index.core.evaluation import FaithfulnessEvaluator query_engine = # from the previous section llm = # from the previous section # query index evaluator = FaithfulnessEvaluator(llm=llm) response = query_engine.query( \"What battles took place in New York City in the American Revolution?\" ) eval_result = evaluator.evaluate_response(response=response) eval_result.passing ``` Even without direct evaluation, we can **gain insights into how our system is performing through observability.** This is especially useful when we are building more complex workflows and want to understand how each component is performing. Install LlamaTrace As introduced in the [section on the LlamaHub](./llama-hub), we can install the LlamaTrace callback from Arize Phoenix with the following command: ``` pip install -U llama-index-callbacks-arize-phoenix ``` Additionally, we need to set the `PHOENIX_API_KEY` environment variable to our LlamaTrace API key. We can get this by: - Creating an account at [LlamaTrace](https://llamatrace.com/login) - Generating an API key in your account settings - Using the API key in the code below to enable tracing ``` import llama_index import os PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\" os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\" llama_index.core.set_global_handler( \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\" ) ``` > Want to learn more about components and how to use them? Continue your journey with theComponents Guidesor theGuide on RAG. We have seen how to use components to create a `QueryEngine`. Now, let‚Äôs see how we can **use the QueryEngine as a tool for an agent!**",
    "metadata": {
      "title": "What are components in LlamaIndex?",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/components",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/components",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/components.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nCongratulations on finishing the `llama-index` module of this second Unit ü•≥\n \nYou‚Äôve just mastered the fundamentals of `llama-index` and you‚Äôve seen how to build your own agentic workflows!\nNow that you have skills in `llama-index`, you can start to create search engines that will solve tasks you‚Äôre interested in.\n \nIn the next module of the unit, you‚Äôre going to learn **how to build Agents with LangGraph**.\n \nFinally, we would love **to hear what you think of the course and how we can improve it**.\nIf you have some feedback then, please üëâ [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \n\n### Keep Learning, and stay awesome ü§ó",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/conclusion",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to LlamaIndex\n\n \nWelcome to this module, where you‚Äôll learn how to build LLM-powered agents using the [LlamaIndex](https://www.llamaindex.ai/) toolkit.\n \nLlamaIndex is **a complete toolkit for creating LLM-powered agents over your data using indexes and workflows**. For this course we‚Äôll focus on three main parts that help build agents in LlamaIndex: **Components**, **Agents and Tools** and **Workflows**.\n \n![LlamaIndex](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/thumbnail.png)\n \nLet‚Äôs look at these key parts of LlamaIndex and how they help with agents:\n \n- **Components**: Are the basic building blocks you use in LlamaIndex. These include things like prompts, models, and databases. Components often help connect LlamaIndex with other tools and libraries.\n- **Tools**: Tools are components that provide specific capabilities like searching, calculating, or accessing external services. They are the building blocks that enable agents to perform tasks.\n- **Agents**: Agents are autonomous components that can use tools and make decisions. They coordinate tool usage to accomplish complex goals.\n- **Workflows**: Are step-by-step processes that process logic together. Workflows or agentic workflows are a way to structure agentic behaviour without the explicit use of agents.\n \n\n## What Makes LlamaIndex Special?\n\n \nWhile LlamaIndex does some things similar to other frameworks like smolagents, it has some key benefits:\n \n- **Clear Workflow System**: Workflows help break down how agents should make decisions step by step using an event-driven and async-first syntax. This helps you clearly compose and organize your logic.\n- **Advanced Document Parsing with LlamaParse**: LlamaParse was made specifically for LlamaIndex, so the integration is seamless, although it is a paid feature.\n- **Many Ready-to-Use Components**: LlamaIndex has been around for a while, so it works with lots of other frameworks. This means it has many tested and reliable components, like LLMs, retrievers, indexes, and more.\n- **LlamaHub**: is a registry of hundreds of these components, agents, and tools that you can use within LlamaIndex.\n \nAll of these concepts are required in different scenarios to create useful agents.\nIn the following sections, we will go over each of these concepts in detail.\nAfter mastering the concepts, we will use our learnings to **create applied use cases with Alfred the agent**!\n \nGetting our hands on LlamaIndex is exciting, right? So, what are we waiting for? Let‚Äôs get started with **finding and installing the integrations we need using LlamaHub! üöÄ**",
    "metadata": {
      "title": "Introduction to LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/introduction",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to the LlamaHub\n\n \n**LlamaHub is a registry of hundreds of integrations, agents and tools that you can use within LlamaIndex.**\n \n![LlamaHub](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/llama-hub.png)\n \nWe will be using various integrations in this course, so let‚Äôs first look at the LlamaHub and how it can help us.\n \nLet‚Äôs see how to find and install the dependencies for the components we need.\n \n\n## Installation\n\n \nLlamaIndex installation instructions are available as a well-structured **overview on LlamaHub**.\nThis might be a bit overwhelming at first, but most of the **installation commands generally follow an easy-to-remember format**:\n  \n```\npip install llama-index-{component-type}-{framework-name}\n```\n \nLet‚Äôs try to install the dependencies for an LLM and embedding component using the [Hugging Face inference API integration](https://llamahub.ai/l/llms/llama-index-llms-huggingface-api?from=llms).\n  \n```\npip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface\n```\n \n\n## Usage\n\n \nOnce installed, we can see the usage patterns. You‚Äôll notice that the import paths follow the install command!\nUnderneath, we can see an example of the usage of **the Hugging Face inference API for an LLM component**.\n  \n```\nfrom llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\nimport os\nfrom dotenv import load_dotenv\n\n# Load the .env file\nload_dotenv()\n\n# Retrieve HF_TOKEN from the environment variables\nhf_token = os.getenv(\"HF_TOKEN\")\n\nllm = HuggingFaceInferenceAPI(\n    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n    temperature=0.7,\n    max_tokens=100,\n    token=hf_token,\n    provider=\"auto\"\n)\n\nresponse = llm.complete(\"Hello, how are you?\")\nprint(response)\n# I am good, how can I help you today?\n```\n \nWonderful, we now know how to find, install and use the integrations for the components we need.\n**Let‚Äôs dive deeper into the components** and see how we can use them to build our own agents.",
    "metadata": {
      "title": "Introduction to the LlamaHub",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/llama-hub",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/llama-hub",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/llama-hub.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Small Quiz (ungraded)\n\n \nSo far we‚Äôve discussed the key components and tools used in LlamaIndex.\nIt‚Äôs time to make a short quiz, since **testing yourself** is the best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf).\nThis will help you find **where you need to reinforce your knowledge**.\n \nThis is an optional quiz and it‚Äôs not graded.\n \n\n### Q1: What is a QueryEngine?\n\n \nWhich of the following best describes a QueryEngine component?\n  A system that only processes static text without any retrieval capabilities.  A component that finds and retrieves relevant information as part of the RAG process.  A tool that only stores vector embeddings without search functionality.  A component that only evaluates response quality.    \n\n### Q2: What is the Purpose of FunctionTools?\n\n \nWhy are FunctionTools important for an Agent?\n  To handle large amounts of data storage.  To convert Python functions into tools that an agent can use.  To allow agents to create random functions definitions.  To only process text data.    \n\n### Q3: What are Toolspecs in LlamaIndex?\n\n \nWhat is the main purpose of Toolspecs?\n  They are redundant components that don't add functionality.  They are sets of community-created tools that extend agent capabilities.  They are used solely for memory management.  They only work with text processing.    \n\n### Q4: What is Required to create a tool?\n\n \nWhat information must be included when creating a tool?\n  A function, a name, and description must be defined.  Only the name is required.  Only the description is required.  Only the function is required.    \nCongrats on finishing this Quiz ü•≥, if you missed some elements, take time to read again the chapter to reinforce your knowledge. If you pass it, you‚Äôre ready to dive deeper into building with these components!",
    "metadata": {
      "title": "Small Quiz (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/quiz1",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/quiz1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/quiz1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Quick Self-Check (ungraded)\n\n \nWhat?! Another Quiz? We know, we know, ‚Ä¶ üòÖ But this short, ungraded quiz is here to **help you reinforce key concepts you‚Äôve just learned**.\n \nThis quiz covers agent workflows and interactions - essential components for building effective AI agents.\n \n\n### Q1: What is the purpose of AgentWorkflow in LlamaIndex?\n\n  To run one or more agents with tools  To create a single agent that can query your data without memory  To automatically build tools for agents  To manage agent memory and state    \n\n### Q2: What object is used for keeping track of the state of the workflow?\n\n  State  Context  WorkflowState  Management    \n\n### Q3: Which method should be used if you want an agent to remember previous interactions?\n\n  run(query_str)  chat(query_str, ctx=ctx)  interact(query_str)  run(query_str, ctx=ctx)    \n\n### Q4: What is a key feature of Agentic RAG?\n\n  It can only use document-based tools, to answer questions in a RAG workflow  It automatically answers questions without tools, like a chatbot  It can decide to use any tool to answer questions, including RAG tools  It only works with Function Calling Agents    \nGot it? Great! Now let‚Äôs **do a brief recap of the unit!**",
    "metadata": {
      "title": "Quick Self-Check (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/quiz2",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/quiz2",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/quiz2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Using Tools in LlamaIndex **Defining a clear set of Tools is crucial to performance.** As we discussed in [unit 1](../../unit1/tools), clear tool interfaces are easier for LLMs to use. Much like a software API interface for human engineers, they can get more out of the tool if it‚Äôs easy to understand how it works. There are **four main types of tools in LlamaIndex**: ![Tools](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/tools.png) 1. `FunctionTool`: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works. 2. `QueryEngineTool`: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools. 3. `Toolspecs`: Sets of tools created by the community, which often include tools for specific services like Gmail. 4. `Utility Tools`: Special tools that help handle large amounts of data from other tools. We will go over each of them in more detail below. ## Creating a FunctionTool > You can follow the code inthis notebookthat you can run using Google Colab. A FunctionTool provides a simple way to wrap any Python function and make it available to an agent. You can pass either a synchronous or asynchronous function to the tool, along with optional `name` and `description` parameters. The name and description are particularly important as they help the agent understand when and how to use the tool effectively. Let‚Äôs look at how to create a FunctionTool below and then call it. ``` from llama_index.core.tools import FunctionTool def get_weather(location: str) -> str: \"\"\"Useful for getting the weather for a given location.\"\"\" print(f\"Getting weather for {location}\") return f\"The weather in {location} is sunny\" tool = FunctionTool.from_defaults( get_weather, name=\"my_weather_tool\", description=\"Useful for getting the weather for a given location.\", ) tool.call(\"New York\") ``` > When using an agent or LLM with function calling, the tool selected (and the arguments written for that tool) rely strongly on the tool name and description of the purpose and arguments of the tool. Learn more about function calling in theFunction Calling Guide. ## Creating a QueryEngineTool The `QueryEngine` we defined in the previous unit can be easily transformed into a tool using the `QueryEngineTool` class. Let‚Äôs see how to create a `QueryEngineTool` from a `QueryEngine` in the example below. ``` from llama_index.core import VectorStoreIndex from llama_index.core.tools import QueryEngineTool from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.vector_stores.chroma import ChromaVectorStore embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\") db = chromadb.PersistentClient(path=\"./alfred_chroma_db\") chroma_collection = db.get_or_create_collection(\"alfred\") vector_store = ChromaVectorStore(chroma_collection=chroma_collection) index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model) llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") query_engine = index.as_query_engine(llm=llm) tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\") ``` ## Creating Toolspecs Think of `ToolSpecs` as collections of tools that work together harmoniously - like a well-organized professional toolkit. Just as a mechanic‚Äôs toolkit contains complementary tools that work together for vehicle repairs, a `ToolSpec` combines related tools for specific purposes. For example, an accounting agent‚Äôs `ToolSpec` might elegantly integrate spreadsheet capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency. Install the Google Toolspec As introduced",
    "metadata": {
      "title": "Using Tools in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/tools",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/tools",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "= chromadb.PersistentClient(path=\"./alfred_chroma_db\") chroma_collection = db.get_or_create_collection(\"alfred\") vector_store = ChromaVectorStore(chroma_collection=chroma_collection) index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model) llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") query_engine = index.as_query_engine(llm=llm) tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\") ``` ## Creating Toolspecs Think of `ToolSpecs` as collections of tools that work together harmoniously - like a well-organized professional toolkit. Just as a mechanic‚Äôs toolkit contains complementary tools that work together for vehicle repairs, a `ToolSpec` combines related tools for specific purposes. For example, an accounting agent‚Äôs `ToolSpec` might elegantly integrate spreadsheet capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency. Install the Google Toolspec As introduced in the [section on the LlamaHub](./llama-hub), we can install the Google toolspec with the following command: ``` pip install llama-index-tools-google ``` And now we can load the toolspec and convert it to a list of tools. ``` from llama_index.tools.google import GmailToolSpec tool_spec = GmailToolSpec() tool_spec_list = tool_spec.to_tool_list() ``` To get a more detailed view of the tools, we can take a look at the `metadata` of each tool. ``` [(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list] ``` ### Model Context Protocol (MCP) in LlamaIndex LlamaIndex also allows using MCP tools through a [ToolSpec on the LlamaHub](https://llamahub.ai/l/tools/llama-index-tools-mcp?from=). You can simply run an MCP server and start using it through the following implementation. If you want to dive deeper about MCP, you can check our [free MCP Course](https://huggingface.co/learn/mcp-course/). Install the MCP Toolspec As introduced in the [section on the LlamaHub](./llama-hub), we can install the MCP toolspec with the following command: ``` pip install llama-index-tools-mcp ``` ``` from llama_index.tools.mcp import BasicMCPClient, McpToolSpec # We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server. mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\") mcp_tool = McpToolSpec(client=mcp_client) # get the agent agent = await get_agent(mcp_tool) # create the agent context agent_context = Context(agent) ``` ## Utility Tools Oftentimes, directly querying an API **can return an excessive amount of data**, some of which may be irrelevant, overflow the context window of the LLM, or unnecessarily increase the number of tokens that you are using. Let‚Äôs walk through our two main utility tools below. 1. `OnDemandToolLoader`: This tool turns any existing LlamaIndex data loader (BaseReader class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger `load_data` from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it ‚Äòon-demand‚Äô. All three of these steps happen in a single tool call. 2. `LoadAndSearchToolSpec`: The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list`, and when that function is called, two tools are returned: a loading tool and then a search tool. The load Tool execution would call the underlying Tool, and then index the output (by default with a vector index). The search Tool execution would take in a query string as input",
    "metadata": {
      "title": "Using Tools in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/tools",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/tools",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "we first load data from the data loader, index it (for instance with a vector store), and then query it ‚Äòon-demand‚Äô. All three of these steps happen in a single tool call. 2. `LoadAndSearchToolSpec`: The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements `to_tool_list`, and when that function is called, two tools are returned: a loading tool and then a search tool. The load Tool execution would call the underlying Tool, and then index the output (by default with a vector index). The search Tool execution would take in a query string as input and call the underlying index. > You can find toolspecs and utility tools on theLlamaHub Now that we understand the basics of agents and tools in LlamaIndex, let‚Äôs see how we can **use LlamaIndex to create configurable and manageable workflows!**",
    "metadata": {
      "title": "Using Tools in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/tools",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/tools",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Creating agentic workflows in LlamaIndex A workflow in LlamaIndex provides a structured way to organize your code into sequential and manageable steps. Such a workflow is created by defining `Steps` which are triggered by `Events`, and themselves emit `Events` to trigger further steps. Let‚Äôs take a look at Alfred showing a LlamaIndex workflow for a RAG task. ![Workflow Schematic](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflows.png) **Workflows offer several key benefits:** - Clear organization of code into discrete steps - Event-driven architecture for flexible control flow - Type-safe communication between steps - Built-in state management - Support for both simple and complex agent interactions As you might have guessed, **workflows strike a great balance between the autonomy of agents while maintaining control over the overall workflow.** So, let‚Äôs learn how to create a workflow ourselves! ## Creating Workflows > You can follow the code inthis notebookthat you can run using Google Colab. ### Basic Workflow Creation Install the Workflow package As introduced in the [section on the LlamaHub](./llama-hub), we can install the Workflow package with the following command: ``` pip install llama-index-utils-workflow ``` We can create a single-step workflow by defining a class that inherits from `Workflow` and decorating your functions with `@step`. We will also need to add `StartEvent` and `StopEvent`, which are special events that are used to indicate the start and end of the workflow. ``` from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step class MyWorkflow(Workflow): @step async def my_step(self, ev: StartEvent) -> StopEvent: # do something here return StopEvent(result=\"Hello, world!\") w = MyWorkflow(timeout=10, verbose=False) result = await w.run() ``` As you can see, we can now run the workflow by calling `w.run()`. ### Connecting Multiple Steps To connect multiple steps, we **create custom events that carry data between steps.** To do so, we need to add an `Event` that is passed between the steps and transfers the output of the first step to the second step. ``` from llama_index.core.workflow import Event class ProcessingEvent(Event): intermediate_result: str class MultiStepWorkflow(Workflow): @step async def step_one(self, ev: StartEvent) -> ProcessingEvent: # Process initial data return ProcessingEvent(intermediate_result=\"Step 1 complete\") @step async def step_two(self, ev: ProcessingEvent) -> StopEvent: # Use the intermediate result final_result = f\"Finished processing: {ev.intermediate_result}\" return StopEvent(result=final_result) w = MultiStepWorkflow(timeout=10, verbose=False) result = await w.run() result ``` The type hinting is important here, as it ensures that the workflow is executed correctly. Let‚Äôs complicate things a bit more! ### Loops and Branches The type hinting is the most powerful part of workflows because it allows us to create branches, loops, and joins to facilitate more complex workflows. Let‚Äôs show an example of **creating a loop** by using the union operator `|`. In the example below, we see that the `LoopEvent` is taken as input for the step and can also be returned as output. ``` from llama_index.core.workflow import Event import random class ProcessingEvent(Event): intermediate_result: str class LoopEvent(Event): loop_output: str class MultiStepWorkflow(Workflow): @step async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent: if random.randint(0, 1) == 0: print(\"Bad thing happened\") return LoopEvent(loop_output=\"Back to step one.\") else:",
    "metadata": {
      "title": "Creating agentic workflows in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/workflows",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/workflows",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/workflows.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "most powerful part of workflows because it allows us to create branches, loops, and joins to facilitate more complex workflows. Let‚Äôs show an example of **creating a loop** by using the union operator `|`. In the example below, we see that the `LoopEvent` is taken as input for the step and can also be returned as output. ``` from llama_index.core.workflow import Event import random class ProcessingEvent(Event): intermediate_result: str class LoopEvent(Event): loop_output: str class MultiStepWorkflow(Workflow): @step async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent: if random.randint(0, 1) == 0: print(\"Bad thing happened\") return LoopEvent(loop_output=\"Back to step one.\") else: print(\"Good thing happened\") return ProcessingEvent(intermediate_result=\"First step complete.\") @step async def step_two(self, ev: ProcessingEvent) -> StopEvent: # Use the intermediate result final_result = f\"Finished processing: {ev.intermediate_result}\" return StopEvent(result=final_result) w = MultiStepWorkflow(verbose=False) result = await w.run() result ``` ### Drawing Workflows We can also draw workflows. Let‚Äôs use the `draw_all_possible_flows` function to draw the workflow. This stores the workflow in an HTML file. ``` from llama_index.utils.workflow import draw_all_possible_flows w = ... # as defined in the previous section draw_all_possible_flows(w, \"flow.html\") ``` ![workflow drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png) There is one last cool trick that we will cover in the course, which is the ability to add state to the workflow. ### State Management State management is useful when you want to keep track of the state of the workflow, so that every step has access to the same state. We can do this by using the `Context` type hint on top of a parameter in the step function. ``` from llama_index.core.workflow import Context, StartEvent, StopEvent @step async def query(self, ctx: Context, ev: StartEvent) -> StopEvent: # store query in the context await ctx.store.set(\"query\", \"What is the capital of France?\") # do something with context and event val = ... # retrieve query from the context query = await ctx.store.get(\"query\") return StopEvent(result=val) ``` Great! Now you know how to create basic workflows in LlamaIndex! > There are some more complex nuances to workflows, which you can learn about inthe LlamaIndex documentation. However, there is another way to create workflows, which relies on the `AgentWorkflow` class. Let‚Äôs take a look at how we can use this to create a multi-agent workflow. ## Automating workflows with Multi-Agent Workflows Instead of manual workflow creation, we can use the **AgentWorkflow class to create a multi-agent workflow**. The `AgentWorkflow` uses Workflow Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle different aspects of a task. Instead of importing classes from `llama_index.core.agent`, we will import the agent classes from `llama_index.core.agent.workflow`. One agent must be designated as the root agent in the `AgentWorkflow` constructor. When a user message comes in, it is first routed to the root agent. Each agent can then: - Handle the request directly using their tools - Handoff to another agent better suited for the task - Return a response to",
    "metadata": {
      "title": "Creating agentic workflows in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/workflows",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/workflows",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/workflows.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle different aspects of a task. Instead of importing classes from `llama_index.core.agent`, we will import the agent classes from `llama_index.core.agent.workflow`. One agent must be designated as the root agent in the `AgentWorkflow` constructor. When a user message comes in, it is first routed to the root agent. Each agent can then: - Handle the request directly using their tools - Handoff to another agent better suited for the task - Return a response to the user Let‚Äôs see how to create a multi-agent workflow. ``` from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI # Define some tools def add(a: int, b: int) -> int: \"\"\"Add two numbers.\"\"\" return a + b def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description multiply_agent = ReActAgent( name=\"multiply_agent\", description=\"Is able to multiply two integers\", system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\", tools=[multiply], llm=llm, ) addition_agent = ReActAgent( name=\"add_agent\", description=\"Is able to add two integers\", system_prompt=\"A helpful assistant that can use a tool to add numbers.\", tools=[add], llm=llm, ) # Create the workflow workflow = AgentWorkflow( agents=[multiply_agent, addition_agent], root_agent=\"multiply_agent\", ) # Run the system response = await workflow.run(user_msg=\"Can you add 5 and 3?\") ``` Agent tools can also modify the workflow state we mentioned earlier. Before starting the workflow, we can provide an initial state dict that will be available to all agents. The state is stored in the state key of the workflow context. It will be injected into the state_prompt which augments each new user message. Let‚Äôs inject a counter to count function calls by modifying the previous example: ``` from llama_index.core.workflow import Context # Define some tools async def add(ctx: Context, a: int, b: int) -> int: \"\"\"Add two numbers.\"\"\" # update our count cur_state = await ctx.store.get(\"state\") cur_state[\"num_fn_calls\"] += 1 await ctx.store.set(\"state\", cur_state) return a + b async def multiply(ctx: Context, a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" # update our count cur_state = await ctx.store.get(\"state\") cur_state[\"num_fn_calls\"] += 1 await ctx.store.set(\"state\", cur_state) return a * b ... workflow = AgentWorkflow( agents=[multiply_agent, addition_agent], root_agent=\"multiply_agent\", initial_state={\"num_fn_calls\": 0}, state_prompt=\"Current state: {state}. User message: {msg}\", ) # run the workflow with context ctx = Context(workflow) response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx) # pull out and inspect the state state = await ctx.store.get(\"state\") print(state[\"num_fn_calls\"]) ``` Congratulations! You have now mastered the basics of Agents in LlamaIndex! üéâ Let‚Äôs continue with one final quiz to solidify your knowledge! üöÄ",
    "metadata": {
      "title": "Creating agentic workflows in LlamaIndex",
      "url": "https://huggingface.co/learn/agents-course/unit2/llama-index/workflows",
      "course": "agents-course",
      "chapter": "Unit 2.2 The LlamaIndex framework",
      "chapter_id": "unit2/llama-index/workflows",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/llama-index/workflows.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building Agents That Use Code Code agents are the default agent type in `smolagents`. They generate Python tool calls to perform actions, achieving action representations that are efficient, expressive, and accurate. Their streamlined approach reduces the number of required actions, simplifies complex operations, and enables reuse of existing code functions. `smolagents` provides a lightweight framework for building code agents, implemented in approximately 1,000 lines of code. ![Code vs JSON Actions](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png) Graphic from the paper [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030) > If you want to learn more about why code agents are effective, check outthis guidefrom the smolagents documentation. ## Why Code Agents? In a multi-step agent process, the LLM writes and executes actions, typically involving external tool calls. Traditional approaches use a JSON format to specify tool names and arguments as strings, **which the system must parse to determine which tool to execute**. However, research shows that **tool-calling LLMs work more effectively with code directly**. This is a core principle of `smolagents`, as shown in the diagram above from [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030). Writing actions in code rather than JSON offers several key advantages: - **Composability**: Easily combine and reuse actions - **Object Management**: Work directly with complex structures like images - **Generality**: Express any computationally possible task - **Natural for LLMs**: High-quality code is already present in LLM training data ## How Does a Code Agent Work? ![From https://huggingface.co/docs/smolagents/conceptual_guides/react](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/codeagent_docs.png) The diagram above illustrates how `CodeAgent.run()` operates, following the ReAct framework we mentioned in Unit 1. The main abstraction for agents in `smolagents` is a `MultiStepAgent`, which serves as the core building block. `CodeAgent` is a special kind of `MultiStepAgent`, as we will see in an example below. A `CodeAgent` performs actions through a cycle of steps, with existing variables and knowledge being incorporated into the agent‚Äôs context, which is kept in an execution log: 1. The system prompt is stored in a `SystemPromptStep`, and the user query is logged in a `TaskStep`. 2. Then, the following while loop is executed: 2.1 Method `agent.write_memory_to_messages()` writes the agent‚Äôs logs into a list of LLM-readable [chat messages](https://huggingface.co/docs/transformers/main/en/chat_templating). 2.2 These messages are sent to a `Model`, which generates a completion. 2.3 The completion is parsed to extract the action, which, in our case, should be a code snippet since we‚Äôre working with a `CodeAgent`. 2.4 The action is executed. 2.5 The results are logged into memory in an `ActionStep`. At the end of each step, if the agent includes any function calls (in `agent.step_callback`), they are executed. ## Let‚Äôs See Some Examples > You can follow the code inthis notebookthat you can run using Google Colab. Alfred is planning a party at the Wayne family mansion and needs your help to ensure everything goes smoothly. To assist him, we‚Äôll apply what we‚Äôve learned about how a multi-step `CodeAgent` operates. ![Alfred Party](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/alfred-party.jpg) If you haven‚Äôt installed `smolagents` yet, you can do so by running the following command: ``` pip install smolagents -U ``` Let‚Äôs also login to the",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "an `ActionStep`. At the end of each step, if the agent includes any function calls (in `agent.step_callback`), they are executed. ## Let‚Äôs See Some Examples > You can follow the code inthis notebookthat you can run using Google Colab. Alfred is planning a party at the Wayne family mansion and needs your help to ensure everything goes smoothly. To assist him, we‚Äôll apply what we‚Äôve learned about how a multi-step `CodeAgent` operates. ![Alfred Party](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/alfred-party.jpg) If you haven‚Äôt installed `smolagents` yet, you can do so by running the following command: ``` pip install smolagents -U ``` Let‚Äôs also login to the Hugging Face Hub to have access to the Serverless Inference API. ``` from huggingface_hub import login login() ``` ### Selecting a Playlist for the Party Using smolagents Music is an essential part of a successful party! Alfred needs some help selecting the playlist. Luckily, `smolagents` has got us covered! We can build an agent capable of searching the web using DuckDuckGo. To give the agent access to this tool, we include it in the tool list when creating the agent. ![Alfred Playlist](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/alfred-playlist.jpg) For the model, we‚Äôll rely on `InferenceClientModel`, which provides access to Hugging Face‚Äôs [Serverless Inference API](https://huggingface.co/docs/api-inference/index). The default model is `\"Qwen/Qwen2.5-Coder-32B-Instruct\"`, which is performant and available for fast inference, but you can select any compatible model from the Hub. Running an agent is quite straightforward: ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=InferenceClientModel()) agent.run(\"Search for the best music recommendations for a party at the Wayne's mansion.\") ``` When you run this example, the output will **display a trace of the workflow steps being executed**. It will also print the corresponding Python code with the message: ``` ‚îÄ Executing parsed code: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ results = web_search(query=\"best music for a Batman party\") print(results) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ``` After a few steps, you‚Äôll see the generated playlist that Alfred can use for the party! üéµ ### Using a Custom Tool to Prepare the Menu ![Alfred Menu](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/alfred-menu.jpg) Now that we have selected a playlist, we need to organize the menu for the guests. Again, Alfred can take advantage of `smolagents` to do so. Here, we use the `@tool` decorator to define a custom function that acts as a tool. We‚Äôll cover tool creation in more detail later, so for now, we can simply run the code. As you can see in the example below, we will create a tool using the `@tool` decorator and include it in the `tools` list. ``` from smolagents import CodeAgent, tool, InferenceClientModel # Tool to suggest a menu based on the occasion @tool def suggest_menu(occasion: str) -> str: \"\"\" Suggests a menu based on the occasion. Args: occasion (str): The type of occasion for the party. Allowed values are: - \"casual\": Menu for casual party. - \"formal\": Menu for formal party. - \"superhero\": Menu for superhero party. - \"custom\": Custom menu. \"\"\" if occasion == \"casual\": return \"Pizza, snacks, and drinks.\" elif occasion == \"formal\": return \"3-course dinner with wine and dessert.\" elif occasion == \"superhero\": return",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "decorator and include it in the `tools` list. ``` from smolagents import CodeAgent, tool, InferenceClientModel # Tool to suggest a menu based on the occasion @tool def suggest_menu(occasion: str) -> str: \"\"\" Suggests a menu based on the occasion. Args: occasion (str): The type of occasion for the party. Allowed values are: - \"casual\": Menu for casual party. - \"formal\": Menu for formal party. - \"superhero\": Menu for superhero party. - \"custom\": Custom menu. \"\"\" if occasion == \"casual\": return \"Pizza, snacks, and drinks.\" elif occasion == \"formal\": return \"3-course dinner with wine and dessert.\" elif occasion == \"superhero\": return \"Buffet with high-energy and healthy food.\" else: return \"Custom menu for the butler.\" # Alfred, the butler, preparing the menu for the party agent = CodeAgent(tools=[suggest_menu], model=InferenceClientModel()) # Preparing the menu for the party agent.run(\"Prepare a formal menu for the party.\") ``` The agent will run for a few steps until finding the answer. Precising allowed values in the docstring helps direct agent to `occasion` argument values which exist and limit hallucinations. The menu is ready! ü•ó ### Using Python Imports Inside the Agent We have the playlist and menu ready, but we need to check one more crucial detail: preparation time! Alfred needs to calculate when everything would be ready if he started preparing now, in case they need assistance from other superheroes. `smolagents` specializes in agents that write and execute Python code snippets, offering sandboxed execution for security. **Code execution has strict security measures** - imports outside a predefined safe list are blocked by default. However, you can authorize additional imports by passing them as strings in `additional_authorized_imports`. For more details on secure code execution, see the official [guide](https://huggingface.co/docs/smolagents/tutorials/secure_code_execution). When creating the agent, we‚Äôll use `additional_authorized_imports` to allow for importing the `datetime` module. ``` from smolagents import CodeAgent, InferenceClientModel import numpy as np import time import datetime agent = CodeAgent(tools=[], model=InferenceClientModel(), additional_authorized_imports=['datetime']) agent.run( \"\"\" Alfred needs to prepare for the party. Here are the tasks: 1. Prepare the drinks - 30 minutes 2. Decorate the mansion - 60 minutes 3. Set up the menu - 45 minutes 4. Prepare the music and playlist - 45 minutes If we start right now, at what time will the party be ready? \"\"\" ) ``` These examples are just the beginning of what you can do with code agents, and we‚Äôre already starting to see their utility for preparing the party. You can learn more about how to build code agents in the [smolagents documentation](https://huggingface.co/docs/smolagents). In summary, `smolagents` specializes in agents that write and execute Python code snippets, offering sandboxed execution for security. It supports both local and API-based language models, making it adaptable to various development environments. ### Sharing Our Custom Party Preparator Agent to the Hub Wouldn‚Äôt it be **amazing to share our very own Alfred agent with the community**? By doing so, anyone can easily download and use the agent directly from the Hub, bringing the ultimate party planner of Gotham to their fingertips! Let‚Äôs make it happen!",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "You can learn more about how to build code agents in the [smolagents documentation](https://huggingface.co/docs/smolagents). In summary, `smolagents` specializes in agents that write and execute Python code snippets, offering sandboxed execution for security. It supports both local and API-based language models, making it adaptable to various development environments. ### Sharing Our Custom Party Preparator Agent to the Hub Wouldn‚Äôt it be **amazing to share our very own Alfred agent with the community**? By doing so, anyone can easily download and use the agent directly from the Hub, bringing the ultimate party planner of Gotham to their fingertips! Let‚Äôs make it happen! üéâ The `smolagents` library makes this possible by allowing you to share a complete agent with the community and download others for immediate use. It‚Äôs as simple as the following: ``` # Change to your username and repo name agent.push_to_hub('sergiopaniego/AlfredAgent') ``` To download the agent again, use the code below: ``` # Change to your username and repo name alfred_agent = agent.from_hub('sergiopaniego/AlfredAgent', trust_remote_code=True) alfred_agent.run(\"Give me the best playlist for a party at Wayne's mansion. The party idea is a 'villain masquerade' theme\") ``` What‚Äôs also exciting is that shared agents are directly available as Hugging Face Spaces, allowing you to interact with them in real-time. You can explore other agents [here](https://huggingface.co/spaces/davidberenstein1957/smolagents-and-tools). For example, the *AlfredAgent* is available [here](https://huggingface.co/spaces/sergiopaniego/AlfredAgent). You can try it out directly below: You may be wondering‚Äîhow did Alfred build such an agent using `smolagents`? By integrating several tools, he can generate an agent as follows. Don‚Äôt worry about the tools for now, as we‚Äôll have a dedicated section later in this unit to explore that in detail: ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, FinalAnswerTool, InferenceClientModel, Tool, tool, VisitWebpageTool @tool def suggest_menu(occasion: str) -> str: \"\"\" Suggests a menu based on the occasion. Args: occasion: The type of occasion for the party. \"\"\" if occasion == \"casual\": return \"Pizza, snacks, and drinks.\" elif occasion == \"formal\": return \"3-course dinner with wine and dessert.\" elif occasion == \"superhero\": return \"Buffet with high-energy and healthy food.\" else: return \"Custom menu for the butler.\" @tool def catering_service_tool(query: str) -> str: \"\"\" This tool returns the highest-rated catering service in Gotham City. Args: query: A search term for finding catering services. \"\"\" # Example list of catering services and their ratings services = { \"Gotham Catering Co.\": 4.9, \"Wayne Manor Catering\": 4.8, \"Gotham City Events\": 4.7, } # Find the highest rated catering service (simulating search query filtering) best_service = max(services, key=services.get) return best_service class SuperheroPartyThemeTool(Tool): name = \"superhero_party_theme_generator\" description = \"\"\" This tool suggests creative superhero-themed party ideas based on a category. It returns a unique party theme idea.\"\"\" inputs = { \"category\": { \"type\": \"string\", \"description\": \"The type of superhero party (e.g., 'classic heroes', 'villain masquerade', 'futuristic Gotham').\", } } output_type = \"string\" def forward(self, category: str): themes = { \"classic heroes\": \"Justice League Gala: Guests come dressed as their favorite DC heroes with themed cocktails like 'The Kryptonite Punch'.\", \"villain masquerade\": \"Gotham Rogues' Ball: A mysterious masquerade where guests",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "catering service (simulating search query filtering) best_service = max(services, key=services.get) return best_service class SuperheroPartyThemeTool(Tool): name = \"superhero_party_theme_generator\" description = \"\"\" This tool suggests creative superhero-themed party ideas based on a category. It returns a unique party theme idea.\"\"\" inputs = { \"category\": { \"type\": \"string\", \"description\": \"The type of superhero party (e.g., 'classic heroes', 'villain masquerade', 'futuristic Gotham').\", } } output_type = \"string\" def forward(self, category: str): themes = { \"classic heroes\": \"Justice League Gala: Guests come dressed as their favorite DC heroes with themed cocktails like 'The Kryptonite Punch'.\", \"villain masquerade\": \"Gotham Rogues' Ball: A mysterious masquerade where guests dress as classic Batman villains.\", \"futuristic Gotham\": \"Neo-Gotham Night: A cyberpunk-style party inspired by Batman Beyond, with neon decorations and futuristic gadgets.\" } return themes.get(category.lower(), \"Themed party idea not found. Try 'classic heroes', 'villain masquerade', or 'futuristic Gotham'.\") # Alfred, the butler, preparing the menu for the party agent = CodeAgent( tools=[ DuckDuckGoSearchTool(), VisitWebpageTool(), suggest_menu, catering_service_tool, SuperheroPartyThemeTool(), FinalAnswerTool() ], model=InferenceClientModel(), max_steps=10, verbosity_level=2 ) agent.run(\"Give me the best playlist for a party at the Wayne's mansion. The party idea is a 'villain masquerade' theme\") ``` As you can see, we‚Äôve created a `CodeAgent` with several tools that enhance the agent‚Äôs functionality, turning it into the ultimate party planner ready to share with the community! üéâ Now, it‚Äôs your turn: build your very own agent and share it with the community using the knowledge we‚Äôve just learned! üïµÔ∏è‚Äç‚ôÇÔ∏èüí° > If you would like to share your agent project, then make a space and tag theagents-courseon the Hugging Face Hub. We‚Äôd love to see what you‚Äôve created! ### Inspecting Our Party Preparator Agent with OpenTelemetry and Langfuse üì° As Alfred fine-tunes the Party Preparator Agent, he‚Äôs growing weary of debugging its runs. Agents, by nature, are unpredictable and difficult to inspect. But since he aims to build the ultimate Party Preparator Agent and deploy it in production, he needs robust traceability for future monitoring and analysis. Once again, `smolagents` comes to the rescue! It embraces the [OpenTelemetry](https://opentelemetry.io/) standard for instrumenting agent runs, allowing seamless inspection and logging. With the help of [Langfuse](https://langfuse.com/) and the `SmolagentsInstrumentor`, Alfred can easily track and analyze his agent‚Äôs behavior. Setting it up is straightforward! First, we need to install the necessary dependencies: ``` pip install opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents langfuse ``` Next, Alfred has already created an account on Langfuse and has his API keys ready. If you haven‚Äôt done so yet, you can sign up for Langfuse Cloud [here](https://cloud.langfuse.com/) or explore [alternatives](https://huggingface.co/docs/smolagents/tutorials/inspect_runs). Once you have your API keys, they need to be properly configured as follows: ``` import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region ``` With the environment variables set, we can now initialize the Langfuse client. get_client() initializes the Langfuse client using the credentials provided in the environment variables. ``` from langfuse import get_client langfuse = get_client() # Verify",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "done so yet, you can sign up for Langfuse Cloud [here](https://cloud.langfuse.com/) or explore [alternatives](https://huggingface.co/docs/smolagents/tutorials/inspect_runs). Once you have your API keys, they need to be properly configured as follows: ``` import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # üá™üá∫ EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # üá∫üá∏ US region ``` With the environment variables set, we can now initialize the Langfuse client. get_client() initializes the Langfuse client using the credentials provided in the environment variables. ``` from langfuse import get_client langfuse = get_client() # Verify connection if langfuse.auth_check(): print(\"Langfuse client is authenticated and ready!\") else: print(\"Authentication failed. Please check your credentials and host.\") ``` Finally, Alfred is ready to initialize the `SmolagentsInstrumentor` and start tracking his agent‚Äôs performance. ``` from openinference.instrumentation.smolagents import SmolagentsInstrumentor SmolagentsInstrumentor().instrument() ``` Alfred is now connected üîå! The runs from `smolagents` are being logged in Langfuse, giving him full visibility into the agent‚Äôs behavior. With this setup, he‚Äôs ready to revisit previous runs and refine his Party Preparator Agent even further. > To learn more about tracing your agents and using the collected data to evaluate their performance, check outBonus Unit 2. ``` from smolagents import CodeAgent, InferenceClientModel agent = CodeAgent(tools=[], model=InferenceClientModel()) alfred_agent = agent.from_hub('sergiopaniego/AlfredAgent', trust_remote_code=True) alfred_agent.run(\"Give me the best playlist for a party at Wayne's mansion. The party idea is a 'villain masquerade' theme\") ``` Alfred can now access these logs [here](https://cloud.langfuse.com/project/cm7bq0abj025rad078ak3luwi/traces/995fc019255528e4f48cf6770b0ce27b?timestamp=2025-02-19T10%3A28%3A36.929Z) to review and analyze them. > Actually, a minor error occurred during execution. Can you spot it in the logs? Try to track how the agent handles it and still returns a valid answer.Hereis the direct link to the error if you want to verify your answer. Of course the error has been fixed in the meantime, more details can be found in thisissue. Meanwhile, the [suggested playlist](https://open.spotify.com/playlist/0gZMMHjuxMrrybQ7wTMTpw) sets the perfect vibe for the party preparations. Cool, right? üé∂ Now that we have created our first Code Agent, let‚Äôs **learn how we can create Tool Calling Agents**, the second type of agent available in `smolagents`. ## Resources - [smolagents Blog](https://huggingface.co/blog/smolagents) - Introduction to smolagents and code interactions - [smolagents: Building Good Agents](https://huggingface.co/docs/smolagents/tutorials/building_good_agents) - Best practices for reliable agents - [Building Effective Agents - Anthropic](https://www.anthropic.com/research/building-effective-agents) - Agent design principles - [Sharing runs with OpenTelemetry](https://huggingface.co/docs/smolagents/tutorials/inspect_runs) - Details about how to setup OpenTelemetry for tracking your agents.",
    "metadata": {
      "title": "Building Agents That Use Code",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/code_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/code_agents",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/code_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nCongratulations on finishing the `smolagents` module of this second Unit ü•≥\n \nYou‚Äôve just mastered the fundamentals of `smolagents` and you‚Äôve built your own Agent! Now that you have skills in `smolagents`, you can now start to create Agents that will solve tasks you‚Äôre interested about.\n \nIn the next module, you‚Äôre going to learn **how to build Agents with LlamaIndex**.\n \nFinally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then, please üëâ [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe9VaONn0eglax0uTwi29rIn4tM7H2sYmmybmG5jJNlE5v0xA/viewform?usp=dialog)\n \n\n### Keep Learning, stay awesome ü§ó",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/conclusion",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Exam Time!\n\n \nWell done on working through the material on `smolagents`! You‚Äôve already achieved a lot. Now, it‚Äôs time to put your knowledge to the test with a quiz. üß†\n \n\n## Instructions\n\n \n- The quiz consists of code questions.\n- You will be given instructions to complete the code snippets.\n- Read the instructions carefully and complete the code snippets accordingly.\n- For each question, you will be given the result and some feedback.\n \nüßò **This quiz is ungraded and uncertified**. It‚Äôs about you understanding the `smolagents` library and knowing whether you should spend more time on the written material. In the coming units you‚Äôll put this knowledge to the test in use cases and projects.\n \nLet‚Äôs get started!\n \n\n## Quiz üöÄ\n\n  \nYou can also access the quiz üëâ [here](https://huggingface.co/spaces/agents-course/unit2_smolagents_quiz)",
    "metadata": {
      "title": "Exam Time!",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/final_quiz",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/final_quiz",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/final_quiz.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to smolagents ![Unit 2.1 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/thumbnail.jpg) Welcome to this module, where you‚Äôll learn **how to build effective agents** using the [smolagents](https://github.com/huggingface/smolagents) library, which provides a lightweight framework for creating capable AI agents. `smolagents` is a Hugging Face library; therefore, we would appreciate your support by **starring** the smolagents [repository](https://github.com/huggingface/smolagents) : ![staring smolagents](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/star_smolagents.gif) ## Module Overview This module provides a comprehensive overview of key concepts and practical strategies for building intelligent agents using `smolagents`. With so many open-source frameworks available, it‚Äôs essential to understand the components and capabilities that make `smolagents` a useful option or to determine when another solution might be a better fit. We‚Äôll explore critical agent types, including code agents designed for software development tasks, tool calling agents for creating modular, function-driven workflows, and retrieval agents that access and synthesize information. Additionally, we‚Äôll cover the orchestration of multiple agents as well as the integration of vision capabilities and web browsing, which unlock new possibilities for dynamic and context-aware applications. In this unit, Alfred, the agent from Unit 1, makes his return. This time, he‚Äôs using the `smolagents` framework for his internal workings. Together, we‚Äôll explore the key concepts behind this framework as Alfred tackles various tasks. Alfred is organizing a party at the Wayne Manor while the Wayne family ü¶á is away, and he has plenty to do. Join us as we showcase his journey and how he handles these tasks with `smolagents`! > In this unit, you will learn to build AI agents with thesmolagentslibrary. Your agents will be able to search for data, execute code, and interact with web pages. You will also learn how to combine multiple agents to create more powerful systems. ![Alfred the agent](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/this-is-alfred.jpg) ## Contents During this unit on `smolagents`, we cover: ### 1Ô∏è‚É£ Why Use smolagents `smolagents` is one of the many open-source agent frameworks available for application development. Alternative options include `LlamaIndex` and `LangGraph`, which are also covered in other modules in this course. `smolagents` offers several key features that might make it a great fit for specific use cases, but we should always consider all options when selecting a framework. We‚Äôll explore the advantages and drawbacks of using `smolagents`, helping you make an informed decision based on your project‚Äôs requirements. ### 2Ô∏è‚É£ CodeAgents `CodeAgents` are the primary type of agent in `smolagents`. Instead of generating JSON or text, these agents produce Python code to perform actions. This module explores their purpose, functionality, and how they work, along with hands-on examples to showcase their capabilities. ### 3Ô∏è‚É£ ToolCallingAgents `ToolCallingAgents` are the second type of agent supported by `smolagents`. Unlike `CodeAgents`, which generate Python code, these agents rely on JSON/text blobs that the system must parse and interpret to execute actions. This module covers their functionality, their key differences from `CodeAgents`, and it provides an example to illustrate their usage. ### 4Ô∏è‚É£ Tools As we saw in Unit 1, tools are functions that an LLM can use within an agentic system, and they act as the essential building blocks for",
    "metadata": {
      "title": "Introduction to smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/introduction",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/introduction",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "explores their purpose, functionality, and how they work, along with hands-on examples to showcase their capabilities. ### 3Ô∏è‚É£ ToolCallingAgents `ToolCallingAgents` are the second type of agent supported by `smolagents`. Unlike `CodeAgents`, which generate Python code, these agents rely on JSON/text blobs that the system must parse and interpret to execute actions. This module covers their functionality, their key differences from `CodeAgents`, and it provides an example to illustrate their usage. ### 4Ô∏è‚É£ Tools As we saw in Unit 1, tools are functions that an LLM can use within an agentic system, and they act as the essential building blocks for agent behavior. This module covers how to create tools, their structure, and different implementation methods using the `Tool` class or the `@tool` decorator. You‚Äôll also learn about the default toolbox, how to share tools with the community, and how to load community-contributed tools for use in your agents. ### 5Ô∏è‚É£ Retrieval Agents Retrieval agents allow models access to knowledge bases, making it possible to search, synthesize, and retrieve information from multiple sources. They leverage vector stores for efficient retrieval and implement **Retrieval-Augmented Generation (RAG)** patterns. These agents are particularly useful for integrating web search with custom knowledge bases while maintaining conversation context through memory systems. This module explores implementation strategies, including fallback mechanisms for robust information retrieval. ### 6Ô∏è‚É£ Multi-Agent Systems Orchestrating multiple agents effectively is crucial for building powerful, multi-agent systems. By combining agents with different capabilities‚Äîsuch as a web search agent with a code execution agent‚Äîyou can create more sophisticated solutions. This module focuses on designing, implementing, and managing multi-agent systems to maximize efficiency and reliability. ### 7Ô∏è‚É£ Vision and Browser agents Vision agents extend traditional agent capabilities by incorporating **Vision-Language Models (VLMs)**, enabling them to process and interpret visual information. This module explores how to design and integrate VLM-powered agents, unlocking advanced functionalities like image-based reasoning, visual data analysis, and multimodal interactions. We will also use vision agents to build a browser agent that can browse the web and extract information from it. ## Resources - [smolagents Documentation](https://huggingface.co/docs/smolagents) - Official docs for the smolagents library - [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - Research paper on agent architectures - [Agent Guidelines](https://huggingface.co/docs/smolagents/tutorials/building_good_agents) - Best practices for building reliable agents - [LangGraph Agents](https://langchain-ai.github.io/langgraph/) - Additional examples of agent implementations - [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling) - Understanding function calling in LLMs - [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/) - Guide to implementing effective RAG",
    "metadata": {
      "title": "Introduction to smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/introduction",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/introduction",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Multi-Agent Systems Multi-agent systems enable **specialized agents to collaborate on complex tasks**, improving modularity, scalability, and robustness. Instead of relying on a single agent, tasks are distributed among agents with distinct capabilities. In **smolagents**, different agents can be combined to generate Python code, call external tools, perform web searches, and more. By orchestrating these agents, we can create powerful workflows. A typical setup might include: - A **Manager Agent** for task delegation - A **Code Interpreter Agent** for code execution - A **Web Search Agent** for information retrieval The diagram below illustrates a simple multi-agent architecture where a **Manager Agent** coordinates a **Code Interpreter Tool** and a **Web Search Agent**, which in turn utilizes tools like the `DuckDuckGoSearchTool` and `VisitWebpageTool` to gather relevant information. ![](https://mermaid.ink/img/pako:eNp1kc1qhTAQRl9FUiQb8wIpdNO76eKubrmFks1oRg3VSYgjpYjv3lFL_2hnMWQOJwn5sqgmelRWleUSKLAtFs09jqhtoWuYUFfFAa6QA9QDTnpzamheuhxn8pt40-6l13UtS0ddhtQXj6dbR4XUGQg6zEYasTF393KjeSDGnDJKNxzj8I_7hLW5IOSmP9CH9hv_NL-d94d4DVNg84p1EnK4qlIj5hGClySWbadT-6OdsrL02MI8sFOOVkciw8zx8kaNspxnrJQE0fXKtjBMMs3JA-MpgOQwftIE9Bzj14w-cMznI_39E9Z3p0uFoA?type=png) ## Multi-Agent Systems in Action A multi-agent system consists of multiple specialized agents working together under the coordination of an **Orchestrator Agent**. This approach enables complex workflows by distributing tasks among agents with distinct roles. For example, a **Multi-Agent RAG system** can integrate: - A **Web Agent** for browsing the internet. - A **Retriever Agent** for fetching information from knowledge bases. - An **Image Generation Agent** for producing visuals. All of these agents operate under an orchestrator that manages task delegation and interaction. ## Solving a complex task with a multi-agent hierarchy > You can follow the code inthis notebookthat you can run using Google Colab. The reception is approaching! With your help, Alfred is now nearly finished with the preparations. But now there‚Äôs a problem: the Batmobile has disappeared. Alfred needs to find a replacement, and find it quickly. Fortunately, a few biopics have been done on Bruce Wayne‚Äôs life, so maybe Alfred could get a car left behind on one of the movie sets, and re-engineer it up to modern standards, which certainly would include a full self-driving option. But this could be anywhere in the filming locations around the world - which could be numerous. So Alfred wants your help. Could you build an agent able to solve this task? > üëâ Find all Batman filming locations in the world, calculate the time to transfer via boat to there, and represent them on a map, with a color varying by boat transfer time. Also represent some supercar factories with the same boat transfer time. Let‚Äôs build this! This example needs some additional packages, so let‚Äôs install them first: ``` pip install 'smolagents[litellm]' plotly geopandas shapely kaleido -q ``` ### We first make a tool to get the cargo plane transfer time. ``` import math from typing import Optional, Tuple from smolagents import tool @tool def calculate_cargo_travel_time( origin_coords: Tuple[float, float], destination_coords: Tuple[float, float], cruising_speed_kmh: Optional[float] = 750.0, # Average speed for cargo planes ) -> float: \"\"\" Calculate the travel time for a cargo plane between two points on Earth using great-circle distance. Args: origin_coords: Tuple of (latitude, longitude) for the starting point destination_coords: Tuple of (latitude, longitude) for the destination cruising_speed_kmh: Optional cruising speed in km/h (defaults to 750 km/h for",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'smolagents[litellm]' plotly geopandas shapely kaleido -q ``` ### We first make a tool to get the cargo plane transfer time. ``` import math from typing import Optional, Tuple from smolagents import tool @tool def calculate_cargo_travel_time( origin_coords: Tuple[float, float], destination_coords: Tuple[float, float], cruising_speed_kmh: Optional[float] = 750.0, # Average speed for cargo planes ) -> float: \"\"\" Calculate the travel time for a cargo plane between two points on Earth using great-circle distance. Args: origin_coords: Tuple of (latitude, longitude) for the starting point destination_coords: Tuple of (latitude, longitude) for the destination cruising_speed_kmh: Optional cruising speed in km/h (defaults to 750 km/h for typical cargo planes) Returns: float: The estimated travel time in hours Example: >>> # Chicago (41.8781¬∞ N, 87.6298¬∞ W) to Sydney (33.8688¬∞ S, 151.2093¬∞ E) >>> result = calculate_cargo_travel_time((41.8781, -87.6298), (-33.8688, 151.2093)) \"\"\" def to_radians(degrees: float) -> float: return degrees * (math.pi / 180) # Extract coordinates lat1, lon1 = map(to_radians, origin_coords) lat2, lon2 = map(to_radians, destination_coords) # Earth's radius in kilometers EARTH_RADIUS_KM = 6371.0 # Calculate great-circle distance using the haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = ( math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2 ) c = 2 * math.asin(math.sqrt(a)) distance = EARTH_RADIUS_KM * c # Add 10% to account for non-direct routes and air traffic controls actual_distance = distance * 1.1 # Calculate flight time # Add 1 hour for takeoff and landing procedures flight_time = (actual_distance / cruising_speed_kmh) + 1.0 # Format the results return round(flight_time, 2) print(calculate_cargo_travel_time((41.8781, -87.6298), (-33.8688, 151.2093))) ``` ### Setting up the agent For the model provider, we use Together AI, one of the new [inference providers on the Hub](https://huggingface.co/blog/inference-providers)! The GoogleSearchTool uses the [Serper API](https://serper.dev) to search the web, so this requires either having setup env variable `SERPAPI_API_KEY` and passing `provider=\"serpapi\"` or having `SERPER_API_KEY` and passing `provider=serper`. If you don‚Äôt have any Serp API provider setup, you can use `DuckDuckGoSearchTool` but beware that it has a rate limit. ``` import os from PIL import Image from smolagents import CodeAgent, GoogleSearchTool, InferenceClientModel, VisitWebpageTool model = InferenceClientModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"together\") ``` We can start by creating a simple agent as a baseline to give us a simple report. ``` task = \"\"\"Find all Batman filming locations in the world, calculate the time to transfer via cargo plane to here (we're in Gotham, 40.7128¬∞ N, 74.0060¬∞ W), and return them to me as a pandas dataframe. Also give me some supercar factories with the same cargo plane transfer time.\"\"\" ``` ``` agent = CodeAgent( model=model, tools=[GoogleSearchTool(\"serper\"), VisitWebpageTool(), calculate_cargo_travel_time], additional_authorized_imports=[\"pandas\"], max_steps=20, ) ``` ``` result = agent.run(task) ``` ``` result ``` In our case, it generates this output: ``` | | Location | Travel Time to Gotham (hours) | |--|------------------------------------------------------|------------------------------| | 0 | Necropolis Cemetery, Glasgow, Scotland, UK | 8.60 | | 1 | St. George's Hall, Liverpool, England, UK | 8.81 | | 2 | Two Temple Place, London, England, UK | 9.17 | | 3 | Wollaton Hall, Nottingham, England,",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "me as a pandas dataframe. Also give me some supercar factories with the same cargo plane transfer time.\"\"\" ``` ``` agent = CodeAgent( model=model, tools=[GoogleSearchTool(\"serper\"), VisitWebpageTool(), calculate_cargo_travel_time], additional_authorized_imports=[\"pandas\"], max_steps=20, ) ``` ``` result = agent.run(task) ``` ``` result ``` In our case, it generates this output: ``` | | Location | Travel Time to Gotham (hours) | |--|------------------------------------------------------|------------------------------| | 0 | Necropolis Cemetery, Glasgow, Scotland, UK | 8.60 | | 1 | St. George's Hall, Liverpool, England, UK | 8.81 | | 2 | Two Temple Place, London, England, UK | 9.17 | | 3 | Wollaton Hall, Nottingham, England, UK | 9.00 | | 4 | Knebworth House, Knebworth, Hertfordshire, UK | 9.15 | | 5 | Acton Lane Power Station, Acton Lane, Acton, UK | 9.16 | | 6 | Queensboro Bridge, New York City, USA | 1.01 | | 7 | Wall Street, New York City, USA | 1.00 | | 8 | Mehrangarh Fort, Jodhpur, Rajasthan, India | 18.34 | | 9 | Turda Gorge, Turda, Romania | 11.89 | | 10 | Chicago, USA | 2.68 | | 11 | Hong Kong, China | 19.99 | | 12 | Cardington Studios, Northamptonshire, UK | 9.10 | | 13 | Warner Bros. Leavesden Studios, Hertfordshire, UK | 9.13 | | 14 | Westwood, Los Angeles, CA, USA | 6.79 | | 15 | Woking, UK (McLaren) | 9.13 | ``` We could already improve this a bit by throwing in some dedicated planning steps, and adding more prompting. Planning steps allow the agent to think ahead and plan its next steps, which can be useful for more complex tasks. ``` agent.planning_interval = 4 detailed_report = agent.run(f\"\"\" You're an expert analyst. You make comprehensive reports after visiting many websites. Don't hesitate to search for many queries at once in a for loop. For each data point that you find, visit the source url to confirm numbers. {task} \"\"\") print(detailed_report) ``` ``` detailed_report ``` In our case, it generates this output: ``` | | Location | Travel Time (hours) | |--|--------------------------------------------------|---------------------| | 0 | Bridge of Sighs, Glasgow Necropolis, Glasgow, UK | 8.6 | | 1 | Wishart Street, Glasgow, Scotland, UK | 8.6 | ``` Thanks to these quick changes, we obtained a much more concise report by simply providing our agent a detailed prompt, and giving it planning capabilities! The model‚Äôs context window is quickly filling up. So **if we ask our agent to combine the results of detailed search with another, it will be slower and quickly ramp up tokens and costs**. ‚û°Ô∏è We need to improve the structure of our system. ### ‚úåÔ∏è Splitting the task between two agents Multi-agent structures allow to separate memories between different sub-tasks, with two great benefits: - Each agent is more focused on its core task, thus more performant - Separating memories reduces the count of input tokens at each step, thus reducing latency and cost. Let‚Äôs create a team with a dedicated web search agent, managed by",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "up. So **if we ask our agent to combine the results of detailed search with another, it will be slower and quickly ramp up tokens and costs**. ‚û°Ô∏è We need to improve the structure of our system. ### ‚úåÔ∏è Splitting the task between two agents Multi-agent structures allow to separate memories between different sub-tasks, with two great benefits: - Each agent is more focused on its core task, thus more performant - Separating memories reduces the count of input tokens at each step, thus reducing latency and cost. Let‚Äôs create a team with a dedicated web search agent, managed by another agent. The manager agent should have plotting capabilities to write its final report: so let us give it access to additional imports, including `plotly`, and `geopandas` + `shapely` for spatial plotting. ``` model = InferenceClientModel( \"Qwen/Qwen2.5-Coder-32B-Instruct\", provider=\"together\", max_tokens=8096 ) web_agent = CodeAgent( model=model, tools=[ GoogleSearchTool(provider=\"serper\"), VisitWebpageTool(), calculate_cargo_travel_time, ], name=\"web_agent\", description=\"Browses the web to find information\", verbosity_level=0, max_steps=10, ) ``` The manager agent will need to do some mental heavy lifting. So we give it the stronger model [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1), and add a `planning_interval` to the mix. ``` from smolagents.utils import encode_image_base64, make_image_url from smolagents import OpenAIServerModel def check_reasoning_and_plot(final_answer, agent_memory): multimodal_model = OpenAIServerModel(\"gpt-4o\", max_tokens=8096) filepath = \"saved_map.png\" assert os.path.exists(filepath), \"Make sure to save the plot under saved_map.png!\" image = Image.open(filepath) prompt = ( f\"Here is a user-given task and the agent steps: {agent_memory.get_succinct_steps()}. Now here is the plot that was made.\" \"Please check that the reasoning process and plot are correct: do they correctly answer the given task?\" \"First list reasons why yes/no, then write your final decision: PASS in caps lock if it is satisfactory, FAIL if it is not.\" \"Don't be harsh: if the plot mostly solves the task, it should pass.\" \"To pass, a plot should be made using px.scatter_map and not any other method (scatter_map looks nicer).\" ) messages = [ { \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": prompt, }, { \"type\": \"image_url\", \"image_url\": {\"url\": make_image_url(encode_image_base64(image))}, }, ], } ] output = multimodal_model(messages).content print(\"Feedback: \", output) if \"FAIL\" in output: raise Exception(output) return True manager_agent = CodeAgent( model=InferenceClientModel(\"deepseek-ai/DeepSeek-R1\", provider=\"together\", max_tokens=8096), tools=[calculate_cargo_travel_time], managed_agents=[web_agent], additional_authorized_imports=[ \"geopandas\", \"plotly\", \"shapely\", \"json\", \"pandas\", \"numpy\", ], planning_interval=5, verbosity_level=2, final_answer_checks=[check_reasoning_and_plot], max_steps=15, ) ``` Let us inspect what this team looks like: ``` manager_agent.visualize() ``` This will generate something like this, helping us understand the structure and relationship between agents and tools used: ``` CodeAgent | deepseek-ai/DeepSeek-R1 ‚îú‚îÄ‚îÄ ‚úÖ Authorized imports: ['geopandas', 'plotly', 'shapely', 'json', 'pandas', 'numpy'] ‚îú‚îÄ‚îÄ üõ†Ô∏è Tools: ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îÇ ‚îÉ Name ‚îÉ Description ‚îÉ Arguments ‚îÉ ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚îÇ ‚îÇ calculate_cargo_travel_time ‚îÇ Calculate the travel time for a cargo ‚îÇ origin_coords (`array`): Tuple of ‚îÇ ‚îÇ ‚îÇ ‚îÇ plane between two points on Earth ‚îÇ (latitude, longitude) for the ‚îÇ ‚îÇ ‚îÇ ‚îÇ using great-circle distance. ‚îÇ starting point ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ destination_coords (`array`): Tuple ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ of (latitude, longitude) for the ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ destination ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ cruising_speed_kmh",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` CodeAgent | deepseek-ai/DeepSeek-R1 ‚îú‚îÄ‚îÄ ‚úÖ Authorized imports: ['geopandas', 'plotly', 'shapely', 'json', 'pandas', 'numpy'] ‚îú‚îÄ‚îÄ üõ†Ô∏è Tools: ‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îÇ ‚îÉ Name ‚îÉ Description ‚îÉ Arguments ‚îÉ ‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚îÇ ‚îÇ calculate_cargo_travel_time ‚îÇ Calculate the travel time for a cargo ‚îÇ origin_coords (`array`): Tuple of ‚îÇ ‚îÇ ‚îÇ ‚îÇ plane between two points on Earth ‚îÇ (latitude, longitude) for the ‚îÇ ‚îÇ ‚îÇ ‚îÇ using great-circle distance. ‚îÇ starting point ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ destination_coords (`array`): Tuple ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ of (latitude, longitude) for the ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ destination ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ cruising_speed_kmh (`number`): ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ Optional cruising speed in km/h ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ (defaults to 750 km/h for typical ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ cargo planes) ‚îÇ ‚îÇ ‚îÇ final_answer ‚îÇ Provides a final answer to the given ‚îÇ answer (`any`): The final answer to ‚îÇ ‚îÇ ‚îÇ ‚îÇ problem. ‚îÇ the problem ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ ü§ñ Managed agents: ‚îî‚îÄ‚îÄ web_agent | CodeAgent | Qwen/Qwen2.5-Coder-32B-Instruct ‚îú‚îÄ‚îÄ ‚úÖ Authorized imports: [] ‚îú‚îÄ‚îÄ üìù Description: Browses the web to find information ‚îî‚îÄ‚îÄ üõ†Ô∏è Tools: ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îÉ Name ‚îÉ Description ‚îÉ Arguments ‚îÉ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚îÇ web_search ‚îÇ Performs a google web search for ‚îÇ query (`string`): The search ‚îÇ ‚îÇ ‚îÇ your query then returns a string ‚îÇ query to perform. ‚îÇ ‚îÇ ‚îÇ of the top search results. ‚îÇ filter_year (`integer`): ‚îÇ ‚îÇ ‚îÇ ‚îÇ Optionally restrict results to a ‚îÇ ‚îÇ ‚îÇ ‚îÇ certain year ‚îÇ ‚îÇ visit_webpage ‚îÇ Visits a webpage at the given url ‚îÇ url (`string`): The url of the ‚îÇ ‚îÇ ‚îÇ and reads its content as a ‚îÇ webpage to visit. ‚îÇ ‚îÇ ‚îÇ markdown string. Use this to ‚îÇ ‚îÇ ‚îÇ ‚îÇ browse webpages. ‚îÇ ‚îÇ ‚îÇ calculate_cargo_travel_time ‚îÇ Calculate the travel time for a ‚îÇ origin_coords (`array`): Tuple of ‚îÇ ‚îÇ ‚îÇ cargo plane between two points on ‚îÇ (latitude, longitude) for the ‚îÇ ‚îÇ ‚îÇ Earth using great-circle ‚îÇ starting point ‚îÇ ‚îÇ ‚îÇ distance. ‚îÇ destination_coords (`array`): ‚îÇ ‚îÇ ‚îÇ ‚îÇ Tuple of (latitude, longitude) ‚îÇ ‚îÇ ‚îÇ ‚îÇ for the destination ‚îÇ ‚îÇ ‚îÇ ‚îÇ cruising_speed_kmh (`number`): ‚îÇ ‚îÇ ‚îÇ ‚îÇ Optional cruising speed in km/h ‚îÇ ‚îÇ ‚îÇ ‚îÇ (defaults to 750 km/h for typical ‚îÇ ‚îÇ ‚îÇ ‚îÇ cargo planes) ‚îÇ ‚îÇ final_answer ‚îÇ Provides a final answer to the ‚îÇ answer (`any`): The final answer ‚îÇ ‚îÇ ‚îÇ given problem. ‚îÇ to the problem ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ``` ``` manager_agent.run(\"\"\" Find all Batman filming locations in the world, calculate the time to transfer via cargo plane to here (we're in Gotham, 40.7128¬∞ N, 74.0060¬∞ W). Also give me some supercar factories with the same cargo plane transfer time. You need at least 6 points in total. Represent this as spatial map of the world, with the locations represented as scatter points with a color that depends on the travel time, and save it to saved_map.png! Here's an example of how to plot and return a",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "answer ‚îÇ ‚îÇ ‚îÇ given problem. ‚îÇ to the problem ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ``` ``` manager_agent.run(\"\"\" Find all Batman filming locations in the world, calculate the time to transfer via cargo plane to here (we're in Gotham, 40.7128¬∞ N, 74.0060¬∞ W). Also give me some supercar factories with the same cargo plane transfer time. You need at least 6 points in total. Represent this as spatial map of the world, with the locations represented as scatter points with a color that depends on the travel time, and save it to saved_map.png! Here's an example of how to plot and return a map: import plotly.express as px df = px.data.carshare() fig = px.scatter_map(df, lat=\"centroid_lat\", lon=\"centroid_lon\", text=\"name\", color=\"peak_hour\", size=100, color_continuous_scale=px.colors.sequential.Magma, size_max=15, zoom=1) fig.show() fig.write_image(\"saved_image.png\") final_answer(fig) Never try to process strings using code: when you have a string to read, just print it and you'll see it. \"\"\") ``` I don‚Äôt know how that went in your run, but in mine, the manager agent skilfully divided tasks given to the web agent in `1. Search for Batman filming locations`, then `2. Find supercar factories`, before aggregating the lists and plotting the map. Let‚Äôs see what the map looks like by inspecting it directly from the agent state: ``` manager_agent.python_executor.state[\"fig\"] ``` This will output the map: ![Multiagent system example output map](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/output_map.png) ## Resources - [Multi-Agent Systems](https://huggingface.co/docs/smolagents/main/en/examples/multiagents) ‚Äì Overview of multi-agent systems. - [What is Agentic RAG?](https://weaviate.io/blog/what-is-agentic-rag) ‚Äì Introduction to Agentic RAG. - [Multi-Agent RAG System ü§ñü§ùü§ñ Recipe](https://huggingface.co/learn/cookbook/multiagent_rag_system) ‚Äì Step-by-step guide to building a multi-agent RAG system.",
    "metadata": {
      "title": "Multi-Agent Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/multi_agent_systems",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/multi_agent_systems",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/multi_agent_systems.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Small Quiz (ungraded)\n\n \nLet‚Äôs test your understanding of `smolagents` with a quick quiz! Remember, testing yourself helps reinforce learning and identify areas that may need review.\n \nThis is an optional quiz and it‚Äôs not graded.\n \n\n### Q1: What is one of the primary advantages of choosing smolagents over other frameworks?\n\n \nWhich statement best captures a core strength of the `smolagents` approach?\n  It uses highly specialized configuration files and a steep learning curve to ensure only expert developers can use it  It supports a code-first approach with minimal abstractions, letting agents interact directly via Python function calls  It focuses on JSON-based actions, removing the need for agents to write any code  It deeply integrates with a single LLM provider and specialized hardware    \n\n### Q2: In which scenario would you likely benefit most from using smolagents?\n\n \nWhich situation aligns well with what smolagents does best?\n  Prototyping or experimenting quickly with agent logic, particularly when your application is relatively straightforward  Building a large-scale enterprise system where you need dozens of microservices and real-time data pipelines  Needing a framework that only supports cloud-based LLMs and forbids local inference  A scenario that requires advanced orchestration, multi-modal perception, and enterprise-scale features out-of-the-box    \n\n### Q3: smolagents offers flexibility in model integration. Which statement best reflects its approach?\n\n \nChoose the most accurate description of how smolagents interoperates with LLMs.\n  It only provides a single built-in model and does not allow custom integrations  It requires you to implement your own model connector for every LLM usage  It only integrates with open-source LLMs but not commercial APIs  It can be used with a wide range of LLMs, offering predefined classes like TransformersModel, InferenceClientModel, and LiteLLMModel    \n\n### Q4: How does smolagents handle the debate between code-based actions and JSON-based actions?\n\n \nWhich statement correctly characterizes smolagents‚Äô philosophy about action formats?\n  It only allows JSON-based actions for all agent tasks, requiring a parser to extract the tool calls  It focuses on code-based actions via a CodeAgent but also supports JSON-based tool calls with a ToolCallingAgent  It disallows any external function calls, instead requiring all logic to reside entirely within the LLM  It requires users to manually convert every code snippet into a JSON object before running the agent    \n\n### Q5: How does smolagents integrate with the Hugging Face Hub for added benefits?\n\n \nWhich statement accurately describes one of the core advantages of Hub integration?\n  It automatically upgrades all public models to commercial license tiers  It disables local inference entirely, forcing remote model usage only  It allows you to push and share agents or tools, making them easily discoverable and reusable by other developers  It permanently stores all your code-based agents, preventing any updates or versioning    \nCongratulations on completing this quiz! üéâ If you missed any questions, consider reviewing the *Why use smolagents* section for a deeper understanding. If you did well, you‚Äôre ready to explore more advanced topics in smolagents!",
    "metadata": {
      "title": "Small Quiz (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/quiz1",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/quiz1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/quiz1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Small Quiz (ungraded) It‚Äôs time to test your understanding of the *Code Agents*, *Tool Calling Agents*, and *Tools* sections. This quiz is optional and not graded. ### Q1: What is the key difference between creating a tool with the @tool decorator versus creating a subclass of Tool in smolagents? Which statement best describes the distinction between these two approaches for defining tools? Using the `@tool` decorator is mandatory for retrieval-based tools, while subclasses of `Tool` are only for text-generation tasks The `@tool` decorator is recommended for simple function-based tools, while subclasses of `Tool` offer more flexibility for complex functionality or custom metadata `@tool` can only be used in multi-agent systems, while creating a `Tool` subclass is for single-agent scenarios Decorating a function with `@tool` replaces the need for a docstring, whereas subclasses must not include docstrings ### Q2: How does a CodeAgent handle multi-step tasks using the ReAct (Reason + Act) approach? Which statement correctly describes how the CodeAgent executes a series of steps to solve a task? It passes each step to a different agent in a multi-agent system, then combines results It stores every action in JSON for easy parsing before executing them all at once It cycles through writing internal thoughts, generating Python code, executing the code, and logging the results until it arrives at a final answer It relies on a vision module to validate code output before continuing to the next step ### Q3: Which of the following is a primary advantage of sharing a tool on the Hugging Face Hub? Select the best reason why a developer might upload and share their custom tool. It automatically integrates the tool with a MultiStepAgent for retrieval-augmented generation It allows others to discover, reuse, and integrate your tool in their smolagents without extra setup It ensures that only CodeAgents can invoke the tool while ToolCallingAgents cannot It converts your tool into a fully vision-capable function for image processing ### Q4: ToolCallingAgent differs from CodeAgent in how it executes actions. Which statement is correct? Choose the option that accurately describes how ToolCallingAgent works. ToolCallingAgent is only compatible with a multi-agent system, while CodeAgent can run alone ToolCallingAgent delegates all reasoning to a separate retrieval agent, then returns a final answer ToolCallingAgent outputs JSON instructions specifying tool calls and arguments, which get parsed and executed ToolCallingAgent is only meant for single-step tasks and automatically stops after calling one tool ### Q5: What is included in the smolagents default toolbox, and why might you use it? Which statement best captures the purpose and contents of the default toolbox in smolagents? It provides a set of commonly-used tools such as DuckDuckGo search, PythonInterpreterTool, and a final answer tool for quick prototyping It only supports vision-based tasks like image classification or OCR by default It is intended solely for multi-agent systems and is incompatible with a single CodeAgent It adds advanced retrieval-based functionality for large-scale question answering from a vector store Congratulations on completing this quiz! üéâ If any questions gave",
    "metadata": {
      "title": "Small Quiz (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/quiz2",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/quiz2",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/quiz2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tool ### Q5: What is included in the smolagents default toolbox, and why might you use it? Which statement best captures the purpose and contents of the default toolbox in smolagents? It provides a set of commonly-used tools such as DuckDuckGo search, PythonInterpreterTool, and a final answer tool for quick prototyping It only supports vision-based tasks like image classification or OCR by default It is intended solely for multi-agent systems and is incompatible with a single CodeAgent It adds advanced retrieval-based functionality for large-scale question answering from a vector store Congratulations on completing this quiz! üéâ If any questions gave you trouble, revisit the *Code Agents*, *Tool Calling Agents*, or *Tools* sections to strengthen your understanding. If you aced it, you‚Äôre well on your way to building robust smolagents applications!",
    "metadata": {
      "title": "Small Quiz (ungraded)",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/quiz2",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/quiz2",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/quiz2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building Agentic RAG Systems > You can follow the code inthis notebookthat you can run using Google Colab. Retrieval Augmented Generation (RAG) systems combine the capabilities of data retrieval and generation models to provide context-aware responses. For example, a user‚Äôs query is passed to a search engine, and the retrieved results are given to the model along with the query. The model then generates a response based on the query and retrieved information. Agentic RAG (Retrieval-Augmented Generation) extends traditional RAG systems by **combining autonomous agents with dynamic knowledge retrieval**. While traditional RAG systems use an LLM to answer queries based on retrieved data, agentic RAG **enables intelligent control of both retrieval and generation processes**, improving efficiency and accuracy. Traditional RAG systems face key limitations, such as **relying on a single retrieval step** and focusing on direct semantic similarity with the user‚Äôs query, which may overlook relevant information. Agentic RAG addresses these issues by allowing the agent to autonomously formulate search queries, critique retrieved results, and conduct multiple retrieval steps for a more tailored and comprehensive output. ## Basic Retrieval with DuckDuckGo Let‚Äôs build a simple agent that can search the web using DuckDuckGo. This agent will retrieve information and synthesize responses to answer queries. With Agentic RAG, Alfred‚Äôs agent can: - Search for latest superhero party trends - Refine results to include luxury elements - Synthesize information into a complete plan Here‚Äôs how Alfred‚Äôs agent can achieve this: ``` from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel # Initialize the search tool search_tool = DuckDuckGoSearchTool() # Initialize the model model = InferenceClientModel() agent = CodeAgent( model=model, tools=[search_tool], ) # Example usage response = agent.run( \"Search for luxury superhero-themed party ideas, including decorations, entertainment, and catering.\" ) print(response) ``` The agent follows this process: 1. **Analyzes the Request:** Alfred‚Äôs agent identifies the key elements of the query‚Äîluxury superhero-themed party planning, with focus on decor, entertainment, and catering. 2. **Performs Retrieval:** The agent leverages DuckDuckGo to search for the most relevant and up-to-date information, ensuring it aligns with Alfred‚Äôs refined preferences for a luxurious event. 3. **Synthesizes Information:** After gathering the results, the agent processes them into a cohesive, actionable plan for Alfred, covering all aspects of the party. 4. **Stores for Future Reference:** The agent stores the retrieved information for easy access when planning future events, optimizing efficiency in subsequent tasks. ## Custom Knowledge Base Tool For specialized tasks, a custom knowledge base can be invaluable. Let‚Äôs create a tool that queries a vector database of technical documentation or specialized knowledge. Using semantic search, the agent can find the most relevant information for Alfred‚Äôs needs. A vector database stores numerical representations (embeddings) of text or other data, created by machine learning models. It enables semantic search by identifying similar meanings in high-dimensional space. This approach combines predefined knowledge with semantic search to provide context-aware solutions for event planning. With specialized knowledge access, Alfred can perfect every detail of the party. In this example, we‚Äôll create a tool that retrieves party planning ideas",
    "metadata": {
      "title": "Building Agentic RAG Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/retrieval_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/retrieval_agents",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/retrieval_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "base can be invaluable. Let‚Äôs create a tool that queries a vector database of technical documentation or specialized knowledge. Using semantic search, the agent can find the most relevant information for Alfred‚Äôs needs. A vector database stores numerical representations (embeddings) of text or other data, created by machine learning models. It enables semantic search by identifying similar meanings in high-dimensional space. This approach combines predefined knowledge with semantic search to provide context-aware solutions for event planning. With specialized knowledge access, Alfred can perfect every detail of the party. In this example, we‚Äôll create a tool that retrieves party planning ideas from a custom knowledge base. We‚Äôll use a BM25 retriever to search the knowledge base and return the top results, and `RecursiveCharacterTextSplitter` to split the documents into smaller chunks for more efficient search. ``` from langchain_community.docstore.document import Document from langchain_text_splitters import RecursiveCharacterTextSplitter from smolagents import Tool from langchain_community.retrievers import BM25Retriever from smolagents import CodeAgent, InferenceClientModel class PartyPlanningRetrieverTool(Tool): name = \"party_planning_retriever\" description = \"Uses semantic search to retrieve relevant party planning ideas for Alfred‚Äôs superhero-themed party at Wayne Manor.\" inputs = { \"query\": { \"type\": \"string\", \"description\": \"The query to perform. This should be a query related to party planning or superhero themes.\", } } output_type = \"string\" def __init__(self, docs, **kwargs): super().__init__(**kwargs) self.retriever = BM25Retriever.from_documents( docs, k=5 # Retrieve the top 5 documents ) def forward(self, query: str) -> str: assert isinstance(query, str), \"Your search query must be a string\" docs = self.retriever.invoke( query, ) return \"\\nRetrieved ideas:\\n\" + \"\".join( [ f\"\\n\\n===== Idea {str(i)} =====\\n\" + doc.page_content for i, doc in enumerate(docs) ] ) # Simulate a knowledge base about party planning party_ideas = [ {\"text\": \"A superhero-themed masquerade ball with luxury decor, including gold accents and velvet curtains.\", \"source\": \"Party Ideas 1\"}, {\"text\": \"Hire a professional DJ who can play themed music for superheroes like Batman and Wonder Woman.\", \"source\": \"Entertainment Ideas\"}, {\"text\": \"For catering, serve dishes named after superheroes, like 'The Hulk's Green Smoothie' and 'Iron Man's Power Steak.'\", \"source\": \"Catering Ideas\"}, {\"text\": \"Decorate with iconic superhero logos and projections of Gotham and other superhero cities around the venue.\", \"source\": \"Decoration Ideas\"}, {\"text\": \"Interactive experiences with VR where guests can engage in superhero simulations or compete in themed games.\", \"source\": \"Entertainment Ideas\"} ] source_docs = [ Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in party_ideas ] # Split the documents into smaller chunks for more efficient search text_splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=50, add_start_index=True, strip_whitespace=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], ) docs_processed = text_splitter.split_documents(source_docs) # Create the retriever tool party_planning_retriever = PartyPlanningRetrieverTool(docs_processed) # Initialize the agent agent = CodeAgent(tools=[party_planning_retriever], model=InferenceClientModel()) # Example usage response = agent.run( \"Find ideas for a luxury superhero-themed party, including entertainment, catering, and decoration options.\" ) print(response) ``` This enhanced agent can: 1. First check the documentation for relevant information 2. Combine insights from the knowledge base 3. Maintain conversation context in memory ## Enhanced Retrieval Capabilities When building agentic RAG systems, the agent can employ sophisticated strategies like: 1. **Query Reformulation:** Instead of using the",
    "metadata": {
      "title": "Building Agentic RAG Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/retrieval_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/retrieval_agents",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/retrieval_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "chunk_size=500, chunk_overlap=50, add_start_index=True, strip_whitespace=True, separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], ) docs_processed = text_splitter.split_documents(source_docs) # Create the retriever tool party_planning_retriever = PartyPlanningRetrieverTool(docs_processed) # Initialize the agent agent = CodeAgent(tools=[party_planning_retriever], model=InferenceClientModel()) # Example usage response = agent.run( \"Find ideas for a luxury superhero-themed party, including entertainment, catering, and decoration options.\" ) print(response) ``` This enhanced agent can: 1. First check the documentation for relevant information 2. Combine insights from the knowledge base 3. Maintain conversation context in memory ## Enhanced Retrieval Capabilities When building agentic RAG systems, the agent can employ sophisticated strategies like: 1. **Query Reformulation:** Instead of using the raw user query, the agent can craft optimized search terms that better match the target documents 2. **Query Decomposition:** Instead of using the user query directly, if it contains multiple pieces of information to query, it can be decomposed to multiple queries 3. **Query Expansion:** Somehow similar to Query Reformulation but done multiple times to put the query in multiple wordings to query them all 4. **Reranking:** Using Cross-Encoders to assign more comprehensive and semantic relevance scores between retrieved documents and search query 5. **Multi-Step Retrieval:** The agent can perform multiple searches, using initial results to inform subsequent queries 6. **Source Integration:** Information can be combined from multiple sources like web search and local documentation 7. **Result Validation:** Retrieved content can be analyzed for relevance and accuracy before being included in responses Effective agentic RAG systems require careful consideration of several key aspects. The agent **should select between available tools based on the query type and context**. Memory systems help maintain conversation history and avoid repetitive retrievals. Having fallback strategies ensures the system can still provide value even when primary retrieval methods fail. Additionally, implementing validation steps helps ensure the accuracy and relevance of retrieved information. ## Resources - [Agentic RAG: turbocharge your RAG with query reformulation and self-query! üöÄ](https://huggingface.co/learn/cookbook/agent_rag) - Recipe for developing an Agentic RAG system using smolagents.",
    "metadata": {
      "title": "Building Agentic RAG Systems",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/retrieval_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/retrieval_agents",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/retrieval_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Writing actions as code snippets or JSON blobs\n\n \n> You can follow the code inthis notebookthat you can run using Google Colab.\n \nTool Calling Agents are the second type of agent available in `smolagents`. Unlike Code Agents that use Python snippets, these agents **use the built-in tool-calling capabilities of LLM providers** to generate tool calls as **JSON structures**. This is the standard approach used by OpenAI, Anthropic, and many other providers.\n \nLet‚Äôs look at an example. When Alfred wants to search for catering services and party ideas, a `CodeAgent` would generate and run Python code like this:\n  \n```\nfor query in [\n    \"Best catering services in Gotham City\", \n    \"Party theme ideas for superheroes\"\n]:\n    print(web_search(f\"Search for: {query}\"))\n```\n \nA `ToolCallingAgent` would instead create a JSON structure:\n  \n```\n[\n    {\"name\": \"web_search\", \"arguments\": \"Best catering services in Gotham City\"},\n    {\"name\": \"web_search\", \"arguments\": \"Party theme ideas for superheroes\"}\n]\n```\n \nThis JSON blob is then used to execute the tool calls.\n \nWhile `smolagents` primarily focuses on `CodeAgents` since [they perform better overall](https://huggingface.co/papers/2402.01030), `ToolCallingAgents` can be effective for simple systems that don‚Äôt require variable handling or complex tool calls.\n \n![Code vs JSON Actions](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png)\n \n\n## How Do Tool Calling Agents Work?\n\n \nTool Calling Agents follow the same multi-step workflow as Code Agents (see the [previous section](./code_agents) for details).\n \nThe key difference is in **how they structure their actions**: instead of executable code, they **generate JSON objects that specify tool names and arguments**. The system then **parses these instructions** to execute the appropriate tools.\n \n\n## Example: Running a Tool Calling Agent\n\n \nLet‚Äôs revisit the previous example where Alfred started party preparations, but this time we‚Äôll use a `ToolCallingAgent` to highlight the difference. We‚Äôll build an agent that can search the web using DuckDuckGo, just like in our Code Agent example. The only difference is the agent type - the framework handles everything else:\n  \n```\nfrom smolagents import ToolCallingAgent, WebSearchTool, InferenceClientModel\n\nagent = ToolCallingAgent(tools=[WebSearchTool()], model=InferenceClientModel())\n\nagent.run(\"Search for the best music recommendations for a party at the Wayne's mansion.\")\n```\n \nWhen you examine the agent‚Äôs trace, instead of seeing `Executing parsed code:`, you‚Äôll see something like:\n  \n```\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ Calling tool: 'web_search' with arguments: {'query': \"best music recommendations for a party at Wayne's         ‚îÇ\n‚îÇ mansion\"}                                                                                                       ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n```\n \nThe agent generates a structured tool call that the system processes to produce the output, rather than directly executing code like a `CodeAgent`.\n \nNow that we understand both agent types, we can choose the right one for our needs. Let‚Äôs continue exploring `smolagents` to make Alfred‚Äôs party a success! üéâ\n \n\n## Resources\n\n \n- [ToolCallingAgent documentation](https://huggingface.co/docs/smolagents/v1.8.1/en/reference/agents#smolagents.ToolCallingAgent) - Official documentation for ToolCallingAgent",
    "metadata": {
      "title": "Writing actions as code snippets or JSON blobs",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tool_calling_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tool_calling_agents",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tool_calling_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Tools As we explored in [unit 1](https://huggingface.co/learn/agents-course/unit1/tools), agents use tools to perform various actions. In `smolagents`, tools are treated as **functions that an LLM can call within an agent system**. To interact with a tool, the LLM needs an **interface description** with these key components: - **Name**: What the tool is called - **Tool description**: What the tool does - **Input types and descriptions**: What arguments the tool accepts - **Output type**: What the tool returns For instance, while preparing for a party at Wayne Manor, Alfred needs various tools to gather information - from searching for catering services to finding party theme ideas. Here‚Äôs how a simple search tool interface might look: - **Name:** `web_search` - **Tool description:** Searches the web for specific queries - **Input:** `query` (string) - The search term to look up - **Output:** String containing the search results By using these tools, Alfred can make informed decisions and gather all the information needed for planning the perfect party. Below, you can see an animation illustrating how a tool call is managed: ![Agentic pipeline from https://huggingface.co/docs/smolagents/conceptual_guides/react](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif) ## Tool Creation Methods In `smolagents`, tools can be defined in two ways: 1. **Using the @tool decorator** for simple function-based tools 2. **Creating a subclass of Tool** for more complex functionality ### The @tool Decorator The `@tool` decorator is the **recommended way to define simple tools**. Under the hood, smolagents will parse basic information about the function from Python. So if you name your function clearly and write a good docstring, it will be easier for the LLM to use. Using this approach, we define a function with: - **A clear and descriptive function name** that helps the LLM understand its purpose. - **Type hints for both inputs and outputs** to ensure proper usage. - **A detailed description**, including an `Args:` section where each argument is explicitly described. These descriptions provide valuable context for the LLM, so it‚Äôs important to write them carefully. #### Generating a tool that retrieves the highest-rated catering ![Alfred Catering](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/smolagents/alfred-catering.jpg) > You can follow the code inthis notebookthat you can run using Google Colab. Let‚Äôs imagine that Alfred has already decided on the menu for the party, but now he needs help preparing food for such a large number of guests. To do so, he would like to hire a catering service and needs to identify the highest-rated options available. Alfred can leverage a tool to search for the best catering services in his area. Below is an example of how Alfred can use the `@tool` decorator to make this happen: ``` from smolagents import CodeAgent, InferenceClientModel, tool # Let's pretend we have a function that fetches the highest-rated catering services. @tool def catering_service_tool(query: str) -> str: \"\"\" This tool returns the highest-rated catering service in Gotham City. Args: query: A search term for finding catering services. \"\"\" # Example list of catering services and their ratings services = { \"Gotham Catering Co.\": 4.9, \"Wayne Manor Catering\": 4.8, \"Gotham City Events\": 4.7, } #",
    "metadata": {
      "title": "Tools",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tools",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tools",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tool to search for the best catering services in his area. Below is an example of how Alfred can use the `@tool` decorator to make this happen: ``` from smolagents import CodeAgent, InferenceClientModel, tool # Let's pretend we have a function that fetches the highest-rated catering services. @tool def catering_service_tool(query: str) -> str: \"\"\" This tool returns the highest-rated catering service in Gotham City. Args: query: A search term for finding catering services. \"\"\" # Example list of catering services and their ratings services = { \"Gotham Catering Co.\": 4.9, \"Wayne Manor Catering\": 4.8, \"Gotham City Events\": 4.7, } # Find the highest rated catering service (simulating search query filtering) best_service = max(services, key=services.get) return best_service agent = CodeAgent(tools=[catering_service_tool], model=InferenceClientModel()) # Run the agent to find the best catering service result = agent.run( \"Can you give me the name of the highest-rated catering service in Gotham City?\" ) print(result) # Output: Gotham Catering Co. ``` ### Defining a Tool as a Python Class This approach involves creating a subclass of [Tool](https://huggingface.co/docs/smolagents/v1.8.1/en/reference/tools#smolagents.Tool). For complex tools, we can implement a class instead of a Python function. The class wraps the function with metadata that helps the LLM understand how to use it effectively. In this class, we define: - `name`: The tool‚Äôs name. - `description`: A description used to populate the agent‚Äôs system prompt. - `inputs`: A dictionary with keys `type` and `description`, providing information to help the Python interpreter process inputs. - `output_type`: Specifies the expected output type. - `forward`: The method containing the inference logic to execute. Below, we can see an example of a tool built using `Tool` and how to integrate it within a `CodeAgent`. #### Generating a tool to generate ideas about the superhero-themed party Alfred‚Äôs party at the mansion is a **superhero-themed event**, but he needs some creative ideas to make it truly special. As a fantastic host, he wants to surprise the guests with a unique theme. To do this, he can use an agent that generates superhero-themed party ideas based on a given category. This way, Alfred can find the perfect party theme to wow his guests. ``` from smolagents import Tool, CodeAgent, InferenceClientModel class SuperheroPartyThemeTool(Tool): name = \"superhero_party_theme_generator\" description = \"\"\" This tool suggests creative superhero-themed party ideas based on a category. It returns a unique party theme idea.\"\"\" inputs = { \"category\": { \"type\": \"string\", \"description\": \"The type of superhero party (e.g., 'classic heroes', 'villain masquerade', 'futuristic Gotham').\", } } output_type = \"string\" def forward(self, category: str): themes = { \"classic heroes\": \"Justice League Gala: Guests come dressed as their favorite DC heroes with themed cocktails like 'The Kryptonite Punch'.\", \"villain masquerade\": \"Gotham Rogues' Ball: A mysterious masquerade where guests dress as classic Batman villains.\", \"futuristic Gotham\": \"Neo-Gotham Night: A cyberpunk-style party inspired by Batman Beyond, with neon decorations and futuristic gadgets.\" } return themes.get(category.lower(), \"Themed party idea not found. Try 'classic heroes', 'villain masquerade', or 'futuristic Gotham'.\") # Instantiate the tool party_theme_tool = SuperheroPartyThemeTool() agent = CodeAgent(tools=[party_theme_tool], model=InferenceClientModel()) # Run the",
    "metadata": {
      "title": "Tools",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tools",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tools",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'classic heroes', 'villain masquerade', 'futuristic Gotham').\", } } output_type = \"string\" def forward(self, category: str): themes = { \"classic heroes\": \"Justice League Gala: Guests come dressed as their favorite DC heroes with themed cocktails like 'The Kryptonite Punch'.\", \"villain masquerade\": \"Gotham Rogues' Ball: A mysterious masquerade where guests dress as classic Batman villains.\", \"futuristic Gotham\": \"Neo-Gotham Night: A cyberpunk-style party inspired by Batman Beyond, with neon decorations and futuristic gadgets.\" } return themes.get(category.lower(), \"Themed party idea not found. Try 'classic heroes', 'villain masquerade', or 'futuristic Gotham'.\") # Instantiate the tool party_theme_tool = SuperheroPartyThemeTool() agent = CodeAgent(tools=[party_theme_tool], model=InferenceClientModel()) # Run the agent to generate a party theme idea result = agent.run( \"What would be a good superhero party idea for a 'villain masquerade' theme?\" ) print(result) # Output: \"Gotham Rogues' Ball: A mysterious masquerade where guests dress as classic Batman villains.\" ``` With this tool, Alfred will be the ultimate super host, impressing his guests with a superhero-themed party they won‚Äôt forget! ü¶∏‚Äç‚ôÇÔ∏èü¶∏‚Äç‚ôÄÔ∏è ## Default Toolbox `smolagents` comes with a set of pre-built tools that can be directly injected into your agent. The [default toolbox](https://huggingface.co/docs/smolagents/guided_tour?build-a-tool=Decorate+a+function+with+%40tool#default-toolbox) includes: - **PythonInterpreterTool** - **FinalAnswerTool** - **UserInputTool** - **DuckDuckGoSearchTool** - **GoogleSearchTool** - **VisitWebpageTool** Alfred could use various tools to ensure a flawless party at Wayne Manor: - First, he could use the `DuckDuckGoSearchTool` to find creative superhero-themed party ideas. - For catering, he‚Äôd rely on the `GoogleSearchTool` to find the highest-rated services in Gotham. - To manage seating arrangements, Alfred could run calculations with the `PythonInterpreterTool`. - Once everything is gathered, he‚Äôd compile the plan using the `FinalAnswerTool`. With these tools, Alfred guarantees the party is both exceptional and seamless. ü¶áüí° ## Sharing and Importing Tools One of the most powerful features of **smolagents** is its ability to share custom tools on the Hub and seamlessly integrate tools created by the community. This includes connecting with **HF Spaces** and **LangChain tools**, significantly enhancing Alfred‚Äôs ability to orchestrate an unforgettable party at Wayne Manor. üé≠ With these integrations, Alfred can tap into advanced event-planning tools‚Äîwhether it‚Äôs adjusting the lighting for the perfect ambiance, curating the ideal playlist for the party, or coordinating with Gotham‚Äôs finest caterers. Here are examples showcasing how these functionalities can elevate the party experience: ### Sharing a Tool to the Hub Sharing your custom tool with the community is easy! Simply upload it to your Hugging Face account using the `push_to_hub()` method. For instance, Alfred can share his `party_theme_tool` to help others find the best catering services in Gotham. Here‚Äôs how to do it: ``` party_theme_tool.push_to_hub(\"{your_username}/party_theme_tool\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\") ``` ### Importing a Tool from the Hub You can easily import tools created by other users using the `load_tool()` function. For example, Alfred might want to generate a promotional image for the party using AI. Instead of building a tool from scratch, he can leverage a predefined one from the community: ``` from smolagents import load_tool, CodeAgent, InferenceClientModel image_generation_tool = load_tool( \"m-ric/text-to-image\", trust_remote_code=True ) agent = CodeAgent( tools=[image_generation_tool], model=InferenceClientModel() ) agent.run(\"Generate an image of a",
    "metadata": {
      "title": "Tools",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tools",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tools",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "instance, Alfred can share his `party_theme_tool` to help others find the best catering services in Gotham. Here‚Äôs how to do it: ``` party_theme_tool.push_to_hub(\"{your_username}/party_theme_tool\", token=\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\") ``` ### Importing a Tool from the Hub You can easily import tools created by other users using the `load_tool()` function. For example, Alfred might want to generate a promotional image for the party using AI. Instead of building a tool from scratch, he can leverage a predefined one from the community: ``` from smolagents import load_tool, CodeAgent, InferenceClientModel image_generation_tool = load_tool( \"m-ric/text-to-image\", trust_remote_code=True ) agent = CodeAgent( tools=[image_generation_tool], model=InferenceClientModel() ) agent.run(\"Generate an image of a luxurious superhero-themed party at Wayne Manor with made-up superheros.\") ``` ### Importing a Hugging Face Space as a Tool You can also import a HF Space as a tool using `Tool.from_space()`. This opens up possibilities for integrating with thousands of spaces from the community for tasks from image generation to data analysis. The tool will connect with the spaces Gradio backend using the `gradio_client`, so make sure to install it via `pip` if you don‚Äôt have it already. For the party, Alfred can use an existing HF Space for the generation of the AI-generated image to be used in the announcement (instead of the pre-built tool we mentioned before). Let‚Äôs build it! ``` from smolagents import CodeAgent, InferenceClientModel, Tool image_generation_tool = Tool.from_space( \"black-forest-labs/FLUX.1-schnell\", name=\"image_generator\", description=\"Generate an image from a prompt\" ) model = InferenceClientModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\") agent = CodeAgent(tools=[image_generation_tool], model=model) agent.run( \"Improve this prompt, then generate an image of it.\", additional_args={'user_prompt': 'A grand superhero-themed party at Wayne Manor, with Alfred overseeing a luxurious gala'} ) ``` ### Importing a LangChain Tool We‚Äôll discuss the `LangChain` framework in upcoming sections. For now, we just note that we can reuse LangChain tools in your smolagents workflow! You can easily load LangChain tools using the `Tool.from_langchain()` method. Alfred, ever the perfectionist, is preparing for a spectacular superhero night at Wayne Manor while the Waynes are away. To make sure every detail exceeds expectations, he taps into LangChain tools to find top-tier entertainment ideas. By using `Tool.from_langchain()`, Alfred effortlessly adds advanced search functionalities to his smolagent, enabling him to discover exclusive party ideas and services with just a few commands. Here‚Äôs how he does it: We first need to install the `langchain` integration for `smolagents`. ``` pip install -U langchain-community ``` After installing the langchain integration, make sure to set your SerpAPI key. This is required for search-based tools such as `SerpAPITool`: ``` os.environ['SERPAPI_API_KEY'] = '...' ``` You can create a SerpAPI key [here](https://serpapi.com/) ``` from langchain.agents import load_tools from smolagents import CodeAgent, InferenceClientModel, Tool search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0]) agent = CodeAgent(tools=[search_tool], model=model) agent.run(\"Search for luxury entertainment ideas for a superhero-themed event, such as live performances and interactive experiences.\") ``` ### Importing a tool collection from any MCP server `smolagents` also allows importing tools from the hundreds of MCP servers available on [glama.ai](https://glama.ai/mcp/servers) or [smithery.ai](https://smithery.ai). If you want to dive deeper about MCP, you can check our [free MCP Course](https://huggingface.co/learn/mcp-course/). Install mcp client We first need to",
    "metadata": {
      "title": "Tools",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tools",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tools",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for search-based tools such as `SerpAPITool`: ``` os.environ['SERPAPI_API_KEY'] = '...' ``` You can create a SerpAPI key [here](https://serpapi.com/) ``` from langchain.agents import load_tools from smolagents import CodeAgent, InferenceClientModel, Tool search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0]) agent = CodeAgent(tools=[search_tool], model=model) agent.run(\"Search for luxury entertainment ideas for a superhero-themed event, such as live performances and interactive experiences.\") ``` ### Importing a tool collection from any MCP server `smolagents` also allows importing tools from the hundreds of MCP servers available on [glama.ai](https://glama.ai/mcp/servers) or [smithery.ai](https://smithery.ai). If you want to dive deeper about MCP, you can check our [free MCP Course](https://huggingface.co/learn/mcp-course/). Install mcp client We first need to install the `mcp` integration for `smolagents`. ``` pip install \"smolagents[mcp]\" ``` The MCP servers tools can be loaded in a ToolCollection object as follow: ``` import os from smolagents import ToolCollection, CodeAgent from mcp import StdioServerParameters from smolagents import InferenceClientModel model = InferenceClientModel(\"Qwen/Qwen2.5-Coder-32B-Instruct\") server_parameters = StdioServerParameters( command=\"uvx\", args=[\"--quiet\", \"pubmedmcp@0.1.3\"], env={\"UV_PYTHON\": \"3.12\", **os.environ}, ) with ToolCollection.from_mcp(server_parameters, trust_remote_code=True) as tool_collection: agent = CodeAgent(tools=[*tool_collection.tools], model=model, add_base_tools=True) agent.run(\"Please find a remedy for hangover.\") ``` With this setup, Alfred can quickly discover luxurious entertainment options, ensuring Gotham‚Äôs elite guests have an unforgettable experience. This tool helps him curate the perfect superhero-themed event for Wayne Manor! üéâ ## Resources - [Tools Tutorial](https://huggingface.co/docs/smolagents/tutorials/tools) - Explore this tutorial to learn how to work with tools effectively. - [Tools Documentation](https://huggingface.co/docs/smolagents/v1.8.1/en/reference/tools) - Comprehensive reference documentation on tools. - [Tools Guided Tour](https://huggingface.co/docs/smolagents/v1.8.1/en/guided_tour#tools) - A step-by-step guided tour to help you build and utilize tools efficiently. - [Building Effective Agents](https://huggingface.co/docs/smolagents/tutorials/building_good_agents) - A detailed guide on best practices for developing reliable and high-performance custom function agents.",
    "metadata": {
      "title": "Tools",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/tools",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/tools",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Vision Agents with smolagents > The examples in this section require access to a powerful VLM model. We tested them using the GPT-4o API.However,Why use smolagentsdiscusses alternative solutions supported by smolagents and Hugging Face. If you‚Äôd like to explore other options, be sure to check that section. Empowering agents with visual capabilities is crucial for solving tasks that go beyond text processing. Many real-world challenges, such as web browsing or document understanding, require analyzing rich visual content. Fortunately, `smolagents` provides built-in support for vision-language models (VLMs), enabling agents to process and interpret images effectively. In this example, imagine Alfred, the butler at Wayne Manor, is tasked with verifying the identities of the guests attending the party. As you can imagine, Alfred may not be familiar with everyone arriving. To help him, we can use an agent that verifies their identity by searching for visual information about their appearance using a VLM. This will allow Alfred to make informed decisions about who can enter. Let‚Äôs build this example! ## Providing Images at the Start of the Agent‚Äôs Execution > You can follow the code inthis notebookthat you can run using Google Colab. In this approach, images are passed to the agent at the start and stored as `task_images` alongside the task prompt. The agent then processes these images throughout its execution. Consider the case where Alfred wants to verify the identities of the superheroes attending the party. He already has a dataset of images from previous parties with the names of the guests. Given a new visitor‚Äôs image, the agent can compare it with the existing dataset and make a decision about letting them in. In this case, a guest is trying to enter, and Alfred suspects that this visitor might be The Joker impersonating Wonder Woman. Alfred needs to verify their identity to prevent anyone unwanted from entering. Let‚Äôs build the example. First, the images are loaded. In this case, we use images from Wikipedia to keep the example minimal, but imagine the possible use-case! ``` from PIL import Image import requests from io import BytesIO image_urls = [ \"https://upload.wikimedia.org/wikipedia/commons/e/e8/The_Joker_at_Wax_Museum_Plus.jpg\", # Joker image \"https://upload.wikimedia.org/wikipedia/en/9/98/Joker_%28DC_Comics_character%29.jpg\" # Joker image ] images = [] for url in image_urls: headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\" } response = requests.get(url,headers=headers) image = Image.open(BytesIO(response.content)).convert(\"RGB\") images.append(image) ``` Now that we have the images, the agent will tell us whether one guest is actually a superhero (Wonder Woman) or a villain (The Joker). ``` from smolagents import CodeAgent, OpenAIServerModel model = OpenAIServerModel(model_id=\"gpt-4o\") # Instantiate the agent agent = CodeAgent( tools=[], model=model, max_steps=20, verbosity_level=2 ) response = agent.run( \"\"\" Describe the costume and makeup that the comic character in these photos is wearing and return the description. Tell me if the guest is The Joker or Wonder Woman. \"\"\", images=images ) ``` In the case of my run, the output is the following, although it could vary in your case, as we‚Äôve already discussed: ``` { 'Costume and",
    "metadata": {
      "title": "Vision Agents with smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/vision_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/vision_agents",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/vision_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tell us whether one guest is actually a superhero (Wonder Woman) or a villain (The Joker). ``` from smolagents import CodeAgent, OpenAIServerModel model = OpenAIServerModel(model_id=\"gpt-4o\") # Instantiate the agent agent = CodeAgent( tools=[], model=model, max_steps=20, verbosity_level=2 ) response = agent.run( \"\"\" Describe the costume and makeup that the comic character in these photos is wearing and return the description. Tell me if the guest is The Joker or Wonder Woman. \"\"\", images=images ) ``` In the case of my run, the output is the following, although it could vary in your case, as we‚Äôve already discussed: ``` { 'Costume and Makeup - First Image': ( 'Purple coat and a purple silk-like cravat or tie over a mustard-yellow shirt.', 'White face paint with exaggerated features, dark eyebrows, blue eye makeup, red lips forming a wide smile.' ), 'Costume and Makeup - Second Image': ( 'Dark suit with a flower on the lapel, holding a playing card.', 'Pale skin, green hair, very red lips with an exaggerated grin.' ), 'Character Identity': 'This character resembles known depictions of The Joker from comic book media.' } ``` In this case, the output reveals that the person is impersonating someone else, so we can prevent The Joker from entering the party! ## Providing Images with Dynamic Retrieval > You can follow the code inthis Python file The previous approach is valuable and has many potential use cases. However, in situations where the guest is not in the database, we need to explore other ways of identifying them. One possible solution is to dynamically retrieve images and information from external sources, such as browsing the web for details. In this approach, images are dynamically added to the agent‚Äôs memory during execution. As we know, agents in `smolagents` are based on the `MultiStepAgent` class, which is an abstraction of the ReAct framework. This class operates in a structured cycle where various variables and knowledge are logged at different stages: 1. **SystemPromptStep:** Stores the system prompt. 2. **TaskStep:** Logs the user query and any provided input. 3. **ActionStep:** Captures logs from the agent‚Äôs actions and results. This structured approach allows agents to incorporate visual information dynamically and respond adaptively to evolving tasks. Below is the diagram we‚Äôve already seen, illustrating the dynamic workflow process and how different steps integrate within the agent lifecycle. When browsing, the agent can take screenshots and save them as `observation_images` in the `ActionStep`. ![Dynamic image retrieval](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/smolagents-can-see/diagram_adding_vlms_smolagents.png) Now that we understand the need, let‚Äôs build our complete example. In this case, Alfred wants full control over the guest verification process, so browsing for details becomes a viable solution. To complete this example, we need a new set of tools for the agent. Additionally, we‚Äôll use Selenium and Helium, which are browser automation tools. This will allow us to build an agent that explores the web, searching for details about a potential guest and retrieving verification information. Let‚Äôs install the tools needed: ``` pip install \"smolagents[all]\" helium selenium python-dotenv ``` We‚Äôll need a set of",
    "metadata": {
      "title": "Vision Agents with smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/vision_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/vision_agents",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/vision_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "![Dynamic image retrieval](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/smolagents-can-see/diagram_adding_vlms_smolagents.png) Now that we understand the need, let‚Äôs build our complete example. In this case, Alfred wants full control over the guest verification process, so browsing for details becomes a viable solution. To complete this example, we need a new set of tools for the agent. Additionally, we‚Äôll use Selenium and Helium, which are browser automation tools. This will allow us to build an agent that explores the web, searching for details about a potential guest and retrieving verification information. Let‚Äôs install the tools needed: ``` pip install \"smolagents[all]\" helium selenium python-dotenv ``` We‚Äôll need a set of agent tools specifically designed for browsing, such as `search_item_ctrl_f`, `go_back`, and `close_popups`. These tools allow the agent to act like a person navigating the web. ``` @tool def search_item_ctrl_f(text: str, nth_result: int = 1) -> str: \"\"\" Searches for text on the current page via Ctrl + F and jumps to the nth occurrence. Args: text: The text to search for nth_result: Which occurrence to jump to (default: 1) \"\"\" elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{text}')]\") if nth_result > len(elements): raise Exception(f\"Match n¬∞{nth_result} not found (only {len(elements)} matches found)\") result = f\"Found {len(elements)} matches for '{text}'.\" elem = elements[nth_result - 1] driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem) result += f\"Focused on element {nth_result} of {len(elements)}\" return result @tool def go_back() -> None: \"\"\"Goes back to previous page.\"\"\" driver.back() @tool def close_popups() -> str: \"\"\" Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows! This does not work on cookie consent banners. \"\"\" webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform() ``` We also need functionality for saving screenshots, as this will be an essential part of what our VLM agent uses to complete the task. This functionality captures the screenshot and saves it in `step_log.observations_images = [image.copy()]`, allowing the agent to store and process the images dynamically as it navigates. ``` def save_screenshot(step_log: ActionStep, agent: CodeAgent) -> None: sleep(1.0) # Let JavaScript animations happen before taking the screenshot driver = helium.get_driver() current_step = step_log.step_number if driver is not None: for step_logs in agent.logs: # Remove previous screenshots from logs for lean processing if isinstance(step_log, ActionStep) and step_log.step_number <= current_step - 2: step_logs.observations_images = None png_bytes = driver.get_screenshot_as_png() image = Image.open(BytesIO(png_bytes)) print(f\"Captured a browser screenshot: {image.size} pixels\") step_log.observations_images = [image.copy()] # Create a copy to ensure it persists, important! # Update observations with current URL url_info = f\"Current url: {driver.current_url}\" step_log.observations = url_info if step_logs.observations is None else step_log.observations + \"\\n\" + url_info return ``` This function is passed to the agent as `step_callback`, as it‚Äôs triggered at the end of each step during the agent‚Äôs execution. This allows the agent to dynamically capture and store screenshots throughout its process. Now, we can generate our vision agent for browsing the web, providing it with the tools we created, along with the `DuckDuckGoSearchTool` to explore the web. This tool will help the agent retrieve necessary information for verifying guests‚Äô identities based on visual cues. ``` from smolagents import CodeAgent, OpenAIServerModel, DuckDuckGoSearchTool model = OpenAIServerModel(model_id=\"gpt-4o\") agent = CodeAgent( tools=[DuckDuckGoSearchTool(),",
    "metadata": {
      "title": "Vision Agents with smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/vision_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/vision_agents",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/vision_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "else step_log.observations + \"\\n\" + url_info return ``` This function is passed to the agent as `step_callback`, as it‚Äôs triggered at the end of each step during the agent‚Äôs execution. This allows the agent to dynamically capture and store screenshots throughout its process. Now, we can generate our vision agent for browsing the web, providing it with the tools we created, along with the `DuckDuckGoSearchTool` to explore the web. This tool will help the agent retrieve necessary information for verifying guests‚Äô identities based on visual cues. ``` from smolagents import CodeAgent, OpenAIServerModel, DuckDuckGoSearchTool model = OpenAIServerModel(model_id=\"gpt-4o\") agent = CodeAgent( tools=[DuckDuckGoSearchTool(), go_back, close_popups, search_item_ctrl_f], model=model, additional_authorized_imports=[\"helium\"], step_callbacks=[save_screenshot], max_steps=20, verbosity_level=2, ) ``` With that, Alfred is ready to check the guests‚Äô identities and make informed decisions about whether to let them into the party: ``` agent.run(\"\"\" I am Alfred, the butler of Wayne Manor, responsible for verifying the identity of guests at party. A superhero has arrived at the entrance claiming to be Wonder Woman, but I need to confirm if she is who she says she is. Please search for images of Wonder Woman and generate a detailed visual description based on those images. Additionally, navigate to Wikipedia to gather key details about her appearance. With this information, I can determine whether to grant her access to the event. \"\"\" + helium_instructions) ``` You can see that we include `helium_instructions` as part of the task. This special prompt is aimed to control the navigation of the agent, ensuring that it follows the correct steps while browsing the web. Let‚Äôs see how this works in the video below: This is the final output: ``` Final answer: Wonder Woman is typically depicted wearing a red and gold bustier, blue shorts or skirt with white stars, a golden tiara, silver bracelets, and a golden Lasso of Truth. She is Princess Diana of Themyscira, known as Diana Prince in the world of men. ``` With all of that, we‚Äôve successfully created our identity verifier for the party! Alfred now has the necessary tools to ensure only the right guests make it through the door. Everything is set to have a good time at Wayne Manor! ## Further Reading - [We just gave sight to smolagents](https://huggingface.co/blog/smolagents-can-see) - Blog describing the vision agent functionality. - [Web Browser Automation with Agents ü§ñüåê](https://huggingface.co/docs/smolagents/examples/web_browser) - Example for Web browsing using a vision agent. - [Web Browser Vision Agent Example](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py) - Example for Web browsing using a vision agent.",
    "metadata": {
      "title": "Vision Agents with smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/vision_agents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/vision_agents",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/vision_agents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "![smolagents banner](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/license_to_call.png) # Why use smolagents In this module, we will explore the pros and cons of using [smolagents](https://huggingface.co/docs/smolagents/en/index), helping you make an informed decision about whether it‚Äôs the right framework for your needs. ## What is smolagents ? `smolagents` is a simple yet powerful framework for building AI agents. It provides LLMs with the *agency* to interact with the real world, such as searching or generating images. As we learned in unit 1, AI agents are programs that use LLMs to generate **‚Äòthoughts‚Äô** based on **‚Äòobservations‚Äô** to perform **‚Äòactions‚Äô**. Let‚Äôs explore how this is implemented in smolagents. ### Key Advantages of smolagents - **Simplicity:** Minimal code complexity and abstractions, to make the framework easy to understand, adopt and extend - **Flexible LLM Support:** Works with any LLM through integration with Hugging Face tools and external APIs - **Code-First Approach:** First-class support for Code Agents that write their actions directly in code, removing the need for parsing and simplifying tool calling - **HF Hub Integration:** Seamless integration with the Hugging Face Hub, allowing the use of Gradio Spaces as tools ### When to use smolagents? With these advantages in mind, when should we use smolagents over other frameworks? smolagents is ideal when: - You need a **lightweight and minimal solution.** - You want to **experiment quickly** without complex configurations. - Your **application logic is straightforward.** ### Code vs. JSON Actions Unlike other frameworks where agents write actions in JSON, `smolagents` **focuses on tool calls in code**, simplifying the execution process. This is because there‚Äôs no need to parse the JSON in order to build code that calls the tools: the output can be executed directly. The following diagram illustrates this difference: ![Code vs. JSON actions](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png) To review the difference between Code vs JSON Actions, you can revisit [the Actions Section in Unit 1](https://huggingface.co/learn/agents-course/unit1/actions#actions-enabling-the-agent-to-engage-with-its-environment). ### Agent Types in smolagents Agents in `smolagents` operate as **multi-step agents**. Each [MultiStepAgent](https://huggingface.co/docs/smolagents/main/en/reference/agents#smolagents.MultiStepAgent) performs: - One thought - One tool call and execution In addition to using **CodeAgent** as the primary type of agent, smolagents also supports **ToolCallingAgent**, which writes tool calls in JSON. We will explore each agent type in more detail in the following sections. > In smolagents, tools are defined using@tooldecorator wrapping a Python function or theToolclass. ### Model Integration in smolagents `smolagents` supports flexible LLM integration, allowing you to use any callable model that meets [certain criteria](https://huggingface.co/docs/smolagents/main/en/reference/models). The framework provides several predefined classes to simplify model connections: - **TransformersModel:** Implements a local `transformers` pipeline for seamless integration. - **InferenceClientModel:** Supports [serverless inference](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference) calls through [Hugging Face‚Äôs infrastructure](https://huggingface.co/docs/api-inference/index), or via a growing number of [third-party inference providers](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference#supported-providers-and-tasks). - **LiteLLMModel:** Leverages [LiteLLM](https://www.litellm.ai/) for lightweight model interactions. - **OpenAIServerModel:** Connects to any service that offers an OpenAI API interface. - **AzureOpenAIServerModel:** Supports integration with any Azure OpenAI deployment. This flexibility ensures that developers can choose the model and service most suitable for their specific use cases, and allows for easy experimentation. Now that we understood why and when to use smolagents, let‚Äôs dive deeper into",
    "metadata": {
      "title": "Why use smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/why_use_smolagents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/why_use_smolagents",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/why_use_smolagents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "classes to simplify model connections: - **TransformersModel:** Implements a local `transformers` pipeline for seamless integration. - **InferenceClientModel:** Supports [serverless inference](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference) calls through [Hugging Face‚Äôs infrastructure](https://huggingface.co/docs/api-inference/index), or via a growing number of [third-party inference providers](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference#supported-providers-and-tasks). - **LiteLLMModel:** Leverages [LiteLLM](https://www.litellm.ai/) for lightweight model interactions. - **OpenAIServerModel:** Connects to any service that offers an OpenAI API interface. - **AzureOpenAIServerModel:** Supports integration with any Azure OpenAI deployment. This flexibility ensures that developers can choose the model and service most suitable for their specific use cases, and allows for easy experimentation. Now that we understood why and when to use smolagents, let‚Äôs dive deeper into this powerful library! ## Resources - [smolagents Blog](https://huggingface.co/blog/smolagents) - Introduction to smolagents and code interactions",
    "metadata": {
      "title": "Why use smolagents",
      "url": "https://huggingface.co/learn/agents-course/unit2/smolagents/why_use_smolagents",
      "course": "agents-course",
      "chapter": "Unit 2.1 The smolagents framework",
      "chapter_id": "unit2/smolagents/why_use_smolagents",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit2/smolagents/why_use_smolagents.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Creating Your Gala Agent Now that we‚Äôve built all the necessary components for Alfred, it‚Äôs time to bring everything together into a complete agent that can help host our extravagant gala. In this section, we‚Äôll combine the guest information retrieval, web search, weather information, and Hub stats tools into a single powerful agent. ## Assembling Alfred: The Complete Agent Instead of reimplementing all the tools we‚Äôve created in previous sections, we‚Äôll import them from their respective modules which we saved in the `tools.py` and `retriever.py` files. > If you haven‚Äôt implemented the tools yet, go back to thetoolsandretrieversections to implement them, and add them to thetools.pyandretriever.pyfiles. Let‚Äôs import the necessary libraries and tools from the previous sections: smolagents llama-index langgraph ``` # Import necessary libraries import random from smolagents import CodeAgent, InferenceClientModel # Import our custom tools from their modules from tools import DuckDuckGoSearchTool, WeatherInfoTool, HubStatsTool from retriever import load_guest_dataset ``` Now, let‚Äôs combine all these tools into a single agent: ``` # Initialize the Hugging Face model model = InferenceClientModel() # Initialize the web search tool search_tool = DuckDuckGoSearchTool() # Initialize the weather tool weather_info_tool = WeatherInfoTool() # Initialize the Hub stats tool hub_stats_tool = HubStatsTool() # Load the guest dataset and initialize the guest info tool guest_info_tool = load_guest_dataset() # Create Alfred with all the tools alfred = CodeAgent( tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], model=model, add_base_tools=True, # Add any additional base tools planning_interval=3 # Enable planning every 3 steps ) ``` Your agent is now ready to use! ## Using Alfred: End-to-End Examples Now that Alfred is fully equipped with all the necessary tools, let‚Äôs see how he can help with various tasks during the gala. ### Example 1: Finding Guest Information Let‚Äôs see how Alfred can help us with our guest information. smolagents llama-index langgraph ``` query = \"Tell me about 'Lady Ada Lovelace'\" response = alfred.run(query) print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: Based on the information I retrieved, Lady Ada Lovelace is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com. ``` ### Example 2: Checking the Weather for Fireworks Let‚Äôs see how Alfred can help us with the weather. smolagents llama-index langgraph ``` query = \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\" response = alfred.run(query) print(\"üé© Alfred's Response:\") print(response) ``` Expected output (will vary due to randomness): ``` üé© Alfred's Response: I've checked the weather in Paris for you. Currently, it's clear with a temperature of 25¬∞C. These conditions are perfect for the fireworks display tonight. The clear skies will provide excellent visibility for the spectacular show, and the comfortable temperature will ensure the guests can enjoy the outdoor event without discomfort. ``` ### Example 3: Impressing AI Researchers Let‚Äôs see how Alfred can help us impress AI researchers. smolagents llama-index langgraph ``` query",
    "metadata": {
      "title": "Creating Your Gala Agent",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/agent",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/agent",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Will it be suitable for our fireworks display?\" response = alfred.run(query) print(\"üé© Alfred's Response:\") print(response) ``` Expected output (will vary due to randomness): ``` üé© Alfred's Response: I've checked the weather in Paris for you. Currently, it's clear with a temperature of 25¬∞C. These conditions are perfect for the fireworks display tonight. The clear skies will provide excellent visibility for the spectacular show, and the comfortable temperature will ensure the guests can enjoy the outdoor event without discomfort. ``` ### Example 3: Impressing AI Researchers Let‚Äôs see how Alfred can help us impress AI researchers. smolagents llama-index langgraph ``` query = \"One of our guests is from Qwen. What can you tell me about their most popular model?\" response = alfred.run(query) print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: The most popular Qwen model is Qwen/Qwen2.5-VL-7B-Instruct with 3,313,345 downloads. ``` ### Example 4: Combining Multiple Tools Let‚Äôs see how Alfred can help us prepare for a conversation with Dr. Nikola Tesla. smolagents llama-index langgraph ``` query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\" response = alfred.run(query) print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: I've gathered information to help you prepare for your conversation with Dr. Nikola Tesla. Guest Information: Name: Dr. Nikola Tesla Relation: old friend from university days Description: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk. Email: nikola.tesla@gmail.com Recent Advancements in Wireless Energy: Based on my web search, here are some recent developments in wireless energy transmission: 1. Researchers have made progress in long-range wireless power transmission using focused electromagnetic waves 2. Several companies are developing resonant inductive coupling technologies for consumer electronics 3. There are new applications in electric vehicle charging without physical connections Conversation Starters: 1. \"I'd love to hear about your new patent on wireless energy transmission. How does it compare to your original concepts from our university days?\" 2. \"Have you seen the recent developments in resonant inductive coupling for consumer electronics? What do you think of their approach?\" 3. \"How are your pigeons doing? I remember your fascination with them.\" This should give you plenty to discuss with Dr. Tesla while demonstrating your knowledge of his interests and recent developments in his field. ``` ## Advanced Features: Conversation Memory To make Alfred even more helpful during the gala, we can enable conversation memory so he remembers previous interactions: smolagents llama-index langgraph ``` # Create Alfred with conversation memory alfred_with_memory = CodeAgent( tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], model=model, add_base_tools=True, planning_interval=3 ) # First interaction response1 = alfred_with_memory.run(\"Tell me about Lady Ada Lovelace.\") print(\"üé© Alfred's First Response:\") print(response1) # Second interaction (referencing the first) response2 = alfred_with_memory.run(\"What projects is she currently working on?\", reset=False) print(\"üé© Alfred's Second Response:\")",
    "metadata": {
      "title": "Creating Your Gala Agent",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/agent",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/agent",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to discuss with Dr. Tesla while demonstrating your knowledge of his interests and recent developments in his field. ``` ## Advanced Features: Conversation Memory To make Alfred even more helpful during the gala, we can enable conversation memory so he remembers previous interactions: smolagents llama-index langgraph ``` # Create Alfred with conversation memory alfred_with_memory = CodeAgent( tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], model=model, add_base_tools=True, planning_interval=3 ) # First interaction response1 = alfred_with_memory.run(\"Tell me about Lady Ada Lovelace.\") print(\"üé© Alfred's First Response:\") print(response1) # Second interaction (referencing the first) response2 = alfred_with_memory.run(\"What projects is she currently working on?\", reset=False) print(\"üé© Alfred's Second Response:\") print(response2) ``` Notice that none of these three agent approaches directly couple memory with the agent. Is there a specific reason for this design choice üßê? - smolagents: Memory is not preserved across different execution runs, you must explicitly state it using `reset=False`. - LlamaIndex: Requires explicitly adding a context object for memory management within a run. - LangGraph: Offers options to retrieve previous messages or utilize a dedicated [MemorySaver](https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-3-adding-memory-to-the-chatbot) component. ## Conclusion Congratulations! You‚Äôve successfully built Alfred, a sophisticated agent equipped with multiple tools to help host the most extravagant gala of the century. Alfred can now: 1. Retrieve detailed information about guests 2. Check weather conditions for planning outdoor activities 3. Provide insights about influential AI builders and their models 4. Search the web for the latest information 5. Maintain conversation context with memory With these capabilities, Alfred is ready to ensure your gala is a resounding success, impressing guests with personalized attention and up-to-date information.",
    "metadata": {
      "title": "Creating Your Gala Agent",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/agent",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/agent",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/agent.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Agentic Retrieval Augmented Generation (RAG)\n\n \nIn this unit, we‚Äôll be taking a look at how we can use Agentic RAG to help Alfred prepare for the amazing gala.\n \n> We know we‚Äôve already discussed Retrieval Augmented Generation (RAG) and agentic RAG in the previous unit, so feel free to skip ahead if you‚Äôre already familiar with the concepts.\n \nLLMs are trained on enormous bodies of data to learn general knowledge.\nHowever, the world knowledge model of LLMs may not always be relevant and up-to-date information.\n**RAG solves this problem by finding and retrieving relevant information from your data and forwarding that to the LLM.**\n \n![RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/rag.png)\n \nNow, think about how Alfred works:\n \n1. We‚Äôve asked Alfred to help plan a gala\n2. Alfred needs to find the latest news and weather information\n3. Alfred needs to structure and search the guest information\n \nJust as Alfred needs to search through your household information to be helpful, any agent needs a way to find and understand relevant data.\n**Agentic RAG is a powerful way to use agents to answer questions about your data.** We can pass various tools to Alfred to help him answer questions.\nHowever, instead of answering the question on top of documents automatically, Alfred can decide to use any other tool or flow to answer the question.\n \n![Agentic RAG](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/agentic-rag.png)\n \nLet‚Äôs start **building our agentic RAG workflow!**\n \nFirst, we‚Äôll create a RAG tool to retrieve up-to-date details about the invitees. Next, we‚Äôll develop tools for web search, weather updates, and Hugging Face Hub model download statistics. Finally, we‚Äôll integrate everything to bring our agentic RAG agent to life!",
    "metadata": {
      "title": "Agentic Retrieval Augmented Generation (RAG)",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/agentic-rag",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/agentic-rag",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/agentic-rag.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nIn this unit, we‚Äôve learned how to create an agentic RAG system to help Alfred, our friendly neighborhood agent, prepare for and manage an extravagant gala.\n \nThe combination of RAG with agentic capabilities demonstrates how powerful AI assistants can become when they have:\n \n- Access to structured knowledge (guest information)\n- Ability to retrieve real-time information (web search)\n- Domain-specific tools (weather information, Hub stats)\n- Memory of past interactions\n \nWith these capabilities, Alfred is now well-equipped to be the perfect host, able to answer questions about guests, provide up-to-date information, and ensure the gala runs smoothly‚Äîeven managing the perfect timing for the fireworks display!\n \n> Now that you‚Äôve built a complete agent, you might want to explore:Creating more specialized tools for your own use casesImplementing more sophisticated RAG systems with embeddingsBuilding multi-agent systems where agents can collaborateDeploying your agent as a service that others can interact with",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/conclusion",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Use Case for Agentic RAG ![Agentic RAG banner](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit3/agentic-rag/thumbnail.jpg) In this unit, we will help Alfred, our friendly agent who is hosting the gala, by using Agentic RAG to create a tool that can be used to answer questions about the guests at the gala. > This is a ‚Äòreal-world‚Äô use case for Agentic RAG, that you could use in your own projects or workplaces. If you want to get more out of this project, why not try it out on your own use case and share in Discord? You can choose any of the frameworks discussed in the course for this use case. We provide code samples for each in separate tabs. ## A Gala to Remember Now, it‚Äôs time to get our hands dirty with an actual use case. Let‚Äôs set the stage! **You decided to host the most extravagant and opulent party of the century.** This means lavish feasts, enchanting dancers, renowned DJs, exquisite drinks, a breathtaking fireworks display, and much more. Alfred, your friendly neighbourhood agent, is getting ready to watch over all of your needs for this party, and **Alfred is going to manage everything himself**. To do so, he needs to have access to all of the information about the party, including the menu, the guests, the schedule, weather forecasts, and much more! Not only that, but he also needs to make sure that the party is going to be a success, so **he needs to be able to answer any questions about the party during the party**, whilst handling unexpected situations that may arise. He can‚Äôt do this alone, so we need to make sure that Alfred has access to all of the information and tools he needs. First, let‚Äôs give him a list of hard requirements for the gala. ## The Gala Requirements A properly educated person in the age of the **Renaissance** needs to have three main traits. He or she needed to be profound in the **knowledge of sports, culture, and science**. So, we need to make sure we can impress our guests with our knowledge and provide them with a truly unforgettable gala. However, to avoid any conflicts, there are some **topics, like politics and religion, that are to be avoided at a gala.** It needs to be a fun party without conflicts related to beliefs and ideals. According to etiquette, **a good host should be aware of guests‚Äô backgrounds**, including their interests and endeavours. A good host also gossips and shares stories about the guests with one another. Lastly, we need to make sure that we‚Äôve got **some general knowledge about the weather** to ensure we can continuously find a real-time update to ensure perfect timing to launch the fireworks and end the gala with a bang! üéÜ As you can see, Alfred needs a lot of information to host the gala. Luckily, we can help and prepare Alfred by giving him some **Retrieval Augmented Generation (RAG) training!** Let‚Äôs start by creating the tools that Alfred",
    "metadata": {
      "title": "Introduction to Use Case for Agentic RAG",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/introduction",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/introduction",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "of guests‚Äô backgrounds**, including their interests and endeavours. A good host also gossips and shares stories about the guests with one another. Lastly, we need to make sure that we‚Äôve got **some general knowledge about the weather** to ensure we can continuously find a real-time update to ensure perfect timing to launch the fireworks and end the gala with a bang! üéÜ As you can see, Alfred needs a lot of information to host the gala. Luckily, we can help and prepare Alfred by giving him some **Retrieval Augmented Generation (RAG) training!** Let‚Äôs start by creating the tools that Alfred needs to be able to host the gala!",
    "metadata": {
      "title": "Introduction to Use Case for Agentic RAG",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/introduction",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/introduction",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Creating a RAG Tool for Guest Stories Alfred, your trusted agent, is preparing for the most extravagant gala of the century. To ensure the event runs smoothly, Alfred needs quick access to up-to-date information about each guest. Let‚Äôs help Alfred by creating a custom Retrieval-Augmented Generation (RAG) tool, powered by our custom dataset. ## Why RAG for a Gala? Imagine Alfred mingling among the guests, needing to recall specific details about each person at a moment‚Äôs notice. A traditional LLM might struggle with this task because: 1. The guest list is specific to your event and not in the model‚Äôs training data 2. Guest information may change or be updated frequently 3. Alfred needs to retrieve precise details like email addresses This is where Retrieval Augmented Generation (RAG) shines! By combining a retrieval system with an LLM, Alfred can access accurate, up-to-date information about your guests on demand. > You can choose any of the frameworks covered in the course for this use case. Select your preferred option from the code tabs. ## Setting up our application In this unit, we‚Äôll develop our agent within a HF Space, as a structured Python project. This approach helps us maintain clean, modular code by organizing different functionalities into separate files. Also, this makes for a more realistic use case where you would deploy the application for public use. ### Project Structure - **tools.py** ‚Äì Provides auxiliary tools for the agent. - **retriever.py** ‚Äì Implements retrieval functions to support knowledge access. - **app.py** ‚Äì Integrates all components into a fully functional agent, which we‚Äôll finalize in the last part of this unit. For a hands-on reference, check out [this HF Space](https://huggingface.co/spaces/agents-course/Unit_3_Agentic_RAG), where the Agentic RAG developed in this unit is live. Feel free to clone it and experiment! You can directly test the agent below: ## Dataset Overview Our dataset [agents-course/unit3-invitees](https://huggingface.co/datasets/agents-course/unit3-invitees/) contains the following fields for each guest: - **Name**: Guest‚Äôs full name - **Relation**: How the guest is related to the host - **Description**: A brief biography or interesting facts about the guest - **Email Address**: Contact information for sending invitations or follow-ups Below is a preview of the dataset: > In a real-world scenario, this dataset could be expanded to include dietary preferences, gift interests, conversation topics to avoid, and other helpful details for a host. ## Building the Guestbook Tool We‚Äôll create a custom tool that Alfred can use to quickly retrieve guest information during the gala. Let‚Äôs break this down into three manageable steps: 1. Load and prepare the dataset 2. Create the Retriever Tool 3. Integrate the Tool with Alfred Let‚Äôs start with loading and preparing the dataset! ### Step 1: Load and Prepare the Dataset First, we need to transform our raw guest data into a format that‚Äôs optimized for retrieval. smolagents llama-index langgraph We will use the Hugging Face `datasets` library to load the dataset and convert it into a list of `Document` objects from the `langchain.docstore.document` module. ``` import datasets from langchain_core.documents import Document",
    "metadata": {
      "title": "Creating a RAG Tool for Guest Stories",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/invitees",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/invitees",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/invitees.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to quickly retrieve guest information during the gala. Let‚Äôs break this down into three manageable steps: 1. Load and prepare the dataset 2. Create the Retriever Tool 3. Integrate the Tool with Alfred Let‚Äôs start with loading and preparing the dataset! ### Step 1: Load and Prepare the Dataset First, we need to transform our raw guest data into a format that‚Äôs optimized for retrieval. smolagents llama-index langgraph We will use the Hugging Face `datasets` library to load the dataset and convert it into a list of `Document` objects from the `langchain.docstore.document` module. ``` import datasets from langchain_core.documents import Document # Load the dataset guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\") # Convert dataset entries into Document objects docs = [ Document( page_content=\"\\n\".join([ f\"Name: {guest['name']}\", f\"Relation: {guest['relation']}\", f\"Description: {guest['description']}\", f\"Email: {guest['email']}\" ]), metadata={\"name\": guest[\"name\"]} ) for guest in guest_dataset ] ``` In the code above, we: - Load the dataset - Convert each guest entry into a `Document` object with formatted content - Store the `Document` objects in a list This means we‚Äôve got all of our data nicely available so we can get started with configuring our retrieval. ### Step 2: Create the Retriever Tool Now, let‚Äôs create a custom tool that Alfred can use to search through our guest information. smolagents llama-index langgraph We will use the `BM25Retriever` from the `langchain_community.retrievers` module to create a retriever tool. > TheBM25Retrieveris a great starting point for retrieval, but for more advanced semantic search, you might consider using embedding-based retrievers like those fromsentence-transformers. ``` from smolagents import Tool from langchain_community.retrievers import BM25Retriever class GuestInfoRetrieverTool(Tool): name = \"guest_info_retriever\" description = \"Retrieves detailed information about gala guests based on their name or relation.\" inputs = { \"query\": { \"type\": \"string\", \"description\": \"The name or relation of the guest you want information about.\" } } output_type = \"string\" def __init__(self, docs): self.is_initialized = False self.retriever = BM25Retriever.from_documents(docs) def forward(self, query: str): results = self.retriever.get_relevant_documents(query) if results: return \"\\n\\n\".join([doc.page_content for doc in results[:3]]) else: return \"No matching guest information found.\" # Initialize the tool guest_info_tool = GuestInfoRetrieverTool(docs) ``` Let‚Äôs understand this tool step-by-step: - The `name` and `description` help the agent understand when and how to use this tool - The `inputs` define what parameters the tool expects (in this case, a search query) - We‚Äôre using a `BM25Retriever`, which is a powerful text retrieval algorithm that doesn‚Äôt require embeddings - The `forward` method processes the query and returns the most relevant guest information ### Step 3: Integrate the Tool with Alfred Finally, let‚Äôs bring everything together by creating our agent and equipping it with our custom tool: smolagents llama-index langgraph ``` from smolagents import CodeAgent, InferenceClientModel # Initialize the Hugging Face model model = InferenceClientModel() # Create Alfred, our gala agent, with the guest info tool alfred = CodeAgent(tools=[guest_info_tool], model=model) # Example query Alfred might receive during the gala response = alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\") print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: Based on the information I retrieved,",
    "metadata": {
      "title": "Creating a RAG Tool for Guest Stories",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/invitees",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/invitees",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/invitees.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "returns the most relevant guest information ### Step 3: Integrate the Tool with Alfred Finally, let‚Äôs bring everything together by creating our agent and equipping it with our custom tool: smolagents llama-index langgraph ``` from smolagents import CodeAgent, InferenceClientModel # Initialize the Hugging Face model model = InferenceClientModel() # Create Alfred, our gala agent, with the guest info tool alfred = CodeAgent(tools=[guest_info_tool], model=model) # Example query Alfred might receive during the gala response = alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\") print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: Based on the information I retrieved, Lady Ada Lovelace is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com. ``` What‚Äôs happening in this final step: - We initialize a Hugging Face model using the `InferenceClientModel` class - We create our agent (Alfred) as a `CodeAgent`, which can execute Python code to solve problems - We ask Alfred to retrieve information about a guest named ‚ÄúLady Ada Lovelace‚Äù ## Example Interaction During the gala, a conversation might flow like this: **You:** ‚ÄúAlfred, who is that gentleman talking to the ambassador?‚Äù **Alfred:** *quickly searches the guest database* ‚ÄúThat‚Äôs Dr. Nikola Tesla, sir. He‚Äôs an old friend from your university days. He‚Äôs recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he‚Äôs passionate about pigeons, so that might make for good small talk.‚Äù ``` { \"name\": \"Dr. Nikola Tesla\", \"relation\": \"old friend from university days\", \"description\": \"Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\", \"email\": \"nikola.tesla@gmail.com\" } ``` ## Taking It Further Now that Alfred can retrieve guest information, consider how you might enhance this system: 1. **Improve the retriever** to use a more sophisticated algorithm like [sentence-transformers](https://www.sbert.net/) 2. **Implement a conversation memory** so Alfred remembers previous interactions 3. **Combine with web search** to get the latest information on unfamiliar guests 4. **Integrate multiple indexes** to get more complete information from verified sources Now Alfred is fully equipped to handle guest inquiries effortlessly, ensuring your gala is remembered as the most sophisticated and delightful event of the century! > Try extending the retriever tool to also return conversation starters based on each guest‚Äôs interests or background. How would you modify the tool to accomplish this?When you‚Äôre done, implement your guest retriever tool in theretriever.pyfile.",
    "metadata": {
      "title": "Creating a RAG Tool for Guest Stories",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/invitees",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/invitees",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/invitees.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building and Integrating Tools for Your Agent In this section, we‚Äôll grant Alfred access to the web, enabling him to find the latest news and global updates. Additionally, he‚Äôll have access to weather data and Hugging Face hub model download statistics, so that he can make relevant conversation about fresh topics. ## Give Your Agent Access to the Web Remember that we want Alfred to establish his presence as a true renaissance host, with a deep knowledge of the world. To do so, we need to make sure that Alfred has access to the latest news and information about the world. Let‚Äôs start by creating a web search tool for Alfred! smolagents llama-index langgraph ``` from smolagents import DuckDuckGoSearchTool # Initialize the DuckDuckGo search tool search_tool = DuckDuckGoSearchTool() # Example usage results = search_tool(\"Who's the current President of France?\") print(results) ``` Expected output: ``` The current President of France in Emmanuel Macron. ``` ## Creating a Custom Tool for Weather Information to Schedule the Fireworks The perfect gala would have fireworks over a clear sky, we need to make sure the fireworks are not cancelled due to bad weather. Let‚Äôs create a custom tool that can be used to call an external weather API and get the weather information for a given location. > For the sake of simplicity, we‚Äôre using a dummy weather API for this example. If you want to use a real weather API, you could implement a weather tool that uses the OpenWeatherMap API, like inUnit 1. smolagents llama-index langgraph ``` from smolagents import Tool import random class WeatherInfoTool(Tool): name = \"weather_info\" description = \"Fetches dummy weather information for a given location.\" inputs = { \"location\": { \"type\": \"string\", \"description\": \"The location to get weather information for.\" } } output_type = \"string\" def forward(self, location: str): # Dummy weather data weather_conditions = [ {\"condition\": \"Rainy\", \"temp_c\": 15}, {\"condition\": \"Clear\", \"temp_c\": 25}, {\"condition\": \"Windy\", \"temp_c\": 20} ] # Randomly select a weather condition data = random.choice(weather_conditions) return f\"Weather in {location}: {data['condition']}, {data['temp_c']}¬∞C\" # Initialize the tool weather_info_tool = WeatherInfoTool() ``` ## Creating a Hub Stats Tool for Influential AI Builders In attendance at the gala are the who‚Äôs who of AI builders. Alfred wants to impress them by discussing their most popular models, datasets, and spaces. We‚Äôll create a tool to fetch model statistics from the Hugging Face Hub based on a username. smolagents llama-index langgraph ``` from smolagents import Tool from huggingface_hub import list_models class HubStatsTool(Tool): name = \"hub_stats\" description = \"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\" inputs = { \"author\": { \"type\": \"string\", \"description\": \"The username of the model author/organization to find models from.\" } } output_type = \"string\" def forward(self, author: str): try: # List models from the specified author, sorted by downloads models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1)) if models: model = models[0] return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\" else: return f\"No models found for author {author}.\" except",
    "metadata": {
      "title": "Building and Integrating Tools for Your Agent",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/tools",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/tools",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` from smolagents import Tool from huggingface_hub import list_models class HubStatsTool(Tool): name = \"hub_stats\" description = \"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\" inputs = { \"author\": { \"type\": \"string\", \"description\": \"The username of the model author/organization to find models from.\" } } output_type = \"string\" def forward(self, author: str): try: # List models from the specified author, sorted by downloads models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1)) if models: model = models[0] return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\" else: return f\"No models found for author {author}.\" except Exception as e: return f\"Error fetching models for {author}: {str(e)}\" # Initialize the tool hub_stats_tool = HubStatsTool() # Example usage print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook ``` Expected output: ``` The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads. ``` With the Hub Stats Tool, Alfred can now impress influential AI builders by discussing their most popular models. ## Integrating Tools with Alfred Now that we have all the tools, let‚Äôs integrate them into Alfred‚Äôs agent: smolagents llama-index langgraph ``` from smolagents import CodeAgent, InferenceClientModel # Initialize the Hugging Face model model = InferenceClientModel() # Create Alfred with all the tools alfred = CodeAgent( tools=[search_tool, weather_info_tool, hub_stats_tool], model=model ) # Example query Alfred might receive during the gala response = alfred.run(\"What is Facebook and what's their most popular model?\") print(\"üé© Alfred's Response:\") print(response) ``` Expected output: ``` üé© Alfred's Response: Facebook is a social networking website where users can connect, share information, and interact with others. The most downloaded model by Facebook on the Hugging Face Hub is ESMFold_v1. ``` ## Conclusion By integrating these tools, Alfred is now equipped to handle a variety of tasks, from web searches to weather updates and model statistics. This ensures he remains the most informed and engaging host at the gala. > Try implementing a tool that can be used to get the latest news about a specific topic.When you‚Äôre done, implement your custom tools in thetools.pyfile.",
    "metadata": {
      "title": "Building and Integrating Tools for Your Agent",
      "url": "https://huggingface.co/learn/agents-course/unit3/agentic-rag/tools",
      "course": "agents-course",
      "chapter": "Unit 3. Use Case for Agentic RAG",
      "chapter_id": "unit3/agentic-rag/tools",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit3/agentic-rag/tools.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# And now? What topics I should learn?\n\n \nAgentic AI is a rapidly evolving field, and understanding foundational protocols is essential for building intelligent, autonomous systems.\n \nTwo important standards you should get familiar with are:\n \n- The **Model Context Protocol (MCP)**\n- The **Agent-to-Agent Protocol (A2A)**\n \n\n## üîå Model Context Protocol (MCP)\n\n \nThe **Model Context Protocol (MCP)** by Anthropic is an open standard that enables AI models to securely and seamlessly **connect with external tools, data sources, and applications**, making agents more capable and autonomous.\n \nThink of MCP as a **universal adapter**, like a USB-C port, that allows AI models to plug into various digital environments **without needing custom integration for each one**.\n \nMCP is quickly gaining traction across the industry, with major companies like OpenAI and Google beginning to adopt it.\n \nüìö Learn more:\n \n- [Anthropic‚Äôs official announcement and documentation](https://www.anthropic.com/news/model-context-protocol)\n- [MCP on Wikipedia](https://en.wikipedia.org/wiki/Model_Context_Protocol)\n- [Blog on MCP](https://huggingface.co/blog/Kseniase/mcp)\n \n\n## ü§ù Agent-to-Agent (A2A) Protocol\n\n \nGoogle has developed the **Agent-to-Agent (A2A) protocol** as a complementary counterpart to Anthropic‚Äôs Model Context Protocol (MCP).\n \nWhile MCP connects agents to external tools, **A2A connects agents to each other**, paving the way for cooperative, multi-agent systems that can work together to solve complex problems.\n \nüìö Dive deeper into A2A:\n \n- [Google‚Äôs A2A announcement](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)",
    "metadata": {
      "title": "And now? What topics I should learn?",
      "url": "https://huggingface.co/learn/agents-course/unit4/additional-readings",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/additional-readings",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/additional-readings.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \n**Congratulations on finishing the Agents Course!**\n \nThrough perseverance and dedication, you‚Äôve built a solid foundation in the world of AI Agents.\n \nBut finishing this course is **not the end of your journey**. It‚Äôs just the beginning: don‚Äôt hesitate to explore the next section where we share curated resources to help you continue learning, including advanced topics like **MCPs** and beyond.\n \n**Thank you** for being part of this course. **We hope you liked this course as much as we loved writing it**.\n \nAnd don‚Äôt forget: **Keep Learning, Stay Awesome ü§ó**",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/agents-course/unit4/conclusion",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/conclusion",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/conclusion.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Claim Your Certificate üéì\n\n \nIf you scored **above 30%, congratulations! üëè You‚Äôre now eligible to claim your official certificate.**\n \nFollow the steps below to receive it:\n \n1. Visit the [certificate page](https://huggingface.co/spaces/agents-course/Unit4-Final-Certificate).\n2. **Sign in** with your Hugging Face account using the button provided.\n3. **Enter your full name**. This is the name that will appear on your certificate.\n4. Click **‚ÄúGet My Certificate‚Äù** to verify your score and download your certificate.\n \n![Congrats!](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/congrats.png)\n \nOnce you‚Äôve got your certificate, feel free to:\n \n- Add it to your **LinkedIn profile** üßë‚Äçüíº\n- Share it on **X**, **Bluesky**, etc. üéâ\n \n**Don‚Äôt forget to tag @huggingface. We‚Äôd be super proud and we‚Äôd love to cheer you on! ü§ó**\n \n> If you have any issues with submission please open a discussion item onThe certification community tab.",
    "metadata": {
      "title": "Claim Your Certificate üéì",
      "url": "https://huggingface.co/learn/agents-course/unit4/get-your-certificate",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/get-your-certificate",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/get-your-certificate.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Hands-On\n\n \nNow that you‚Äôre ready to dive deeper into the creation of your final agent, let‚Äôs see how you can submit it for review.\n \n\n## The Dataset\n\n \nThe Dataset used in this leaderboard consist of 20 questions extracted from the level 1 questions of the **validation** set from GAIA.\n \nThe chosen question were filtered based on the number of tools and steps needed to answer a question.\n \nBased on the current look of the GAIA benchmark, we think that getting you to try to aim for 30% on level 1 question is a fair test.\n \n![GAIA current status!](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/leaderboard%20GAIA%2024%3A04%3A2025.png)\n \n\n## The process\n\n \nNow the big question in your mind is probably : ‚ÄúHow do I start submitting ?‚Äù\n \nFor this Unit, we created an API that will allow you to get the questions, and send your answers for scoring.\nHere is a summary of the routes (see the [live documentation](https://agents-course-unit4-scoring.hf.space/docs) for interactive details):\n \n- **GET /questions**: Retrieve the full list of filtered evaluation questions.\n- **GET /random-question**: Fetch a single random question from the list.\n- **GET /files/{task_id}**: Download a specific file associated with a given task ID.\n- **POST /submit**: Submit agent answers, calculate the score, and update the leaderboard.\n \nThe submit function will compare the answer to the ground truth in an **EXACT MATCH** manner, hence prompt it well ! The GAIA team shared a prompting example for your agent [here](https://huggingface.co/spaces/gaia-benchmark/leaderboard) (for the sake of this course, make sure you don‚Äôt include the text ‚ÄúFINAL ANSWER‚Äù in your submission, just make your agent reply with the answer and nothing else).\n \nüé® **Make the Template Your Own!**\n \nTo demonstrate the process of interacting with the API, we‚Äôve included a [basic template](https://huggingface.co/spaces/agents-course/Final_Assignment_Template) as a starting point.\n \nPlease feel free‚Äîand **actively encouraged**‚Äîto change, add to, or completely restructure it! Modify it in any way that best suits your approach and creativity.\n \nIn order to submit this templates compute 3 things needed by the API :\n \n- **Username:** Your Hugging Face username (here obtained via Gradio login), which is used to identify your submission.\n- **Code Link (agent_code):** the URL linking to your Hugging Face Space code (`.../tree/main`) for verification purposes, so please keep your space public.\n- **Answers (answers):** The list of responses (`{\"task_id\": ..., \"submitted_answer\": ...}`) generated by your Agent for scoring.\n \nHence we encourage you to start by duplicating this [template](https://huggingface.co/spaces/agents-course/Final_Assignment_Template) on your own huggingface profile.\n \nüèÜ Check out the leaderboard [here](https://huggingface.co/spaces/agents-course/Students_leaderboard)\n \n*A friendly note: This leaderboard is meant for fun! We know it‚Äôs possible to submit scores without full verification. If we see too many high scores posted without a public link to back them up, we might need to review, adjust, or remove some entries to keep the leaderboard useful.*\nThe leaderboard will show the link to your space code-base, since this leaderboard is for students only, please keep your space public if you get a score you‚Äôre proud of.",
    "metadata": {
      "title": "Hands-On",
      "url": "https://huggingface.co/learn/agents-course/unit4/hands-on",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/hands-on",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/hands-on.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Welcome to the final Unit\n\n \n![AI Agents Course thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/thumbnail.jpg)\n \nWelcome to the final unit of the course! üéâ\n \nSo far, you‚Äôve **built a strong foundation in AI Agents**, from understanding their components to creating your own. With this knowledge, you‚Äôre now ready to **build powerful agents** and stay up-to-date with the latest advancements in this fast-evolving field.\n \nThis unit is all about applying what you‚Äôve learned. It‚Äôs your **final hands-on project**, and completing it is your ticket to earning the **course certificate**.\n \n\n## What‚Äôs the challenge?\n\n \nYou‚Äôll create your own agent and **evaluate its performance using a subset of the GAIA benchmark**.\n \nTo successfully complete the course, your agent needs to score **30% or higher** on the benchmark. Achieve that, and you‚Äôll earn your **Certificate of Completion**, officially recognizing your expertise. üèÖ\n \nAdditionally, see how you stack up against your peers! A dedicated **Student Leaderboard** is available for you to submit your scores and see the community‚Äôs progress.\n \n> üö® Heads Up: Advanced & Hands-On UnitPlease be aware that this unit shifts towards a more practical, hands-on approach. Success in this section will requiremore advanced coding knowledgeand relies on you navigating tasks withless explicit guidancecompared to earlier parts of the course.\n \nSounds exciting? Let‚Äôs get started! üöÄ",
    "metadata": {
      "title": "Welcome to the final Unit",
      "url": "https://huggingface.co/learn/agents-course/unit4/introduction",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/introduction",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/introduction.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What is GAIA?\n\n \n[GAIA](https://huggingface.co/papers/2311.12983) is a **benchmark designed to evaluate AI assistants on real-world tasks** that require a combination of core capabilities‚Äîsuch as reasoning, multimodal understanding, web browsing, and proficient tool use.\n \nIt was introduced in the paper *‚ÄùGAIA: A Benchmark for General AI Assistants‚Äù*.\n \nThe benchmark features **466 carefully curated questions** that are **conceptually simple for humans**, yet **remarkably challenging for current AI systems**.\n \nTo illustrate the gap:\n \n- **Humans**: ~92% success rate\n- **GPT-4 with plugins**: ~15%\n- **Deep Research (OpenAI)**: 67.36% on the validation set\n \nGAIA highlights the current limitations of AI models and provides a rigorous benchmark to evaluate progress toward truly general-purpose AI assistants.\n \n\n## üå± GAIA‚Äôs Core Principles\n\n \nGAIA is carefully designed around the following pillars:\n \n- üîç **Real-world difficulty**: Tasks require multi-step reasoning, multimodal understanding, and tool interaction.\n- üßæ **Human interpretability**: Despite their difficulty for AI, tasks remain conceptually simple and easy to follow for humans.\n- üõ°Ô∏è **Non-gameability**: Correct answers demand full task execution, making brute-forcing ineffective.\n- üß∞ **Simplicity of evaluation**: Answers are concise, factual, and unambiguous‚Äîideal for benchmarking.\n \n\n## Difficulty Levels\n\n \nGAIA tasks are organized into **three levels of increasing complexity**, each testing specific skills:\n \n- **Level 1**: Requires less than 5 steps and minimal tool usage.\n- **Level 2**: Involves more complex reasoning and coordination between multiple tools and 5-10 steps.\n- **Level 3**: Demands long-term planning and advanced integration of various tools.\n \n![GAIA levels](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/gaia_levels.png)\n \n\n## Example of a Hard GAIA Question\n\n \n> Which of the fruits shown in the 2008 painting ‚ÄúEmbroidery from Uzbekistan‚Äù were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film ‚ÄúThe Last Voyage‚Äù? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o‚Äôclock position. Use the plural form of each fruit.\n \nAs you can see, this question challenges AI systems in several ways:\n \n- Requires a **structured response format**\n- Involves **multimodal reasoning** (e.g., analyzing images)\n- Demands **multi-hop retrieval** of interdependent facts:\n\n- Identifying the fruits in the painting\n- Discovering which ocean liner was used in *The Last Voyage*\n- Looking up the breakfast menu from October 1949 for that ship\n- Needs **correct sequencing** and high-level planning to solve in the right order\n \nThis kind of task highlights where standalone LLMs often fall short, making GAIA an ideal benchmark for **agent-based systems** that can reason, retrieve, and execute over multiple steps and modalities.\n \n![GAIA capabilities plot](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/gaia_capabilities.png)\n \n\n## Live Evaluation\n\n \nTo encourage continuous benchmarking, **GAIA provides a public leaderboard hosted on Hugging Face**, where you can test your models against **300 testing questions**.\n \nüëâ Check out the leaderboard [here](https://huggingface.co/spaces/gaia-benchmark/leaderboard)\n  \nWant to dive deeper into GAIA?\n \n- üìÑ [Read the full paper](https://huggingface.co/papers/2311.12983)\n- üìÑ [Deep Research release post by OpenAI](https://openai.com/index/introducing-deep-research/)\n- üìÑ [Open-source DeepResearch ‚Äì Freeing our search agents](https://huggingface.co/blog/open-deep-research)",
    "metadata": {
      "title": "What is GAIA?",
      "url": "https://huggingface.co/learn/agents-course/unit4/what-is-gaia",
      "course": "agents-course",
      "chapter": "Unit 4. Final Project - Create, Test, and Certify Your Agent",
      "chapter_id": "unit4/what-is-gaia",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/agents-course/unit4/what-is-gaia.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction Welcome to the Hugging Face course! This introduction will guide you through setting up a working environment. If you‚Äôre just starting the course, we recommend you first take a look at [Chapter 1](/course/chapter1), then come back and set up your environment so you can try the code yourself. All the libraries that we‚Äôll be using in this course are available as Python packages, so here we‚Äôll show you how to set up a Python environment and install the specific libraries you‚Äôll need. We‚Äôll cover two ways of setting up your working environment, using a Colab notebook or a Python virtual environment. Feel free to choose the one that resonates with you the most. For beginners, we strongly recommend that you get started by using a Colab notebook. Note that we will not be covering the Windows system. If you‚Äôre running on Windows, we recommend following along using a Colab notebook. If you‚Äôre using a Linux distribution or macOS, you can use either approach described here. Most of the course relies on you having a Hugging Face account. We recommend creating one now: [create an account](https://huggingface.co/join). ## Using a Google Colab notebook Using a Colab notebook is the simplest possible setup; boot up a notebook in your browser and get straight to coding! If you‚Äôre not familiar with Colab, we recommend you start by following the [introduction](https://colab.research.google.com/notebooks/intro.ipynb). Colab allows you to use some accelerating hardware, like GPUs or TPUs, and it is free for smaller workloads. Once you‚Äôre comfortable moving around in Colab, create a new notebook and get started with the setup: ![An empty colab notebook](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/new_colab.png) The next step is to install the libraries that we‚Äôll be using in this course. We‚Äôll use `pip` for the installation, which is the package manager for Python. In notebooks, you can run system commands by preceding them with the `!` character, so you can install the ü§ó Transformers library as follows: ``` !pip install transformers ``` You can make sure the package was correctly installed by importing it within your Python runtime: ``` import transformers ``` ![A gif showing the result of the two commands above: installation and import](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter0/install.gif) This installs a very light version of ü§ó Transformers. In particular, no specific machine learning frameworks (like PyTorch or TensorFlow) are installed. Since we‚Äôll be using a lot of different features of the library, we recommend installing the development version, which comes with all the required dependencies for pretty much any imaginable use case: ``` !pip install transformers[sentencepiece] ``` This will take a bit of time, but then you‚Äôll be ready to go for the rest of the course! ## Using a Python virtual environment If you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started. Once you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter0/1",
      "course": "llm-course",
      "chapter": "0. Setup",
      "chapter_id": "chapter0/1",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter0/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "with all the required dependencies for pretty much any imaginable use case: ``` !pip install transformers[sentencepiece] ``` This will take a bit of time, but then you‚Äôll be ready to go for the rest of the course! ## Using a Python virtual environment If you prefer to use a Python virtual environment, the first step is to install Python on your system. We recommend following [this guide](https://realpython.com/installing-python/) to get started. Once you have Python installed, you should be able to run Python commands in your terminal. You can start by running the following command to ensure that it is correctly installed before proceeding to the next steps: `python --version`. This should print out the Python version now available on your system. When running a Python command in your terminal, such as `python --version`, you should think of the program running your command as the ‚Äúmain‚Äù Python on your system. We recommend keeping this main installation free of any packages, and using it to create separate environments for each application you work on ‚Äî this way, each application can have its own dependencies and packages, and you won‚Äôt need to worry about potential compatibility issues with other applications. In Python this is done with [virtual environments](https://docs.python.org/3/tutorial/venv.html), which are self-contained directory trees that each contain a Python installation with a particular Python version alongside all the packages the application needs. Creating such a virtual environment can be done with a number of different tools, but we‚Äôll use the official Python package for that purpose, which is called [venv](https://docs.python.org/3/library/venv.html#module-venv). First, create the directory you‚Äôd like your application to live in ‚Äî for example, you might want to make a new directory called *transformers-course* at the root of your home directory: ``` mkdir ~/transformers-course cd ~/transformers-course ``` From inside this directory, create a virtual environment using the Python `venv` module: ``` python -m venv .env ``` You should now have a directory called *.env* in your otherwise empty folder: ``` ls -a ``` ``` . .. .env ``` You can jump in and out of your virtual environment with the `activate` and `deactivate` scripts: ``` # Activate the virtual environment source .env/bin/activate # Deactivate the virtual environment deactivate ``` You can make sure that the environment is activated by running the `which python` command: if it points to the virtual environment, then you have successfully activated it! ``` which python ``` ``` /home/<user>/transformers-course/.env/bin/python ``` ### Installing dependencies As in the previous section on using Google Colab instances, you‚Äôll now need to install the packages required to continue. Again, you can install the development version of ü§ó Transformers using the `pip` package manager: ``` pip install \"transformers[sentencepiece]\" ``` You‚Äôre now all set up and ready to go!",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter0/1",
      "course": "llm-course",
      "chapter": "0. Setup",
      "chapter_id": "chapter0/1",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter0/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction ## Welcome to the ü§ó Course! This course will teach you about large language models (LLMs) and natural language processing (NLP) using libraries from the [Hugging Face](https://huggingface.co/) ecosystem ‚Äî [ü§ó Transformers](https://github.com/huggingface/transformers), [ü§ó Datasets](https://github.com/huggingface/datasets), [ü§ó Tokenizers](https://github.com/huggingface/tokenizers), and [ü§ó Accelerate](https://github.com/huggingface/accelerate) ‚Äî as well as the [Hugging Face Hub](https://huggingface.co/models). We‚Äôll also cover libraries outside the Hugging Face ecosystem. These are amazing contributions to the AI community and incredibly useful tools. It‚Äôs completely free and without ads. ## Understanding NLP and LLMs While this course was originally focused on NLP (Natural Language Processing), it has evolved to emphasize Large Language Models (LLMs), which represent the latest advancement in the field. **What‚Äôs the difference?** - **NLP (Natural Language Processing)** is the broader field focused on enabling computers to understand, interpret, and generate human language. NLP encompasses many techniques and tasks such as sentiment analysis, named entity recognition, and machine translation. - **LLMs (Large Language Models)** are a powerful subset of NLP models characterized by their massive size, extensive training data, and ability to perform a wide range of language tasks with minimal task-specific training. Models like the Llama, GPT, or Claude series are examples of LLMs that have revolutionized what‚Äôs possible in NLP. Throughout this course, you‚Äôll learn about both traditional NLP concepts and cutting-edge LLM techniques, as understanding the foundations of NLP is crucial for working effectively with LLMs. ## What to expect? Here is a brief overview of the course: ![Brief overview of the chapters of the course.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg) ![Brief overview of the chapters of the course.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg) - Chapters 1 to 4 provide an introduction to the main concepts of the ü§ó Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the [Hugging Face Hub](https://huggingface.co/models), fine-tune it on a dataset, and share your results on the Hub! - Chapters 5 to 8 teach the basics of ü§ó Datasets and ü§ó Tokenizers before diving into classic NLP tasks and LLM techniques. By the end of this part, you will be able to tackle the most common language processing challenges by yourself. - Chapter 9 goes beyond NLP to cover how to build and share demos of your models on the ü§ó Hub. By the end of this part, you will be ready to showcase your ü§ó Transformers application to the world! - Chapters 10 to 12 dive into advanced LLM topics like fine-tuning, curating high-quality datasets, and building reasoning models. This course: - Requires a good knowledge of Python - Is better taken after an introductory deep learning course, such as [fast.ai‚Äôs](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) or one of the programs developed by [DeepLearning.AI](https://www.deeplearning.ai/) - Does not expect prior [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/) knowledge, though some familiarity with either of those will help After you‚Äôve completed this course, we recommend checking out DeepLearning.AI‚Äôs [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter1/1",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/1",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "10 to 12 dive into advanced LLM topics like fine-tuning, curating high-quality datasets, and building reasoning models. This course: - Requires a good knowledge of Python - Is better taken after an introductory deep learning course, such as [fast.ai‚Äôs](https://www.fast.ai/) [Practical Deep Learning for Coders](https://course.fast.ai/) or one of the programs developed by [DeepLearning.AI](https://www.deeplearning.ai/) - Does not expect prior [PyTorch](https://pytorch.org/) or [TensorFlow](https://www.tensorflow.org/) knowledge, though some familiarity with either of those will help After you‚Äôve completed this course, we recommend checking out DeepLearning.AI‚Äôs [Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh), which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about! ## Who are we? About the authors: [Abubakar Abid](https://huggingface.co/abidlabs) completed his PhD at Stanford in applied machine learning. During his PhD, he founded [Gradio](https://github.com/gradio-app/gradio), an open-source Python library that has been used to build over 600,000 machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead. [Ben Burtenshaw](https://huggingface.co/burtenshaw) is a Machine Learning Engineer at Hugging Face. He completed his PhD in Natural Language Processing at the University of Antwerp, where he applied Transformer models to generate children stories for the purpose of improving literacy skills. Since then, he has focused on educational materials and tools for the wider community. [Matthew Carrigan](https://huggingface.co/Rocketknight1) is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we‚Äôre going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless. [Lysandre Debut](https://huggingface.co/lysandre) is a Machine Learning Engineer at Hugging Face and has been working on the ü§ó Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API. [Sylvain Gugger](https://huggingface.co/sgugger) is a Research Engineer at Hugging Face and one of the core maintainers of the ü§ó Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote *Deep Learning for Coders with fastai and PyTorch* with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources. [Dawood Khan](https://huggingface.co/dawoodkhan82) is a Machine Learning Engineer at Hugging Face. He‚Äôs from NYC and graduated from New York University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face. [Merve Noyan](https://huggingface.co/merve) is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone. [Lucile Saulnier](https://huggingface.co/SaulLu) is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience. [Lewis Tunstall](https://huggingface.co/lewtun) is a machine",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter1/1",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/1",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face. [Merve Noyan](https://huggingface.co/merve) is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone. [Lucile Saulnier](https://huggingface.co/SaulLu) is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience. [Lewis Tunstall](https://huggingface.co/lewtun) is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O‚ÄôReilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). [Leandro von Werra](https://huggingface.co/lvwerra) is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the O‚ÄôReilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack.. ## FAQ Here are some answers to frequently asked questions: - **Does taking this course lead to a certification?** Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem ‚Äî stay tuned! - **How much time should I spend on this course?** Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course. - **Where can I ask a question if I have one?** If you have a question about any section of the course, just click on the ‚Äù*Ask a question*‚Äù banner at the top of the page to be automatically redirected to the right section of the [Hugging Face forums](https://discuss.huggingface.co/): ![Link to the Hugging Face forums](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png) Note that a list of [project ideas](https://discuss.huggingface.co/c/course/course-event/25) is also available on the forums if you wish to practice more once you have completed the course. - **Where can I get the code for the course?** For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab: ![Link to the Hugging Face course notebooks](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png) The Jupyter notebooks containing all the code from the course are hosted on the [huggingface/notebooks](https://github.com/huggingface/notebooks) repo. If you wish to generate them locally, check out the instructions in the [course](https://github.com/huggingface/course#-jupyter-notebooks) repo on GitHub. - **How can I contribute to the course?** There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on the [course](https://github.com/huggingface/course) repo. If you would like to help translate the course into your native language, check out the instructions [here](https://github.com/huggingface/course#translating-the-course-into-your-language). - **What were the choices made for each translation?** Each translation has a glossary and `TRANSLATING.txt` file that details the choices that were",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter1/1",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/1",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "from the course are hosted on the [huggingface/notebooks](https://github.com/huggingface/notebooks) repo. If you wish to generate them locally, check out the instructions in the [course](https://github.com/huggingface/course#-jupyter-notebooks) repo on GitHub. - **How can I contribute to the course?** There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on the [course](https://github.com/huggingface/course) repo. If you would like to help translate the course into your native language, check out the instructions [here](https://github.com/huggingface/course#translating-the-course-into-your-language). - **What were the choices made for each translation?** Each translation has a glossary and `TRANSLATING.txt` file that details the choices that were made for machine learning jargon etc. You can find an example for German [here](https://github.com/huggingface/course/blob/main/chapters/de/TRANSLATING.txt). - **Can I reuse this course?** Of course! The course is released under the permissive [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0.html). This means that you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you would like to cite the course, please use the following BibTeX: ``` @misc{huggingfacecourse, author = {Hugging Face}, title = {The Hugging Face Course, 2022}, howpublished = \"\\url{https://huggingface.co/course}\", year = {2022}, note = \"[Online; accessed <today>]\" } ``` ## Languages and translations Thanks to our wonderful community, the course is available in many languages beyond English üî•! Check out the table below to see which languages are available and who contributed to the translations: Language Authors [French](https://huggingface.co/course/fr/chapter1/1) [@lbourdois](https://github.com/lbourdois), [@ChainYo](https://github.com/ChainYo), [@melaniedrevet](https://github.com/melaniedrevet), [@abdouaziz](https://github.com/abdouaziz) [Vietnamese](https://huggingface.co/course/vi/chapter1/1) [@honghanhh](https://github.com/honghanhh) [Chinese (simplified)](https://huggingface.co/course/zh-CN/chapter1/1) [@zhlhyx](https://github.com/zhlhyx), [petrichor1122](https://github.com/petrichor1122), [@yaoqih](https://github.com/yaoqih) [Bengali](https://huggingface.co/course/bn/chapter1/1) (WIP) [@avishek-018](https://github.com/avishek-018), [@eNipu](https://github.com/eNipu) [German](https://huggingface.co/course/de/chapter1/1) (WIP) [@JesperDramsch](https://github.com/JesperDramsch), [@MarcusFra](https://github.com/MarcusFra), [@fabridamicelli](https://github.com/fabridamicelli) [Spanish](https://huggingface.co/course/es/chapter1/1) (WIP) [@camartinezbu](https://github.com/camartinezbu), [@munozariasjm](https://github.com/munozariasjm), [@fordaz](https://github.com/fordaz) [Persian](https://huggingface.co/course/fa/chapter1/1) (WIP) [@jowharshamshiri](https://github.com/jowharshamshiri), [@schoobani](https://github.com/schoobani) [Gujarati](https://huggingface.co/course/gu/chapter1/1) (WIP) [@pandyaved98](https://github.com/pandyaved98) [Hebrew](https://huggingface.co/course/he/chapter1/1) (WIP) [@omer-dor](https://github.com/omer-dor) [Hindi](https://huggingface.co/course/hi/chapter1/1) (WIP) [@pandyaved98](https://github.com/pandyaved98) [Bahasa Indonesia](https://huggingface.co/course/id/chapter1/1) (WIP) [@gstdl](https://github.com/gstdl) [Italian](https://huggingface.co/course/it/chapter1/1) (WIP) [@CaterinaBi](https://github.com/CaterinaBi), [@ClonedOne](https://github.com/ClonedOne), [@Nolanogenn](https://github.com/Nolanogenn), [@EdAbati](https://github.com/EdAbati), [@gdacciaro](https://github.com/gdacciaro) [Japanese](https://huggingface.co/course/ja/chapter1/1) (WIP) [@hiromu166](https://github.com/@hiromu166), [@younesbelkada](https://github.com/@younesbelkada), [@HiromuHota](https://github.com/@HiromuHota) [Korean](https://huggingface.co/course/ko/chapter1/1) (WIP) [@Doohae](https://github.com/Doohae), [@wonhyeongseo](https://github.com/wonhyeongseo), [@dlfrnaos19](https://github.com/dlfrnaos19) [Portuguese](https://huggingface.co/course/pt/chapter1/1) (WIP) [@johnnv1](https://github.com/johnnv1), [@victorescosta](https://github.com/victorescosta), [@LincolnVS](https://github.com/LincolnVS) [Russian](https://huggingface.co/course/ru/chapter1/1) (WIP) [@pdumin](https://github.com/pdumin), [@svv73](https://github.com/svv73) [Thai](https://huggingface.co/course/th/chapter1/1) (WIP) [@peeraponw](https://github.com/peeraponw), [@a-krirk](https://github.com/a-krirk), [@jomariya23156](https://github.com/jomariya23156), [@ckingkan](https://github.com/ckingkan) [Turkish](https://huggingface.co/course/tr/chapter1/1) (WIP) [@tanersekmen](https://github.com/tanersekmen), [@mertbozkir](https://github.com/mertbozkir), [@ftarlaci](https://github.com/ftarlaci), [@akkasayaz](https://github.com/akkasayaz) [Chinese (traditional)](https://huggingface.co/course/zh-TW/chapter1/1) (WIP) [@davidpeng86](https://github.com/davidpeng86) For some languages, the [course YouTube videos](https://youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o) have subtitles in the language. You can enable them by first clicking the *CC* button in the bottom right corner of the video. Then, under the settings icon ‚öôÔ∏è, you can select the language you want by selecting the *Subtitles/CC* option. ![Activating subtitles for the Hugging Face course YouTube videos](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/subtitles.png) > Don‚Äôt see your language in the above table or you‚Äôd like to contribute to an existing translation? You can help us translate the course by following the instructionshere. ## Let‚Äôs go üöÄ Are you ready to roll? In this chapter, you will learn: - How to use the `pipeline()` function to solve NLP tasks such as text generation and classification - About the Transformer architecture - How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter1/1",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/1",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Summary\n\n   \nIn this chapter, you‚Äôve been introduced to the fundamentals of Transformer models, Large Language Models (LLMs), and how they‚Äôre revolutionizing AI and beyond.\n \n\n## Key concepts covered\n\n \n\n### Natural Language Processing and LLMs\n\n \nWe explored what NLP is and how Large Language Models have transformed the field. You learned that:\n \n- NLP encompasses a wide range of tasks from classification to generation\n- LLMs are powerful models trained on massive amounts of text data\n- These models can perform multiple tasks within a single architecture\n- Despite their capabilities, LLMs have limitations including hallucinations and bias\n \n\n### Transformer capabilities\n\n \nYou saw how the `pipeline()` function from ü§ó Transformers makes it easy to use pre-trained models for various tasks:\n \n- Text classification, token classification, and question answering\n- Text generation and summarization\n- Translation and other sequence-to-sequence tasks\n- Speech recognition and image classification\n \n\n### Transformer architecture\n\n \nWe discussed how Transformer models work at a high level, including:\n \n- The importance of the attention mechanism\n- How transfer learning enables models to adapt to specific tasks\n- The three main architectural variants: encoder-only, decoder-only, and encoder-decoder\n \n\n### Model architectures and their applications\n\n \nA key aspect of this chapter was understanding which architecture to use for different tasks:\n Model Examples Tasks Encoder-only BERT, DistilBERT, ModernBERT Sentence classification, named entity recognition, extractive question answering Decoder-only GPT, LLaMA, Gemma, SmolLM Text generation, conversational AI, creative writing Encoder-decoder BART, T5, Marian, mBART Summarization, translation, generative question answering \n\n### Modern LLM developments\n\n \nYou also learned about recent developments in the field:\n \n- How LLMs have grown in size and capability over time\n- The concept of scaling laws and how they guide model development\n- Specialized attention mechanisms that help models process longer sequences\n- The two-phase training approach of pretraining and instruction tuning\n \n\n### Practical applications\n\n \nThroughout the chapter, you‚Äôve seen how these models can be applied to real-world problems:\n \n- Using the Hugging Face Hub to find and use pre-trained models\n- Leveraging the Inference API to test models directly in your browser\n- Understanding which models are best suited for specific tasks\n \n\n## Looking ahead\n\n \nNow that you have a solid understanding of what Transformer models are and how they work at a high level, you‚Äôre ready to dive deeper into how to use them effectively. In the next chapters, you‚Äôll learn how to:\n \n- Use the Transformers library to load and fine-tune models\n- Process different types of data for model input\n- Adapt pre-trained models to your specific tasks\n- Deploy models for practical applications\n \nThe foundation you‚Äôve built in this chapter will serve you well as you explore more advanced topics and techniques in the coming sections.",
    "metadata": {
      "title": "Summary",
      "url": "https://huggingface.co/learn/llm-course/chapter1/10",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/10",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/10.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Exam Time!\n\n \nIt‚Äôs time to put your knowledge to the test! We‚Äôve prepared a short quiz for you to test your understanding of the concepts covered in this chapter.\n \nTo take the quiz, you will need to follow these steps:\n \n1. Sign in to your Hugging Face account.\n2. Answer the questions in the quiz.\n3. Submit your answers.\n \n\n## Multiple Choice Quiz\n\n \nIn this quiz, you will be asked to select the correct answer from a list of options. We‚Äôll test you on the fundamentals of supervised finetuning.",
    "metadata": {
      "title": "Exam Time!",
      "url": "https://huggingface.co/learn/llm-course/chapter1/11",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/11",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/11.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Natural Language Processing and Large Language Models Before jumping into Transformer models, let‚Äôs do a quick overview of what natural language processing is, how large language models have transformed the field, and why we care about it. ## What is NLP? NLP is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words. The following is a list of common NLP tasks, with some examples of each: - **Classifying whole sentences**: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not - **Classifying each word in a sentence**: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization) - **Generating text content**: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words - **Extracting an answer from a text**: Given a question and a context, extracting the answer to the question based on the information provided in the context - **Generating a new sentence from an input text**: Translating a text into another language, summarizing a text NLP isn‚Äôt limited to written text though. It also tackles complex challenges in speech recognition and computer vision, such as generating a transcript of an audio sample or a description of an image. ## The Rise of Large Language Models (LLMs) In recent years, the field of NLP has been revolutionized by Large Language Models (LLMs). These models, which include architectures like GPT (Generative Pre-trained Transformer) and [Llama](https://huggingface.co/meta-llama), have transformed what‚Äôs possible in language processing. > A large language model (LLM) is an AI model trained on massive amounts of text data that can understand and generate human-like text, recognize patterns in language, and perform a wide variety of language tasks without task-specific training. They represent a significant advancement in the field of natural language processing (NLP). LLMs are characterized by: - **Scale**: They contain millions, billions, or even hundreds of billions of parameters - **General capabilities**: They can perform multiple tasks without task-specific training - **In-context learning**: They can learn from examples provided in the prompt - **Emergent abilities**: As these models grow in size, they demonstrate capabilities that weren‚Äôt explicitly programmed or anticipated The advent of LLMs has shifted the paradigm from building specialized models for specific NLP tasks to using a single, large model that can be prompted or fine-tuned to address a wide range of language tasks. This has made sophisticated language processing more accessible while also introducing new challenges in areas like efficiency, ethics, and deployment. However, LLMs also have important limitations: - **Hallucinations**: They can generate incorrect information confidently - **Lack of true understanding**: They lack true understanding of the world and operate purely on statistical patterns - **Bias**: They may reproduce biases present in their",
    "metadata": {
      "title": "Natural Language Processing and Large Language Models",
      "url": "https://huggingface.co/learn/llm-course/chapter1/2",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/2",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "programmed or anticipated The advent of LLMs has shifted the paradigm from building specialized models for specific NLP tasks to using a single, large model that can be prompted or fine-tuned to address a wide range of language tasks. This has made sophisticated language processing more accessible while also introducing new challenges in areas like efficiency, ethics, and deployment. However, LLMs also have important limitations: - **Hallucinations**: They can generate incorrect information confidently - **Lack of true understanding**: They lack true understanding of the world and operate purely on statistical patterns - **Bias**: They may reproduce biases present in their training data or inputs. - **Context windows**: They have limited context windows (though this is improving) - **Computational resources**: They require significant computational resources ## Why is language processing challenging? Computers don‚Äôt process information in the same way as humans. For example, when we read the sentence ‚ÄúI am hungry,‚Äù we can easily understand its meaning. Similarly, given two sentences such as ‚ÄúI am hungry‚Äù and ‚ÄúI am sad,‚Äù we‚Äôre able to easily determine how similar they are. For machine learning (ML) models, such tasks are more difficult. The text needs to be processed in a way that enables the model to learn from it. And because language is complex, we need to think carefully about how this processing must be done. There has been a lot of research done on how to represent text, and we will look at some methods in the next chapter. Even with the advances in LLMs, many fundamental challenges remain. These include understanding ambiguity, cultural context, sarcasm, and humor. LLMs address these challenges through massive training on diverse datasets, but still often fall short of human-level understanding in many complex scenarios.",
    "metadata": {
      "title": "Natural Language Processing and Large Language Models",
      "url": "https://huggingface.co/learn/llm-course/chapter1/2",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/2",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Transformers, what can they do? In this section, we will look at what Transformer models can do and use our first tool from the ü§ó Transformers library: the `pipeline()` function. > üëÄ See thatOpen in Colabbutton on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples.If you want to run the examples locally, we recommend taking a look at thesetup. ## Transformers are everywhere! Transformer models are used to solve all kinds of tasks across different modalities, including natural language processing (NLP), computer vision, audio processing, and more. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models: ![Companies using Hugging Face](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG) The [ü§ó Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains millions of pretrained models that anyone can download and use. You can also upload your own models to the Hub! > ‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want!Create a huggingface.coaccount to benefit from all available features! Before diving into how Transformer models work under the hood, let‚Äôs look at a few examples of how they can be used to solve some interesting NLP problems. ## Working with pipelines The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer: ``` from transformers import pipeline classifier = pipeline(\"sentiment-analysis\") classifier(\"I've been waiting for a HuggingFace course my whole life.\") ``` ``` [{'label': 'POSITIVE', 'score': 0.9598047137260437}] ``` We can even pass several sentences! ``` classifier( [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"] ) ``` ``` [{'label': 'POSITIVE', 'score': 0.9598047137260437}, {'label': 'NEGATIVE', 'score': 0.9994558095932007}] ``` By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again. There are three main steps involved when you pass some text to a pipeline: 1. The text is preprocessed into a format the model can understand. 2. The preprocessed inputs are passed to the model. 3. The predictions of the model are post-processed, so you can make sense of them. ## Available pipelines for different modalities The `pipeline()` function supports multiple modalities, allowing you to work with text, images, audio, and even multimodal tasks. In this course we‚Äôll focus on text tasks, but it‚Äôs useful to understand the transformer architecture‚Äôs potential, so we‚Äôll briefly outline it. Here‚Äôs an overview of what‚Äôs available: > For a full and updated list of pipelines,",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 1,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to a pipeline: 1. The text is preprocessed into a format the model can understand. 2. The preprocessed inputs are passed to the model. 3. The predictions of the model are post-processed, so you can make sense of them. ## Available pipelines for different modalities The `pipeline()` function supports multiple modalities, allowing you to work with text, images, audio, and even multimodal tasks. In this course we‚Äôll focus on text tasks, but it‚Äôs useful to understand the transformer architecture‚Äôs potential, so we‚Äôll briefly outline it. Here‚Äôs an overview of what‚Äôs available: > For a full and updated list of pipelines, see theü§ó Transformers documentation. ### Text pipelines - `text-generation`: Generate text from a prompt - `text-classification`: Classify text into predefined categories - `summarization`: Create a shorter version of a text while preserving key information - `translation`: Translate text from one language to another - `zero-shot-classification`: Classify text without prior training on specific labels - `feature-extraction`: Extract vector representations of text ### Image pipelines - `image-to-text`: Generate text descriptions of images - `image-classification`: Identify objects in an image - `object-detection`: Locate and identify objects in images ### Audio pipelines - `automatic-speech-recognition`: Convert speech to text - `audio-classification`: Classify audio into categories - `text-to-speech`: Convert text to spoken audio ### Multimodal pipelines - `image-text-to-text`: Respond to an image based on a text prompt Let‚Äôs explore some of these pipelines in more detail! ## Zero-shot classification We‚Äôll start by tackling a more challenging task where we need to classify texts that haven‚Äôt been labelled. This is a common scenario in real-world projects because annotating text is usually time-consuming and requires domain expertise. For this use case, the `zero-shot-classification` pipeline is very powerful: it allows you to specify which labels to use for the classification, so you don‚Äôt have to rely on the labels of the pretrained model. You‚Äôve already seen how the model can classify a sentence as positive or negative using those two labels ‚Äî but it can also classify the text using any other set of labels you like. ``` from transformers import pipeline classifier = pipeline(\"zero-shot-classification\") classifier( \"This is a course about the Transformers library\", candidate_labels=[\"education\", \"politics\", \"business\"], ) ``` ``` {'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]} ``` This pipeline is called *zero-shot* because you don‚Äôt need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want! > ‚úèÔ∏èTry it out!Play around with your own sequences and labels and see how the model behaves. ## Text generation Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it‚Äôs normal if you don‚Äôt get the same results as shown below. ``` from transformers import pipeline",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 2,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "directly return probability scores for any list of labels you want! > ‚úèÔ∏èTry it out!Play around with your own sequences and labels and see how the model behaves. ## Text generation Now let‚Äôs see how to use a pipeline to generate some text. The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text. This is similar to the predictive text feature that is found on many phones. Text generation involves randomness, so it‚Äôs normal if you don‚Äôt get the same results as shown below. ``` from transformers import pipeline generator = pipeline(\"text-generation\") generator(\"In this course, we will teach you how to\") ``` ``` [{'generated_text': 'In this course, we will teach you how to understand and use ' 'data flow and data interchange when handling user data. We ' 'will be working with one or more of the most commonly used ' 'data flows ‚Äî data flows of various types, as seen by the ' 'HTTP'}] ``` You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`. > ‚úèÔ∏èTry it out!Use thenum_return_sequencesandmax_lengtharguments to generate two sentences of 15 words each. ## Using any model from the Hub in a pipeline The previous examples used the default model for the task at hand, but you can also choose a particular model from the Hub to use in a pipeline for a specific task ‚Äî say, text generation. Go to the [Model Hub](https://huggingface.co/models) and click on the corresponding tag on the left to display only the supported models for that task. You should get to a page like [this one](https://huggingface.co/models?pipeline_tag=text-generation). Let‚Äôs try the [HuggingFaceTB/SmolLM2-360M](https://huggingface.co/HuggingFaceTB/SmolLM2-360M) model! Here‚Äôs how to load it in the same pipeline as before: ``` from transformers import pipeline generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\") generator( \"In this course, we will teach you how to\", max_length=30, num_return_sequences=2, ) ``` ``` [{'generated_text': 'In this course, we will teach you how to manipulate the world and ' 'move your mental and physical capabilities to your advantage.'}, {'generated_text': 'In this course, we will teach you how to become an expert and ' 'practice realtime, and with a hands on experience on both real ' 'time and real'}] ``` You can refine your search for a model by clicking on the language tags, and pick a model that will generate text in another language. The Model Hub even contains checkpoints for multilingual models that support several languages. Once you select a model by clicking on it, you‚Äôll see that there is a widget enabling you to try it directly online. This way you can quickly test the model‚Äôs capabilities before downloading it. > ‚úèÔ∏èTry it out!Use the filters to find a text generation model for another language. Feel free to play with the widget and use it in a pipeline! ### Inference Providers All the models can be tested directly through your browser using the Inference Providers, which is available on",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 3,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in another language. The Model Hub even contains checkpoints for multilingual models that support several languages. Once you select a model by clicking on it, you‚Äôll see that there is a widget enabling you to try it directly online. This way you can quickly test the model‚Äôs capabilities before downloading it. > ‚úèÔ∏èTry it out!Use the filters to find a text generation model for another language. Feel free to play with the widget and use it in a pipeline! ### Inference Providers All the models can be tested directly through your browser using the Inference Providers, which is available on the Hugging Face [website](https://huggingface.co/docs/inference-providers/en/index). You can play with the model directly on this page by inputting custom text and watching the model process the input data. Inference Providers that powers the widget is also available as a paid product, which comes in handy if you need it for your workflows. See the [pricing page](https://huggingface.co/docs/inference-providers/en/pricing) for more details. ## Mask filling The next pipeline you‚Äôll try is `fill-mask`. The idea of this task is to fill in the blanks in a given text: ``` from transformers import pipeline unmasker = pipeline(\"fill-mask\") unmasker(\"This course will teach you all about <mask> models.\", top_k=2) ``` ``` [{'sequence': 'This course will teach you all about mathematical models.', 'score': 0.19619831442832947, 'token': 30412, 'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052725434303284, 'token': 38163, 'token_str': ' computational'}] ``` The `top_k` argument controls how many possibilities you want to be displayed. Note that here the model fills in the special `<mask>` word, which is often referred to as a *mask token*. Other mask-filling models might have different mask tokens, so it‚Äôs always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget. > ‚úèÔ∏èTry it out!Search for thebert-base-casedmodel on the Hub and identify its mask word in the Inference API widget. What does this model predict for the sentence in ourpipelineexample above? ## Named entity recognition Named entity recognition (NER) is a task where the model has to find which parts of the input text correspond to entities such as persons, locations, or organizations. Let‚Äôs look at an example: ``` from transformers import pipeline ner = pipeline(\"ner\", grouped_entities=True) ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` ``` [{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57} ] ``` Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC). We pass the option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 4,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57} ] ``` Here the model correctly identified that Sylvain is a person (PER), Hugging Face an organization (ORG), and Brooklyn a location (LOC). We pass the option `grouped_entities=True` in the pipeline creation function to tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: here the model correctly grouped ‚ÄúHugging‚Äù and ‚ÄúFace‚Äù as a single organization, even though the name consists of multiple words. In fact, as we will see in the next chapter, the preprocessing even splits some words into smaller parts. For instance, `Sylvain` is split into four pieces: `S`, `##yl`, `##va`, and `##in`. In the post-processing step, the pipeline successfully regrouped those pieces. > ‚úèÔ∏èTry it out!Search the Model Hub for a model able to do part-of-speech tagging (usually abbreviated as POS) in English. What does this model predict for the sentence in the example above? ## Question answering The `question-answering` pipeline answers questions using information from a given context: ``` from transformers import pipeline question_answerer = pipeline(\"question-answering\") question_answerer( question=\"Where do I work?\", context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\", ) ``` ``` {'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'} ``` Note that this pipeline works by extracting information from the provided context; it does not generate the answer. ## Summarization Summarization is the task of reducing a text into a shorter text while keeping all (or most) of the important aspects referenced in the text. Here‚Äôs an example: ``` from transformers import pipeline summarizer = pipeline(\"summarization\") summarizer( \"\"\" America has changed dramatically during recent years. Not only has the number of graduates in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering declined, but in most of the premier American universities engineering curricula now concentrate on and encourage largely the study of engineering science. As a result, there are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues, and greater concentration on high technology subjects, largely supporting increasingly complex scientific developments. While the latter is important, it should not be at the expense of more traditional engineering. Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers. \"\"\" ) ``` ``` [{'summary_text': ' America has changed dramatically during recent years . The ' 'number of engineering graduates in the U.S. has declined in ' 'traditional engineering disciplines such as mechanical, civil ' ', electrical, chemical, and aeronautical engineering . Rapidly ' 'developing economies such",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 5,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "encourage and advance the teaching of engineering. Both China and India, respectively, graduate six and eight times as many traditional engineers as does the United States. Other industrial countries at minimum maintain their output, while America suffers an increasingly serious decline in the number of engineering graduates and a lack of well-educated engineers. \"\"\" ) ``` ``` [{'summary_text': ' America has changed dramatically during recent years . The ' 'number of engineering graduates in the U.S. has declined in ' 'traditional engineering disciplines such as mechanical, civil ' ', electrical, chemical, and aeronautical engineering . Rapidly ' 'developing economies such as China and India, as well as other ' 'industrial countries in Europe and Asia, continue to encourage ' 'and advance engineering .'}] ``` Like with text generation, you can specify a `max_length` or a `min_length` for the result. ## Translation For translation, you can use a default model if you provide a language pair in the task name (such as `\"translation_en_to_fr\"`), but the easiest way is to pick the model you want to use on the [Model Hub](https://huggingface.co/models). Here we‚Äôll try translating from French to English: ``` from transformers import pipeline translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\") translator(\"Ce cours est produit par Hugging Face.\") ``` ``` [{'translation_text': 'This course is produced by Hugging Face.'}] ``` Like with text generation and summarization, you can specify a `max_length` or a `min_length` for the result. > ‚úèÔ∏èTry it out!Search for translation models in other languages and try to translate the previous sentence into a few different languages. ## Image and audio pipelines Beyond text, Transformer models can also work with images and audio. Here are a few examples: ### Image classification ``` from transformers import pipeline image_classifier = pipeline( task=\"image-classification\", model=\"google/vit-base-patch16-224\" ) result = image_classifier( \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\" ) print(result) ``` ``` [{'label': 'lynx, catamount', 'score': 0.43350091576576233}, {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.034796204417943954}, {'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.03240183740854263}, {'label': 'Egyptian cat', 'score': 0.02394474856555462}, {'label': 'tiger cat', 'score': 0.02288915030658245}] ``` ### Automatic speech recognition ``` from transformers import pipeline transcriber = pipeline( task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\" ) result = transcriber( \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\" ) print(result) ``` ``` {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'} ``` ## Combining data from multiple sources One powerful application of Transformer models is their ability to combine and process data from multiple sources. This is especially useful when you need to: 1. Search across multiple databases or repositories 2. Consolidate information from different formats (text, images, audio) 3. Create a unified view of related information For example, you could build a system that: - Searches for information across databases in multiple modalities like text and image. - Combines results from different sources into a single coherent response. For example, from an audio file and text description. - Presents the most relevant information from a database of documents and metadata. ## Conclusion The pipelines shown in this chapter are mostly for demonstrative",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 6,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "multiple sources. This is especially useful when you need to: 1. Search across multiple databases or repositories 2. Consolidate information from different formats (text, images, audio) 3. Create a unified view of related information For example, you could build a system that: - Searches for information across databases in multiple modalities like text and image. - Combines results from different sources into a single coherent response. For example, from an audio file and text description. - Presents the most relevant information from a database of documents and metadata. ## Conclusion The pipelines shown in this chapter are mostly for demonstrative purposes. They were programmed for specific tasks and cannot perform variations of them. In the next chapter, you‚Äôll learn what‚Äôs inside a `pipeline()` function and how to customize its behavior.",
    "metadata": {
      "title": "Transformers, what can they do?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/3",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/3",
      "part": 7,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# How do Transformers work? In this section, we will take a look at the architecture of Transformer models and dive deeper into the concepts of attention, encoder-decoder architecture, and more. > üöÄ We‚Äôre taking things up a notch here. This section is detailed and technical, so don‚Äôt worry if you don‚Äôt understand everything right away. We‚Äôll come back to these concepts later in the course. ## A bit of Transformer history Here are some reference points in the (short) history of Transformer models: ![A brief chronology of Transformers models.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg) ![A brief chronology of Transformers models.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg) The [Transformer architecture](https://arxiv.org/abs/1706.03762) was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including: - **June 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results - **October 2018**: [BERT](https://arxiv.org/abs/1810.04805), another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!) - **February 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns - **October 2019**: [T5](https://huggingface.co/papers/1910.10683), A multi-task focused implementation of the sequence-to-sequence Transformer architecture. - **May 2020**, [GPT-3](https://huggingface.co/papers/2005.14165), an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called *zero-shot learning*) - **January 2022**: [InstructGPT](https://huggingface.co/papers/2203.02155), a version of GPT-3 that was trained to follow instructions better. - **January 2023**: [Llama](https://huggingface.co/papers/2302.13971), a large language model that is able to generate text in a variety of languages. - **March 2023**: [Mistral](https://huggingface.co/papers/2310.06825), a 7-billion-parameter language model that outperforms Llama 2 13B across all evaluated benchmarks, leveraging grouped-query attention for faster inference and sliding window attention to handle sequences of arbitrary length. - **May 2024**: [Gemma 2](https://huggingface.co/papers/2408.00118), a family of lightweight, state-of-the-art open models ranging from 2B to 27B parameters that incorporate interleaved local-global attentions and group-query attention, with smaller models trained using knowledge distillation to deliver performance competitive with models 2-3 times larger. - **November 2024**: [SmolLM2](https://huggingface.co/papers/2502.02737), a state-of-the-art small language model (135 million to 1.7 billion parameters) that achieves impressive performance despite its compact size, and unlocking new possibilities for mobile and edge devices. This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories: - GPT-like (also called *auto-regressive* Transformer models) - BERT-like (also called *auto-encoding* Transformer models) - T5-like (also called *sequence-to-sequence* Transformer models) We will dive into these families in more depth later on. ## Transformers are language models All the Transformer models mentioned above (GPT, BERT, T5, etc.) have been trained as *language models*. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "three categories: - GPT-like (also called *auto-regressive* Transformer models) - BERT-like (also called *auto-encoding* Transformer models) - T5-like (also called *sequence-to-sequence* Transformer models) We will dive into these families in more depth later on. ## Transformers are language models All the Transformer models mentioned above (GPT, BERT, T5, etc.) have been trained as *language models*. This means they have been trained on large amounts of raw text in a self-supervised fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data! This type of model develops a statistical understanding of the language it has been trained on, but it‚Äôs less useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called *transfer learning* or *fine-tuning*. During this process, the model is fine-tuned in a supervised way ‚Äî that is, using human-annotated labels ‚Äî on a given task. An example of a task is predicting the next word in a sentence having read the *n* previous words. This is called *causal language modeling* because the output depends on the past and present inputs, but not the future ones. ![Example of causal language modeling in which the next word from a sentence is predicted.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg) ![Example of causal language modeling in which the next word from a sentence is predicted.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg) Another example is *masked language modeling*, in which the model predicts a masked word in the sentence. ![Example of masked language modeling in which a masked word from a sentence is predicted.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg) ![Example of masked language modeling in which a masked word from a sentence is predicted.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg) ## Transformers are big models Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models‚Äô sizes as well as the amount of data they are pretrained on. ![Number of parameters of recent Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png) Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes very costly in terms of time and compute resources. It even translates to environmental impact, as can be seen in the following graph. ![The carbon footprint of a large language model.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg) ![The carbon footprint of a large language model.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg) And this is showing a project for a (very big) model led by a team consciously trying to reduce the environmental impact of pretraining. The footprint of running lots of trials to get the best hyperparameters would be even higher. Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs! This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community. By the way, you can evaluate the carbon footprint of your models‚Äô training through several tools. For example",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "environmental impact of pretraining. The footprint of running lots of trials to get the best hyperparameters would be even higher. Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs! This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community. By the way, you can evaluate the carbon footprint of your models‚Äô training through several tools. For example [ML CO2 Impact](https://mlco2.github.io/impact/) or [Code Carbon](https://codecarbon.io/) which is integrated in ü§ó Transformers. To learn more about this, you can read this [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) which will show you how to generate an `emissions.csv` file with an estimate of the footprint of your training, as well as the [documentation](https://huggingface.co/docs/hub/model-cards-co2) of ü§ó Transformers addressing this topic. ## Transfer Learning *Pretraining* is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge. ![The pretraining of a language model is costly in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg) ![The pretraining of a language model is costly in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg) This pretraining is usually done on very large amounts of data. Therefore, it requires a very large corpus of data, and training can take up to several weeks. *Fine-tuning*, on the other hand, is the training done **after** a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait ‚Äî why not simply train the model for your final use case from the start (**scratch**)? There are a couple of reasons: - The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task). - Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results. - For the same reason, the amount of time and resources needed to get good results are much lower. For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is ‚Äútransferred,‚Äù hence the term *transfer learning*. ![The fine-tuning of a language model is cheaper than pretraining in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg) ![The fine-tuning of a language model is cheaper than pretraining in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg) Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is ‚Äútransferred,‚Äù hence the term *transfer learning*. ![The fine-tuning of a language model is cheaper than pretraining in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg) ![The fine-tuning of a language model is cheaper than pretraining in both time and money.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg) Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining. This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model ‚Äî one as close as possible to the task you have at hand ‚Äî and fine-tune it. ## General Transformer architecture In this section, we‚Äôll go over the general architecture of the Transformer model. Don‚Äôt worry if you don‚Äôt understand some of the concepts; there are detailed sections later covering each of the components. The model is primarily composed of two blocks: - **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input. - **Decoder (right)**: The decoder uses the encoder‚Äôs representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs. ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg) ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg) Each of these parts can be used independently, depending on the task: - **Encoder-only models**: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition. - **Decoder-only models**: Good for generative tasks such as text generation. - **Encoder-decoder models** or **sequence-to-sequence models**: Good for generative tasks that require an input, such as translation or summarization. We will dive into those architectures independently in later sections. ## Attention layers A key feature of Transformer models is that they are built with special layers called *attention layers*. In fact, the title of the paper introducing the Transformer architecture was [‚ÄúAttention Is All You Need‚Äù](https://arxiv.org/abs/1706.03762)! We will explore the details of attention layers later in the course; for now, all you need to know is that this layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word. To put this into context, consider the task of translating text from English to French. Given the input ‚ÄúYou like this course‚Äù, a translation model will need to also attend to the adjacent word ‚ÄúYou‚Äù to get the proper translation for the word ‚Äúlike‚Äù, because in French the verb ‚Äúlike‚Äù is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word. To put this into context, consider the task of translating text from English to French. Given the input ‚ÄúYou like this course‚Äù, a translation model will need to also attend to the adjacent word ‚ÄúYou‚Äù to get the proper translation for the word ‚Äúlike‚Äù, because in French the verb ‚Äúlike‚Äù is conjugated differently depending on the subject. The rest of the sentence, however, is not useful for the translation of that word. In the same vein, when translating ‚Äúthis‚Äù the model will also need to pay attention to the word ‚Äúcourse‚Äù, because ‚Äúthis‚Äù translates differently depending on whether the associated noun is masculine or feminine. Again, the other words in the sentence will not matter for the translation of ‚Äúcourse‚Äù. With more complex sentences (and more complex grammar rules), the model would need to pay special attention to words that might appear farther away in the sentence to properly translate each word. The same concept applies to any task associated with natural language: a word by itself has a meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the word being studied. Now that you have an idea of what attention layers are all about, let‚Äôs take a closer look at the Transformer architecture. ## The original architecture The Transformer architecture was originally designed for translation. During training, the encoder receives inputs (sentences) in a certain language, while the decoder receives the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder which then uses all the inputs of the encoder to try to predict the fourth word. To speed things up during training (when the model has access to target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3. The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right: ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg) ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg) Note that the",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "target sentences), the decoder is fed the whole target, but it is not allowed to use future words (if it had access to the word at position 2 when trying to predict the word at position 2, the problem would not be very hard!). For instance, when trying to predict the fourth word, the attention layer will only have access to the words in positions 1 to 3. The original Transformer architecture looked like this, with the encoder on the left and the decoder on the right: ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg) ![Architecture of a Transformers models](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg) Note that the first attention layer in a decoder block pays attention to all (past) inputs to the decoder, but the second attention layer uses the output of the encoder. It can thus access the whole input sentence to best predict the current word. This is very useful as different languages can have grammatical rules that put the words in different orders, or some context provided later in the sentence may be helpful to determine the best translation of a given word. The *attention mask* can also be used in the encoder/decoder to prevent the model from paying attention to some special words ‚Äî for instance, the special padding word used to make all the inputs the same length when batching together sentences. ## Architectures vs. checkpoints As we dive into Transformer models in this course, you‚Äôll see mentions of *architectures* and *checkpoints* as well as *models*. These terms all have slightly different meanings: - **Architecture**: This is the skeleton of the model ‚Äî the definition of each layer and each operation that happens within the model. - **Checkpoints**: These are the weights that will be loaded in a given architecture. - **Model**: This is an umbrella term that isn‚Äôt as precise as ‚Äúarchitecture‚Äù or ‚Äúcheckpoint‚Äù: it can mean both. This course will specify *architecture* or *checkpoint* when it matters to reduce ambiguity. For example, BERT is an architecture while `bert-base-cased`, a set of weights trained by the Google team for the first release of BERT, is a checkpoint. However, one can say ‚Äúthe BERT model‚Äù and ‚Äúthe `bert-base-cased` model.‚Äù",
    "metadata": {
      "title": "How do Transformers work?",
      "url": "https://huggingface.co/learn/llm-course/chapter1/4",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/4",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# How ü§ó Transformers solve tasks In [Transformers, what can they do?](/course/chapter1/3), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what‚Äôs happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, a decoder, or an encoder-decoder structure. > Before diving into specific architectural variants, it‚Äôs helpful to understand that most tasks follow a similar pattern: input data is processed through a model, and the output is interpreted for a specific task. The differences lie in how the data is prepared, what model architecture variant is used, and how the output is processed. To explain how tasks are solved, we‚Äôll walk through what goes on inside the model to output useful predictions. We‚Äôll cover the following models and their corresponding tasks: - [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2) for audio classification and automatic speech recognition (ASR) - [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit) and [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext) for image classification - [DETR](https://huggingface.co/docs/transformers/model_doc/detr) for object detection - [Mask2Former](https://huggingface.co/docs/transformers/model_doc/mask2former) for image segmentation - [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn) for depth estimation - [BERT](https://huggingface.co/docs/transformers/model_doc/bert) for NLP tasks like text classification, token classification and question answering that use an encoder - [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) for NLP tasks like text generation that use a decoder - [BART](https://huggingface.co/docs/transformers/model_doc/bart) for NLP tasks like summarization and translation that use an encoder-decoder > Before you go further, it is good to have some basic knowledge of the original Transformer architecture. Knowing how encoders, decoders, and attention work will aid you in understanding how different Transformer models work. Be sure to check out ourthe previous sectionfor more information! ## Transformer models for language Language models are at the heart of modern NLP. They‚Äôre designed to understand and generate human language by learning the statistical patterns and relationships between words or tokens in text. The Transformer was initially designed for machine translation, and since then, it has become the default architecture for solving all AI tasks. Some tasks lend themselves to the Transformer‚Äôs encoder structure, while others are better suited for the decoder. Still, other tasks make use of both the Transformer‚Äôs encoder-decoder structure. ### How language models work Language models work by being trained to predict the probability of a word given the context of surrounding words. This gives them a foundational understanding of language that can generalize to other tasks. There are two main approaches for training a transformer model: 1. **Masked language modeling (MLM)**: Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word). 2. **Causal language modeling (CLM)**: Used by decoder models",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "being trained to predict the probability of a word given the context of surrounding words. This gives them a foundational understanding of language that can generalize to other tasks. There are two main approaches for training a transformer model: 1. **Masked language modeling (MLM)**: Used by encoder models like BERT, this approach randomly masks some tokens in the input and trains the model to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional context (looking at words both before and after the masked word). 2. **Causal language modeling (CLM)**: Used by decoder models like GPT, this approach predicts the next token based on all previous tokens in the sequence. The model can only use context from the left (previous tokens) to predict the next token. ### Types of language models In the Transformers library, language models generally fall into three architectural categories: 1. **Encoder-only models** (like BERT): These models use a bidirectional approach to understand context from both directions. They‚Äôre best suited for tasks that require deep understanding of text, such as classification, named entity recognition, and question answering. 2. **Decoder-only models** (like GPT, Llama): These models process text from left to right and are particularly good at text generation tasks. They can complete sentences, write essays, or even generate code based on a prompt. 3. **Encoder-decoder models** (like T5, BART): These models combine both approaches, using an encoder to understand the input and a decoder to generate output. They excel at sequence-to-sequence tasks like translation, summarization, and question answering. ![transformer-models-for-language](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_architecture.png) As we covered in the previous section, language models are typically pretrained on large amounts of text data in a self-supervised manner (without human annotations), then fine-tuned on specific tasks. This approach, known as transfer learning, allows these models to adapt to many different NLP tasks with relatively small amounts of task-specific data. In the following sections, we‚Äôll explore specific model architectures and how they‚Äôre applied to various tasks across speech, vision, and text domains. > Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional context use encoders, tasks generating text use decoders, and tasks converting one sequence to another use encoder-decoders. ### Text generation Text generation involves creating coherent and contextually relevant text based on a prompt or input. [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2) is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png) 1. GPT-2 uses [byte pair encoding (BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can‚Äôt",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is a decoder-only model pretrained on a large amount of text. It can generate convincing (though not always true!) text given a prompt and complete other NLP tasks like question answering despite not being explicitly trained to. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png) 1. GPT-2 uses [byte pair encoding (BPE)](https://huggingface.co/docs/transformers/tokenizer_summary#bytepair-encoding-bpe) to tokenize words and generate a token embedding. Positional encodings are added to the token embeddings to indicate the position of each token in the sequence. The input embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder block, GPT-2 uses a *masked self-attention* layer which means GPT-2 can‚Äôt attend to future tokens. It is only allowed to attend to tokens on the left. This is different from BERT‚Äôs [`mask`] token because, in masked self-attention, an attention mask is used to set the score to `0` for future tokens. 2. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The label is the next token in the sequence, which are created by shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits and the labels to output the next most likely token. GPT-2‚Äôs pretraining objective is based entirely on [causal language modeling](https://huggingface.co/docs/transformers/glossary#causal-language-modeling), predicting the next word in a sequence. This makes GPT-2 especially good at tasks that involve generating text. Ready to try your hand at text generation? Check out our complete [causal language modeling guide](https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling) to learn how to finetune DistilGPT-2 and use it for inference! > For more information about text generation, check out thetext generation strategiesguide! ### Text classification Text classification involves assigning predefined categories to text documents, such as sentiment analysis, topic classification, or spam detection. [BERT](https://huggingface.co/docs/transformers/model_doc/bert) is an encoder-only model and is the first model to effectively implement deep bidirectionality to learn richer representations of the text by attending to words on both sides. 1. BERT uses [WordPiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) tokenization to generate a token embedding of the text. To tell the difference between a single sentence and a pair of sentences, a special `[SEP]` token is added to differentiate them. A special `[CLS]` token is added to the beginning of every sequence of text. The final output with the `[CLS]` token is used as the input to the classification head for classification tasks. BERT also adds a segment embedding to denote whether a token belongs to the first or second sentence in a pair of sentences. 2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ‚Äúpredict‚Äù the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word. The second pretraining object is next-sentence prediction.",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "token belongs to the first or second sentence in a pair of sentences. 2. BERT is pretrained with two objectives: masked language modeling and next-sentence prediction. In masked language modeling, some percentage of the input tokens are randomly masked, and the model needs to predict these. This solves the issue of bidirectionality, where the model could cheat and see all the words and ‚Äúpredict‚Äù the next word. The final hidden states of the predicted mask tokens are passed to a feedforward network with a softmax over the vocabulary to predict the masked word. The second pretraining object is next-sentence prediction. The model must predict whether sentence B follows sentence A. Half of the time sentence B is the next sentence, and the other half of the time, sentence B is a random sentence. The prediction, whether it is the next sentence or not, is passed to a feedforward network with a softmax over the two classes (`IsNext` and `NotNext`). 3. The input embeddings are passed through multiple encoder layers to output some final hidden states. To use the pretrained model for text classification, add a sequence classification head on top of the base BERT model. The sequence classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and target to find the most likely label. Ready to try your hand at text classification? Check out our complete [text classification guide](https://huggingface.co/docs/transformers/tasks/sequence_classification) to learn how to finetune DistilBERT and use it for inference! ### Token classification Token classification involves assigning a label to each token in a sequence, such as in named entity recognition or part-of-speech tagging. To use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label. Ready to try your hand at token classification? Check out our complete [token classification guide](https://huggingface.co/docs/transformers/tasks/token_classification) to learn how to finetune DistilBERT and use it for inference! ### Question answering Question answering involves finding the answer to a question within a given context or passage. To use BERT for question answering, add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer. Ready to try your hand at question answering? Check out our complete [question answering guide](https://huggingface.co/docs/transformers/tasks/question_answering) to learn how to finetune DistilBERT and use it for inference! > üí° Notice how easy it is to use BERT for different tasks",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "add a span classification head on top of the base BERT model. This linear layer accepts the final hidden states and performs a linear transformation to compute the `span` start and end logits corresponding to the answer. The cross-entropy loss is calculated between the logits and the label position to find the most likely span of text corresponding to the answer. Ready to try your hand at question answering? Check out our complete [question answering guide](https://huggingface.co/docs/transformers/tasks/question_answering) to learn how to finetune DistilBERT and use it for inference! > üí° Notice how easy it is to use BERT for different tasks once it‚Äôs been pretrained. You only need to add a specific head to the pretrained model to manipulate the hidden states into your desired output! ### Summarization Summarization involves condensing a longer text into a shorter version while preserving its key information and meaning. Encoder-decoder models like [BART](https://huggingface.co/docs/transformers/model_doc/bart) and [T5](model_doc/t5) are designed for the sequence-to-sequence pattern of a summarization task. We‚Äôll explain how BART works in this section, and then you can try finetuning T5 at the end. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png) 1. BART‚Äôs encoder architecture is very similar to BERT and accepts a token and positional embedding of the text. BART is pretrained by corrupting the input and then reconstructing it with the decoder. Unlike other encoders with specific corruption strategies, BART can apply any type of corruption. The *text infilling* corruption strategy works the best though. In text infilling, a number of text spans are replaced with a **single** [`mask`] token. This is important because the model has to predict the masked tokens, and it teaches the model to predict the number of missing tokens. The input embeddings and masked spans are passed through the encoder to output some final hidden states, but unlike BERT, BART doesn‚Äôt add a final feedforward network at the end to predict a word. 2. The encoder‚Äôs output is passed to the decoder, which must predict the masked tokens and any uncorrupted tokens from the encoder‚Äôs output. This gives additional context to help the decoder restore the original text. The output from the decoder is passed to a language modeling head, which performs a linear transformation to convert the hidden states into logits. The cross-entropy loss is calculated between the logits and the label, which is just the token shifted to the right. Ready to try your hand at summarization? Check out our complete [summarization guide](https://huggingface.co/docs/transformers/tasks/summarization) to learn how to finetune T5 and use it for inference! > For more information about text generation, check out thetext generation strategiesguide! ### Translation Translation involves converting text from one language to another while preserving its meaning. Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](https://huggingface.co/docs/transformers/model_doc/bart) or [T5](model_doc/t5) to do it. We‚Äôll explain how BART works in this section, and then you can try finetuning T5 at the end. BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to learn how to finetune T5 and use it for inference! > For more information about text generation, check out thetext generation strategiesguide! ### Translation Translation involves converting text from one language to another while preserving its meaning. Translation is another example of a sequence-to-sequence task, which means you can use an encoder-decoder model like [BART](https://huggingface.co/docs/transformers/model_doc/bart) or [T5](model_doc/t5) to do it. We‚Äôll explain how BART works in this section, and then you can try finetuning T5 at the end. BART adapts to translation by adding a separate randomly initialized encoder to map a source language to an input that can be decoded into the target language. This new encoder‚Äôs embeddings are passed to the pretrained encoder instead of the original word embeddings. The source encoder is trained by updating the source encoder, positional embeddings, and input embeddings with the cross-entropy loss from the model output. The model parameters are frozen in this first step, and all the model parameters are trained together in the second step. BART has since been followed up by a multilingual version, mBART, intended for translation and pretrained on many different languages. Ready to try your hand at translation? Check out our complete [translation guide](https://huggingface.co/docs/transformers/tasks/translation) to learn how to finetune T5 and use it for inference! > As you‚Äôve seen throughout this guide, many models follow similar patterns despite addressing different tasks. Understanding these common patterns can help you quickly grasp how new models work and how to adapt existing models to your specific needs. ## Modalities beyond text Transformers are not limited to text. They can also be applied to other modalities like speech and audio, images, and video. Of course, on this course we will focus on text, but we can briefly introduce the other modalities. ### Speech and audio Let‚Äôs start by exploring how Transformer models handle speech and audio data, which presents unique challenges compared to text or images. [Whisper](https://huggingface.co/docs/transformers/main/en/model_doc/whisper) is a encoder-decoder (sequence-to-sequence) transformer pretrained on 680,000 hours of labeled audio data. This amount of pretraining data enables zero-shot performance on audio tasks in English and many other languages. The decoder allows Whisper to map the encoders learned speech representations to useful outputs, such as text, without additional fine-tuning. Whisper just works out of the box. ![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png) Diagram is from [Whisper paper](https://huggingface.co/papers/2212.04356). This model has two main components: 1. An **encoder** processes the input audio. The raw audio is first converted into a log-Mel spectrogram. This spectrogram is then passed through a Transformer encoder network. 2. A **decoder** takes the encoded audio representation and autoregressively predicts the corresponding text tokens. It‚Äôs a standard Transformer decoder trained to predict the next text token given the previous tokens and the encoder output. Special tokens are used at the beginning of the decoder input to steer the model towards specific tasks like transcription, translation, or language identification. Whisper was pretrained on a massive and diverse dataset of 680,000 hours of labeled audio data collected from the web. This large-scale, weakly supervised pretraining is the",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "into a log-Mel spectrogram. This spectrogram is then passed through a Transformer encoder network. 2. A **decoder** takes the encoded audio representation and autoregressively predicts the corresponding text tokens. It‚Äôs a standard Transformer decoder trained to predict the next text token given the previous tokens and the encoder output. Special tokens are used at the beginning of the decoder input to steer the model towards specific tasks like transcription, translation, or language identification. Whisper was pretrained on a massive and diverse dataset of 680,000 hours of labeled audio data collected from the web. This large-scale, weakly supervised pretraining is the key to its strong zero-shot performance across many languages and tasks. Now that Whisper is pretrained, you can use it directly for zero-shot inference or finetune it on your data for improved performance on specific tasks like automatic speech recognition or speech translation! > The key innovation in Whisper is its training on an unprecedented scale of diverse, weakly supervised audio data from the internet. This allows it to generalize remarkably well to different languages, accents, and tasks without task-specific finetuning. ### Automatic speech recognition To use the pretrained model for automatic speech recognition, you leverage its full encoder-decoder structure. The encoder processes the audio input, and the decoder autoregressively generates the transcript token by token. When fine-tuning, the model is typically trained using a standard sequence-to-sequence loss (like cross-entropy) to predict the correct text tokens based on the audio input. The easiest way to use a fine-tuned model for inference is within a `pipeline`. ``` from transformers import pipeline transcriber = pipeline( task=\"automatic-speech-recognition\", model=\"openai/whisper-base.en\" ) transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\") # Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'} ``` Ready to try your hand at automatic speech recognition? Check out our complete [automatic speech recognition guide](https://huggingface.co/docs/transformers/tasks/asr) to learn how to finetune Whisper and use it for inference! ### Computer vision Now let‚Äôs move on to computer vision tasks, which deal with understanding and interpreting visual information from images or videos. There are two ways to approach computer vision tasks: 1. Split an image into a sequence of patches and process them in parallel with a Transformer. 2. Use a modern CNN, like [ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext), which relies on convolutional layers but adopts modern network designs. > A third approach mixes Transformers with convolutions (for example,Convolutional Vision TransformerorLeViT). We won‚Äôt discuss those because they just combine the two approaches we examine here. ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we‚Äôll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks. ### Image classification Image classification is one of the fundamental computer vision tasks. Let‚Äôs see how different model architectures approach this problem. ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions. [ViT](https://huggingface.co/docs/transformers/model_doc/vit) replaces convolutions entirely",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "won‚Äôt discuss those because they just combine the two approaches we examine here. ViT and ConvNeXT are commonly used for image classification, but for other vision tasks like object detection, segmentation, and depth estimation, we‚Äôll look at DETR, Mask2Former and GLPN, respectively; these models are better suited for those tasks. ### Image classification Image classification is one of the fundamental computer vision tasks. Let‚Äôs see how different model architectures approach this problem. ViT and ConvNeXT can both be used for image classification; the main difference is that ViT uses an attention mechanism while ConvNeXT uses convolutions. [ViT](https://huggingface.co/docs/transformers/model_doc/vit) replaces convolutions entirely with a pure Transformer architecture. If you‚Äôre familiar with the original Transformer, then you‚Äôre already most of the way toward understanding ViT. ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg) The main change ViT introduced was in how images are fed to a Transformer: 1. An image is split into square non-overlapping patches, each of which gets turned into a vector or *patch embedding*. The patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions (which for a base Transformer is 768 values for each patch embedding). If you had a 224x224 pixel image, you could split it into 196 16x16 image patches. Just like how text is tokenized into words, an image is ‚Äútokenized‚Äù into a sequence of patches. 2. A *learnable embedding* - a special `[CLS]` token - is added to the beginning of the patch embeddings just like BERT. The final hidden state of the `[CLS]` token is used as the input to the attached classification head; other outputs are ignored. This token helps the model learn how to encode a representation of the image. 3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn‚Äôt know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder. 4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT‚Äôs pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class. Ready to try your hand at image classification? Check out our complete [image classification guide](https://huggingface.co/docs/transformers/tasks/image_classification) to learn how to fine-tune ViT and use it for inference! > Notice the parallel between ViT and BERT: both use a special token ([CLS]) to capture the overall representation, both add position information to their embeddings, and both use a Transformer encoder to process the sequence of tokens/patches.",
    "metadata": {
      "title": "How ü§ó Transformers solve tasks",
      "url": "https://huggingface.co/learn/llm-course/chapter1/5",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/5",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Transformer Architectures In the previous sections, we introduced the general Transformer architecture and explored how these models can solve various tasks. Now, let‚Äôs take a closer look at the three main architectural variants of Transformer models and understand when to use each one. Then, we look at how those architectures are applied to different language tasks. In this section, we‚Äôre going to dive deeper into the three main architectural variants of Transformer models and understand when to use each one. > Remember that most Transformer models use one of three architectures: encoder-only, decoder-only, or encoder-decoder (sequence-to-sequence). Understanding these differences will help you choose the right model for your specific task. ## Encoder models Encoder models use only the encoder of a Transformer model. At each stage, the attention layers can access all the words in the initial sentence. These models are often characterized as having ‚Äúbi-directional‚Äù attention, and are often called *auto-encoding models*. The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence. Encoder models are best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition (and more generally word classification), and extractive question answering. > As we saw inHow ü§ó Transformers solve tasks, encoder models like BERT excel at understanding text because they can look at the entire context in both directions. This makes them perfect for tasks where comprehension of the whole input is important. Representatives of this family of models include: - [BERT](https://huggingface.co/docs/transformers/model_doc/bert) - [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) - [ModernBERT](https://huggingface.co/docs/transformers/en/model_doc/modernbert) ## Decoder models Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called *auto-regressive models*. The pretraining of decoder models usually revolves around predicting the next word in the sentence. These models are best suited for tasks involving text generation. > Decoder models like GPT are designed to generate text by predicting one token at a time. As we explored inHow ü§ó Transformers solve tasks, they can only see previous tokens, which makes them excellent for creative text generation but less ideal for tasks requiring bidirectional understanding. Representatives of this family of models include: - [Hugging Face SmolLM Series](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) - [Meta‚Äôs Llama Series](https://huggingface.co/docs/transformers/en/model_doc/llama4) - [Google‚Äôs Gemma Series](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3) - [DeepSeek‚Äôs V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) ### Modern Large Language Models (LLMs) Most modern Large Language Models (LLMs) use the decoder-only architecture. These models have grown dramatically in size and capabilities over the past few years, with some of the largest models containing hundreds of billions of parameters. Modern LLMs are typically trained in two phases: 1. **Pretraining**: The model learns to predict the next token on vast amounts of text data 2. **Instruction tuning**: The model is fine-tuned to follow instructions and generate helpful responses This approach has led to models that can understand and generate human-like text",
    "metadata": {
      "title": "Transformer Architectures",
      "url": "https://huggingface.co/learn/llm-course/chapter1/6",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/6",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "- [Google‚Äôs Gemma Series](https://huggingface.co/docs/transformers/main/en/model_doc/gemma3) - [DeepSeek‚Äôs V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) ### Modern Large Language Models (LLMs) Most modern Large Language Models (LLMs) use the decoder-only architecture. These models have grown dramatically in size and capabilities over the past few years, with some of the largest models containing hundreds of billions of parameters. Modern LLMs are typically trained in two phases: 1. **Pretraining**: The model learns to predict the next token on vast amounts of text data 2. **Instruction tuning**: The model is fine-tuned to follow instructions and generate helpful responses This approach has led to models that can understand and generate human-like text across a wide range of topics and tasks. #### Key capabilities of modern LLMs Modern decoder-based LLMs have demonstrated impressive capabilities: Capability Description Example Text generation Creating coherent and contextually relevant text Writing essays, stories, or emails Summarization Condensing long documents into shorter versions Creating executive summaries of reports Translation Converting text between languages Translating English to Spanish Question answering Providing answers to factual questions ‚ÄúWhat is the capital of France?‚Äù Code generation Writing or completing code snippets Creating a function based on a description Reasoning Working through problems step by step Solving math problems or logical puzzles Few-shot learning Learning from a few examples in the prompt Classifying text after seeing just 2-3 examples You can experiment with decoder-based LLMs directly in your browser via model repo pages on the Hub. Here‚Äôs an example with the classic [GPT-2](https://huggingface.co/openai-community/gpt2) (OpenAI‚Äôs finest open source model!): ## Sequence-to-sequence models Encoder-decoder models (also called *sequence-to-sequence models*) use both parts of the Transformer architecture. At each stage, the attention layers of the encoder can access all the words in the initial sentence, whereas the attention layers of the decoder can only access the words positioned before a given word in the input. The pretraining of these models can take different forms, but it often involves reconstructing a sentence for which the input has been somehow corrupted (for instance by masking random words). The pretraining of the T5 model consists of replacing random spans of text (that can contain several words) with a single mask special token, and the task is then to predict the text that this mask token replaces. Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering. > As we saw inHow ü§ó Transformers solve tasks, encoder-decoder models like BART and T5 combine the strengths of both architectures. The encoder provides deep bidirectional understanding of the input, while the decoder generates appropriate output text. This makes them perfect for tasks that transform one sequence into another, like translation or summarization. ### Practical applications Sequence-to-sequence models excel at tasks that require transforming one form of text into another while preserving meaning. Some practical applications include: Application Description Example Model Machine translation Converting text between languages Marian, T5 Text summarization Creating concise summaries of longer texts BART, T5 Data-to-text generation Converting structured data into natural language T5 Grammar",
    "metadata": {
      "title": "Transformer Architectures",
      "url": "https://huggingface.co/learn/llm-course/chapter1/6",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/6",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "encoder-decoder models like BART and T5 combine the strengths of both architectures. The encoder provides deep bidirectional understanding of the input, while the decoder generates appropriate output text. This makes them perfect for tasks that transform one sequence into another, like translation or summarization. ### Practical applications Sequence-to-sequence models excel at tasks that require transforming one form of text into another while preserving meaning. Some practical applications include: Application Description Example Model Machine translation Converting text between languages Marian, T5 Text summarization Creating concise summaries of longer texts BART, T5 Data-to-text generation Converting structured data into natural language T5 Grammar correction Fixing grammatical errors in text T5 Question answering Generating answers based on context BART, T5 Here‚Äôs an interactive demo of a sequence-to-sequence model for translation: Representatives of this family of models include: - [BART](https://huggingface.co/docs/transformers/model_doc/bart) - [mBART](https://huggingface.co/docs/transformers/model_doc/mbart) - [Marian](https://huggingface.co/docs/transformers/model_doc/marian) - [T5](https://huggingface.co/docs/transformers/model_doc/t5) ## Choosing the right architecture When working on a specific NLP task, how do you decide which architecture to use? Here‚Äôs a quick guide: Task Suggested Architecture Examples Text classification (sentiment, topic) Encoder BERT, RoBERTa Text generation (creative writing) Decoder GPT, LLaMA Translation Encoder-Decoder T5, BART Summarization Encoder-Decoder BART, T5 Named entity recognition Encoder BERT, RoBERTa Question answering (extractive) Encoder BERT, RoBERTa Question answering (generative) Encoder-Decoder or Decoder T5, GPT Conversational AI Decoder GPT, LLaMA > When in doubt about which model to use, consider:What kind of understanding does your task need? (Bidirectional or unidirectional)Are you generating new text or analyzing existing text?Do you need to transform one sequence into another?The answers to these questions will guide you toward the right architecture. ## The evolution of LLMs Large Language Models have evolved rapidly in recent years, with each generation bringing significant improvements in capabilities. ## Attention mechanisms Most transformer models use full attention in the sense that the attention matrix is square. It can be a big computational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and use a sparse version of the attention matrix to speed up training. > Standard attention mechanisms have a computational complexity of O(n¬≤), where n is the sequence length. This becomes problematic for very long sequences. The specialized attention mechanisms below help address this limitation. ### LSH attention [Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) uses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax dimension) of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only the keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is modified to mask the current token (except at the first position), because it will give a query and a key equal (so very similar to each other). Since the hash can be a bit random, several hash functions are used in practice (determined by a n_rounds parameter) and then are averaged together. ### Local attention [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) uses local attention: often, the local context",
    "metadata": {
      "title": "Transformer Architectures",
      "url": "https://huggingface.co/learn/llm-course/chapter1/6",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/6",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for each query q in Q, we can consider only the keys k in K that are close to q. A hash function is used to determine if q and k are close. The attention mask is modified to mask the current token (except at the first position), because it will give a query and a key equal (so very similar to each other). Since the hash can be a bit random, several hash functions are used in practice (determined by a n_rounds parameter) and then are averaged together. ### Local attention [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) uses local attention: often, the local context (e.g., what are the two tokens to the left and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small window, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a representation of the whole sentence. Some preselected input tokens are also given global attention: for those few tokens, the attention matrix can access all tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in their local window). This is shown in Figure 2d of the paper, see below for a sample attention mask: ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png) Using those attention matrices with less parameters then allows the model to have inputs having a bigger sequence length. ### Axial positional encodings [Reformer](https://huggingface.co/docs/transformers/model_doc/reformer) uses axial positional encodings: in traditional transformer models, the positional encoding E is a matrix of sizelll byddd,lll being the sequence length andddd the dimension of the hidden state. If you have very long texts, this matrix can be huge and take way too much space on the GPU. To alleviate that, axial positional encodings consist of factorizing that big matrix E in two smaller matrices E1 and E2, with dimensionsl1√ód1l_{1} \\times d_{1}l1‚Äã√ód1‚Äã andl2√ód2l_{2} \\times d_{2}l2‚Äã√ód2‚Äã, such thatl1√ól2=ll_{1} \\times l_{2} = ll1‚Äã√ól2‚Äã=l andd1+d2=dd_{1} + d_{2} = dd1‚Äã+d2‚Äã=d (with the product for the lengths, this ends up being way smaller). The embedding for time stepjjj in E is obtained by concatenating the embeddings for timestepj%l1j \\% l1j%l1 in E1 andj//l1j // l1j//l1 in E2. ## Conclusion In this section, we‚Äôve explored the three main Transformer architectures and some specialized attention mechanisms. Understanding these architectural differences is crucial for selecting the right model for your specific NLP task. As we move forward in the course, you‚Äôll get hands-on experience with these different architectures and learn how to fine-tune them for your specific needs. In the next section, we‚Äôll look at some of the limitations and biases present in these models that you should be aware of when deploying them.",
    "metadata": {
      "title": "Transformer Architectures",
      "url": "https://huggingface.co/learn/llm-course/chapter1/6",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/6",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Ungraded quiz So far, this chapter has covered a lot of ground! Don‚Äôt worry if you didn‚Äôt grasp all the details, but it‚Äôs to reflect on what you‚Äôve learned so far with a quiz. This quiz is ungraded, so you can try it as many times as you want. If you struggle with some questions, follow the tips and revisit the material. You‚Äôll be quizzed on this material again in the certification exam. ### 1. Explore the Hub and look for the roberta-large-mnli checkpoint. What task does it perform? Summarization Text classification Text generation ### 2. What will the following code return? ``` from transformers import pipeline ner = pipeline(\"ner\", grouped_entities=True) ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` It will return classification scores for this sentence, with labels \"positive\" or \"negative\". It will return a generated text completing this sentence. It will return the words representing persons, organizations or locations. ### 3. What should replace ‚Ä¶ in this code sample? ``` from transformers import pipeline filler = pipeline(\"fill-mask\", model=\"bert-base-cased\") result = filler(\"...\") ``` This <mask> has been waiting for you. This [MASK] has been waiting for you. This man has been waiting for you. ### 4. Why will this code fail? ``` from transformers import pipeline classifier = pipeline(\"zero-shot-classification\") result = classifier(\"This is a course about the Transformers library\") ``` This pipeline requires that labels be given to classify this text. This pipeline requires several sentences, not just one. The ü§ó Transformers library is broken, as usual. This pipeline requires longer inputs; this one is too short. ### 5. What does ‚Äútransfer learning‚Äù mean? Transferring the knowledge of a pretrained model to a new model by training it on the same dataset. Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights. Transferring the knowledge of a pretrained model to a new model by building the second model with the same architecture as the first model. ### 6. True or false? A language model usually does not need labels for its pretraining. True False ### 7. Select the sentence that best describes the terms ‚Äúmodel‚Äù, ‚Äúarchitecture‚Äù, and ‚Äúweights‚Äù. If a model is a building, its architecture is the blueprint and the weights are the people living inside. An architecture is a map to build a model and its weights are the cities represented on the map. An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters. ### 8. Which of these types of models would you use for completing prompts with generated text? An encoder model A decoder model A sequence-to-sequence model ### 9. Which of those types of models would you use for summarizing texts? An encoder model A decoder model A sequence-to-sequence model ### 10. Which of these types of models would you use for classifying text inputs according to certain labels? An encoder model A decoder model A sequence-to-sequence model",
    "metadata": {
      "title": "Ungraded quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter1/7",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/7",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "represented on the map. An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters. ### 8. Which of these types of models would you use for completing prompts with generated text? An encoder model A decoder model A sequence-to-sequence model ### 9. Which of those types of models would you use for summarizing texts? An encoder model A decoder model A sequence-to-sequence model ### 10. Which of these types of models would you use for classifying text inputs according to certain labels? An encoder model A decoder model A sequence-to-sequence model ### 11. What possible source can the bias observed in a model have? The model is a fine-tuned version of a pretrained model and it picked up its bias from it. The data the model was trained on is biased. The metric the model was optimizing for is biased.",
    "metadata": {
      "title": "Ungraded quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter1/7",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/7",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Deep dive into Text Generation Inference with LLMs So far, we‚Äôve explored the transformer architecture in relation to a range of discrete tasks, like text classification or summarization. However, Large Language Models are most used for text generation, and this is what we‚Äôll explore in this chapter. In this page, we‚Äôll explore the core concepts behind LLM inference, providing a comprehensive understanding of how these models generate text and the key components involved in the inference process. ## Understanding the Basics Let‚Äôs start with the fundamentals. Inference is the process of using a trained LLM to generate human-like text from a given input prompt. Language models use their knowledge from training to formulate responses one word at a time. The model leverages learned probabilities from billions of parameters to predict and generate the next token in a sequence. This sequential generation is what allows LLMs to produce coherent and contextually relevant text. ## The Role of Attention The attention mechanism is what gives LLMs their ability to understand context and generate coherent responses. When predicting the next word, not every word in a sentence carries equal weight - for example, in the sentence *‚ÄúThe capital of France is ‚Ä¶‚Äù*, the words ‚ÄúFrance‚Äù and ‚Äúcapital‚Äù are crucial for determining that ‚ÄúParis‚Äù should come next. This ability to focus on relevant information is what we call attention. ![Visual Gif of Attention](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AttentionSceneFinal.gif) This process of identifying the most relevant words to predict the next token has proven to be incredibly effective. Although the basic principle of training LLMs‚Äîpredicting the next token‚Äîhas remained generally consistent since BERT and GPT-2, there have been significant advancements in scaling neural networks and making the attention mechanism work for longer and longer sequences, at lower and lower costs. > In short, the attention mechanism is the key to LLMs being able to generate text that is both coherent and context-aware. It sets modern LLMs apart from previous generations of language models. ### Context Length and Attention Span Now that we understand attention, let‚Äôs explore how much context an LLM can actually handle. This brings us to context length, or the model‚Äôs ‚Äòattention span‚Äô. The context length refers to the maximum number of tokens (words or parts of words) that the LLM can process at once. Think of it as the size of the model‚Äôs working memory. These capabilities are limited by several practical factors: - The model‚Äôs architecture and size - Available computational resources - The complexity of the input and desired output In an ideal world, we could feed unlimited context to the model, but hardware constraints and computational costs make this impractical. This is why different models are designed with different context lengths to balance capability with efficiency. > The context length is the maximum number of tokens the model can consider at once when generating a response. ### The Art of Prompting When we pass information to LLMs, we structure our input in a way that guides the generation of the LLM toward the",
    "metadata": {
      "title": "Deep dive into Text Generation Inference with LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter1/8",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/8",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "architecture and size - Available computational resources - The complexity of the input and desired output In an ideal world, we could feed unlimited context to the model, but hardware constraints and computational costs make this impractical. This is why different models are designed with different context lengths to balance capability with efficiency. > The context length is the maximum number of tokens the model can consider at once when generating a response. ### The Art of Prompting When we pass information to LLMs, we structure our input in a way that guides the generation of the LLM toward the desired output. This is called *prompting*. Understanding how LLMs process information helps us craft better prompts. Since the model‚Äôs primary task is to predict the next token by analyzing the importance of each input token, the wording of your input sequence becomes crucial. > Careful design of the prompt makes it easierto guide the generation of the LLM toward the desired output. ## The Two-Phase Inference Process Now that we understand the basic components, let‚Äôs dive into how LLMs actually generate text. The process can be broken down into two main phases: prefill and decode. These phases work together like an assembly line, each playing a crucial role in producing coherent text. ### The Prefill Phase The prefill phase is like the preparation stage in cooking - it‚Äôs where all the initial ingredients are processed and made ready. This phase involves three key steps: 1. **Tokenization**: Converting the input text into tokens (think of these as the basic building blocks the model understands) 2. **Embedding Conversion**: Transforming these tokens into numerical representations that capture their meaning 3. **Initial Processing**: Running these embeddings through the model‚Äôs neural networks to create a rich understanding of the context This phase is computationally intensive because it needs to process all input tokens at once. Think of it as reading and understanding an entire paragraph before starting to write a response. You can experiment with different tokenizers in the interactive playground below: ### The Decode Phase After the prefill phase has processed the input, we move to the decode phase - this is where the actual text generation happens. The model generates one token at a time in what we call an autoregressive process (where each new token depends on all previous tokens). The decode phase involves several key steps that happen for each new token: 1. **Attention Computation**: Looking back at all previous tokens to understand context 2. **Probability Calculation**: Determining the likelihood of each possible next token 3. **Token Selection**: Choosing the next token based on these probabilities 4. **Continuation Check**: Deciding whether to continue or stop generation This phase is memory-intensive because the model needs to keep track of all previously generated tokens and their relationships. ## Sampling Strategies Now that we understand how the model generates text, let‚Äôs explore the various ways we can control this generation process. Just like a writer might choose between being more creative or more precise,",
    "metadata": {
      "title": "Deep dive into Text Generation Inference with LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter1/8",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/8",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "token: 1. **Attention Computation**: Looking back at all previous tokens to understand context 2. **Probability Calculation**: Determining the likelihood of each possible next token 3. **Token Selection**: Choosing the next token based on these probabilities 4. **Continuation Check**: Deciding whether to continue or stop generation This phase is memory-intensive because the model needs to keep track of all previously generated tokens and their relationships. ## Sampling Strategies Now that we understand how the model generates text, let‚Äôs explore the various ways we can control this generation process. Just like a writer might choose between being more creative or more precise, we can adjust how the model makes its token selections. You can interact with the basic decoding process yourself with SmolLM2 in this Space (remember, it decodes until reaching an **EOS** token which is **<|im_end|>** for this model): ### Understanding Token Selection: From Probabilities to Token Choices When the model needs to choose the next token, it starts with raw probabilities (called logits) for every word in its vocabulary. But how do we turn these probabilities into actual choices? Let‚Äôs break down the process: ![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/1.png) 1. **Raw Logits**: Think of these as the model‚Äôs initial gut feelings about each possible next word 2. **Temperature Control**: Like a creativity dial - higher settings (>1.0) make choices more random and creative, lower settings (<1.0) make them more focused and deterministic 3. **Top-p (Nucleus) Sampling**: Instead of considering all possible words, we only look at the most likely ones that add up to our chosen probability threshold (e.g., top 90%) 4. **Top-k Filtering**: An alternative approach where we only consider the k most likely next words ### Managing Repetition: Keeping Output Fresh One common challenge with LLMs is their tendency to repeat themselves - much like a speaker who keeps returning to the same points. To address this, we use two types of penalties: 1. **Presence Penalty**: A fixed penalty applied to any token that has appeared before, regardless of how often. This helps prevent the model from reusing the same words. 2. **Frequency Penalty**: A scaling penalty that increases based on how often a token has been used. The more a word appears, the less likely it is to be chosen again. ![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/2.png) These penalties are applied early in the token selection process, adjusting the raw probabilities before other sampling strategies are applied. Think of them as gentle nudges encouraging the model to explore new vocabulary. ### Controlling Generation Length: Setting Boundaries Just as a good story needs proper pacing and length, we need ways to control how much text our LLM generates. This is crucial for practical applications - whether we‚Äôre generating a tweet-length response or a full blog post. We can control generation length in several ways: 1. **Token Limits**: Setting minimum and maximum token counts 2. **Stop Sequences**: Defining specific patterns that signal the end of generation 3. **End-of-Sequence Detection**: Letting the model naturally conclude its response For example, if we want to generate a single paragraph, we might",
    "metadata": {
      "title": "Deep dive into Text Generation Inference with LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter1/8",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/8",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to explore new vocabulary. ### Controlling Generation Length: Setting Boundaries Just as a good story needs proper pacing and length, we need ways to control how much text our LLM generates. This is crucial for practical applications - whether we‚Äôre generating a tweet-length response or a full blog post. We can control generation length in several ways: 1. **Token Limits**: Setting minimum and maximum token counts 2. **Stop Sequences**: Defining specific patterns that signal the end of generation 3. **End-of-Sequence Detection**: Letting the model naturally conclude its response For example, if we want to generate a single paragraph, we might set a maximum of 100 tokens and use ‚Äú\\n\\n‚Äù as a stop sequence. This ensures our output stays focused and appropriately sized for its purpose. ![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/3.png) ### Beam Search: Looking Ahead for Better Coherence While the strategies we‚Äôve discussed so far make decisions one token at a time, beam search takes a more holistic approach. Instead of committing to a single choice at each step, it explores multiple possible paths simultaneously - like a chess player thinking several moves ahead. ![image](https://huggingface.co/reasoning-course/images/resolve/main/inference/4.png) Here‚Äôs how it works: 1. At each step, maintain multiple candidate sequences (typically 5-10) 2. For each candidate, compute probabilities for the next token 3. Keep only the most promising combinations of sequences and next tokens 4. Continue this process until reaching the desired length or stop condition 5. Select the sequence with the highest overall probability You can explore beam search visually here: This approach often produces more coherent and grammatically correct text, though it requires more computational resources than simpler methods. ## Practical Challenges and Optimization As we wrap up our exploration of LLM inference, let‚Äôs look at the practical challenges you‚Äôll face when deploying these models, and how to measure and optimize their performance. ### Key Performance Metrics When working with LLMs, four critical metrics will shape your implementation decisions: 1. **Time to First Token (TTFT)**: How quickly can you get the first response? This is crucial for user experience and is primarily affected by the prefill phase. 2. **Time Per Output Token (TPOT)**: How fast can you generate subsequent tokens? This determines the overall generation speed. 3. **Throughput**: How many requests can you handle simultaneously? This affects scaling and cost efficiency. 4. **VRAM Usage**: How much GPU memory do you need? This often becomes the primary constraint in real-world applications. ### The Context Length Challenge One of the most significant challenges in LLM inference is managing context length effectively. Longer contexts provide more information but come with substantial costs: - **Memory Usage**: Grows quadratically with context length - **Processing Speed**: Decreases linearly with longer contexts - **Resource Allocation**: Requires careful balancing of VRAM usage Recent models like [Qwen2.5-1M](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M) offer impressive 1M token context windows, but this comes at the cost of significantly slower inference times. The key is finding the right balance for your specific use case. Input Text (Raw) ‚Üí Tokenized Input Context Window(e.g., 4K tokens) Memory Usage‚àù Length¬≤ Processing Time‚àù Length ### The",
    "metadata": {
      "title": "Deep dive into Text Generation Inference with LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter1/8",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/8",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "One of the most significant challenges in LLM inference is managing context length effectively. Longer contexts provide more information but come with substantial costs: - **Memory Usage**: Grows quadratically with context length - **Processing Speed**: Decreases linearly with longer contexts - **Resource Allocation**: Requires careful balancing of VRAM usage Recent models like [Qwen2.5-1M](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M) offer impressive 1M token context windows, but this comes at the cost of significantly slower inference times. The key is finding the right balance for your specific use case. Input Text (Raw) ‚Üí Tokenized Input Context Window(e.g., 4K tokens) Memory Usage‚àù Length¬≤ Processing Time‚àù Length ### The KV Cache Optimization To address these challenges, one of the most powerful optimizations is KV (Key-Value) caching. This technique significantly improves inference speed by storing and reusing intermediate calculations. This optimization: - Reduces repeated calculations - Improves generation speed - Makes long-context generation practical The trade-off is additional memory usage, but the performance benefits usually far outweigh this cost. ## Conclusion Understanding LLM inference is crucial for effectively deploying and optimizing these powerful models. We‚Äôve covered the key components: - The fundamental role of attention and context - The two-phase inference process - Various sampling strategies for controlling generation - Practical challenges and optimizations By mastering these concepts, you‚Äôll be better equipped to build applications that leverage LLMs effectively and efficiently. Remember that the field of LLM inference is rapidly evolving, with new techniques and optimizations emerging regularly. Stay curious and keep experimenting with different approaches to find what works best for your specific use cases.",
    "metadata": {
      "title": "Deep dive into Text Generation Inference with LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter1/8",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/8",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Bias and limitations\n\n    \nIf your intent is to use a pretrained model or a fine-tuned version in production, please be aware that, while these models are powerful tools, they come with limitations. The biggest of these is that, to enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet.\n \nTo give a quick illustration, let‚Äôs go back to the example of a `fill-mask` pipeline with the BERT model:\n  \n```\nfrom transformers import pipeline\n\nunmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\nresult = unmasker(\"This man works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])\n\nresult = unmasker(\"This woman works as a [MASK].\")\nprint([r[\"token_str\"] for r in result])\n```\n  \n```\n['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']\n['nurse', 'waitress', 'teacher', 'maid', 'prostitute']\n```\n \nWhen asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender ‚Äî and yes, prostitute ended up in the top 5 possibilities the model associates with ‚Äúwoman‚Äù and ‚Äúwork.‚Äù This happens even though BERT is one of the rare Transformer models not built by scraping data from all over the internet, but rather using apparently neutral data (it‚Äôs trained on the [English Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) datasets).\n \nWhen you use these tools, you therefore need to keep in the back of your mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won‚Äôt make this intrinsic bias disappear.",
    "metadata": {
      "title": "Bias and limitations",
      "url": "https://huggingface.co/learn/llm-course/chapter1/9",
      "course": "llm-course",
      "chapter": "1. Transformer models",
      "chapter_id": "chapter1/9",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter1/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Argilla\n\n   \nIn Chapter 5 you learnt how to build a dataset using the ü§ó Datasets library and in Chapter 6 you explored how to fine-tune models for some common NLP tasks. In this chapter, you will learn how to use [Argilla](https://argilla.io) to **annotate and curate datasets** that you can use to train and evaluate your models.\n \nThe key to training models that perform well is to have high-quality data. Although there are some good datasets in the Hub that you could use to train and evaluate your models, these may not be relevant for your specific application or use case. In this scenario, you may want to build and curate a dataset of your own. Argilla will help you to do this efficiently.\n \n![Argilla sign in page.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/signin-hf-page.png)\n \nWith Argilla you can:\n \n- turn unstructured data into **structured data** to be used in NLP tasks.\n- curate a dataset to go from a low-quality dataset to a **high-quality dataset**.\n- gather **human feedback** for LLMs and multi-modal models.\n- invite experts to collaborate with you in Argilla, or crowdsource annotations!\n \nHere are some of the things that you will learn in this chapter:\n \n- How to set up your own Argilla instance.\n- How to load a dataset and configure it based on some popular NLP tasks.\n- How to use the Argilla UI to annotate your dataset.\n- How to use your curated dataset and export it to the Hub.",
    "metadata": {
      "title": "Introduction to Argilla",
      "url": "https://huggingface.co/learn/llm-course/chapter10/1",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Set up your Argilla instance\n\n    \nTo start using Argilla, you will need to set up your own Argilla instance first. Then you will need to install the Python SDK so that you can manage Argilla using Python code.\n \n\n## Deploy the Argilla UI\n\n \nThe easiest way to set up your Argilla instance is through Hugging Face Spaces. To create your Argilla Space, simply follow [this form](https://huggingface.co/new-space?template=argilla%2Fargilla-template-space). If you need further guidance, check the [Argilla quickstart](https://docs.argilla.io/latest/getting_started/quickstart/).\n \n![Space configuration form.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/space_config.png)\n \n> ‚ö†Ô∏è You may want to enablePersistent storageso the data isn‚Äôt lost if the Space is paused or restarted.\n> You can do that from the Settings of your Space.\n \nOnce Argilla is up and running, you can log in with your credentials.\n \n\n## Install and connect the Python SDK\n\n \nNow you can go to your Python environment or notebook and install the argilla library:\n \n`!pip install argilla`\n \nLet‚Äôs connect with our Argilla instance. To do that you will need the following information:\n \n- **Your API URL**: This is the URL where Argilla is running. If you are using a Space, you can open the Space, click on the three dots in the top right corner, then ‚ÄúEmbed this Space‚Äù and copy the **Direct URL**. It should look something like `https://<your-username>.<space-name>.hf.space`.\n- **Your API key**: To get your key, log in to your Argilla instance and go to ‚ÄúMy Settings‚Äù, then copy the API key.\n- **Your HF token**: If your Space is private, you will need to an Access Token in your Hugging Face Hub account with writing permissions.\n  \n```\nimport argilla as rg\n\nHF_TOKEN = \"...\"  # only for private spaces\n\nclient = rg.Argilla(\n    api_url=\"...\",\n    api_key=\"...\",\n    headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"},  # only for private spaces\n)\n```\n \nTo check that everything is working properly, we‚Äôll call `me`. This should return our user:\n  \n```\nclient.me\n```\n \nIf this worked, your Argilla instance is up and running and you‚Äôre connected to it! Congrats!\n \nWe can now get started with loading our first dataset to Argilla.",
    "metadata": {
      "title": "Set up your Argilla instance",
      "url": "https://huggingface.co/learn/llm-course/chapter10/2",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/2",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Load your dataset to Argilla Depending on the NLP task that you‚Äôre working with and the specific use case or application, your data and the annotation task will look differently. For this section of the course, we‚Äôll use [a dataset collecting news](https://huggingface.co/datasets/SetFit/ag_news) to complete two tasks: a text classification on the topic of each text and a token classification to identify the named entities mentioned. It is possible to import datasets from the Hub using the Argilla UI directly, but we‚Äôll be using the SDK to learn how we can make further edits to the data if needed. ## Configure your dataset The first step is to connect to our Argilla instance as we did in the previous section: ``` import argilla as rg HF_TOKEN = \"...\" # only for private spaces client = rg.Argilla( api_url=\"...\", api_key=\"...\", headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}, # only for private spaces ) ``` We can now think about the settings of our dataset in Argilla. These represent the annotation task we‚Äôll do over our data. First, we can load the dataset from the Hub and inspect its features, so that we can make sure that we configure the dataset correctly. ``` from datasets import load_dataset data = load_dataset(\"SetFit/ag_news\", split=\"train\") data.features ``` These are the features of our dataset: ``` {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'label_text': Value(dtype='string', id=None)} ``` It contains a `text` and also some initial labels for the text classification. We‚Äôll add those to our dataset settings together with a `spans` question for the named entities: ``` settings = rg.Settings( fields=[rg.TextField(name=\"text\")], questions=[ rg.LabelQuestion( name=\"label\", title=\"Classify the text:\", labels=data.unique(\"label_text\") ), rg.SpanQuestion( name=\"entities\", title=\"Highlight all the entities in the text:\", labels=[\"PERSON\", \"ORG\", \"LOC\", \"EVENT\"], field=\"text\", ), ], ) ``` Let‚Äôs dive a bit deeper into what these settings mean. First, we‚Äôve defined **fields**, these include the information that we‚Äôll be annotating. In this case, we only have one field and it comes in the form of a text, so we‚Äôve choosen a `TextField`. Then, we define **questions** that represent the tasks that we want to perform on our data: - For the text classification task we‚Äôve chosen a `LabelQuestion` and we used the unique values of the `label_text` column as our labels, to make sure that the question is compatible with the labels that already exist in the dataset. - For the token classification task, we‚Äôll need a `SpanQuestion`. We‚Äôve defined a set of labels that we‚Äôll be using for that task, plus the field on which we‚Äôll be drawing the spans. To learn more about all the available types of fields and questions and other advanced settings, like metadata and vectors, go to the [Argilla docs](https://docs.argilla.io/latest/how_to_guides/dataset/#define-dataset-settings). ## Upload the dataset Now that we‚Äôve defined some settings, we can create the dataset: ``` dataset = rg.Dataset(name=\"ag_news\", settings=settings) dataset.create() ``` The dataset now appears in our Argilla instance, but you will see that it‚Äôs empty: ![Screenshot of the empty dataset.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/empty_dataset.png) Now we need to add the records that we‚Äôll be annotating i.e., the rows in our",
    "metadata": {
      "title": "Load your dataset to Argilla",
      "url": "https://huggingface.co/learn/llm-course/chapter10/3",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/3",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "that we‚Äôll be using for that task, plus the field on which we‚Äôll be drawing the spans. To learn more about all the available types of fields and questions and other advanced settings, like metadata and vectors, go to the [Argilla docs](https://docs.argilla.io/latest/how_to_guides/dataset/#define-dataset-settings). ## Upload the dataset Now that we‚Äôve defined some settings, we can create the dataset: ``` dataset = rg.Dataset(name=\"ag_news\", settings=settings) dataset.create() ``` The dataset now appears in our Argilla instance, but you will see that it‚Äôs empty: ![Screenshot of the empty dataset.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/empty_dataset.png) Now we need to add the records that we‚Äôll be annotating i.e., the rows in our dataset. To do that, we‚Äôll simply need to log the data as records and provide a mapping for those elements that don‚Äôt have the same name in the hub and Argilla datasets: ``` dataset.records.log(data, mapping={\"label_text\": \"label\"}) ``` In our mapping, we‚Äôve specified that the `label_text` column in the dataset should be mapped to the question with the name `label`. In this way, we‚Äôll use the existing labels in the dataset as pre-annotations so we can annotate faster. While the records continue to log, you can already start working with your dataset in the Argilla UI. At this point, it should look like this: ![Screenshot of the dataset in Argilla.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/argilla_initial_dataset.png) Now our dataset is ready to start annotating!",
    "metadata": {
      "title": "Load your dataset to Argilla",
      "url": "https://huggingface.co/learn/llm-course/chapter10/3",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/3",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Annotate your dataset\n\n   \nNow it is time to start working from the Argilla UI to annotate our dataset.\n \n\n## Align your team with annotation guidelines\n\n \nBefore you start annotating your dataset, it is always good practice to write some guidelines, especially if you‚Äôre working as part of a team. This will help you align on the task and the use of the different labels, and resolve questions or conflicts when they come up.\n \nIn Argilla, you can go to your dataset settings page in the UI and modify the guidelines and the descriptions of your questions to help with alignment.\n \n![Screenshot of the Dataset Settings page in Argilla.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/argilla_dataset_settings.png)\n \nIf you want to dive deeper into the topic of how to write good guidelines, we recommend reading [this blogpost](https://argilla.io/blog/annotation-guidelines-practices) and the bibliographical references mentioned there.\n \n\n## Distribute the task\n\n \nIn the dataset settings page, you can also change the dataset distribution settings. This will help you annotate more efficiently when you‚Äôre working as part of a team. The default value for the minimum submitted responses is 1, meaning that as soon as a record has 1 submitted response it will be considered complete and count towards the progress in your dataset.\n \nSometimes, you want to have more than one submitted response per record, for example, if you want to analyze the inter-annotator agreement in your task. In that case, make sure to change this setting to a higher number, but always smaller or equal to the total number of annotators. If you‚Äôre working on the task alone, you want this setting to be 1.\n \n\n## Annotate records\n\n \n> üí° If you are deploying Argilla in a Hugging Face Space, any team members will be able to log in using the Hugging Face OAuth. Otherwise, you may need to create users for them followingthis guide.\n \nWhen you open your dataset, you will realize that the first question is already filled in with some suggested labels. That‚Äôs because in the previous section we mapped our question called `label` to the `label_text` column in the dataset, so that we simply need to review and correct the already existing labels:\n \n![Screenshot of the dataset in Argilla.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/argilla_initial_dataset.png)\n \nFor the token classification, we‚Äôll need to add all labels manually, as we didn‚Äôt include any suggestions. This is how it might look after the span annotations:\n \n![Screenshot of the dataset in Argilla with spans annotated.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter10/argilla_dataset_with_spans.png)\n \nAs you move through the different records, there are different actions you can take:\n \n- submit your responses, once you‚Äôre done with the record.\n- save them as a draft, in case you want to come back to them later.\n- discard them, if the record souldn‚Äôt be part of the dataset or you won‚Äôt give responses to it.\n \nIn the next section, you will learn how you can export and use those annotations.",
    "metadata": {
      "title": "Annotate your dataset",
      "url": "https://huggingface.co/learn/llm-course/chapter10/4",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/4",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Use your annotated dataset\n\n    \nWe will learn now how to export and use the annotated data that we have in Argilla.\n \n\n## Load the dataset\n\n \nFirst, we‚Äôll need to make sure that we‚Äôre connected to our Argilla instance as in the previous steps:\n  \n```\nimport argilla as rg\n\nHF_TOKEN = \"...\"  # only for private spaces\n\nclient = rg.Argilla(\n    api_url=\"...\",\n    api_key=\"...\",\n    headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"},  # only for private spaces\n)\n```\n \nAnd now, we‚Äôll load the dataset that we‚Äôll be working with:\n  \n```\ndataset = client.datasets(name=\"ag_news\")\n```\n \nLoading the dataset and calling its records with `dataset.records` is enough to start using your dataset and records for your own purposes and pipelines. However, we‚Äôll also learn how to do a few optional operations, like filtering the records and exporting your dataset to the Hugging Face Hub.\n \n\n## Filter the dataset\n\n \nSometimes you only want to use the records that have been completed, so we will first filter the records in our dataset based on their status:\n  \n```\nstatus_filter = rg.Query(filter=rg.Filter([(\"status\", \"==\", \"completed\")]))\n\nfiltered_records = dataset.records(status_filter)\n```\n \n> ‚ö†Ô∏è Note that the records withcompletedstatus (i.e., records that meet the minimum submitted responses configured in the task distribution settings) could have more than one response and that each response can have any status fromsubmitted,draftordiscarded.\n \nLearn more about querying and filtering records in the [Argilla docs](https://docs.argilla.io/latest/how_to_guides/query/).\n \n\n## Export to the Hub\n\n \nWe can now export our annotations to the Hugging Face Hub, so we can share them with others. To do this, we‚Äôll need to convert the records into a ü§ó Dataset and then push it to the Hub:\n  \n```\nfiltered_records.to_datasets().push_to_hub(\"argilla/ag_news_annotated\")\n```\n \nAlternatively, we can export directly the complete Argilla dataset (including pending records) like this:\n  \n```\ndataset.to_hub(repo_id=\"argilla/ag_news_annotated\")\n```\n \nThis is an interesting choice in case others want to open the dataset in their Argilla instances, as the settings are automatically saved and they can simply import the full dataset using a single line of code:\n  \n```\ndataset = rg.Dataset.from_hub(repo_id=\"argilla/ag_news_annotated\")\n```",
    "metadata": {
      "title": "Use your annotated dataset",
      "url": "https://huggingface.co/learn/llm-course/chapter10/5",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/5",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Argilla, check!\n\n   \nThat‚Äôs all! Congrats! üëè\n \nIn this chapter, you learnt the basic steps to:\n \n- set up Argilla.\n- annotate to improve the quality of your dataset.\n- adapt an existing dataset and re-use it for a different NLP task.\n- share your annotated dataset with the community in the Hugging Face Hub.\n \n\n## What‚Äôs next?\n\n \n- Check more step-by-step tutorials for other popular tasks in the [tutorials page](https://docs.argilla.io/latest/tutorials/).\n- You can also explore other examples of datasets in this [demo](https://demo.argilla.io/sign-in?auth=ZGVtbzoxMjM0NTY3OA==).\n- If you‚Äôd like to keep learning about Argilla and more advanced features, check the [Argilla documentation](https://docs.argilla.io/latest/).",
    "metadata": {
      "title": "Argilla, check!",
      "url": "https://huggingface.co/learn/llm-course/chapter10/6",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/6",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter quiz\n\n   \nLet‚Äôs test what you learned in this chapter!\n \n\n### 1. What can you use Argilla for?\n\n  Turn unstructured data into structured data for NLP tasks  Scrap a public website to build a dataset  Improve the quality of an existing dataset  Adapt an existing dataset to your own use case  Train your model  Generate synthetic datasets   \n\n### 2. Argilla ONLY works in the Hugging Face Spaces and with Hugging Face Datasets.\n\n  True  False   \n\n### 3. You need a Hugging Face token to connect the Python SDK to your Argilla server.\n\n  True  False   \n\n### 4. What are fields in Argilla? How many fields can you use?\n\n  Fields show the data that we are annotating. All this information needs to be collected in a single field.  Fields show the data that we are annotating. All this information can be spread across multiple fields.  Fields contain the metadata of the records. You can use as many as you need.   \n\n### 5. What‚Äôs the best type of question for a token classification task?\n\n  A SpanQuestion  A LabelQuestion  A TextQuestion  None of the above   \n\n### 6. What is the purpose of the ‚ÄúSave as draft‚Äù button?\n\n  Submit your responses  Save your responses without submitting them  Discard a record   \n\n### 7. Argilla does not offer suggested labels automatically, you need to provide that data yourself.\n\n  True  False   \n\n### 8. Select all the necessary steps to export an Argilla dataset in full to the Hub:\n\n  You need to be connected to your Argilla server: `client= rg.Argilla(api_url='...', api_key='...')`  Import the dataset from the hub: `dataset = rg.Dataset.from_hub(repo_id='argilla/ag_news_annotated')`  Load the dataset: `dataset = client.datasets(name='my_dataset')`  Convert the Argilla dataset into a Datasets dataset: `dataset = dataset.to_datasets()`  Use the to_hub method to export the dataset: `dataset.to_hub(repo_id='my_username/dataset_name')`",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter10/7",
      "course": "llm-course",
      "chapter": "10. Curate high-quality datasets",
      "chapter_id": "chapter10/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter10/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Supervised Fine-Tuning\n\n \nIn [Chapter 2 Section 2](/course/chapter2/2), we saw that generative language models can be fine-tuned on specific tasks like summarization and question answering. However, nowadays it is far more common to fine-tune language models on a broad range of tasks simultaneously; a method known as supervised fine-tuning (SFT). This process helps models become more versatile and capable of handling diverse use cases. Most LLMs that people interact with on platforms like ChatGPT have undergone SFT to make them more helpful and aligned with human preferences. We will separate this chapter into four sections:\n \n\n## 1Ô∏è‚É£ Chat Templates\n\n \nChat templates structure interactions between users and AI models, ensuring consistent and contextually appropriate responses. They include components like system prompts and role-based messages.\n \n\n## 2Ô∏è‚É£ Supervised Fine-Tuning\n\n \nSupervised Fine-Tuning (SFT) is a critical process for adapting pre-trained language models to specific tasks. It involves training the model on a task-specific dataset with labeled examples. For a detailed guide on SFT, including key steps and best practices, see [the supervised fine-tuning section of the TRL documentation](https://huggingface.co/docs/trl/en/sft_trainer).\n \n\n## 3Ô∏è‚É£ Low Rank Adaptation (LoRA)\n\n \nLow Rank Adaptation (LoRA) is a technique for fine-tuning language models by adding low-rank matrices to the model‚Äôs layers. This allows for efficient fine-tuning while preserving the model‚Äôs pre-trained knowledge. One of the key benefits of LoRA is the significant memory savings it offers, making it possible to fine-tune large models on hardware with limited resources.\n \n\n## 4Ô∏è‚É£ Evaluation\n\n \nEvaluation is a crucial step in the fine-tuning process. It allows us to measure the performance of the model on a task-specific dataset.\n \n> ‚ö†Ô∏è In order to benefit from all features available with the Model Hub and ü§ó Transformers, we recommendcreating an account.\n \n\n## References\n\n \n- [Transformers documentation on chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating)\n- [Script for Supervised Fine-Tuning in TRL](https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py)\n- [SFTTrainerin TRL](https://huggingface.co/docs/trl/main/en/sft_trainer)\n- [Direct Preference Optimization Paper](https://arxiv.org/abs/2305.18290)\n- [Supervised Fine-Tuning with TRL](https://huggingface.co/docs/trl/sft_trainer)\n- [How to fine-tune Google Gemma with ChatML and Hugging Face TRL](https://github.com/huggingface/alignment-handbook)\n- [Fine-tuning LLM to Generate Persian Product Catalogs in JSON Format](https://huggingface.co/learn/cookbook/en/fine_tuning_llm_to_generate_persian_product_catalogs_in_json_format)",
    "metadata": {
      "title": "Supervised Fine-Tuning",
      "url": "https://huggingface.co/learn/llm-course/chapter11/1",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Chat Templates ## Introduction Chat templates are essential for structuring interactions between language models and users. Whether you‚Äôre building a simple chatbot or a complex AI agent, understanding how to properly format your conversations is crucial for getting the best results from your model. In this guide, we‚Äôll explore what chat templates are, why they matter, and how to use them effectively. > Chat templates are crucial for:Maintaining consistent conversation structureEnsuring proper role identificationManaging context across multiple turnsSupporting advanced features like tool use ## Model Types and Templates ### Base Models vs Instruct Models A base model is trained on raw text data to predict the next token, while an instruct model is fine-tuned specifically to follow instructions and engage in conversations. For example, [SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) is a base model, while [SmolLM2-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) is its instruction-tuned variant. Instruction tuned models are trained to follow a specific conversational structure, making them more suitable for chatbot applications. Moreover, instruct models can handle complex interactions, including tool use, multimodal inputs, and function calling. To make a base model behave like an instruct model, we need to format our prompts in a consistent way that the model can understand. This is where chat templates come in. ChatML is one such template format that structures conversations with clear role indicators (system, user, assistant). Here‚Äôs a guide on [ChatML](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/blob/e2c3f7557efbdec707ae3a336371d169783f1da1/tokenizer_config.json#L146). > When using an instruct model, always verify you‚Äôre using the correct chat template format. Using the wrong template can result in poor model performance or unexpected behavior. The easiest way to ensure this is to check the model tokenizer configuration on the Hub. For example, theSmolLM2-135M-Instructmodel usesthis configuration. ### Common Template Formats Before diving into specific implementations, it‚Äôs important to understand how different models expect their conversations to be formatted. Let‚Äôs explore some common template formats using a simple example conversation: We‚Äôll use the following conversation structure for all examples: ``` messages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}, {\"role\": \"assistant\", \"content\": \"Hi! How can I help you today?\"}, {\"role\": \"user\", \"content\": \"What's the weather?\"}, ] ``` This is the ChatML template used in models like SmolLM2 and Qwen 2: ``` <|im_start|>system You are a helpful assistant.<|im_end|> <|im_start|>user Hello!<|im_end|> <|im_start|>assistant Hi! How can I help you today?<|im_end|> <|im_start|>user What's the weather?<|im_start|>assistant ``` This is using the `mistral` template format: ``` <s>[INST] You are a helpful assistant. [/INST] Hi! How can I help you today?</s> [INST] Hello! [/INST] ``` Key differences between these formats include: 1. **System Message Handling**: - Llama 2 wraps system messages in `<<SYS>>` tags - Llama 3 uses `<|system|>` tags with `</s>` endings - Mistral includes system message in the first instruction - Qwen uses explicit `system` role with `<|im_start|>` tags - ChatGPT uses `SYSTEM:` prefix 2. **Message Boundaries**: - Llama 2 uses `[INST]` and `[/INST]` tags - Llama 3 uses role-specific tags (`<|system|>`, `<|user|>`, `<|assistant|>`) with `</s>` endings - Mistral uses `[INST]` and `[/INST]` with `<s>` and `</s>` - Qwen uses role-specific start/end tokens 3. **Special",
    "metadata": {
      "title": "Chat Templates",
      "url": "https://huggingface.co/learn/llm-course/chapter11/2",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/2",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "I help you today?</s> [INST] Hello! [/INST] ``` Key differences between these formats include: 1. **System Message Handling**: - Llama 2 wraps system messages in `<<SYS>>` tags - Llama 3 uses `<|system|>` tags with `</s>` endings - Mistral includes system message in the first instruction - Qwen uses explicit `system` role with `<|im_start|>` tags - ChatGPT uses `SYSTEM:` prefix 2. **Message Boundaries**: - Llama 2 uses `[INST]` and `[/INST]` tags - Llama 3 uses role-specific tags (`<|system|>`, `<|user|>`, `<|assistant|>`) with `</s>` endings - Mistral uses `[INST]` and `[/INST]` with `<s>` and `</s>` - Qwen uses role-specific start/end tokens 3. **Special Tokens**: - Llama 2 uses `<s>` and `</s>` for conversation boundaries - Llama 3 uses `</s>` to end each message - Mistral uses `<s>` and `</s>` for turn boundaries - Qwen uses role-specific start/end tokens Understanding these differences is key to working with various models. Let‚Äôs look at how the transformers library helps us handle these variations automatically: ``` from transformers import AutoTokenizer # These will use different templates automatically mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\") qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\") smol_tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\") messages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}, ] # Each will format according to its model's template mistral_chat = mistral_tokenizer.apply_chat_template(messages, tokenize=False) qwen_chat = qwen_tokenizer.apply_chat_template(messages, tokenize=False) smol_chat = smol_tokenizer.apply_chat_template(messages, tokenize=False) ``` Click to see template examples Qwen 2 and SmolLM2 ChatML template: ``` <|im_start|>system You are a helpful assistant.<|im_end|> <|im_start|>user Hello!<|im_end|> <|im_start|>assistant Hi! How can I help you today?<|im_end|> <|im_start|>user What's the weather?<|im_start|>assistant ``` Mistral template: ``` <s>[INST] You are a helpful assistant. [/INST] Hi! How can I help you today?</s> [INST] Hello! [/INST] ``` ### Advanced Features Chat templates can handle more complex scenarios beyond just conversational interactions, including: 1. **Tool Use**: When models need to interact with external tools or APIs 2. **Multimodal Inputs**: For handling images, audio, or other media types 3. **Function Calling**: For structured function execution 4. **Multi-turn Context**: For maintaining conversation history > When implementing advanced features:Test thoroughly with your specific model. Vision and tool use template are particularly diverse.Monitor token usage carefully between each feature and model.Document the expected format for each feature For multimodal conversations, chat templates can include image references or base64-encoded images: ``` messages = [ { \"role\": \"system\", \"content\": \"You are a helpful vision assistant that can analyze images.\", }, { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"What's in this image?\"}, {\"type\": \"image\", \"image_url\": \"https://example.com/image.jpg\"}, ], }, ] ``` Here‚Äôs an example of a chat template with tool use: ``` messages = [ { \"role\": \"system\", \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\", }, {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"}, { \"role\": \"assistant\", \"content\": \"Let me help you with that.\", \"tool_calls\": [ { \"tool\": \"calculator\", \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456}, }, {\"tool\": \"weather_api\", \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}}, ], }, {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"}, { \"role\": \"tool\", \"tool_name\": \"weather_api\", \"content\": \"{'condition': 'rain', 'temperature': 15}\",",
    "metadata": {
      "title": "Chat Templates",
      "url": "https://huggingface.co/learn/llm-course/chapter11/2",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/2",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "\"image\", \"image_url\": \"https://example.com/image.jpg\"}, ], }, ] ``` Here‚Äôs an example of a chat template with tool use: ``` messages = [ { \"role\": \"system\", \"content\": \"You are an AI assistant that can use tools. Available tools: calculator, weather_api\", }, {\"role\": \"user\", \"content\": \"What's 123 * 456 and is it raining in Paris?\"}, { \"role\": \"assistant\", \"content\": \"Let me help you with that.\", \"tool_calls\": [ { \"tool\": \"calculator\", \"parameters\": {\"operation\": \"multiply\", \"x\": 123, \"y\": 456}, }, {\"tool\": \"weather_api\", \"parameters\": {\"city\": \"Paris\", \"country\": \"France\"}}, ], }, {\"role\": \"tool\", \"tool_name\": \"calculator\", \"content\": \"56088\"}, { \"role\": \"tool\", \"tool_name\": \"weather_api\", \"content\": \"{'condition': 'rain', 'temperature': 15}\", }, ] ``` ## Best Practices ### General Guidelines When working with chat templates, follow these key practices: 1. **Consistent Formatting**: Always use the same template format throughout your application 2. **Clear Role Definition**: Clearly specify roles (system, user, assistant, tool) for each message 3. **Context Management**: Be mindful of token limits when maintaining conversation history 4. **Error Handling**: Include proper error handling for tool calls and multimodal inputs 5. **Validation**: Validate message structure before sending to the model > Common pitfalls to avoid:Mixing different template formats in the same applicationExceeding token limits with long conversation historiesNot properly escaping special characters in messagesForgetting to validate input message structureIgnoring model-specific template requirements ## Hands-on Exercise Let‚Äôs practice implementing chat templates with a real-world example. > Follow these steps to convert theHuggingFaceTB/smoltalkdataset into chatml format:Load the dataset:Copiedfromdatasetsimportload_dataset > > dataset = load_dataset(\"HuggingFaceTB/smoltalk\")Create a processing function:Copieddefconvert_to_chatml(example):return{\"messages\": [ > {\"role\":\"user\",\"content\": example[\"input\"]}, > {\"role\":\"assistant\",\"content\": example[\"output\"]}, > ] > }Apply the chat template using your chosen model‚Äôs tokenizerRemember to validate your output format matches your target model‚Äôs requirements! ## Additional Resources - [Hugging Face Chat Templating Guide](https://huggingface.co/docs/transformers/main/en/chat_templating) - [Transformers Documentation](https://huggingface.co/docs/transformers) - [Chat Templates Examples Repository](https://github.com/chujiezheng/chat_templates)",
    "metadata": {
      "title": "Chat Templates",
      "url": "https://huggingface.co/learn/llm-course/chapter11/2",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/2",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Supervised Fine-Tuning Supervised Fine-Tuning (SFT) is a process primarily used to adapt pre-trained language models to follow instructions, engage in dialogue, and use specific output formats. While pre-trained models have impressive general capabilities, SFT helps transform them into assistant-like models that can better understand and respond to user prompts. This is typically done by training on datasets of human-written conversations and instructions. This page provides a step-by-step guide to fine-tuning the [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) model using the [SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer). By following these steps, you can adapt the model to perform specific tasks more effectively. ## When to Use SFT Before diving into implementation, it‚Äôs important to understand when SFT is the right choice for your project. As a first step, you should consider whether using an existing instruction-tuned model with well-crafted prompts would suffice for your use case. SFT involves significant computational resources and engineering effort, so it should only be pursued when prompting existing models proves insufficient. > Consider SFT only if you:Need additional performance beyond what prompting can achieveHave a specific use case where the cost of using a large general-purpose model outweighs the cost of fine-tuning a smaller modelRequire specialized output formats or domain-specific knowledge that existing models struggle with If you determine that SFT is necessary, the decision to proceed depends on two primary factors: ### Template Control SFT allows precise control over the model‚Äôs output structure. This is particularly valuable when you need the model to: 1. Generate responses in a specific chat template format 2. Follow strict output schemas 3. Maintain consistent styling across responses ### Domain Adaptation When working in specialized domains, SFT helps align the model with domain-specific requirements by: 1. Teaching domain terminology and concepts 2. Enforcing professional standards 3. Handling technical queries appropriately 4. Following industry-specific guidelines > Before starting SFT, evaluate whether your use case requires:Precise output formattingDomain-specific knowledgeConsistent response patternsAdherence to specific guidelinesThis evaluation will help determine if SFT is the right approach for your needs. ## Dataset Preparation The supervised fine-tuning process requires a task-specific dataset structured with input-output pairs. Each pair should consist of: 1. An input prompt 2. The expected model response 3. Any additional context or metadata The quality of your training data is crucial for successful fine-tuning. Let‚Äôs look at how to prepare and validate your dataset: ## Training Configuration The success of your fine-tuning depends heavily on choosing the right training parameters. Let‚Äôs explore each important parameter and how to configure them effectively: The SFTTrainer configuration requires consideration of several parameters that control the training process. Let‚Äôs explore each parameter and their purpose: 1. **Training Duration Parameters**: - `num_train_epochs`: Controls total training duration - `max_steps`: Alternative to epochs, sets maximum number of training steps - More epochs allow better learning but risk overfitting 2. **Batch Size Parameters**: - `per_device_train_batch_size`: Determines memory usage and training stability - `gradient_accumulation_steps`: Enables larger effective batch sizes - Larger batches provide more stable gradients but require more memory 3. **Learning Rate Parameters**: - `learning_rate`: Controls size of weight updates",
    "metadata": {
      "title": "Supervised Fine-Tuning",
      "url": "https://huggingface.co/learn/llm-course/chapter11/3",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/3",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "each important parameter and how to configure them effectively: The SFTTrainer configuration requires consideration of several parameters that control the training process. Let‚Äôs explore each parameter and their purpose: 1. **Training Duration Parameters**: - `num_train_epochs`: Controls total training duration - `max_steps`: Alternative to epochs, sets maximum number of training steps - More epochs allow better learning but risk overfitting 2. **Batch Size Parameters**: - `per_device_train_batch_size`: Determines memory usage and training stability - `gradient_accumulation_steps`: Enables larger effective batch sizes - Larger batches provide more stable gradients but require more memory 3. **Learning Rate Parameters**: - `learning_rate`: Controls size of weight updates - `warmup_ratio`: Portion of training used for learning rate warmup - Too high can cause instability, too low results in slow learning 4. **Monitoring Parameters**: - `logging_steps`: Frequency of metric logging - `eval_steps`: How often to evaluate on validation data - `save_steps`: Frequency of model checkpoint saves > Start with conservative values and adjust based on monitoring:Begin with 1-3 epochsUse smaller batch sizes initiallyMonitor validation metrics closelyAdjust learning rate if training is unstable ## Implementation with TRL Now that we understand the key components, let‚Äôs implement the training with proper validation and monitoring. We will use the `SFTTrainer` class from the Transformers Reinforcement Learning (TRL) library, which is built on top of the `transformers` library. Here‚Äôs a complete example using the TRL library: ``` from datasets import load_dataset from trl import SFTConfig, SFTTrainer import torch # Set device device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Load dataset dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"all\") # Configure model and tokenizer model_name = \"HuggingFaceTB/SmolLM2-135M\" model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to( device ) tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name) # Setup chat template model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer) # Configure trainer training_args = SFTConfig( output_dir=\"./sft_output\", max_steps=1000, per_device_train_batch_size=4, learning_rate=5e-5, logging_steps=10, save_steps=100, eval_strategy=\"steps\", eval_steps=50, ) # Initialize trainer trainer = SFTTrainer( model=model, args=training_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], processing_class=tokenizer, ) # Start training trainer.train() ``` > When using a dataset with a ‚Äúmessages‚Äù field (like the example above), the SFTTrainer automatically applies the model‚Äôs chat template, which it retrieves from the hub. This means you don‚Äôt need any additional configuration to handle chat-style conversations - the trainer will format the messages according to the model‚Äôs expected template format. ## Packing the Dataset The SFTTrainer supports example packing to optimize training efficiency. This feature allows multiple short examples to be packed into the same input sequence, maximizing GPU utilization during training. To enable packing, simply set `packing=True` in the SFTConfig constructor. When using packed datasets with `max_steps`, be aware that you may train for more epochs than expected depending on your packing configuration. You can customize how examples are combined using a formatting function - particularly useful when working with datasets that have multiple fields like question-answer pairs. For evaluation datasets, you can disable packing by setting `eval_packing=False` in the SFTConfig. Here‚Äôs a basic example of customizing the packing configuration: ``` # Configure packing training_args = SFTConfig(packing=True) trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args) trainer.train() ``` When packing the dataset with multiple fields, you can define a custom formatting",
    "metadata": {
      "title": "Supervised Fine-Tuning",
      "url": "https://huggingface.co/learn/llm-course/chapter11/3",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/3",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in the SFTConfig constructor. When using packed datasets with `max_steps`, be aware that you may train for more epochs than expected depending on your packing configuration. You can customize how examples are combined using a formatting function - particularly useful when working with datasets that have multiple fields like question-answer pairs. For evaluation datasets, you can disable packing by setting `eval_packing=False` in the SFTConfig. Here‚Äôs a basic example of customizing the packing configuration: ``` # Configure packing training_args = SFTConfig(packing=True) trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args) trainer.train() ``` When packing the dataset with multiple fields, you can define a custom formatting function to combine the fields into a single input sequence. This function should take a list of examples and return a dictionary with the packed input sequence. Here‚Äôs an example of a custom formatting function: ``` def formatting_func(example): text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\" return text training_args = SFTConfig(packing=True) trainer = SFTTrainer( \"facebook/opt-350m\", train_dataset=dataset, args=training_args, formatting_func=formatting_func, ) ``` ## Monitoring Training Progress Effective monitoring is crucial for successful fine-tuning. Let‚Äôs explore what to watch for during training: ### Understanding Loss Patterns Training loss typically follows three distinct phases: 1. Initial Sharp Drop: Rapid adaptation to new data distribution 2. Gradual Stabilization: Learning rate slows as model fine-tunes 3. Convergence: Loss values stabilize, indicating training completion ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/nlp_course_sft_loss_graphic.png) ### Metrics to Monitor Effective monitoring involves tracking quantitative metrics, and evaluating qualitative metrics. Available metrics are: - Training loss - Validation loss - Learning rate progression - Gradient norms > Watch for these warning signs during training:Validation loss increasing while training loss decreases (overfitting)No significant improvement in loss values (underfitting)Extremely low loss values (potential memorization)Inconsistent output formatting (template learning issues) ### The Path to Convergence As training progresses, the loss curve should gradually stabilize. The key indicator of healthy training is a small gap between training and validation loss, suggesting the model is learning generalizable patterns rather than memorizing specific examples. The absolute loss values will vary depending on your task and dataset. ### Monitoring Training Progress The graph above shows a typical training progression. Notice how both training and validation loss decrease sharply at first, then gradually level off. This pattern indicates the model is learning effectively while maintaining generalization ability. ### Warning Signs to Watch For Several patterns in the loss curves can indicate potential issues. Below we illustrate common warning signs and solutions that we can consider. ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_1.png) If the validation loss decreases at a significantly slower rate than training loss, your model is likely overfitting to the training data. Consider: - Reducing the training steps - Increasing the dataset size - Validating dataset quality and diversity ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_2.png) If the loss doesn‚Äôt show significant improvement, the model might be: - Learning too slowly (try increasing the learning rate) - Struggling with the task (check data quality and task complexity) - Hitting architecture limitations (consider a different model) ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_3.png) Extremely low loss values could suggest memorization rather than learning. This is particularly concerning if: -",
    "metadata": {
      "title": "Supervised Fine-Tuning",
      "url": "https://huggingface.co/learn/llm-course/chapter11/3",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/3",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_1.png) If the validation loss decreases at a significantly slower rate than training loss, your model is likely overfitting to the training data. Consider: - Reducing the training steps - Increasing the dataset size - Validating dataset quality and diversity ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_2.png) If the loss doesn‚Äôt show significant improvement, the model might be: - Learning too slowly (try increasing the learning rate) - Struggling with the task (check data quality and task complexity) - Hitting architecture limitations (consider a different model) ![SFTTrainer Training](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/sft_loss_3.png) Extremely low loss values could suggest memorization rather than learning. This is particularly concerning if: - The model performs poorly on new, similar examples - The outputs lack diversity - The responses are too similar to training examples > Monitor both the loss values and the model‚Äôs actual outputs during training. Sometimes the loss can look good while the model develops unwanted behaviors. Regular qualitative evaluation of the model‚Äôs responses helps catch issues that metrics alone might miss. We should note that the interpretation of the loss values we outline here is aimed on the most common case, and in fact, loss values can behave on various ways depending on the model, the dataset, the training parameters, etc. If you interested in exploring more about outlined patterns, you should check out this blog post by the people at [Fast AI](https://www.fast.ai/posts/2023-09-04-learning-jumps/). ## Evaluation after SFT In section [11.4](/en/chapter11/4) we will learn how to evaluate the model using benchmark datasets. For now, we will focus on the qualitative evaluation of the model. After completing SFT, consider these follow-up actions: 1. Evaluate the model thoroughly on held-out test data 2. Validate template adherence across various inputs 3. Test domain-specific knowledge retention 4. Monitor real-world performance metrics > Document your training process, including:Dataset characteristicsTraining parametersPerformance metricsKnown limitations > This documentation will be valuable for future model iterations. ## Quiz ### 1. What parameters control the training duration in SFT? num_train_epochs and max_steps batch_size and learning_rate gradient_checkpointing and warmup_ratio ### 2. Which pattern in the loss curves indicates potential overfitting? Validation loss increases while training loss continues to decrease Both training and validation loss decrease steadily Training loss remains constant while validation loss decreases ### 3. What is gradient_accumulation_steps used for? To increase effective batch size without using more memory To save checkpoints during training To control the learning rate schedule ### 4. What should you monitor during SFT training? Both quantitative metrics and qualitative outputs Only the training loss Only the model's output quality ### 5. What indicates healthy convergence during training? A small gap between training and validation loss Training loss reaching zero Validation loss being lower than training loss ## üíê Nice work! You‚Äôve learned how to fine-tune models using SFT! To continue your learning: 1. Try the notebook with different parameters 2. Experiment with other datasets 3. Contribute improvements to the course material ## Additional Resources - [TRL Documentation](https://huggingface.co/docs/trl) - [SFT Examples Repository](https://github.com/huggingface/trl/blob/main/trl/scripts/sft.py) - [Fine-tuning Best Practices](https://huggingface.co/docs/transformers/training)",
    "metadata": {
      "title": "Supervised Fine-Tuning",
      "url": "https://huggingface.co/learn/llm-course/chapter11/3",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/3",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# LoRA (Low-Rank Adaptation) Fine-tuning large language models is a resource intensive process. LoRA is a technique that allows us to fine-tune large language models with a small number of parameters. It works by adding and optimizing smaller matrices to the attention weights, typically reducing trainable parameters by about 90%. ## Understanding LoRA LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the pre-trained model weights and injects trainable rank decomposition matrices into the model‚Äôs layers. Instead of training all model parameters during fine-tuning, LoRA decomposes the weight updates into smaller matrices through low-rank decomposition, significantly reducing the number of trainable parameters while maintaining model performance. For example, when applied to GPT-3 175B, LoRA reduced trainable parameters by 10,000x and GPU memory requirements by 3x compared to full fine-tuning. You can read more about LoRA in the [LoRA paper](https://arxiv.org/pdf/2106.09685). LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on attention weights. During inference, these adapter weights can be merged with the base model, resulting in no additional latency overhead. LoRA is particularly useful for adapting large language models to specific tasks or domains while keeping resource requirements manageable. ## Key advantages of LoRA 1. **Memory Efficiency**: - Only adapter parameters are stored in GPU memory - Base model weights remain frozen and can be loaded in lower precision - Enables fine-tuning of large models on consumer GPUs 2. **Training Features**: - Native PEFT/LoRA integration with minimal setup - Support for QLoRA (Quantized LoRA) for even better memory efficiency 3. **Adapter Management**: - Adapter weight saving during checkpoints - Features to merge adapters back into base model ## Loading LoRA Adapters with PEFT [PEFT](https://github.com/huggingface/peft) is a library that provides a unified interface for loading and managing PEFT methods, including LoRA. It allows you to easily load and switch between different PEFT methods, making it easier to experiment with different fine-tuning techniques. Adapters can be loaded onto a pretrained model with `load_adapter()`, which is useful for trying out different adapters whose weights aren‚Äôt merged. Set the active adapter weights with the `set_adapter()` function. To return the base model, you could use unload() to unload all of the LoRA modules. This makes it easy to switch between different task-specific weights. ``` from peft import PeftModel, PeftConfig config = PeftConfig.from_pretrained(\"ybelkada/opt-350m-lora\") model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path) lora_model = PeftModel.from_pretrained(model, \"ybelkada/opt-350m-lora\") ``` ![lora_load_adapter](https://github.com/huggingface/smol-course/raw/main/3_parameter_efficient_finetuning/images/lora_adapter.png) ## Fine-tune LLM using trl and the SFTTrainer with LoRA The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. This means that we can fine-tune a model in the same way as we did with SFT, but use LoRA to reduce the number of parameters we need to train. We‚Äôll use the `LoRAConfig` class from PEFT in our example. The setup requires just a few configuration steps: 1. Define the LoRA configuration (rank, alpha, dropout) 2. Create the SFTTrainer with PEFT config 3. Train and save the adapter weights ## LoRA Configuration Let‚Äôs walk through the LoRA configuration and key parameters. Parameter Description `r` (rank)",
    "metadata": {
      "title": "LoRA (Low-Rank Adaptation)",
      "url": "https://huggingface.co/learn/llm-course/chapter11/4",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/4",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "LoRA The [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` provides integration with LoRA adapters through the [PEFT](https://huggingface.co/docs/peft/en/index) library. This means that we can fine-tune a model in the same way as we did with SFT, but use LoRA to reduce the number of parameters we need to train. We‚Äôll use the `LoRAConfig` class from PEFT in our example. The setup requires just a few configuration steps: 1. Define the LoRA configuration (rank, alpha, dropout) 2. Create the SFTTrainer with PEFT config 3. Train and save the adapter weights ## LoRA Configuration Let‚Äôs walk through the LoRA configuration and key parameters. Parameter Description `r` (rank) Dimension of the low-rank matrices used for weight updates. Typically between 4-32. Lower values provide more compression but potentially less expressiveness. `lora_alpha` Scaling factor for LoRA layers, usually set to 2x the rank value. Higher values result in stronger adaptation effects. `lora_dropout` Dropout probability for LoRA layers, typically 0.05-0.1. Higher values help prevent overfitting during training. `bias` Controls training of bias terms. Options are ‚Äúnone‚Äù, ‚Äúall‚Äù, or ‚Äúlora_only‚Äù. ‚Äúnone‚Äù is most common for memory efficiency. `target_modules` Specifies which model modules to apply LoRA to. Can be ‚Äúall-linear‚Äù or specific modules like ‚Äúq_proj,v_proj‚Äù. More modules enable greater adaptability but increase memory usage. > When implementing PEFT methods, start with small rank values (4-8) for LoRA and monitor training loss. Use validation sets to prevent overfitting and compare results with full fine-tuning baselines when possible. The effectiveness of different methods can vary by task, so experimentation is key. ## Using TRL with PEFT PEFT methods can be combined with TRL for fine-tuning to reduce memory requirements. We can pass the `LoraConfig` to the model when loading it. ``` from peft import LoraConfig # TODO: Configure LoRA parameters # r: rank dimension for LoRA update matrices (smaller = more compression) rank_dimension = 6 # lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation) lora_alpha = 8 # lora_dropout: dropout probability for LoRA layers (helps prevent overfitting) lora_dropout = 0.05 peft_config = LoraConfig( r=rank_dimension, # Rank dimension - typically between 4-32 lora_alpha=lora_alpha, # LoRA scaling factor - typically 2x rank lora_dropout=lora_dropout, # Dropout probability for LoRA layers bias=\"none\", # Bias type for LoRA. the corresponding biases will be updated during training. target_modules=\"all-linear\", # Which modules to apply LoRA to task_type=\"CAUSAL_LM\", # Task type for model architecture ) ``` Above, we used `device_map=\"auto\"` to automatically assign the model to the correct device. You can also manually assign the model to a specific device using `device_map={\"\": device_index}`. We will also need to define the `SFTTrainer` with the LoRA configuration. ``` # Create SFTTrainer with LoRA configuration trainer = SFTTrainer( model=model, args=args, train_dataset=dataset[\"train\"], peft_config=peft_config, # LoRA configuration max_seq_length=max_seq_length, # Maximum sequence length processing_class=tokenizer, ) ``` > ‚úèÔ∏èTry it out!Build on your fine-tuned model from the previous section, but fine-tune it with LoRA. Use theHuggingFaceTB/smoltalkdataset to fine-tune adeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5Bmodel, using the LoRA configuration we defined above. ## Merging LoRA Adapters After training with LoRA, you might want to merge the adapter weights back into the base model for",
    "metadata": {
      "title": "LoRA (Low-Rank Adaptation)",
      "url": "https://huggingface.co/learn/llm-course/chapter11/4",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/4",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "also manually assign the model to a specific device using `device_map={\"\": device_index}`. We will also need to define the `SFTTrainer` with the LoRA configuration. ``` # Create SFTTrainer with LoRA configuration trainer = SFTTrainer( model=model, args=args, train_dataset=dataset[\"train\"], peft_config=peft_config, # LoRA configuration max_seq_length=max_seq_length, # Maximum sequence length processing_class=tokenizer, ) ``` > ‚úèÔ∏èTry it out!Build on your fine-tuned model from the previous section, but fine-tune it with LoRA. Use theHuggingFaceTB/smoltalkdataset to fine-tune adeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5Bmodel, using the LoRA configuration we defined above. ## Merging LoRA Adapters After training with LoRA, you might want to merge the adapter weights back into the base model for easier deployment. This creates a single model with the combined weights, eliminating the need to load adapters separately during inference. The merging process requires attention to memory management and precision. Since you‚Äôll need to load both the base model and adapter weights simultaneously, ensure sufficient GPU/CPU memory is available. Using `device_map=\"auto\"` in `transformers` will find the correct device for the model based on your hardware. Maintain consistent precision (e.g., float16) throughout the process, matching the precision used during training and saving the merged model in the same format for deployment. ## Merging Implementation After training a LoRA adapter, you can merge the adapter weights back into the base model. Here‚Äôs how to do it: ``` import torch from transformers import AutoModelForCausalLM from peft import PeftModel # 1. Load the base model base_model = AutoModelForCausalLM.from_pretrained( \"base_model_name\", torch_dtype=torch.float16, device_map=\"auto\" ) # 2. Load the PEFT model with adapter peft_model = PeftModel.from_pretrained( base_model, \"path/to/adapter\", torch_dtype=torch.float16 ) # 3. Merge adapter weights with base model merged_model = peft_model.merge_and_unload() ``` If you encounter size discrepancies in the saved model, ensure you‚Äôre also saving the tokenizer: ``` # Save both model and tokenizer tokenizer = AutoTokenizer.from_pretrained(\"base_model_name\") merged_model.save_pretrained(\"path/to/save/merged_model\") tokenizer.save_pretrained(\"path/to/save/merged_model\") ``` > ‚úèÔ∏èTry it out!Merge the adapter weights back into the base model. Use theHuggingFaceTB/smoltalkdataset to fine-tune adeepseek-ai/DeepSeek-R1-Distill-Qwen-1.5Bmodel, using the LoRA configuration we defined above. # Resources - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685) - [PEFT Documentation](https://huggingface.co/docs/peft) - [Hugging Face blog post on PEFT](https://huggingface.co/blog/peft)",
    "metadata": {
      "title": "LoRA (Low-Rank Adaptation)",
      "url": "https://huggingface.co/learn/llm-course/chapter11/4",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/4",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Evaluation With a finetuned model through either SFT or LoRA SFT, we should evaluate it on standard benchmarks. As machine learning engineers you should maintain a suite of relevant evaluations for your targeted domain of interest. In this page, we will look at some of the most common benchmarks and how to use them to evaluate your model. We‚Äôll also look at how to create custom benchmarks for your specific use case. ## Automatic Benchmarks Automatic benchmarks serve as standardized tools for evaluating language models across different tasks and capabilities. While they provide a useful starting point for understanding model performance, it‚Äôs important to recognize that they represent only one piece of a comprehensive evaluation strategy. ## Understanding Automatic Benchmarks Automatic benchmarks typically consist of curated datasets with predefined tasks and evaluation metrics. These benchmarks aim to assess various aspects of model capability, from basic language understanding to complex reasoning. The key advantage of using automatic benchmarks is their standardization - they allow for consistent comparison across different models and provide reproducible results. However, it‚Äôs crucial to understand that benchmark performance doesn‚Äôt always translate directly to real-world effectiveness. A model that excels at academic benchmarks may still struggle with specific domain applications or practical use cases. ## General Knowledge Benchmarks [MMLU](https://huggingface.co/datasets/cais/mmlu) (Massive Multitask Language Understanding) tests knowledge across 57 subjects, from science to humanities. While comprehensive, it may not reflect the depth of expertise needed for specific domains. TruthfulQA evaluates a model‚Äôs tendency to reproduce common misconceptions, though it can‚Äôt capture all forms of misinformation. ## Reasoning Benchmarks [BBH](https://huggingface.co/datasets/lukaemon/bbh) (Big Bench Hard) and [GSM8K](https://huggingface.co/datasets/openai/gsm8k) focus on complex reasoning tasks. BBH tests logical thinking and planning, while GSM8K specifically targets mathematical problem-solving. These benchmarks help assess analytical capabilities but may not capture the nuanced reasoning required in real-world scenarios. ## Language Understanding [HELM](https://github.com/stanford-crfm/helm) provides a holistic evaluation framework. Benchmarks like HELM offer insights into language processing capabilities on aspects like commonsense, world knowledge, and reasoning. But may not fully represent the complexity of natural conversation or domain-specific terminology. ## Domain-Specific Benchmarks Let‚Äôs look at a few benchmarks that focus on specific domains like math, coding, and chat. The [MATH benchmark](https://huggingface.co/papers/2103.03874) is another important evaluation tool for mathematical reasoning. It consists of 12,500 problems from mathematics competitions, covering algebra, geometry, number theory, counting, probability, and more. What makes MATH particularly challenging is that it requires multi-step reasoning, formal mathematical notation understanding, and the ability to generate step-by-step solutions. Unlike simpler arithmetic tasks, MATH problems often demand sophisticated problem-solving strategies and mathematical concept applications. The [HumanEval Benchmark](https://github.com/openai/human-eval) is a coding-focused evaluation dataset consisting of 164 programming problems. The benchmark tests a model‚Äôs ability to generate functionally correct Python code that solves the given programming tasks. What makes HumanEval particularly valuable is that it evaluates both code generation capabilities and functional correctness through actual test case execution, rather than just superficial similarity to reference solutions. The problems range from basic string manipulation to more complex algorithms and data structures. [Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) is an",
    "metadata": {
      "title": "Evaluation",
      "url": "https://huggingface.co/learn/llm-course/chapter11/5",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/5",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and the ability to generate step-by-step solutions. Unlike simpler arithmetic tasks, MATH problems often demand sophisticated problem-solving strategies and mathematical concept applications. The [HumanEval Benchmark](https://github.com/openai/human-eval) is a coding-focused evaluation dataset consisting of 164 programming problems. The benchmark tests a model‚Äôs ability to generate functionally correct Python code that solves the given programming tasks. What makes HumanEval particularly valuable is that it evaluates both code generation capabilities and functional correctness through actual test case execution, rather than just superficial similarity to reference solutions. The problems range from basic string manipulation to more complex algorithms and data structures. [Alpaca Eval](https://tatsu-lab.github.io/alpaca_eval/) is an automated evaluation framework designed to assess the quality of instruction-following language models. It uses GPT-4 as a judge to evaluate model outputs across various dimensions including helpfulness, honesty, and harmlessness. The framework includes a dataset of 805 carefully curated prompts and can evaluate responses against multiple reference models like Claude, GPT-4, and others. What makes Alpaca Eval particularly useful is its ability to provide consistent, scalable evaluations without requiring human annotators, while still capturing nuanced aspects of model performance that traditional metrics might miss. ## Alternative Evaluation Approaches Many organizations have developed alternative evaluation methods to address the limitations of standard benchmarks: ### LLM-as-Judge Using one language model to evaluate another‚Äôs outputs has become increasingly popular. This approach can provide more nuanced feedback than traditional metrics, though it comes with its own biases and limitations. ### Evaluation Arenas Evaluation arenas like [Chatbot Arena](https://lmarena.ai/) offer a unique approach to LLM assessment through crowdsourced feedback. In these platforms, users engage in anonymous ‚Äúbattles‚Äù between two LLMs, asking questions and voting on which model provides better responses. This approach captures real-world usage patterns and preferences through diverse, challenging questions, with studies showing strong agreement between crowd-sourced votes and expert evaluations. While powerful, these platforms have limitations including potential user base bias, skewed prompt distributions, and a primary focus on helpfulness rather than safety considerations. ### Custom Benchmark Suites Organizations often develop internal benchmark suites tailored to their specific needs and use cases. These might include domain-specific knowledge tests or evaluation scenarios that mirror actual deployment conditions. ## Custom Evaluation While standard benchmarks provide a useful baseline, they shouldn‚Äôt be your only evaluation method. Here‚Äôs how to develop a more comprehensive approach: 1. Start with relevant standard benchmarks to establish a baseline and enable comparison with other models. 2. Identify the specific requirements and challenges of your use case. What tasks will your model actually perform? What kinds of errors would be most problematic? 3. Develop custom evaluation datasets that reflect your actual use case. This might include: - Real user queries from your domain - Common edge cases you‚Äôve encountered - Examples of particularly challenging scenarios 4. Consider implementing a multi-layered evaluation strategy: - Automated metrics for quick feedback - Human evaluation for nuanced understanding - Domain expert review for specialized applications - A/B testing in controlled environments ## Implementing Custom Evaluations In this section, we will implement evaluation for our finetuned model.",
    "metadata": {
      "title": "Evaluation",
      "url": "https://huggingface.co/learn/llm-course/chapter11/5",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/5",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "challenges of your use case. What tasks will your model actually perform? What kinds of errors would be most problematic? 3. Develop custom evaluation datasets that reflect your actual use case. This might include: - Real user queries from your domain - Common edge cases you‚Äôve encountered - Examples of particularly challenging scenarios 4. Consider implementing a multi-layered evaluation strategy: - Automated metrics for quick feedback - Human evaluation for nuanced understanding - Domain expert review for specialized applications - A/B testing in controlled environments ## Implementing Custom Evaluations In this section, we will implement evaluation for our finetuned model. We can use [lighteval](https://github.com/huggingface/lighteval) to evaluate our finetuned model on standard benchmarks, which contains a wide range of tasks built into the library. We just need to define the tasks we want to evaluate and the parameters for the evaluation. LightEval tasks are defined using a specific format: ``` {suite}|{task}|{num_few_shot}|{auto_reduce} ``` Parameter Description `suite` The benchmark suite (e.g., ‚Äòmmlu‚Äô, ‚Äòtruthfulqa‚Äô) `task` Specific task within the suite (e.g., ‚Äòabstract_algebra‚Äô) `num_few_shot` Number of examples to include in prompt (0 for zero-shot) `auto_reduce` Whether to automatically reduce few-shot examples if prompt is too long (0 or 1) Example: `\"mmlu|abstract_algebra|0|0\"` evaluates on MMLU‚Äôs abstract algebra task with zero-shot inference. ## Example Evaluation Pipeline Let‚Äôs set up an evaluation pipeline for our finetuned model. We will evaluate the model on set of sub tasks that relate to the domain of medicine. Here‚Äôs a complete example of evaluating on automatic benchmarks relevant to one specific domain using Lighteval with the VLLM backend: ``` lighteval accelerate \\ \"pretrained=your-model-name\" \\ \"mmlu|anatomy|0|0\" \\ \"mmlu|high_school_biology|0|0\" \\ \"mmlu|high_school_chemistry|0|0\" \\ \"mmlu|professional_medicine|0|0\" \\ --max_samples 40 \\ --batch_size 1 \\ --output_path \"./results\" \\ --save_generations true ``` Results are displayed in a tabular format showing: ``` | Task |Version|Metric|Value | |Stderr| |----------------------------------------|------:|------|-----:|---|-----:| |all | |acc |0.3333|¬± |0.1169| |leaderboard:mmlu:_average:5 | |acc |0.3400|¬± |0.1121| |leaderboard:mmlu:anatomy:5 | 0|acc |0.4500|¬± |0.1141| |leaderboard:mmlu:high_school_biology:5 | 0|acc |0.1500|¬± |0.0819| ``` Lighteval also include a python API for more detailed evaluation tasks, which is useful for manipulating the results in a more flexible way. Check out the [Lighteval documentation](https://huggingface.co/docs/lighteval/using-the-python-api) for more information. > ‚úèÔ∏èTry it out!Evaluate your finetuned model on a specific task in lighteval. # End-of-chapter quiz ### 1. What are the main advantages of using automatic benchmarks for model evaluation? They provide perfect real-world performance metrics They allow for standardized comparison between models and provide reproducible results They eliminate the need for any other form of evaluation ### 2. Which benchmark specifically tests knowledge across 57 different subjects? BBH (Big Bench Hard) GSM8K MMLU ### 3. What is LLM-as-Judge? Using one language model to evaluate another's outputs A benchmark that tests judicial reasoning A method for training models on legal datasets ### 4. What should be included in a comprehensive evaluation strategy? Only standard benchmarks Standard benchmarks, custom evaluation datasets, and domain-specific testing Only custom datasets specific to your use case ### 5. What is a limitation of automatic benchmarks? They are too expensive to run Benchmark performance doesn't always translate",
    "metadata": {
      "title": "Evaluation",
      "url": "https://huggingface.co/learn/llm-course/chapter11/5",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/5",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for any other form of evaluation ### 2. Which benchmark specifically tests knowledge across 57 different subjects? BBH (Big Bench Hard) GSM8K MMLU ### 3. What is LLM-as-Judge? Using one language model to evaluate another's outputs A benchmark that tests judicial reasoning A method for training models on legal datasets ### 4. What should be included in a comprehensive evaluation strategy? Only standard benchmarks Standard benchmarks, custom evaluation datasets, and domain-specific testing Only custom datasets specific to your use case ### 5. What is a limitation of automatic benchmarks? They are too expensive to run Benchmark performance doesn't always translate directly to real-world effectiveness They can only evaluate small models ### 6. What is the purpose of creating custom evaluation datasets? To reflect your specific use case and include real user queries from your domain To replace standard benchmarks entirely To make evaluation easier",
    "metadata": {
      "title": "Evaluation",
      "url": "https://huggingface.co/learn/llm-course/chapter11/5",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/5",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Conclusion\n\n \nIn this chapter, we explored the essential components of fine-tuning language models:\n \n1. **Chat Templates** provide structure to model interactions, ensuring consistent and appropriate responses through standardized formatting.\n2. **Supervised Fine-Tuning (SFT)** allows adaptation of pre-trained models to specific tasks while maintaining their foundational knowledge.\n3. **LoRA** offers an efficient approach to fine-tuning by reducing trainable parameters while preserving model performance.\n4. **Evaluation** helps measure and validate the effectiveness of fine-tuning through various metrics and benchmarks.\n \nThese techniques, when combined, enable the creation of specialized language models that can excel at specific tasks while remaining computationally efficient. Whether you‚Äôre building a customer service bot or a domain-specific assistant, understanding these concepts is crucial for successful model adaptation.",
    "metadata": {
      "title": "Conclusion",
      "url": "https://huggingface.co/learn/llm-course/chapter11/6",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/6",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Exam Time!\n\n \nIt‚Äôs time to put your knowledge to the test! We‚Äôve prepared a short quiz for you to test your understanding of the concepts covered in this chapter.\n \nTo take the quiz, you will need to follow these steps:\n \n1. Sign in to your Hugging Face account.\n2. Answer the questions in the quiz.\n3. Submit your answers.\n \n\n## Multiple Choice Quiz\n\n \nIn this quiz, you will be asked to select the correct answer from a list of options. We‚Äôll test you on the fundamentals of supervised finetuning.\n  \n\n## Code Quiz\n\n \nIn this quiz, you will be asked to write code to complete a task. We‚Äôll test you on the code you‚Äôve studied in the course from libraries like `datasets`, `transformers`, `peft`, and `TRL`.",
    "metadata": {
      "title": "Exam Time!",
      "url": "https://huggingface.co/learn/llm-course/chapter11/7",
      "course": "llm-course",
      "chapter": "11. Fine-tune Large Language Models",
      "chapter_id": "chapter11/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter11/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Open R1 for Students Welcome to an exciting journey into the world of open-source AI with reinforcement learning! This chapter is designed to help students understand reinforcement learning and its role in LLMs. We will also explore [Open R1](https://github.com/huggingface/open-r1), a groundbreaking community project that‚Äôs making advanced AI accessible to everyone. Specifically, this course is to help students and learners to use and contribute to [Open R1](https://github.com/huggingface/open-r1). ## What You‚Äôll Learn In this chapter, we‚Äôll break down complex concepts into easy-to-understand pieces and show you how you can be part of this exciting project to make LLMs reason on complex problems. LLMs have shown excellent performance on many generative tasks. However, up until recently they have struggled on complex problems that require reasoning. For example, they struggle to deal with puzzles or math problems that require multiple steps of reasoning. Open R1 is a project that aims to make LLMs reason on complex problems. It does this by using reinforcement learning to encourage LLMs to ‚Äòthink‚Äô and reason. In simple terms, the model is trained to generate thoughts as well as outputs, and to structure these thoughts and outputs so that they can be handled separately by the user. Let‚Äôs take a look at an example. As we gave ourself the task of solving the following problem, we might think like this: ``` Problem: \"I have 3 apples and 2 oranges. How many pieces of fruit do I have in total?\" Thought: \"I need to add the number of apples and oranges to get the total number of pieces of fruit.\" Answer: \"5\" ``` We can then structure this thought and answer so that they can be handled separately by the user. For reasoning tasks, LLMs can be trained to generate thoughts and answers in the following format: ``` <think>I need to add the number of apples and oranges to get the total number of pieces of fruit.</think> 5 ``` As a user, we can then extract the thought and answer from the model‚Äôs output and use them to solve the problem. ## Why This Matters for Students As a student, understanding Open R1 and the role of reinforcement learning in LLMs is valuable because: - It shows you how cutting-edge AI is developed - It gives you hands-on opportunities to learn and contribute - It helps you understand where AI technology is heading - It opens doors to future career opportunities in AI ## Chapter Overview This chapter is divided into four sections, each focusing on a different aspect of Open R1: ### 1Ô∏è‚É£ Introduction to Reinforcement Learning and its Role in LLMs We‚Äôll explore the basics of Reinforcement Learning (RL) and its role in training LLMs. - What is RL? - How is RL used in LLMs? - What is DeepSeek R1? - What are the key innovations of DeepSeek R1? ### 2Ô∏è‚É£ Understanding the DeepSeek R1 Paper We‚Äôll break down the research paper that inspired [Open R1](https://huggingface.co/open-r1): - Key innovations and breakthroughs - The training process and",
    "metadata": {
      "title": "Open R1 for Students",
      "url": "https://huggingface.co/learn/llm-course/chapter12/1",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/1",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "career opportunities in AI ## Chapter Overview This chapter is divided into four sections, each focusing on a different aspect of Open R1: ### 1Ô∏è‚É£ Introduction to Reinforcement Learning and its Role in LLMs We‚Äôll explore the basics of Reinforcement Learning (RL) and its role in training LLMs. - What is RL? - How is RL used in LLMs? - What is DeepSeek R1? - What are the key innovations of DeepSeek R1? ### 2Ô∏è‚É£ Understanding the DeepSeek R1 Paper We‚Äôll break down the research paper that inspired [Open R1](https://huggingface.co/open-r1): - Key innovations and breakthroughs - The training process and architecture - Results and their significance ### 3Ô∏è‚É£ Implementing GRPO in TRL We‚Äôll get practical with code examples: - How to use the Transformer Reinforcement Learning (TRL) library - Setting up GRPO training ### 4Ô∏è‚É£ Practical use case to align a model We‚Äôll look at a practical use case to align a model using Open R1. - How to train a model using GRPO in TRL - Share your model on the [Hugging Face Hub](https://huggingface.co/models) ## Prerequisites To get the most out of this chapter, it‚Äôs helpful to have: - Solid understanding of Python programming - Familiarity with machine learning concepts - Interest in AI and language models Don‚Äôt worry if you‚Äôre missing some of these ‚Äì we‚Äôll explain key concepts as we go along! üöÄ > If you don‚Äôt have all the prerequisites, check out thiscoursefrom units 1 to 11 ## How to Use This Chapter 1. **Read Sequentially**: The sections build on each other, so it‚Äôs best to read them in order 2. **Share Notes**: Write down key concepts and questions and discuss them within the community in [Discord](https://discord.gg/UrrTSsSyjb) 3. **Try the Code**: When we get to practical examples, try them yourself 4. **Join the Community**: Use the resources we provide to connect with other learners Let‚Äôs begin our exploration of Open R1 and discover how you can be part of making AI more accessible to everyone! üöÄ",
    "metadata": {
      "title": "Open R1 for Students",
      "url": "https://huggingface.co/learn/llm-course/chapter12/1",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/1",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Reinforcement Learning and its Role in LLMs Welcome to the first page! We‚Äôre going to start our journey into the exciting world of Reinforcement Learning (RL) and discover how it‚Äôs revolutionizing the way we train Language Models like the ones you might use every day. > In this chapter, we are focusing on reinforcement learning for language models. However, reinforcement learning is a broad field with many applications beyond language models. If you‚Äôre interested in learning more about reinforcement learning, you should check out theDeep Reinforcement Learning course. This page will give you a friendly and clear introduction to RL, even if you‚Äôve never encountered it before. We‚Äôll break down the core ideas and see why RL is becoming so important in the field of Large Language Models (LLMs). ## What is Reinforcement Learning (RL)? Imagine you‚Äôre training a dog. You want to teach it to sit. You might say ‚ÄúSit!‚Äù and then, if the dog sits, you give it a treat and praise. If it doesn‚Äôt sit, you might gently guide it or just try again. Over time, the dog learns to associate sitting with the positive reward (treat and praise) and is more likely to sit when you say ‚ÄúSit!‚Äù again. In reinforcement learning, we refer to this feedback as a **reward**. That, in a nutshell, is the basic idea behind Reinforcement Learning! Instead of a dog, we have a **language model** (in reinforcement learning, we call it an **agent**), and instead of you, we have the **environment** that gives feedback. ![RL terms Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/3.jpg) Let‚Äôs break down the key pieces of RL: ### Agent This is our learner. In the dog example, the dog is the agent. In the context of LLMs, the LLM itself becomes the agent we want to train. The agent is the one making decisions and learning from the environment and its rewards. ### Environment This is the world the agent lives in and interacts with. For the dog, the environment is your house and you. For an LLM, the environment is a bit more abstract ‚Äì it could be the users it interacts with, or a simulated scenario we set up for it. The environment provides feedback to the agent. ### Action These are the choices the agent can make in the environment. The dog‚Äôs actions are things like ‚Äúsit‚Äù, ‚Äústand‚Äù, ‚Äúbark‚Äù, etc. For an LLM, actions could be generating words in a sentence, choosing which answer to give to a question, or deciding how to respond in a conversation. ### Reward This is the feedback the environment gives to the agent after it takes an action. Rewards are usually numbers. **Positive rewards** are like treats and praise ‚Äì they tell the agent ‚Äúgood job, you did something right!‚Äú. **Negative rewards** (or penalties) are like a gentle ‚Äúno‚Äù ‚Äì they tell the agent ‚Äúthat wasn‚Äôt quite right, try something else‚Äù. For the dog, the treat is the reward. For an LLM, rewards are designed to reflect how well the LLM",
    "metadata": {
      "title": "Introduction to Reinforcement Learning and its Role in LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter12/2",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/2",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "words in a sentence, choosing which answer to give to a question, or deciding how to respond in a conversation. ### Reward This is the feedback the environment gives to the agent after it takes an action. Rewards are usually numbers. **Positive rewards** are like treats and praise ‚Äì they tell the agent ‚Äúgood job, you did something right!‚Äú. **Negative rewards** (or penalties) are like a gentle ‚Äúno‚Äù ‚Äì they tell the agent ‚Äúthat wasn‚Äôt quite right, try something else‚Äù. For the dog, the treat is the reward. For an LLM, rewards are designed to reflect how well the LLM is doing at a specific task ‚Äì maybe it‚Äôs how helpful, truthful, or harmless its response is. ### Policy This is the agent‚Äôs strategy for choosing actions. It‚Äôs like the dog‚Äôs understanding of what it should do when you say ‚ÄúSit!‚Äú. In RL, the policy is what we‚Äôre really trying to learn and improve. It‚Äôs a set of rules or a function that tells the agent what action to take in different situations. Initially, the policy might be random, but as the agent learns, the policy becomes better at choosing actions that lead to higher rewards. ## The RL Process: Trial and Error ![RL Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/1.jpg) Reinforcement Learning happens through a process of trial and error: Step Process Description 1. Observation The agent observes the environment The agent takes in information about its current state and surroundings 2. Action The agent takes an action based on its current policy Using its learned strategy (policy), the agent decides what to do next 3. Feedback The environment gives the agent a reward The agent receives feedback on how good or bad its action was 4. Learning The agent updates its policy based on the reward The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards 5. Iteration Repeat the process This cycle continues, allowing the agent to continuously improve its decision-making Think about learning to ride a bike. You might wobble and fall at first (negative reward!). But when you manage to balance and pedal smoothly, you feel good (positive reward!). You adjust your actions based on this feedback ‚Äì leaning slightly, pedaling faster, etc. ‚Äì until you learn to ride well. RL is similar ‚Äì it‚Äôs about learning through interaction and feedback. ## Role of RL in Large Language Models (LLMs) Now, why is RL so important for Large Language Models? Well, training really good LLMs is tricky. We can train them on massive amounts of text from the internet, and they become very good at predicting the next word in a sentence. This is how they learn to generate fluent and grammatically correct text, as we learned in [chapter 2](/course/chapter2/1). However, just being fluent isn‚Äôt enough. We want our LLMs to be more than just good at stringing words together. We want them to be: - **Helpful:** Provide useful and relevant information. - **Harmless:** Avoid generating toxic, biased, or harmful content.",
    "metadata": {
      "title": "Introduction to Reinforcement Learning and its Role in LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter12/2",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/2",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "why is RL so important for Large Language Models? Well, training really good LLMs is tricky. We can train them on massive amounts of text from the internet, and they become very good at predicting the next word in a sentence. This is how they learn to generate fluent and grammatically correct text, as we learned in [chapter 2](/course/chapter2/1). However, just being fluent isn‚Äôt enough. We want our LLMs to be more than just good at stringing words together. We want them to be: - **Helpful:** Provide useful and relevant information. - **Harmless:** Avoid generating toxic, biased, or harmful content. - **Aligned with Human Preferences:** Respond in ways that humans find natural, helpful, and engaging. Pre-training LLM methods, which mostly rely on predicting the next word from text data, sometimes fall short on these aspects. Whilst supervised training is excellent at producing structured outputs, it can be less effective at producing helpful, harmless, and aligned responses. We explore supervised training in [chapter 11](/course/chapter11/1). Fine-tuned models might generate fluent and structured text that is still factually incorrect, biased, or doesn‚Äôt really answer the user‚Äôs question in a helpful way. **Enter Reinforcement Learning!** RL gives us a way to fine-tune these pre-trained LLMs to better achieve these desired qualities. It‚Äôs like giving our LLM dog extra training to become a well-behaved and helpful companion, not just a dog that knows how to bark fluently! ## Reinforcement Learning from Human Feedback (RLHF) A very popular technique for aligning language models is **Reinforcement Learning from Human Feedback (RLHF)**. In RLHF, we use human feedback as a proxy for the ‚Äúreward‚Äù signal in RL. Here‚Äôs how it works: 1. **Get Human Preferences:** We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question ‚ÄúWhat is the capital of France?‚Äù and ask them ‚ÄúWhich answer is better?‚Äú. 2. **Train a Reward Model:** We use this human preference data to train a separate model called a **reward model**. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences. 3. **Fine-tune the LLM with RL:** Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we‚Äôre training the LLM to produce text that our reward model (which learned from human preferences) thinks is good. ![RL Basic Concept](https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg) From a general perspective, let‚Äôs look at the benefits of using RL in LLMs: Benefit Description Improved Control RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise. Enhanced Alignment with Human Values RLHF, in particular, helps us align LLMs with complex and often subjective",
    "metadata": {
      "title": "Introduction to Reinforcement Learning and its Role in LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter12/2",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/2",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "reward model scores these responses (provides rewards). In essence, we‚Äôre training the LLM to produce text that our reward model (which learned from human preferences) thinks is good. ![RL Basic Concept](https://huggingface.co/reasoning-course/images/resolve/main/grpo/2.jpg) From a general perspective, let‚Äôs look at the benefits of using RL in LLMs: Benefit Description Improved Control RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise. Enhanced Alignment with Human Values RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It‚Äôs hard to write down rules for ‚Äúwhat makes a good answer,‚Äù but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments. Mitigating Undesirable Behaviors RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them. Reinforcement Learning from Human Feedback has been used to train many of the most popular LLMs today, such as OpenAI‚Äôs GPT-4, Google‚Äôs Gemini, and DeepSeek‚Äôs R1. There are a wide range of techniques for RLHF, with varying degrees of complexity and sophistication. In this chapter, we will focus on Group Relative Policy Optimization (GRPO), which is a technique for RLHF that has been shown to be effective at training LLMs that are helpful, harmless, and aligned with human preferences. ## Why should we care about GRPO (Group Relative Policy Optimization)? There are many techniques for RLHF but this course is focused on GRPO because it represents a significant advancement in reinforcement learning for language models. Let‚Äôs briefly consider two of other popular techniques for RLHF: - Proximal Policy Optimization (PPO) - Direct Preference Optimization (DPO) Proximal Policy Optimization (PPO) was one of the first highly effective techniques for RLHF. It uses a policy gradient method to update the policy based on the reward from a separate reward model. Direct Preference Optimization (DPO) was later developed as a simpler technique that eliminates the need for a separate reward model using preference data directly. Essentially, framing the problem as a classification task between the chosen and rejected responses. > DPO and PPO are complex reinforcement learning algorithms in their own right, which we will not cover in this course. If you‚Äôre interested in learning more about them, you can check out the following resources:Proximal Policy OptimizationDirect Preference Optimization Unlike DPO and PPO, GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods. GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function. GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn‚Äôt strictly require one. This is because GRPO can incorporate",
    "metadata": {
      "title": "Introduction to Reinforcement Learning and its Role in LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter12/2",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/2",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "learning more about them, you can check out the following resources:Proximal Policy OptimizationDirect Preference Optimization Unlike DPO and PPO, GRPO groups similar samples together and compares them as a group. The group-based approach provides more stable gradients and better convergence properties compared to other methods. GRPO does not use preference data like DPO, but instead compares groups of similar samples using a reward signal from a model or function. GRPO is flexible in how it obtains reward signals - it can work with a reward model (like PPO does) but doesn‚Äôt strictly require one. This is because GRPO can incorporate reward signals from any function or model that can evaluate the quality of responses. For example, we could use a length function to reward shorter responses, a mathematical solver to verify solution correctness, or a factual correctness function to reward responses that are more factually accurate. This flexibility makes GRPO particularly versatile for different types of alignment tasks. Congratulations on completing Module 1! You‚Äôve now got a solid introduction to Reinforcement Learning and its crucial role in shaping the future of Large Language Models. You understand the basic concepts of RL, why it‚Äôs used for LLMs, and you‚Äôve been introduced to GRPO, a key algorithm in this field. In the next module, we‚Äôll get our hands dirty and dive into the DeepSeek R1 paper to see these concepts in action! ## Quiz ### 1. What are the key components of Reinforcement Learning? Agent, Environment, Action, Reward, and Policy Model, Data, Loss Function, and Optimizer Input, Output, and Hidden Layers ### 2. What is the main advantage of RLHF for training language models? It helps align models with human preferences and values It makes models generate text faster It reduces the model's memory usage ### 3. In the context of RL for LLMs, what represents an ‚Äúaction‚Äù? Generating words or choosing responses in a conversation Updating model weights Processing input tokens ### 4. What is the role of the reward in RL training of language models? To provide feedback on how well the model's responses align with desired behavior To measure the model's vocabulary size To determine the model's training speed ### 5. What is a reward in the context of RL for LLMs? A numerical score that measures the quality of a response A function that generates responses A model that evaluates the quality of responses",
    "metadata": {
      "title": "Introduction to Reinforcement Learning and its Role in LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter12/2",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/2",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Understanding the DeepSeek R1 Paper This chapter is a crash course paper reading. We will walk through the paper in simple terms, and then we will break down the key concepts and takeaways. DeepSeek R1 represents a significant advancement in language model training, particularly in developing reasoning capabilities through reinforcement learning. The paper introduces a new reinforcement learning algorithm called Group Relative Policy Optimization (GRPO). ![DeepSeek R1 Overview](https://huggingface.co/reasoning-course/images/resolve/main/grpo/4.png) In the next chapter, we will build on this knowledge and implement GRPO in practice. The initial goal of the paper was to explore whether pure reinforcement learning could develop reasoning capabilities without supervised fine-tuning. > Up until that point, all the popular LLMs required some supervised fine-tuning, which we explored inchapter 11. ## The Breakthrough ‚ÄòAha‚Äô Moment ![The 'Aha Moment'](https://huggingface.co/reasoning-course/images/resolve/main/grpo/9.png) One of the most remarkable discoveries in R1-Zero‚Äôs training was the emergence of a phenomenon known as the ‚ÄúAha Moment.‚Äù This phenomenon is somewhat similar to how humans experience sudden realizations while problem-solving. Here‚Äôs how it works: 1. Initial Attempt: The model makes an initial attempt at solving a problem 2. Recognition: It recognizes potential errors or inconsistencies 3. Self-Correction: It adjusts its approach based on this recognition 4. Explanation: It can explain why the new approach is better This breakthrough resonates with learners and feels like a ‚ÄúEureka‚Äù moment. It demonstrates learning rather than mere memorization, so let‚Äôs take a moment to imagine what it feels like to have an ‚ÄúAha‚Äù moment. For example, imagine you‚Äôre trying to solve a puzzle: - First try: ‚ÄúThis piece should go here based on the color‚Äù - Recognition: ‚ÄúBut wait, the shape doesn‚Äôt quite fit‚Äù - Correction: ‚ÄúAh, it actually belongs over there‚Äù - Explanation: ‚ÄúBecause both the color and shape pattern match in this position‚Äù This ability emerged naturally from RL training, without being explicitly programmed, demonstrating learning rather than mere memorization of a process from the training data. The easiest way to understand the ‚ÄòAha‚Äô moment is to see it in action. Let‚Äôs take a look at an example. In the chat below, we ask the model to solve a problem and the UI shows the model‚Äôs thought process as it solves the problem. If you want to try Deepseek‚Äôs R1, you can also check out [Hugging Chat](https://huggingface.co/chat/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B). ## The Training Process Training R1 was a multi-phase process. Let‚Äôs break down the phases and the key innovations in each phase. The final process results in two models: - DeepSeek-R1-Zero: A model trained purely using reinforcement learning. - DeepSeek-R1: A model that builds on the foundation of DeepSeek-R1-Zero and adds supervised fine-tuning. Feature DeepSeek-R1-Zero DeepSeek-R1 Training Approach Pure RL Multi-phase (SFT + RL) Fine-tuning None Supervised fine-tuning Reasoning Capability Emergent Enhanced AIME Performance 71.0% 79.8% Key Characteristics Strong reasoning but readability issues Better language consistency and readability While DeepSeek-R1-Zero demonstrates the potential of pure reinforcement learning for developing reasoning capabilities, DeepSeek-R1 builds upon this foundation with a more balanced approach that prioritizes both reasoning performance and usability. The training process involves",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "The final process results in two models: - DeepSeek-R1-Zero: A model trained purely using reinforcement learning. - DeepSeek-R1: A model that builds on the foundation of DeepSeek-R1-Zero and adds supervised fine-tuning. Feature DeepSeek-R1-Zero DeepSeek-R1 Training Approach Pure RL Multi-phase (SFT + RL) Fine-tuning None Supervised fine-tuning Reasoning Capability Emergent Enhanced AIME Performance 71.0% 79.8% Key Characteristics Strong reasoning but readability issues Better language consistency and readability While DeepSeek-R1-Zero demonstrates the potential of pure reinforcement learning for developing reasoning capabilities, DeepSeek-R1 builds upon this foundation with a more balanced approach that prioritizes both reasoning performance and usability. The training process involves four phases: 1. Cold Start Phase 2. Reasoning RL Phase 3. Rejection Sampling Phase 4. Diverse RL Phase Let‚Äôs break down each phase: ### Cold Start Phase (Quality Foundation) ![Cold Start Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/5.png) This phase is designed to establish a strong foundation for the model‚Äôs readability and response quality. It uses a small dataset of high-quality samples from R1-Zero to fine-tune the V3-Base model. Starting with the DeepSeek-V3-Base model, the team used thousands of validated, high-quality samples from R1-Zero for supervised fine-tuning. This innovative approach uses a small but high quality dataset to establish strong baseline readability and response quality. ### Reasoning RL Phase (Capability Building) ![Reasoning RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/6.png) The Reasoning RL Phase focuses on developing core reasoning capabilities across domains including mathematics, coding, science, and logic. This phase employs rule-based reinforcement learning, with rewards directly tied to solution correctness. Crucially, all the tasks in this phase are ‚Äòverifiable‚Äô so we can check if the model‚Äôs answer is correct or not. For example, in the case of mathematics, we can check if the model‚Äôs answer is correct by using a mathematical solver. What makes this phase particularly innovative is its direct optimization approach that eliminates the need for a separate reward model, streamlining the training process. ### Rejection Sampling Phase (Quality Control) ![Rejection Sampling Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/7.png) During the Rejection Sampling Phase, the model generates samples which are then filtered through a quality control process. DeepSeek-V3 serves as the quality judge, evaluating outputs across a broad scope that extends beyond pure reasoning tasks. The filtered data is then used for supervised fine-tuning. This phase‚Äôs innovation lies in its ability to combine multiple quality signals to ensure high-standard outputs. ### Diverse RL Phase (Broad Alignment) ![Diverse RL Phase](https://huggingface.co/reasoning-course/images/resolve/main/grpo/8.png) The final Diverse RL Phase tackles multiple task types using a sophisticated hybrid approach. For deterministic tasks, it employs rule-based rewards, while subjective tasks are evaluated through LLM feedback. This phase aims to achieve human preference alignment through its innovative hybrid reward approach, combining the precision of rule-based systems with the flexibility of language model evaluation. ## The Algorithm: Group Relative Policy Optimization (GRPO) Now that we have a good understanding of the training process, let‚Äôs look at the algorithm that was used to train the model. The authors describe GRPO as a breakthrough in model fine-tuning: ![GRPO Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/10.png) GRPO‚Äôs novelty lies in its capacity to ‚Äúdirectly optimize for preference rectification.‚Äù This signifies a more direct",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tasks, it employs rule-based rewards, while subjective tasks are evaluated through LLM feedback. This phase aims to achieve human preference alignment through its innovative hybrid reward approach, combining the precision of rule-based systems with the flexibility of language model evaluation. ## The Algorithm: Group Relative Policy Optimization (GRPO) Now that we have a good understanding of the training process, let‚Äôs look at the algorithm that was used to train the model. The authors describe GRPO as a breakthrough in model fine-tuning: ![GRPO Process](https://huggingface.co/reasoning-course/images/resolve/main/grpo/10.png) GRPO‚Äôs novelty lies in its capacity to ‚Äúdirectly optimize for preference rectification.‚Äù This signifies a more direct and efficient route to aligning the model with desired outputs, contrasting with traditional Reinforcement Learning algorithms such as PPO. Let‚Äôs break down how GRPO works through its three main components. ### Group Formation: Creating Multiple Solutions The first step in GRPO is remarkably intuitive - it‚Äôs similar to how a student might solve a difficult problem by trying multiple approaches. When given a prompt, the model doesn‚Äôt just generate one response; instead, it creates multiple attempts at solving the same problem (usually 4, 8, or 16 different attempts). Imagine you‚Äôre teaching a model to solve math problems. For a question about counting chickens on a farm, the model might generate several different solutions: - One solution might break down the problem step by step: first counting total chickens, then subtracting roosters, and finally accounting for non-laying hens - Another might use a different but equally valid approach - Some attempts might contain mistakes or less efficient solutions All these attempts are kept together as a group, much like having multiple students‚Äô solutions to compare and learn from. ![Group Formation](https://huggingface.co/reasoning-course/images/resolve/main/grpo/11.jpg) ### Preference Learning: Understanding What Makes a Good Solution This is where GRPO really shines in its simplicity. Unlike other methods for RLHF that need always require a separate reward model to predict how good a solution might be, GRPO can use any function or model to evaluate the quality of a solution. For example, we could use a length function to reward shorter responses or a mathematical solver to reward accurate mathematical solutions. The evaluation process looks at various aspects of each solution: - Is the final answer correct? - Did the solution follow proper formatting (like using the right XML tags)? - Does the reasoning match the answer provided? What makes this approach particularly clever is how it handles the scoring. Instead of just giving absolute scores, GRPO normalizes the rewards within each group. It uses a simple but effective formula for group relative advantage estimation: ``` Advantage = (reward - mean(group_rewards)) / std(group_rewards) ``` ![Preference Learning](https://huggingface.co/reasoning-course/images/resolve/main/grpo/12.jpg) This normalization is like grading on a curve, but for AI. It helps the model understand which solutions within the group were better or worse compared to their peers, rather than just looking at absolute scores. ### Optimization: Learning from Experience The final step is where GRPO teaches the model to improve based on what it learned from evaluating the group of",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the scoring. Instead of just giving absolute scores, GRPO normalizes the rewards within each group. It uses a simple but effective formula for group relative advantage estimation: ``` Advantage = (reward - mean(group_rewards)) / std(group_rewards) ``` ![Preference Learning](https://huggingface.co/reasoning-course/images/resolve/main/grpo/12.jpg) This normalization is like grading on a curve, but for AI. It helps the model understand which solutions within the group were better or worse compared to their peers, rather than just looking at absolute scores. ### Optimization: Learning from Experience The final step is where GRPO teaches the model to improve based on what it learned from evaluating the group of solutions. This process is both powerful and stable, using two main principles: 1. It encourages the model to produce more solutions like the successful ones while moving away from less effective approaches 2. It includes a safety mechanism (called KL divergence penalty) that prevents the model from changing too drastically all at once This approach proves more stable than traditional methods because: - It looks at multiple solutions together rather than comparing just two at a time - The group-based normalization helps prevent issues with reward scaling - The KL penalty acts like a safety net, ensuring the model doesn‚Äôt forget what it already knows while learning new things > GRPO‚Äôs key innovations are:Learning directly from any function or model, eliminating the reliance on a separate reward model.Group-based learning, which is more stable and efficient than traditional methods like pairwise comparisons. This breakdown is complex, but the key takeaway is that GRPO is a more efficient and stable way to train a model to reason. ### GRPO Algorithm in Pseudocode Now that we understand the key components of GRPO, let‚Äôs look at the algorithm in pseudocode. This is a simplified version of the algorithm, but it captures the key ideas. ``` Input: - initial_policy: Starting model to be trained - reward_function: Function that evaluates outputs - training_prompts: Set of training examples - group_size: Number of outputs per prompt (typically 4-16) Algorithm GRPO: 1. For each training iteration: a. Set reference_policy = initial_policy (snapshot current policy) b. For each prompt in batch: i. Generate group_size different outputs using initial_policy ii. Compute rewards for each output using reward_function iii. Normalize rewards within group: normalized_advantage = (reward - mean(rewards)) / std(rewards) iv. Update policy by maximizing the clipped ratio: min(prob_ratio * normalized_advantage, clip(prob_ratio, 1-epsilon, 1+epsilon) * normalized_advantage) - kl_weight * KL(initial_policy || reference_policy) where prob_ratio is current_prob / reference_prob Output: Optimized policy model ``` This algorithm shows how GRPO combines group-based advantage estimation with policy optimization while maintaining stability through clipping and KL divergence constraints. ## Results and Impact Now that we‚Äôve explored the algorithm, let‚Äôs look at the results. DeepSeek R1 achieves state-of-the-art performance across multiple domains: Domain Key Results Mathematics ‚Ä¢ 79.8% on AIME 2024‚Ä¢ 97.3% on MATH-500 Coding ‚Ä¢ Codeforces Rating: 2029‚Ä¢ LiveCodeBench: 65.9% General Knowledge ‚Ä¢ MMLU: 90.8%‚Ä¢ GPQA Diamond: 71.5% Language Tasks ‚Ä¢ AlpacaEval 2.0: 87.6% win rate‚Ä¢ FRAMES: 82.5% The model‚Äôs practical impact extends beyond benchmarks through",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "where prob_ratio is current_prob / reference_prob Output: Optimized policy model ``` This algorithm shows how GRPO combines group-based advantage estimation with policy optimization while maintaining stability through clipping and KL divergence constraints. ## Results and Impact Now that we‚Äôve explored the algorithm, let‚Äôs look at the results. DeepSeek R1 achieves state-of-the-art performance across multiple domains: Domain Key Results Mathematics ‚Ä¢ 79.8% on AIME 2024‚Ä¢ 97.3% on MATH-500 Coding ‚Ä¢ Codeforces Rating: 2029‚Ä¢ LiveCodeBench: 65.9% General Knowledge ‚Ä¢ MMLU: 90.8%‚Ä¢ GPQA Diamond: 71.5% Language Tasks ‚Ä¢ AlpacaEval 2.0: 87.6% win rate‚Ä¢ FRAMES: 82.5% The model‚Äôs practical impact extends beyond benchmarks through its cost-effective API pricing ($0.14 per million input tokens) and successful model distillation across various sizes (1.5B to 70B parameters). Notably, even the 7B model achieves 55.5% on AIME 2024, while the 70B distilled version approaches o1-mini performance on MATH-500 (94.5%), demonstrating effective capability preservation at different scales. ## Limitations and Challenges of GRPO While GRPO represents a significant advancement in reinforcement learning for language models, it‚Äôs important to understand its limitations and challenges: - **Generation Cost**: Generating multiple completions (4-16) for each prompt increases computational requirements compared to methods that generate only one or two completions. - **Batch Size Constraints**: The need to process groups of completions together can limit effective batch sizes, adding complexity to the training process and potentially slowing down training. - **Reward Function Design**: The quality of training heavily depends on well-designed reward functions. Poorly designed rewards can lead to unintended behaviors or optimization for the wrong objectives. - **Group Size Tradeoffs**: Choosing the optimal group size involves balancing diversity of solutions against computational cost. Too few samples may not provide enough diversity, while too many increase training time and resource requirements. - **KL Divergence Tuning**: Finding the right balance for the KL divergence penalty requires careful tuning - too high and the model won‚Äôt learn effectively, too low and it may diverge too far from its initial capabilities. ## Conclusion The DeepSeek R1 paper represents a significant milestone in language model development. The Group Relative Policy Optimization (GRPO) algorithm has demonstrated that pure reinforcement learning can indeed develop strong reasoning capabilities, challenging previous assumptions about the necessity of supervised fine-tuning. Perhaps most importantly, DeepSeek R1 has shown that it‚Äôs possible to balance high performance with practical considerations like cost-effectiveness and accessibility. The successful distillation of the model‚Äôs capabilities across different sizes, from 1.5B to 70B parameters, demonstrates a path forward for making advanced AI capabilities more widely available. In the next section, we‚Äôll explore practical implementations of these concepts, focusing on how to leverage GRPO and RFTrans in your own language model development projects. ## Quiz ### 1. What is the main innovation of the DeepSeek R1 paper? The GRPO algorithm that enables learning from preferences with and without a reward model Using more GPUs for training than any previous model Creating a larger language model than existing ones ### 2. What are the four phases of the DeepSeek R1 training process? Cold Start,",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "70B parameters, demonstrates a path forward for making advanced AI capabilities more widely available. In the next section, we‚Äôll explore practical implementations of these concepts, focusing on how to leverage GRPO and RFTrans in your own language model development projects. ## Quiz ### 1. What is the main innovation of the DeepSeek R1 paper? The GRPO algorithm that enables learning from preferences with and without a reward model Using more GPUs for training than any previous model Creating a larger language model than existing ones ### 2. What are the four phases of the DeepSeek R1 training process? Cold Start, Reasoning RL, Rejection Sampling, and Diverse RL Pre-training, Fine-tuning, Testing, and Deployment Data Collection, Model Training, Evaluation, and Optimization ### 3. What is the ‚ÄòAha Moment‚Äô phenomenon in R1-Zero‚Äôs training? A process where the model recognizes errors, self-corrects, and explains its corrections The point where the model reaches human-level performance When the model completes its training process ### 4. How does GRPO‚Äôs group formation work? It generates multiple solutions (4-16) for the same problem and evaluates them together It combines multiple models into one ensemble It splits the training data into different groups ### 5. What is the key difference between DeepSeek-R1-Zero and DeepSeek-R1? R1-Zero uses pure RL while R1 combines RL with supervised fine-tuning R1-Zero is smaller than R1 R1-Zero was trained on less data",
    "metadata": {
      "title": "Understanding the DeepSeek R1 Paper",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath > This section dives into the technical and mathematical details of GRPO. It was authored byShirin Yamani. Let‚Äôs deepen our understanding of GRPO so that we can improve our model‚Äôs training process. GRPO directly evaluates the model-generated responses by comparing them within groups of generation to optimize policy model, instead of training a separate value model (Critic). This approach leads to significant reduction in computational cost! GRPO can be applied to any verifiable task where the correctness of the response can be determined. For instance, in math reasoning, the correctness of the response can be easily verified by comparing it to the ground truth. Before diving into the technical details, let‚Äôs visualize how GRPO works at a high level: ![deep](https://huggingface.co/reasoning-course/images/resolve/main/grpo/16.png) Now that we have a visual overview, let‚Äôs break down how GRPO works step by step. ## The GRPO Algorithm The core innovation of GRPO is its approach to evaluating and learning from multiple generated responses simultaneously. Instead of relying on a separate reward model, it compares outputs within the same group to determine which ones should be reinforced. Let‚Äôs walk through each step of the algorithm in detail: ### Step 1: Group Sampling The first step is to generate multiple possible answers for each question. This creates a diverse set of outputs that can be compared against each other. For each questionq q q, the model will generate G G G outputs (group size) from the trained policy: {o1,o2,o3,‚Ä¶,oGœÄŒ∏old {o_1, o_2, o_3, \\dots, o_G}\\pi_{\\theta_{\\text{old}}} o1‚Äã,o2‚Äã,o3‚Äã,‚Ä¶,oG‚ÄãœÄŒ∏old‚Äã‚Äã },G=8 G=8 G=8 where eachoi o_i oi‚Äã represents one completion from the model. #### Example To make this concrete, let‚Äôs look at a simple arithmetic problem: **Question** q q q :Calculate 2+2√ó6 \\text{Calculate}\\space2 + 2 \\times 6 Calculate 2+2√ó6 **Outputs** (G=8) (G = 8) (G=8):{o1:14 (correct),o2:16 (wrong),o3:10 (wrong),‚Ä¶,o8:14 (correct)} \\{o_1:14 \\text{ (correct)}, o_2:16 \\text{ (wrong)}, o_3:10 \\text{ (wrong)}, \\ldots, o_8:14 \\text{ (correct)}\\} {o1‚Äã:14 (correct),o2‚Äã:16 (wrong),o3‚Äã:10 (wrong),‚Ä¶,o8‚Äã:14 (correct)} Notice how some of the generated answers are correct (14) while others are wrong (16 or 10). This diversity is crucial for the next step. ### Step 2: Advantage Calculation Once we have multiple responses, we need a way to determine which ones are better than others. This is where the advantage calculation comes in. #### Reward Distribution First, we assign a reward score to each generated response. In this example, we‚Äôll use a reward model, but as we learnt in the previous section, we can use any reward returning function. Assign a RM score to each of the generated responses based on the correctnessri r_i ri‚Äã *(e.g. 1 for correct response, 0 for wrong response)* then for each of theri r_i ri‚Äã calculate the following Advantage value. #### Advantage Value Formula The key insight of GRPO is that we don‚Äôt need absolute measures of quality - we can compare outputs within the same group. This is done using standardization: Ai=ri‚àímean({r1,r2,‚Ä¶,rG})std({r1,r2,‚Ä¶,rG})A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})}Ai‚Äã=std({r1‚Äã,r2‚Äã,‚Ä¶,rG‚Äã})ri‚Äã‚àímean({r1‚Äã,r2‚Äã,‚Ä¶,rG‚Äã})‚Äã #### Example Continuing with our",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "but as we learnt in the previous section, we can use any reward returning function. Assign a RM score to each of the generated responses based on the correctnessri r_i ri‚Äã *(e.g. 1 for correct response, 0 for wrong response)* then for each of theri r_i ri‚Äã calculate the following Advantage value. #### Advantage Value Formula The key insight of GRPO is that we don‚Äôt need absolute measures of quality - we can compare outputs within the same group. This is done using standardization: Ai=ri‚àímean({r1,r2,‚Ä¶,rG})std({r1,r2,‚Ä¶,rG})A_i = \\frac{r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\})}{\\text{std}(\\{r_1, r_2, \\ldots, r_G\\})}Ai‚Äã=std({r1‚Äã,r2‚Äã,‚Ä¶,rG‚Äã})ri‚Äã‚àímean({r1‚Äã,r2‚Äã,‚Ä¶,rG‚Äã})‚Äã #### Example Continuing with our arithmetic example for the same example above, imagine we have 8 responses, 4 of which is correct and the rest wrong, therefore; Metric Value Group Average mean(ri)=0.5 mean(r_i) = 0.5 mean(ri‚Äã)=0.5 Standard Deviation std(ri)=0.53 std(r_i) = 0.53 std(ri‚Äã)=0.53 Advantage Value (Correct response) Ai=1‚àí0.50.53=0.94 A_i = \\frac{1 - 0.5}{0.53}= 0.94 Ai‚Äã=0.531‚àí0.5‚Äã=0.94 Advantage Value (Wrong response) Ai=0‚àí0.50.53=‚àí0.94 A_i = \\frac{0 - 0.5}{0.53}= -0.94 Ai‚Äã=0.530‚àí0.5‚Äã=‚àí0.94 #### Interpretation Now that we have calculated the advantage values, let‚Äôs understand what they mean: This standardization (i.e.Ai A_i Ai‚Äã weighting) allows the model to assess each response‚Äôs relative performance, guiding the optimization process to favorable responses that are better than average (high reward) and discourage those that are worse. For instance ifAi>0 A_i > 0 Ai‚Äã>0, then theoi o_i oi‚Äã is better response than the average level within its group; and ifAi<0 A_i < 0 Ai‚Äã<0, then theoi o_i oi‚Äã then the quality of the response is less than the average (i.e. poor quality/performance). For the example above, ifAi=0.94(correct output) A_i = 0.94 \\text{(correct output)} Ai‚Äã=0.94(correct output) then during optimization steps its generation probability will be increased. With our advantage values calculated, we‚Äôre now ready to update the policy. ### Step 3: Policy Update The final step is to use these advantage values to update our model so that it becomes more likely to generate good responses in the future. The target function for policy update is: JGRPO(Œ∏)=[1G‚àëi=1Gmin‚Å°(œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q)Aiclip(œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q),1‚àíœµ,1+œµ)Ai)]‚àíŒ≤DKL(œÄŒ∏‚à•‚à•œÄref)J_{GRPO}(\\theta) = \\left[\\frac{1}{G} \\sum_{i=1}^{G} \\min \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} A_i \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon \\right) A_i \\right)\\right]- \\beta D_{KL}(\\pi_{\\theta} \\|\\| \\pi_{ref})JGRPO‚Äã(Œ∏)=[G1‚Äãi=1‚àëG‚Äãmin(œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚ÄãAi‚Äãclip(œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã,1‚àíœµ,1+œµ)Ai‚Äã)]‚àíŒ≤DKL‚Äã(œÄŒ∏‚Äã‚à•‚à•œÄref‚Äã) This formula might look intimidating at first, but it‚Äôs built from several components that each serve an important purpose. Let‚Äôs break them down one by one. ## Key Components of the Target Function The GRPO update function combines several techniques to ensure stable and effective learning. Let‚Äôs examine each component: ### 1. Probability Ratio The probability ratio is defined as: (œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q)) \\left(\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}\\right) (œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã) Intuitively, the formula compares how much the new model‚Äôs response probability differs from the old model‚Äôs response probability while incorporating a preference for responses that improve the expected outcome. #### Interpretation - If ratio>1 \\text{ratio} > 1 ratio>1, the new model assigns a higher probability to responseoi o_i oi‚Äã than the old model. - If ratio<1 \\text{ratio} < 1 ratio<1, the new model assigns a lower probability tooi o_i oi‚Äã This ratio allows us to control how much the model changes at each step,",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "examine each component: ### 1. Probability Ratio The probability ratio is defined as: (œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q)) \\left(\\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}\\right) (œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã) Intuitively, the formula compares how much the new model‚Äôs response probability differs from the old model‚Äôs response probability while incorporating a preference for responses that improve the expected outcome. #### Interpretation - If ratio>1 \\text{ratio} > 1 ratio>1, the new model assigns a higher probability to responseoi o_i oi‚Äã than the old model. - If ratio<1 \\text{ratio} < 1 ratio<1, the new model assigns a lower probability tooi o_i oi‚Äã This ratio allows us to control how much the model changes at each step, which leads us to the next component. ### 2. Clip Function The clipping function is defined as: clip(œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q),1‚àíœµ,1+œµ) \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1 - \\epsilon, 1 + \\epsilon\\right) clip(œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã,1‚àíœµ,1+œµ) Limit the ratio discussed above to be within[1‚àíœµ,1+œµ] [1 - \\epsilon, 1 + \\epsilon] [1‚àíœµ,1+œµ] to avoid/control drastic changes or crazy updates and stepping too far off from the old policy. In other words, it limit how much the probability ratio can increase to help maintaining stability by avoiding updates that push the new model too far from the old one. #### Example (Œµ = 0.2) Let‚Äôs look at two different scenarios to better understand this clipping function: - **Case 1**: if the new policy has a probability of 0.9 for a specific response and the old policy has a probabiliy of 0.5, it means this response is getting reinforeced by the new policy to have higher probability, but within a controlled limit which is the clipping to tight up its hands to not get drastic -Ratio:œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q)=0.90.5=1.8‚ÜíClip 1.2 \\text{Ratio}: \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} = \\frac{0.9}{0.5} = 1.8 ‚Üí \\text{Clip}\\space1.2 Ratio:œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã=0.50.9‚Äã=1.8‚ÜíClip 1.2 (upper bound limit 1.2) - **Case 2**: If the new policy is not in favour of a response (lower probability e.g. 0.2), meaning if the response is not beneficial the increase might be incorrect, and the model would be penalized. -Ratio:œÄŒ∏(oi‚à£q)œÄŒ∏old(oi‚à£q)=0.20.5=0.4‚ÜíClip 0.8 \\text{Ratio}: \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)} = \\frac{0.2}{0.5} = 0.4 ‚Üí\\text{Clip}\\space0.8 Ratio:œÄŒ∏old‚Äã‚Äã(oi‚Äã‚à£q)œÄŒ∏‚Äã(oi‚Äã‚à£q)‚Äã=0.50.2‚Äã=0.4‚ÜíClip 0.8 (lower bound limit 0.8) #### Interpretation - The formula encourages the new model to favour responses that the old model underweighted **if they improve the outcome**. - If the old model already favoured a response with a high probability, the new model can still reinforce it **but only within a controlled limit[1‚àíœµ,1+œµ] [1 - \\epsilon, 1 + \\epsilon] [1‚àíœµ,1+œµ],(e.g., œµ=0.2, so [0.8‚àí1.2]) \\text{(e.g., }\\epsilon = 0.2, \\space \\text{so} \\space [0.8-1.2]) (e.g., œµ=0.2, so [0.8‚àí1.2])**. - If the old model overestimated a response that performs poorly, the new model is **discouraged** from maintaining that high probability. - Therefore, intuitively, By incorporating the probability ratio, the objective function ensures that updates to the policy are proportional to the advantageAi A_i Ai‚Äã while being moderated to prevent drastic changes. T While the clipping function helps prevent drastic changes, we need one more safeguard to ensure our model doesn‚Äôt deviate too far from its original behavior. ### 3. KL Divergence The KL divergence term is: Œ≤DKL(œÄŒ∏‚à•‚à•œÄref) \\beta D_{KL}(\\pi_{\\theta} \\|\\| \\pi_{ref}) Œ≤DKL‚Äã(œÄŒ∏‚Äã‚à•‚à•œÄref‚Äã) In the KL divergence term, theœÄref \\pi_{ref}",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "- If the old model overestimated a response that performs poorly, the new model is **discouraged** from maintaining that high probability. - Therefore, intuitively, By incorporating the probability ratio, the objective function ensures that updates to the policy are proportional to the advantageAi A_i Ai‚Äã while being moderated to prevent drastic changes. T While the clipping function helps prevent drastic changes, we need one more safeguard to ensure our model doesn‚Äôt deviate too far from its original behavior. ### 3. KL Divergence The KL divergence term is: Œ≤DKL(œÄŒ∏‚à•‚à•œÄref) \\beta D_{KL}(\\pi_{\\theta} \\|\\| \\pi_{ref}) Œ≤DKL‚Äã(œÄŒ∏‚Äã‚à•‚à•œÄref‚Äã) In the KL divergence term, theœÄref \\pi_{ref} œÄref‚Äã is basically the pre-update model‚Äôs output, `per_token_logps` andœÄŒ∏ \\pi_{\\theta} œÄŒ∏‚Äã is the new model‚Äôs output, `new_per_token_logps`. Theoretically, KL divergence is minimized to prevent the model from deviating too far from its original behavior during optimization. This helps strike a balance between improving performance based on the reward signal and maintaining coherence. In this context, minimizing KL divergence reduces the risk of the model generating nonsensical text or, in the case of mathematical reasoning, producing extremely incorrect answers. #### Interpretation - A KL divergence penalty keeps the model‚Äôs outputs close to its original distribution, preventing extreme shifts. - Instead of drifting towards completely irrational outputs, the model would refine its understanding while still allowing some exploration #### Math Definition For those interested in the mathematical details, let‚Äôs look at the formal definition: Recall that KL distance is defined as follows:DKL(P‚à•‚à•Q)=‚àëx‚ààXP(x)log‚Å°P(x)Q(x)D_{KL}(P \\|\\| Q) = \\sum_{x \\in X} P(x) \\log \\frac{P(x)}{Q(x)}DKL‚Äã(P‚à•‚à•Q)=x‚ààX‚àë‚ÄãP(x)logQ(x)P(x)‚Äã In RLHF, the two distributions of interest are often the distribution of the new model version, P(x), and a distribution of the reference policy, Q(x). #### The Role of Œ≤ Parameter The coefficientŒ≤ \\beta Œ≤ controls how strongly we enforce the KL divergence constraint: - **Higher Œ≤ (Stronger KL Penalty)** - More constraint on policy updates. The model remains close to its reference distribution. - Can slow down adaptation: The model may struggle to explore better responses. - **Lower Œ≤ (Weaker KL Penalty)** - More freedom to update policy: The model can deviate more from the reference. - Faster adaptation but risk of instability: The model might learn reward-hacking behaviors. - Over-optimization risk: If the reward model is flawed, the policy might generate nonsensical outputs. - **Original** [DeepSeekMath](https://arxiv.org/abs/2402.03300) paper set thisŒ≤=0.04 \\beta= 0.04 Œ≤=0.04 Now that we understand the components of GRPO, let‚Äôs see how they work together in a complete example. ## Worked Example with GRPO To solidify our understanding of GRPO, let‚Äôs walk through a complete example from start to finish. ### Example Problem Q: Calculate 2+2√ó6\\text{Q: Calculate}\\space2 + 2 \\times 6Q: Calculate 2+2√ó6 ### Step 1: Group Sampling First, we generate multiple responses from our model. Generate (G=8) (G = 8) (G=8) responses,4 4 4 of which are correct answer (\\( 14, \\text{reward=} 1 \\)) and4 4 4 incorrect(reward= 0) \\text{(reward= 0)} (reward= 0), Therefore: o1:14(correct),o2:10(wrong),o3:16(wrong),...oG:14(correct){o_1:14(correct), o_2:10 (wrong), o_3:16 (wrong), ... o_G:14(correct)}o1‚Äã:14(correct),o2‚Äã:10(wrong),o3‚Äã:16(wrong),...oG‚Äã:14(correct) ### Step 2: Advantage Calculation Next, we calculate the advantage values to determine which responses are",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "example. ## Worked Example with GRPO To solidify our understanding of GRPO, let‚Äôs walk through a complete example from start to finish. ### Example Problem Q: Calculate 2+2√ó6\\text{Q: Calculate}\\space2 + 2 \\times 6Q: Calculate 2+2√ó6 ### Step 1: Group Sampling First, we generate multiple responses from our model. Generate (G=8) (G = 8) (G=8) responses,4 4 4 of which are correct answer (\\( 14, \\text{reward=} 1 \\)) and4 4 4 incorrect(reward= 0) \\text{(reward= 0)} (reward= 0), Therefore: o1:14(correct),o2:10(wrong),o3:16(wrong),...oG:14(correct){o_1:14(correct), o_2:10 (wrong), o_3:16 (wrong), ... o_G:14(correct)}o1‚Äã:14(correct),o2‚Äã:10(wrong),o3‚Äã:16(wrong),...oG‚Äã:14(correct) ### Step 2: Advantage Calculation Next, we calculate the advantage values to determine which responses are better than average: Statistic Value Group Average mean(ri)=0.5 mean(r_i) = 0.5 mean(ri‚Äã)=0.5 Standard Deviation std(ri)=0.53 std(r_i) = 0.53 std(ri‚Äã)=0.53 Advantage Value (Correct response) Ai=1‚àí0.50.53=0.94 A_i = \\frac{1 - 0.5}{0.53}= 0.94 Ai‚Äã=0.531‚àí0.5‚Äã=0.94 Advantage Value (Wrong response) Ai=0‚àí0.50.53=‚àí0.94 A_i = \\frac{0 - 0.5}{0.53}= -0.94 Ai‚Äã=0.530‚àí0.5‚Äã=‚àí0.94 ### Step 3: Policy Update Finally, we update our model to reinforce the correct responses: - Assuming the probability of old policy (\\( \\pi*{\\theta*{old}} \\)) for a correct outputo1 o_1 o1‚Äã is0.5 0.5 0.5 and the new policy increases it to0.7 0.7 0.7 then:Ratio:0.70.5=1.4‚Üíafter Clip 1.2 (œµ=0.2)\\text{Ratio}: \\frac{0.7}{0.5} = 1.4 ‚Üí\\text{after Clip}\\space1.2 \\space (\\epsilon = 0.2)Ratio:0.50.7‚Äã=1.4‚Üíafter Clip 1.2 (œµ=0.2) - Then when the target function is re-weighted, the model tends to reinforce the generation of correct output, and theKL Divergence \\text{KL Divergence} KL Divergence limits the deviation from the reference policy. With the theoretical understanding in place, let‚Äôs see how GRPO can be implemented in code. ## Implementation Example Let‚Äôs put everything together in a practical example. The following code demonstrates how to implement GRPO in PyTorch. ### 1. Loading the Model and Generating Responses First, we need to load a model and generate multiple responses for a given question: ``` import torch import torch.nn.functional as F from transformers import AutoModelForCausalLM, AutoTokenizer # Load the model and tokenizer model_name = \"Qwen/Qwen2-Math-1.5B\" model = AutoModelForCausalLM.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) model.eval() # Move model to GPU if available device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") model.to(device) # Input prompt prompt = \"Solve y = 2x + 1 for x = 2, y = \" # Correct answer: 5 inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True) input_ids = inputs[\"input_ids\"].to(device) # Shape: (1, prompt_len) attention_mask = inputs[\"attention_mask\"].to(device) # Step 1: Generate 8 responses (B = 2 groups, G = 4 responses per group) batch_size, num_generations = 2, 4 outputs = model.generate( input_ids=input_ids, # Shape: (1, prompt_len) attention_mask=attention_mask, max_new_tokens=1, # seq_len = 1 (single token per response) num_return_sequences=batch_size * num_generations, # 8 responses total do_sample=True, top_k=10, temperature=0.7, pad_token_id=tokenizer.eos_token_id, return_dict_in_generate=True, output_scores=True, ) ``` this initial Generation (Before Any Steps) will output sth like this: ``` Output 1: 5.0 Output 2: 6.0 Output 3: 7.0 Output 4: 5.0 Output 5: 10.0 Output 6: 2.0 Output 7: 5.0 Output 8: 5.0 ``` ### 2. Calculating Rewards Now, we need to determine which responses are correct and assign rewards accordingly: With GRPO, with the same sample prompt, we generate multiple completions. So for instance, for our prompts of `\"Solve",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(1, prompt_len) attention_mask=attention_mask, max_new_tokens=1, # seq_len = 1 (single token per response) num_return_sequences=batch_size * num_generations, # 8 responses total do_sample=True, top_k=10, temperature=0.7, pad_token_id=tokenizer.eos_token_id, return_dict_in_generate=True, output_scores=True, ) ``` this initial Generation (Before Any Steps) will output sth like this: ``` Output 1: 5.0 Output 2: 6.0 Output 3: 7.0 Output 4: 5.0 Output 5: 10.0 Output 6: 2.0 Output 7: 5.0 Output 8: 5.0 ``` ### 2. Calculating Rewards Now, we need to determine which responses are correct and assign rewards accordingly: With GRPO, with the same sample prompt, we generate multiple completions. So for instance, for our prompts of `\"Solve y = 2x + 1 for x = 2, y = \"` and `Solve y = 2x + 1 for x = 4, y = \"` we have two group of generated outputs for the given prompt one is say - `[5, 6, 7, 5]` and the other is - `[10, 2, 9, 9]` while the correct answer is 5 and 9. Note that in practice these reward scores are achieved by a rule-based reward function that assigns rewards based on the correctness of the response or a more complex neural network-based model that can be trained to assign rewards based on the correctness of the response or a mixed of both. But for sake of simplicity let‚Äôs say our reward per response is 1 if the response is correct and 0 if it is wrong, therefore; ``` reward_1 = [1, 0, 0, 1] reward_2 = [0, 0, 1, 1] ``` next we get the group_wise mean and std of the rewards; ``` # Shape: (B * G,) = (8,) bc we have 2 groups of 4 generations that we flatten rewards = torch.tensor([1, 0, 0, 1, 0, 0, 1, 1], dtype=torch.float32) num_generations = 4 # Group rewards: Shape (B, G) = 2, 4) rewards_grouped = rewards.view(-1, num_generations) # Mean per group: Shape (B,) = (2,) mean_grouped_rewards = rewards_grouped.mean(dim=1) # Std per group: Shape (B,) = (2,) std_grouped_rewards = rewards_grouped.std(dim=1) # Broadcast to match rewards and normalize: Shape (B * G,) = (8,) # why we need to broadcast? because we need to calculate the advantage values for each response within the group mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_generations, dim=0) std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_generations, dim=0) ``` this will output: ``` Grouped Rewards: tensor([[1., 0., 0., 1.], [0., 0., 1., 1.]]) Mean per group: tensor([0.5000, 0.5000]) Std per group: tensor([0.5774, 0.5774]) Broadcasted Mean: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]) Broadcasted Std: tensor([0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774]) ``` Now we can calculate the advantage values for each response: ``` # Advantages: Shape (B * G,) = (8,) advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-8) ``` this will output: ``` Advantages: tensor([ 0.8659, -0.8660, -0.8660, 0.8659, -0.8660, -0.8660, 0.8659, 0.8659]) ``` which is coming from the Advantage formula above, so: ``` For reward_1 = [1, 0, 0, 1]: 1 - 0.5 / 0.5774 ‚âà 0.8659 0 - 0.5 / 0.5774 ‚âà -0.8660 For reward_2 = [0, 0, 1, 1]: Same",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]) Broadcasted Std: tensor([0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774, 0.5774]) ``` Now we can calculate the advantage values for each response: ``` # Advantages: Shape (B * G,) = (8,) advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-8) ``` this will output: ``` Advantages: tensor([ 0.8659, -0.8660, -0.8660, 0.8659, -0.8660, -0.8660, 0.8659, 0.8659]) ``` which is coming from the Advantage formula above, so: ``` For reward_1 = [1, 0, 0, 1]: 1 - 0.5 / 0.5774 ‚âà 0.8659 0 - 0.5 / 0.5774 ‚âà -0.8660 For reward_2 = [0, 0, 1, 1]: Same pattern. ``` however, the shape here is `(B*G,) = (8,)` but in practice, we need to have the shape of `(B, G) = (2, 4)` to match the logits shape, right? Therefore, we need to unsqueeze the advantages tensor to have the shape of `(B*G, 1) = (8, 1)` to match the logits shape. ``` # Shape (B * G, 1) = (8, 1) to match the logits shape advantages = advantages.unsqueeze(1) ``` which will output: ``` Advantages: tensor([[ 0.8659], [-0.8660], [-0.8660], [ 0.8659], [-0.8660], [-0.8660], [ 0.8659], [ 0.8659]]) ``` now we are good, let‚Äôs move to the next step of updating the policy model based on the advantage values. ### 3. Updating the Policy Finally, we use the advantage values to update our model: ``` # Compute probability ratio between new and old policies ratio = torch.exp( new_per_token_logps - per_token_logps ) # Shape: (B*G, seq_len) seq_len is the length of the output i.e. the num of generated tokens so here for simplicity let's assume it is 1 # (8, 1) ``` Note that the `per_token_logps` can be achieved by passing the generated outputs to the model and get the logits and then apply the softmax function to get the probabilities `F.softmax(logits, dim=-1)`. ``` # Clipping Function eps = self.cliprange # e.g. 0.2 pg_losses1 = -advantages * ratio # Shape: (B*G, seq_len) #(8, 1) pg_losses2 = -advantages * torch.clamp( ratio, 1.0 - eps, 1.0 + eps ) # Shape: (B*G, seq_len) #(8, 1) pg_loss_max = torch.max(pg_losses1, pg_losses2) # Shape: (B*G, seq_len) #(8, 1) # Now Combine with KL penalty # Shape: (B*G, seq_len) #(8, 1) per_token_loss = pg_loss_max + self.beta * per_token_kl ``` `per_token_kl` can also be calculated as follows: ``` # Shape: (B*G, seq_len) #(8, 1) per_token_kl = F.kl_div( F.log_softmax(new_per_token_logps, dim=-1), F.softmax(per_token_logps, dim=-1), reduction=\"none\", ).sum(dim=-1, keepdim=True) ``` Complete example can be found [here](./basic_example.py). GRPO is also implemented by the excellent TRL team, you can check the implementation [TRL/GRPO_trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py) for more details. ## Summary and Next Steps Congratulations! You‚Äôve now learned about Group Relative Policy Optimization (GRPO). To recap what we‚Äôve covered: 1. GRPO compares multiple outputs within a group to determine which ones are better than others, without requiring a separate value model. 2. The advantage calculation standardizes rewards to identify which responses are above or below average. 3. The policy update uses a clipped objective function with a KL divergence penalty to ensure stable learning. This approach",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "example can be found [here](./basic_example.py). GRPO is also implemented by the excellent TRL team, you can check the implementation [TRL/GRPO_trainer](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py) for more details. ## Summary and Next Steps Congratulations! You‚Äôve now learned about Group Relative Policy Optimization (GRPO). To recap what we‚Äôve covered: 1. GRPO compares multiple outputs within a group to determine which ones are better than others, without requiring a separate value model. 2. The advantage calculation standardizes rewards to identify which responses are above or below average. 3. The policy update uses a clipped objective function with a KL divergence penalty to ensure stable learning. This approach is particularly powerful for mathematical reasoning tasks, where correctness can be objectively verified. The GRPO method allows for more efficient training compared to traditional RLHF approaches that require a separate critic model. As you continue exploring GRPO, consider experimenting with different group sizes, reward functions, and KL penalty coefficients to see how they affect your model‚Äôs performance. Happy training! üöÄ ## References 1. [RLHF Book by Nathan Lambert](https://github.com/natolambert/rlhf-book) 2. [DeepSeek-V3 Technical Report](https://huggingface.co/papers/2412.19437) 3. [DeepSeekMath](https://huggingface.co/papers/2402.03300)",
    "metadata": {
      "title": "Advanced Understanding of Group Relative Policy Optimization (GRPO) in DeepSeekMath",
      "url": "https://huggingface.co/learn/llm-course/chapter12/3b",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/3b",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Implementing GRPO in TRL In this page, we‚Äôll learn how to implement Group Relative Policy Optimization (GRPO) using the Transformer Reinforcement Learning (TRL) library. We‚Äôll focus on practical implementation with minimal code. We‚Äôll explore the core concepts of GRPO as they are embodied in TRL‚Äôs GRPOTrainer, using snippets from the official TRL documentation to guide us. > This chapter is aimed at TRL beginners. If you are already familiar with TRL, you might want to also check out theOpen R1 implementationof GRPO. First, let‚Äôs remind ourselves of some of the important concepts of GRPO algorithm: - Group Formation: The model generates multiple completions for each prompt. - Preference Learning: The model learns from a reward function that compares groups of completions. - Training Configuration: The model uses a configuration to control the training process. What do we need to do to implement GRPO? - Define a dataset of prompts. - Define a reward function that takes a list of completions and returns a list of rewards. - Configure the training process with a GRPOConfig. - Train the model using the GRPOTrainer. Here‚Äôs a minimal example to get started with GRPO training: ``` from trl import GRPOTrainer, GRPOConfig from datasets import load_dataset # 1. Load your dataset dataset = load_dataset(\"your_dataset\", split=\"train\") # 2. Define a simple reward function def reward_func(completions, **kwargs): \"\"\"Example: Reward longer completions\"\"\" return [float(len(completion)) for completion in completions] # 3. Configure training training_args = GRPOConfig( output_dir=\"output\", num_train_epochs=3, per_device_train_batch_size=4, gradient_accumulation_steps=2, logging_steps=10, ) # 4. Initialize and train trainer = GRPOTrainer( model=\"your_model\", # e.g. \"Qwen/Qwen2-0.5B-Instruct\" args=training_args, train_dataset=dataset, reward_funcs=reward_func, ) trainer.train() ``` ## Key Components ### 1. Dataset Format Your dataset should contain prompts that the model will respond to. The GRPO trainer will generate multiple completions for each prompt and use the reward function to compare them. ### 2. Reward Function The reward function is crucial - it determines how the model learns. Here are two practical examples: ``` # Example 1: Reward based on completion length def reward_length(completions, **kwargs): return [float(len(completion)) for completion in completions] # Example 2: Reward based on matching a pattern import re def reward_format(completions, **kwargs): pattern = r\"^<think>.*?</think><answer>.*?</answer>$\" return [1.0 if re.match(pattern, c) else 0.0 for c in completions] ``` ### 3. Training Configuration Key parameters to consider in `GRPOConfig`: ``` training_args = GRPOConfig( # Essential parameters output_dir=\"output\", num_train_epochs=3, num_generation=4, # Number of completions to generate for each prompt per_device_train_batch_size=4, # We want to get all generations in one device batch # Optional but useful gradient_accumulation_steps=2, learning_rate=1e-5, logging_steps=10, # GRPO specific (optional) use_vllm=True, # Speed up generation ) ``` The `num_generation` parameter is particularly important for GRPO as it defines the group size - how many different completions the model will generate for each prompt. This is a key differentiator from other RL methods: - Too small (e.g., 2-3): May not provide enough diversity for meaningful comparisons - Recommended (4-16): Provides good balance between diversity and computational efficiency - Larger values: May improve learning but significantly increases computational cost The group size should be",
    "metadata": {
      "title": "Implementing GRPO in TRL",
      "url": "https://huggingface.co/learn/llm-course/chapter12/4",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/4",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "get all generations in one device batch # Optional but useful gradient_accumulation_steps=2, learning_rate=1e-5, logging_steps=10, # GRPO specific (optional) use_vllm=True, # Speed up generation ) ``` The `num_generation` parameter is particularly important for GRPO as it defines the group size - how many different completions the model will generate for each prompt. This is a key differentiator from other RL methods: - Too small (e.g., 2-3): May not provide enough diversity for meaningful comparisons - Recommended (4-16): Provides good balance between diversity and computational efficiency - Larger values: May improve learning but significantly increases computational cost The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16). ## Tips for Success 1. **Memory Management**: Adjust `per_device_train_batch_size` and `gradient_accumulation_steps` based on your GPU memory. 2. **Speed**: Enable `use_vllm=True` for faster generation if your model is supported. 3. **Monitoring**: Watch the logged metrics during training: - `reward`: Average reward across completions - `reward_std`: Standard deviation within reward groups - `kl`: KL divergence from reference model ## Reward Function Design The DeepSeek R1 paper demonstrates several effective approaches to reward function design that you can adapt for your own GRPO implementation: ### 1. Length-Based Rewards One of the easiest rewards to implement is a length-based reward. You can reward longer completions: ``` def reward_len(completions, **kwargs): ideal_length = 20 return [-abs(ideal_length - len(completion)) for completion in completions] ``` This reward function penalizes completions that are too short or too long, encouraging the model to generate completions that are close to the ideal length of 20 tokens. ## 2. Rule-Based Rewards for Verifiable Tasks For tasks with objectively correct answers (like mathematics or coding), you can implement rule-based reward functions: ``` def problem_reward(completions, answers, **kwargs): \"\"\"Reward function for math problems with verifiable answers completions: list of completions to evaluate answers: list of answers to the problems from the dataset \"\"\" rewards = [] for completion, correct_answer in zip(completions, answers): # Extract the answer from the completion try: # This is a simplified example - you'd need proper parsing answer = extract_final_answer(completion) # Binary reward: 1 for correct, 0 for incorrect reward = 1.0 if answer == correct_answer else 0.0 rewards.append(reward) except: # If we can't parse an answer, give a low reward rewards.append(0.0) return rewards ``` ## 3. Format-Based Rewards You can also reward proper formatting, which was important in the DeepSeek R1 training: ``` def format_reward(completions, **kwargs): \"\"\"Reward completions that follow the desired format\"\"\" # Example: Check if the completion follows a think-then-answer format pattern = r\"<think>(.*?)</think>\\s*<answer>(.*?)</answer>\" rewards = [] for completion in completions: match = re.search(pattern, completion, re.DOTALL) if match: # Check if there's substantial content in both sections think_content = match.group(1).strip() answer_content = match.group(2).strip() if len(think_content) > 20 and len(answer_content) > 0: rewards.append(1.0) else: rewards.append( 0.5 ) # Partial reward for correct format but limited content else: rewards.append(0.0) # No reward for incorrect format return rewards ```",
    "metadata": {
      "title": "Implementing GRPO in TRL",
      "url": "https://huggingface.co/learn/llm-course/chapter12/4",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/4",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "You can also reward proper formatting, which was important in the DeepSeek R1 training: ``` def format_reward(completions, **kwargs): \"\"\"Reward completions that follow the desired format\"\"\" # Example: Check if the completion follows a think-then-answer format pattern = r\"<think>(.*?)</think>\\s*<answer>(.*?)</answer>\" rewards = [] for completion in completions: match = re.search(pattern, completion, re.DOTALL) if match: # Check if there's substantial content in both sections think_content = match.group(1).strip() answer_content = match.group(2).strip() if len(think_content) > 20 and len(answer_content) > 0: rewards.append(1.0) else: rewards.append( 0.5 ) # Partial reward for correct format but limited content else: rewards.append(0.0) # No reward for incorrect format return rewards ``` These examples demonstrate how you can implement reward functions inspired by the DeepSeek R1 training process, focusing on correctness, formatting, and combined signals. ## That‚Äôs it! In the next section, you will follow an exercise to implement GRPO in TRL.",
    "metadata": {
      "title": "Implementing GRPO in TRL",
      "url": "https://huggingface.co/learn/llm-course/chapter12/4",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/4",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Practical Exercise: Fine-tune a model with GRPO Now that you‚Äôve seen the theory, let‚Äôs put it into practice! In this exercise, you‚Äôll fine-tune a model with GRPO. > This exercise was written by LLM fine-tuning expert@mlabonne. ## Install dependencies First, let‚Äôs install the dependencies for this exercise. ``` !pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off !pip install -qqq flash-attn --no-build-isolation --progress-bar off ``` Now we‚Äôll import the necessary libraries. ``` import torch from datasets import load_dataset from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM, AutoTokenizer from trl import GRPOConfig, GRPOTrainer ``` ## Import and log in to Weights & Biases Weights & Biases is a tool for logging and monitoring your experiments. We‚Äôll use it to log our fine-tuning process. ``` import wandb wandb.login() ``` You can do this exercise without logging in to Weights & Biases, but it‚Äôs recommended to do so to track your experiments and interpret the results. ## Load the dataset Now, let‚Äôs load the dataset. In this case, we‚Äôll use the [mlabonne/smoltldr](https://huggingface.co/datasets/mlabonne/smoltldr) dataset, which contains a list of short stories. ``` dataset = load_dataset(\"mlabonne/smoltldr\") print(dataset) ``` ## Load model Now, let‚Äôs load the model. For this exercise, we‚Äôll use the [SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M) model. This is a small 135M parameter model that runs on limited hardware. This makes the model ideal for learning, but it‚Äôs not the most powerful model out there. If you have access to more powerful hardware, you can try to fine-tune a larger model like [SmolLM2-1.7B](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B). ``` model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\" model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\", ) tokenizer = AutoTokenizer.from_pretrained(model_id) ``` ## Load LoRA Now, let‚Äôs load the LoRA configuration. We‚Äôll take advantage of LoRA to reduce the number of trainable parameters, and in turn the memory footprint we need to fine-tune the model. If you‚Äôre not familiar with LoRA, you can read more about it in [Chapter 11](https://huggingface.co/learn/course/en/chapter11/3). ``` # Load LoRA lora_config = LoraConfig( task_type=\"CAUSAL_LM\", r=16, lora_alpha=32, target_modules=\"all-linear\", ) model = get_peft_model(model, lora_config) print(model.print_trainable_parameters()) ``` ``` Total trainable parameters: 135M ``` ## Define the reward function As mentioned in the previous section, GRPO can use any reward function to improve the model. In this case, we‚Äôll use a simple reward function that encourages the model to generate text that is 50 tokens long. ``` # Reward function ideal_length = 50 def reward_len(completions, **kwargs): return [-abs(ideal_length - len(completion)) for completion in completions] ``` ## Define the training arguments Now, let‚Äôs define the training arguments. We‚Äôll use the `GRPOConfig` class to define the training arguments in a typical `transformers` style. If this is the first time you‚Äôre defining training arguments, you can check the [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainingarguments) class for more information, or [Chapter 2](https://huggingface.co/learn/course/en/chapter2/1) for a detailed introduction. ``` # Training arguments training_args = GRPOConfig( output_dir=\"GRPO\", learning_rate=2e-5, per_device_train_batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=96, num_generations=8, optim=\"adamw_8bit\", num_train_epochs=1, bf16=True, report_to=[\"wandb\"], remove_unused_columns=False, logging_steps=1, ) ``` Now, we can initialize the trainer with model, dataset, and training arguments and start training. ``` # Trainer trainer = GRPOTrainer( model=model, reward_funcs=[reward_len], args=training_args, train_dataset=dataset[\"train\"], ) #",
    "metadata": {
      "title": "Practical Exercise: Fine-tune a model with GRPO",
      "url": "https://huggingface.co/learn/llm-course/chapter12/5",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/5",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Define the training arguments Now, let‚Äôs define the training arguments. We‚Äôll use the `GRPOConfig` class to define the training arguments in a typical `transformers` style. If this is the first time you‚Äôre defining training arguments, you can check the [TrainingArguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainingarguments) class for more information, or [Chapter 2](https://huggingface.co/learn/course/en/chapter2/1) for a detailed introduction. ``` # Training arguments training_args = GRPOConfig( output_dir=\"GRPO\", learning_rate=2e-5, per_device_train_batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=96, num_generations=8, optim=\"adamw_8bit\", num_train_epochs=1, bf16=True, report_to=[\"wandb\"], remove_unused_columns=False, logging_steps=1, ) ``` Now, we can initialize the trainer with model, dataset, and training arguments and start training. ``` # Trainer trainer = GRPOTrainer( model=model, reward_funcs=[reward_len], args=training_args, train_dataset=dataset[\"train\"], ) # Train model wandb.init(project=\"GRPO\") trainer.train() ``` Training takes around 1 hour on a single A10G GPU which is available on Google Colab or via Hugging Face Spaces. ## Push the model to the Hub during training If we set the `push_to_hub` argument to `True` and the `model_id` argument to a valid model name, the model will be pushed to the Hugging Face Hub whilst we‚Äôre training. This is useful if you want to start vibe testing the model straight away! ## Interpret training results `GRPOTrainer` logs the reward from your reward function, the loss, and a range of other metrics. We will focus on the reward from the reward function and the loss. As you can see, the reward from the reward function moves closer to 0 as the model learns. This is a good sign that the model is learning to generate text of the correct length. ![Reward from reward function](https://huggingface.co/reasoning-course/images/resolve/main/grpo/13.png) You might notice that the loss starts at zero and then increases during training, which may seem counterintuitive. This behavior is expected in GRPO and is directly related to the mathematical formulation of the algorithm. The loss in GRPO is proportional to the KL divergence (the cap relative to original policy) . As training progresses, the model learns to generate text that better matches the reward function, causing it to diverge more from its initial policy. This increasing divergence is reflected in the rising loss value, which actually indicates that the model is successfully adapting to optimize for the reward function. ![Loss](https://huggingface.co/reasoning-course/images/resolve/main/grpo/14.png) ## Save and publish the model Let‚Äôs share the model with the community! ``` merged_model = trainer.model.merge_and_unload() merged_model.push_to_hub( \"SmolGRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"] ) ``` ## Generate text üéâ You‚Äôve successfully fine-tuned a model with GRPO! Now, let‚Äôs generate some text with the model. First, we‚Äôll define a really long document! ``` prompt = \"\"\" # A long document about the Cat The cat (Felis catus), also referred to as the domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Its",
    "metadata": {
      "title": "Practical Exercise: Fine-tune a model with GRPO",
      "url": "https://huggingface.co/learn/llm-course/chapter12/5",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/5",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "really long document! ``` prompt = \"\"\" # A long document about the Cat The cat (Felis catus), also referred to as the domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Its retractable claws are adapted to killing small prey species such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations‚Äîincluding meowing, purring, trilling, hissing, growling, and grunting‚Äîas well as body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It secretes and perceives pheromones. \"\"\" messages = [ {\"role\": \"user\", \"content\": prompt}, ] ``` Now, we can generate text with the model. ``` # Generate text from transformers import pipeline generator = pipeline(\"text-generation\", model=\"SmolGRPO-135M\") ## Or use the model and tokenizer we defined earlier # generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) generate_kwargs = { \"max_new_tokens\": 256, \"do_sample\": True, \"temperature\": 0.5, \"min_p\": 0.1, } generated_text = generator(messages, generate_kwargs=generate_kwargs) print(generated_text) ``` # Conclusion In this chapter, we‚Äôve seen how to fine-tune a model with GRPO. We‚Äôve also seen how to interpret the training results and generate text with the model.",
    "metadata": {
      "title": "Practical Exercise: Fine-tune a model with GRPO",
      "url": "https://huggingface.co/learn/llm-course/chapter12/5",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/5",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Practical Exercise: GRPO with Unsloth In this exercise, you‚Äôll fine-tune a model with GRPO (Group Relative Policy Optimization) using Unsloth, to improve a model‚Äôs reasoning capabilities. We covered GRPO in [Chapter 3](/course/chapter3/3). Unsloth is a library that accelerates LLM fine-tuning, making it possible to train models faster and with less computational resources. Unsloth is plugs into TRL, so we‚Äôll build on what we learned in the previous sections, and adapt it for Unsloth specifics. > This exercise can be run on a free Google Colab T4 GPU. For the best experience, follow along with the notebook linked above and try it out yourself. ## Install dependencies First, let‚Äôs install the necessary libraries. We‚Äôll need Unsloth for the accelerated fine-tuning and vLLM for fast inference. ``` pip install unsloth vllm pip install --upgrade pillow ``` ## Setting up Unsloth Unsloth provides a class (`FastLanguageModel`) that integrates transformers with Unsloth optimizations. Let‚Äôs import it: ``` from unsloth import FastLanguageModel ``` Now, let‚Äôs load Google‚Äôs Gemma 3 1B Instruct model and configure it for fine-tuning: ``` from unsloth import FastLanguageModel import torch max_seq_length = 1024 # Can increase for longer reasoning traces lora_rank = 32 # Larger rank = smarter, but slower model, tokenizer = FastLanguageModel.from_pretrained( model_name=\"google/gemma-3-1b-it\", max_seq_length=max_seq_length, load_in_4bit=True, # False for LoRA 16bit fast_inference=True, # Enable vLLM fast inference max_lora_rank=lora_rank, gpu_memory_utilization=0.6, # Reduce if out of memory ) model = FastLanguageModel.get_peft_model( model, r=lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128 target_modules=[ \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", ], # Remove QKVO if out of memory lora_alpha=lora_rank, use_gradient_checkpointing=\"unsloth\", # Enable long context finetuning random_state=3407, ) ``` This code loads the model in 4-bit quantization to save memory and applies LoRA (Low-Rank Adaptation) for efficient fine-tuning. The `target_modules` parameter specifies which layers of the model to fine-tune, and `use_gradient_checkpointing` enables training with longer contexts. > We won‚Äôt cover the details of LoRA in this chapter, but you can learn more inChapter 11. ## Data Preparation For this exercise, we‚Äôll use the GSM8K dataset, which contains grade school math problems. We‚Äôll format the data to encourage the model to show its reasoning before providing an answer. First, we will define the format of the prompts and answers: ``` # Define the system prompt that instructs the model to use a specific format SYSTEM_PROMPT = \"\"\" Respond in the following format: <reasoning> ... </reasoning> <answer> ... </answer> \"\"\" XML_COT_FORMAT = \"\"\"\\ <reasoning> {reasoning} </reasoning> <answer> {answer} </answer> \"\"\" ``` Now, let‚Äôs prepare the dataset: ``` import re from datasets import load_dataset, Dataset # Helper functions to extract answers from different formats def extract_xml_answer(text: str) -> str: answer = text.split(\"<answer>\")[-1] answer = answer.split(\"</answer>\")[0] return answer.strip() def extract_hash_answer(text: str) -> str | None: if \"####\" not in text: return None return text.split(\"####\")[1].strip() # Function to prepare the GSM8K dataset def get_gsm8k_questions(split=\"train\") -> Dataset: data = load_dataset(\"openai/gsm8k\", \"main\")[split] data = data.map( lambda x: { \"prompt\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": x[\"question\"]}, ], \"answer\": extract_hash_answer(x[\"answer\"]), } ) return data",
    "metadata": {
      "title": "Practical Exercise: GRPO with Unsloth",
      "url": "https://huggingface.co/learn/llm-course/chapter12/6",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/6",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "XML_COT_FORMAT = \"\"\"\\ <reasoning> {reasoning} </reasoning> <answer> {answer} </answer> \"\"\" ``` Now, let‚Äôs prepare the dataset: ``` import re from datasets import load_dataset, Dataset # Helper functions to extract answers from different formats def extract_xml_answer(text: str) -> str: answer = text.split(\"<answer>\")[-1] answer = answer.split(\"</answer>\")[0] return answer.strip() def extract_hash_answer(text: str) -> str | None: if \"####\" not in text: return None return text.split(\"####\")[1].strip() # Function to prepare the GSM8K dataset def get_gsm8k_questions(split=\"train\") -> Dataset: data = load_dataset(\"openai/gsm8k\", \"main\")[split] data = data.map( lambda x: { \"prompt\": [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": x[\"question\"]}, ], \"answer\": extract_hash_answer(x[\"answer\"]), } ) return data dataset = get_gsm8k_questions() ``` The dataset is prepared by extracting the answer from the dataset and formatting it as a string. ## Defining Reward Functions As we discussed in [an earlier page](/course/chapter13/4), GRPO can use reward functions to guide the model‚Äôs learning based on verifiable criteria like length and formatting. In this exercise, we‚Äôll define several reward functions that encourage different aspects of good reasoning. For example, we‚Äôll reward the model for providing an integer answer, and for following the strict format. ``` # Reward function that checks if the answer is correct def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]: responses = [completion[0][\"content\"] for completion in completions] q = prompts[0][-1][\"content\"] extracted_responses = [extract_xml_answer(r) for r in responses] print( \"-\" * 20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\", ) return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)] # Reward function that checks if the answer is an integer def int_reward_func(completions, **kwargs) -> list[float]: responses = [completion[0][\"content\"] for completion in completions] extracted_responses = [extract_xml_answer(r) for r in responses] return [0.5 if r.isdigit() else 0.0 for r in extracted_responses] # Reward function that checks if the completion follows the strict format def strict_format_reward_func(completions, **kwargs) -> list[float]: pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\" responses = [completion[0][\"content\"] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches] # Reward function that checks if the completion follows a more relaxed format def soft_format_reward_func(completions, **kwargs) -> list[float]: pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\" responses = [completion[0][\"content\"] for completion in completions] matches = [re.match(pattern, r) for r in responses] return [0.5 if match else 0.0 for match in matches] # Reward function that counts XML tags and penalizes extra content def count_xml(text) -> float: count = 0.0 if text.count(\"<reasoning>\\n\") == 1: count += 0.125 if text.count(\"\\n</reasoning>\\n\") == 1: count += 0.125 if text.count(\"\\n<answer>\\n\") == 1: count += 0.125 count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001 if text.count(\"\\n</answer>\") == 1: count += 0.125 count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001 return count def xmlcount_reward_func(completions, **kwargs) -> list[float]: contents = [completion[0][\"content\"] for completion in completions] return [count_xml(c) for c in contents] ``` These reward functions serve different purposes: Reward Function Purpose `correctness_reward_func` Rewards the model when its answer matches the correct answer `int_reward_func` Rewards the model for providing a numeric answer `strict_format_reward_func` and `soft_format_reward_func` Reward the model for following the specified format `xmlcount_reward_func` Rewards proper XML tag usage and penalizes extra content",
    "metadata": {
      "title": "Practical Exercise: GRPO with Unsloth",
      "url": "https://huggingface.co/learn/llm-course/chapter12/6",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/6",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "count += 0.125 if text.count(\"\\n<answer>\\n\") == 1: count += 0.125 count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001 if text.count(\"\\n</answer>\") == 1: count += 0.125 count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001 return count def xmlcount_reward_func(completions, **kwargs) -> list[float]: contents = [completion[0][\"content\"] for completion in completions] return [count_xml(c) for c in contents] ``` These reward functions serve different purposes: Reward Function Purpose `correctness_reward_func` Rewards the model when its answer matches the correct answer `int_reward_func` Rewards the model for providing a numeric answer `strict_format_reward_func` and `soft_format_reward_func` Reward the model for following the specified format `xmlcount_reward_func` Rewards proper XML tag usage and penalizes extra content after the closing tags ## Training with GRPO Now we‚Äôll set up the GRPO trainer with our model, tokenizer, and reward functions. This part follows the same approach as the [previous exercise](/course/chapter12/5). ``` from trl import GRPOConfig, GRPOTrainer max_prompt_length = 256 training_args = GRPOConfig( learning_rate=5e-6, adam_beta1=0.9, adam_beta2=0.99, weight_decay=0.1, warmup_ratio=0.1, lr_scheduler_type=\"cosine\", optim=\"paged_adamw_8bit\", logging_steps=1, per_device_train_batch_size=1, gradient_accumulation_steps=1, # Increase to 4 for smoother training num_generations=6, # Decrease if out of memory max_prompt_length=max_prompt_length, max_completion_length=max_seq_length - max_prompt_length, # num_train_epochs = 1, # Set to 1 for a full training run max_steps=250, save_steps=250, max_grad_norm=0.1, report_to=\"none\", # Can use Weights & Biases output_dir=\"outputs\", ) trainer = GRPOTrainer( model=model, processing_class=tokenizer, reward_funcs=[ xmlcount_reward_func, soft_format_reward_func, strict_format_reward_func, int_reward_func, correctness_reward_func, ], args=training_args, train_dataset=dataset, ) ``` The `GRPOConfig` sets various hyperparameters for training: - `use_vllm`: Enables fast inference with vLLM - `learning_rate`: Controls how quickly the model learns - `num_generations`: Number of completions to generate for each prompt - `max_steps`: Total number of training steps to perform Now let‚Äôs start the training: ``` trainer.train() ``` > Training may take some time. You might not see rewards increase immediately - it can take 150-200 steps before you start seeing improvements. Be patient! ## Testing the Model After training, let‚Äôs test our model to see how it performs. First, we‚Äôll save the LoRA weights: ``` model.save_lora(\"grpo_saved_lora\") ``` Now, let‚Äôs test the model with a new question: ``` from vllm import SamplingParams text = tokenizer.apply_chat_template( [ {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": \"Calculate pi.\"}, ], tokenize=False, add_generation_prompt=True, ) sampling_params = SamplingParams( temperature=0.8, top_p=0.95, max_tokens=1024, ) output = ( model.fast_generate( text, sampling_params=sampling_params, lora_request=model.load_lora(\"grpo_saved_lora\"), )[0] .outputs[0] .text ) print(output) ``` You should see that the model now follows the specified format, showing its reasoning before providing an answer. ## Saving the Model Unsloth provides several options for saving your fine-tuned model, but we‚Äôll focus on the most common. ``` # Save to 16-bit precision model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\") ``` ## Pushing to Hugging Face Hub We‚Äôll push the model to the Hugging Face Hub using the `push_to_hub_merged` method. This method allows us to push the model in multiple quantization formats. ``` # Push to Hugging Face Hub (requires a token) model.push_to_hub_merged( \"your-username/model-name\", tokenizer, save_method=\"merged_16bit\", token=\"your-token\" ) ``` Unsloth also supports saving to GGUF format for use with llama.cpp: ``` model.push_to_hub_gguf( \"your-username/model-name\", tokenizer, quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"], token=\"your-token\", ) ``` The GGUF files can be used with llama.cpp or UI-based systems like Jan or Open WebUI. ## Conclusion In this",
    "metadata": {
      "title": "Practical Exercise: GRPO with Unsloth",
      "url": "https://huggingface.co/learn/llm-course/chapter12/6",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/6",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "common. ``` # Save to 16-bit precision model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\") ``` ## Pushing to Hugging Face Hub We‚Äôll push the model to the Hugging Face Hub using the `push_to_hub_merged` method. This method allows us to push the model in multiple quantization formats. ``` # Push to Hugging Face Hub (requires a token) model.push_to_hub_merged( \"your-username/model-name\", tokenizer, save_method=\"merged_16bit\", token=\"your-token\" ) ``` Unsloth also supports saving to GGUF format for use with llama.cpp: ``` model.push_to_hub_gguf( \"your-username/model-name\", tokenizer, quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\"], token=\"your-token\", ) ``` The GGUF files can be used with llama.cpp or UI-based systems like Jan or Open WebUI. ## Conclusion In this exercise, you‚Äôve learned how to: 1. Set up Unsloth for accelerated fine-tuning 2. Prepare data for GRPO training 3. Define custom reward functions to guide the model‚Äôs learning 4. Train a model using GRPO 5. Test the fine-tuned model 6. Save the model in various formats GRPO is a powerful technique for aligning language models with specific behaviors, and Unsloth makes it accessible even on limited hardware. By combining multiple reward functions, you can guide the model to follow a specific format while also improving its reasoning capabilities. For more information and resources, check out: - [Unsloth Documentation](https://docs.unsloth.ai/) - [Unsloth Discord](https://discord.gg/unsloth) - [Unsloth GitHub](https://github.com/unslothai/unsloth)",
    "metadata": {
      "title": "Practical Exercise: GRPO with Unsloth",
      "url": "https://huggingface.co/learn/llm-course/chapter12/6",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/6",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Coming soon‚Ä¶\n\n \nThis chapter is being run as a live cohort now! If you‚Äôve finished the material so far, here‚Äôs what to expect:\n \n\n## Course Schedule\n\n Date Unit ~March 7th, 2025~ ~No-Code Exam and Certification~ ~March 14th, 2025~ ~Next Practical Exercise~ March 21st, 2025 Interactive code review April 2025 More written material on building reasoning models April 2025 Live sessions on building Open R1 April 2025 Code Exam and Certification \n\n## Staying Up to Date\n\n \nIf you want to follow the course, follow the [The Reasoning Course](https://huggingface.co/reasoning-course) and join the [Discord community](https://discord.gg/F3vZujJH)!",
    "metadata": {
      "title": "Coming soon‚Ä¶",
      "url": "https://huggingface.co/learn/llm-course/chapter12/7",
      "course": "llm-course",
      "chapter": "12. Build Reasoning Models",
      "chapter_id": "chapter12/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter12/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n   \nAs you saw in [Chapter 1](/course/chapter1), Transformer models are usually very large. With millions to tens of *billions* of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.\n \nThe ü§ó Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library‚Äôs main features are:\n \n- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` classes and can be handled like any other models in their respective machine learning (ML) frameworks.\n- **Simplicity**: Hardly any abstractions are made across the library. The ‚ÄúAll in one file‚Äù is a core concept: a model‚Äôs forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.\n \nThis last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules\nthat are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.\n \nThis chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we‚Äôll discuss the model API: we‚Äôll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.\n \nThen we‚Äôll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we‚Äôll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n \n> ‚ö†Ô∏è In order to benefit from all features available with the Model Hub and ü§ó Transformers, we recommendcreating an account.",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter2/1",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Behind the pipeline Let‚Äôs start with a complete example, taking a look at what happened behind the scenes when we executed the following code in [Chapter 1](/course/chapter1): ``` from transformers import pipeline classifier = pipeline(\"sentiment-analysis\") classifier( [ \"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\", ] ) ``` and obtained: ``` [{'label': 'POSITIVE', 'score': 0.9598047137260437}, {'label': 'NEGATIVE', 'score': 0.9994558095932007}] ``` As we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing: ![The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg) ![The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg) Let‚Äôs quickly go over each of these. ## Preprocessing with a tokenizer Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for: - Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens* - Mapping each token to an integer - Adding additional inputs that may be useful to the model All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below). Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following: ``` from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) ``` Once we have the tokenizer, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors. You can use ü§ó Transformers without having to worry about which ML framework is used as a backend; it might be PyTorch or Flax for some models. However, Transformer models only accept *tensors* as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays. To specify the type of tensors we want to get back (PyTorch or plain NumPy), we use the `return_tensors` argument: ``` raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\",",
    "metadata": {
      "title": "Behind the pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter2/2",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/2",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "only accept *tensors* as input. If this is your first time hearing about tensors, you can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays. To specify the type of tensors we want to get back (PyTorch or plain NumPy), we use the `return_tensors` argument: ``` raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\", ] inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") print(inputs) ``` Don‚Äôt worry about padding and truncation just yet; we‚Äôll explain those later. The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result). Here‚Äôs what the results look like as PyTorch tensors: ``` { 'input_ids': tensor([ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [ 101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0] ]), 'attention_mask': tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0] ]) } ``` The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`. `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. We‚Äôll explain what the `attention_mask` is later in this chapter. ## Going through the model We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an `AutoModel` class which also has a `from_pretrained()` method: ``` from transformers import AutoModel checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModel.from_pretrained(checkpoint) ``` In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it. This architecture contains only the base Transformer module: given some inputs, it outputs what we‚Äôll call *hidden states*, also known as *features*. For each model input, we‚Äôll retrieve a high-dimensional vector representing the **contextual understanding of that input by the Transformer model**. If this doesn‚Äôt make sense, don‚Äôt worry about it. We‚Äôll explain it all later. While these hidden states can be useful on their own, they‚Äôre usually inputs to another part of the model, known as the *head*. In [Chapter 1](/course/chapter1), the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it. ### A high-dimensional vector? The vector output by the Transformer module is usually large. It generally has three dimensions: - **Batch size**: The number of sequences processed",
    "metadata": {
      "title": "Behind the pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter2/2",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/2",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "**contextual understanding of that input by the Transformer model**. If this doesn‚Äôt make sense, don‚Äôt worry about it. We‚Äôll explain it all later. While these hidden states can be useful on their own, they‚Äôre usually inputs to another part of the model, known as the *head*. In [Chapter 1](/course/chapter1), the different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it. ### A high-dimensional vector? The vector output by the Transformer module is usually large. It generally has three dimensions: - **Batch size**: The number of sequences processed at a time (2 in our example). - **Sequence length**: The length of the numerical representation of the sequence (16 in our example). - **Hidden size**: The vector dimension of each model input. It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more). We can see this if we feed the inputs we preprocessed to our model: ``` outputs = model(**inputs) print(outputs.last_hidden_state.shape) ``` ``` torch.Size([2, 16, 768]) ``` Note that the outputs of ü§ó Transformers models behave like `namedtuple`s or dictionaries. You can access the elements by attributes (like we did) or by key (`outputs[\"last_hidden_state\"]`), or even by index if you know exactly where the thing you are looking for is (`outputs[0]`). ### Model heads: Making sense out of numbers The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers: ![A Transformer network alongside its head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg) ![A Transformer network alongside its head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg) The output of the Transformer model is sent directly to the model head to be processed. In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences. There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list: - `*Model` (retrieve the hidden states) - `*ForCausalLM` - `*ForMaskedLM` - `*ForMultipleChoice` - `*ForQuestionAnswering` - `*ForSequenceClassification` - `*ForTokenClassification` - and others ü§ó For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the `AutoModel` class, but `AutoModelForSequenceClassification`: ``` from transformers import AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModelForSequenceClassification.from_pretrained(checkpoint) outputs = model(**inputs) ``` Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): ``` print(outputs.logits.shape) ``` ``` torch.Size([2, 2]) ``` Since we have just two sentences and",
    "metadata": {
      "title": "Behind the pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter2/2",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/2",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the `AutoModel` class, but `AutoModelForSequenceClassification`: ``` from transformers import AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" model = AutoModelForSequenceClassification.from_pretrained(checkpoint) outputs = model(**inputs) ``` Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label): ``` print(outputs.logits.shape) ``` ``` torch.Size([2, 2]) ``` Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2. ## Postprocessing the output The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look: ``` print(outputs.logits) ``` ``` tensor([[-1.5607, 1.6123], [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>) ``` Our model predicted `[-1.5607, 1.6123]` for the first sentence and `[ 4.1692, -3.3464]` for the second one. Those are not probabilities but *logits*, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy): ``` import torch predictions = torch.nn.functional.softmax(outputs.logits, dim=-1) print(predictions) ``` ``` tensor([[4.0195e-02, 9.5980e-01], [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>) ``` Now we can see that the model predicted `[0.0402, 0.9598]` for the first sentence and `[0.9995, 0.0005]` for the second one. These are recognizable probability scores. To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config (more on this in the next section): ``` model.config.id2label ``` ``` {0: 'NEGATIVE', 1: 'POSITIVE'} ``` Now we can conclude that the model predicted the following: - First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598 - Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005 We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing! Now let‚Äôs take some time to dive deeper into each of those steps. > ‚úèÔ∏èTry it out!Choose two (or more) texts of your own and run them through thesentiment-analysispipeline. Then replicate the steps you saw here yourself and check that you obtain the same results!",
    "metadata": {
      "title": "Behind the pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter2/2",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/2",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Models In this section, we‚Äôll take a closer look at creating and using models. We‚Äôll use the `AutoModel` class, which is handy when you want to instantiate any model from a checkpoint. ## Creating a Transformer Let‚Äôs begin by examining what happens when we instantiate an `AutoModel`: ``` from transformers import AutoModel model = AutoModel.from_pretrained(\"bert-base-cased\") ``` Similar to the tokenizer, the `from_pretrained()` method will download and cache the model data from the Hugging Face Hub. As mentioned previously, the checkpoint name corresponds to a specific model architecture and weights, in this case a BERT model with a basic architecture (12 layers, 768 hidden size, 12 attention heads) and cased inputs (meaning that the uppercase/lowercase distinction is important). There are many checkpoints available on the Hub ‚Äî you can explore them [here](https://huggingface.co/models). The `AutoModel` class and its associates are actually simple wrappers designed to fetch the appropriate model architecture for a given checkpoint. It‚Äôs an ‚Äúauto‚Äù class meaning it will guess the appropriate model architecture for you and instantiate the correct model class. However, if you know the type of model you want to use, you can use the class that defines its architecture directly: ``` from transformers import BertModel model = BertModel.from_pretrained(\"bert-base-cased\") ``` ## Loading and saving Saving a model is as simple as saving a tokenizer. In fact, the models actually have the same `save_pretrained()` method, which saves the model‚Äôs weights and architecture configuration: ``` model.save_pretrained(\"directory_on_my_computer\") ``` This will save two files to your disk: ``` ls directory_on_my_computer config.json model.safetensors ``` If you look inside the *config.json* file, you‚Äôll see all the necessary attributes needed to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what ü§ó Transformers version you were using when you last saved the checkpoint. The *pytorch_model.safetensors* file is known as the state dictionary; it contains all your model‚Äôs weights. The two files work together: the configuration file is needed to know about the model architecture, while the model weights are the parameters of the model. To reuse a saved model, use the `from_pretrained()` method again: ``` from transformers import AutoModel model = AutoModel.from_pretrained(\"directory_on_my_computer\") ``` A wonderful feature of the ü§ó Transformers library is the ability to easily share models and tokenizers with the community. To do this, make sure you have an account on [Hugging Face](https://huggingface.co). If you‚Äôre using a notebook, you can easily log in with this: ``` from huggingface_hub import notebook_login notebook_login() ``` Otherwise, at your terminal run: ``` huggingface-cli login ``` Then you can push the model to the Hub with the `push_to_hub()` method: ``` model.push_to_hub(\"my-awesome-model\") ``` This will upload the model files to the Hub, in a repository under your namespace named *my-awesome-model*. Then, anyone can load your model with the `from_pretrained()` method! ``` from transformers import AutoModel model = AutoModel.from_pretrained(\"your-username/my-awesome-model\") ``` You can do a lot more with the Hub API: - Push a model from a local repository - Update specific files without re-uploading everything - Add model cards",
    "metadata": {
      "title": "Models",
      "url": "https://huggingface.co/learn/llm-course/chapter2/3",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/3",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in with this: ``` from huggingface_hub import notebook_login notebook_login() ``` Otherwise, at your terminal run: ``` huggingface-cli login ``` Then you can push the model to the Hub with the `push_to_hub()` method: ``` model.push_to_hub(\"my-awesome-model\") ``` This will upload the model files to the Hub, in a repository under your namespace named *my-awesome-model*. Then, anyone can load your model with the `from_pretrained()` method! ``` from transformers import AutoModel model = AutoModel.from_pretrained(\"your-username/my-awesome-model\") ``` You can do a lot more with the Hub API: - Push a model from a local repository - Update specific files without re-uploading everything - Add model cards to document the model‚Äôs abilities, limitations, known biases, etc. See [the documentation](https://huggingface.co/docs/huggingface_hub/how-to-upstream) for a complete tutorial on this, or check out the advanced [Chapter 4](/course/chapter4). ## Encoding text Transformer models handle text by turning the inputs into numbers. Here we will look at exactly what happens when your text is processed by the tokenizer. We‚Äôve already seen in [Chapter 1](/course/chapter1) that tokenizers split the text into tokens and then convert these tokens into numbers. We can see this conversion through a simple tokenizer: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") encoded_input = tokenizer(\"Hello, I'm a single sentence!\") print(encoded_input) ``` ``` {'input_ids': [101, 8667, 117, 1000, 1045, 1005, 1049, 2235, 17662, 12172, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} ``` We get a dictionary with the following fields: - input_ids: numerical representations of your tokens - token_type_ids: these tell the model which part of the input is sentence A and which is sentence B (discussed more in the next section) - attention_mask: this indicates which tokens should be attended to and which should not (discussed more in a bit) We can decode the input IDs to get back the original text: ``` tokenizer.decode(encoded_input[\"input_ids\"]) ``` ``` \"[CLS] Hello, I'm a single sentence! [SEP]\" ``` You‚Äôll notice that the tokenizer has added special tokens ‚Äî `[CLS]` and `[SEP]` ‚Äî required by the model. Not all models need special tokens; they‚Äôre utilized when a model was pretrained with them, in which case the tokenizer needs to add them as that model expects these tokens. You can encode multiple sentences at once, either by batching them together (we‚Äôll discuss this soon) or by passing a list: ``` encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\") print(encoded_input) ``` ``` {'input_ids': [[101, 1731, 1132, 1128, 136, 102], [101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} ``` Note that when passing multiple sentences, the tokenizer returns a list for each sentence for each dictionary value. We can also ask the tokenizer to return tensors directly from PyTorch: ``` encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\") print(encoded_input)",
    "metadata": {
      "title": "Models",
      "url": "https://huggingface.co/learn/llm-course/chapter2/3",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/3",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "fine, thank you!\") print(encoded_input) ``` ``` {'input_ids': [[101, 1731, 1132, 1128, 136, 102], [101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} ``` Note that when passing multiple sentences, the tokenizer returns a list for each sentence for each dictionary value. We can also ask the tokenizer to return tensors directly from PyTorch: ``` encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\") print(encoded_input) ``` ``` {'input_ids': tensor([[ 101, 1731, 1132, 1128, 136, 102], [ 101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} ``` But there‚Äôs a problem: the two lists don‚Äôt have the same length! Arrays and tensors need to be rectangular, so we can‚Äôt simply convert these lists to a PyTorch tensor (or NumPy array). The tokenizer provides an option for that: padding. ### Padding inputs If we ask the tokenizer to pad the inputs, it will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one: ``` encoded_input = tokenizer( [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\" ) print(encoded_input) ``` ``` {'input_ids': tensor([[ 101, 1731, 1132, 1128, 136, 102, 0, 0, 0, 0], [ 101, 1045, 1005, 1049, 2503, 117, 5763, 1128, 136, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} ``` Now we have rectangular tensors! Note that the padding tokens have been encoded into input IDs with ID 0, and they have an attention mask value of 0 as well. This is because those padding tokens shouldn‚Äôt be analyzed by the model: they‚Äôre not part of the actual sentence. ### Truncating inputs The tensors might get too big to be processed by the model. For instance, BERT was only pretrained with sequences up to 512 tokens, so it cannot process longer sequences. If you have sequences longer than the model can handle, you‚Äôll need to truncate them with the `truncation` parameter: ``` encoded_input = tokenizer( \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\", truncation=True, ) print(encoded_input[\"input_ids\"]) ``` ``` [101, 1188, 1110, 170, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505,",
    "metadata": {
      "title": "Models",
      "url": "https://huggingface.co/learn/llm-course/chapter2/3",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/3",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "have sequences longer than the model can handle, you‚Äôll need to truncate them with the `truncation` parameter: ``` encoded_input = tokenizer( \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\", truncation=True, ) print(encoded_input[\"input_ids\"]) ``` ``` [101, 1188, 1110, 170, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1179, 5650, 119, 102] ``` By combining the padding and truncation arguments, you can make sure your tensors have the exact size you need: ``` encoded_input = tokenizer( [\"How are you?\", \"I'm fine, thank you!\"], padding=True, truncation=True, max_length=5, return_tensors=\"pt\", ) print(encoded_input) ``` ``` {'input_ids': tensor([[ 101, 1731, 1132, 1128, 102], [ 101, 1045, 1005, 1049, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]])} ``` ### Adding special tokens Special tokens (or at least the concept of them) is particularly important to BERT and derived models. These tokens are added to better represent the sentence boundaries, such as the beginning of a sentence (`[CLS]`) or separator between sentences (`[SEP]`). Let‚Äôs look at a simple example: ``` encoded_input = tokenizer(\"How are you?\") print(encoded_input[\"input_ids\"]) tokenizer.decode(encoded_input[\"input_ids\"]) ``` ``` [101, 1731, 1132, 1128, 136, 102] '[CLS] How are you? [SEP]' ``` These special tokens are automatically added by the tokenizer. Not all models need special tokens; they are primarily used when a model was pretrained with them, in which case the tokenizer will add them since the model expects them. ### Why is all of this necessary? Here‚Äôs a concrete example. Consider these encoded sequences: ``` sequences = [ \"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\", ] ``` Once tokenized, we have: ``` encoded_sequences = [ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, ], [101, 1045, 5223, 2023, 2061, 2172, 999, 102], ] ``` This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This ‚Äúarray‚Äù is already of rectangular shape, so converting it to a",
    "metadata": {
      "title": "Models",
      "url": "https://huggingface.co/learn/llm-course/chapter2/3",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/3",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is all of this necessary? Here‚Äôs a concrete example. Consider these encoded sequences: ``` sequences = [ \"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\", ] ``` Once tokenized, we have: ``` encoded_sequences = [ [ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, ], [101, 1045, 5223, 2023, 2061, 2172, 999, 102], ] ``` This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This ‚Äúarray‚Äù is already of rectangular shape, so converting it to a tensor is easy: ``` import torch model_inputs = torch.tensor(encoded_sequences) ``` ### Using the tensors as inputs to the model Making use of the tensors with the model is extremely simple ‚Äî we just call the model with the inputs: ``` output = model(model_inputs) ``` While the model accepts a lot of different arguments, only the input IDs are necessary. We‚Äôll explain what the other arguments do and when they are required later, but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.",
    "metadata": {
      "title": "Models",
      "url": "https://huggingface.co/learn/llm-course/chapter2/3",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/3",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Tokenizers Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we‚Äôll explore exactly what happens in the tokenization pipeline. In NLP tasks, the data that is generally processed is raw text. Here‚Äôs an example of such text: ``` Jim Henson was a puppeteer ``` However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That‚Äôs what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation ‚Äî that is, the one that makes the most sense to the model ‚Äî and, if possible, the smallest representation. Let‚Äôs take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization. ## Word-based The first type of tokenizer that comes to mind is *word-based*. It‚Äôs generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them: ![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg) ![An example of word-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg) There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python‚Äôs `split()` function: ``` tokenized_text = \"Jim Henson was a puppeteer\".split() print(tokenized_text) ``` ``` ['Jim', 'Henson', 'was', 'a', 'puppeteer'] ``` There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large ‚Äúvocabularies,‚Äù where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word. If we want to completely cover a language with a word-based tokenizer, we‚Äôll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we‚Äôd need to keep track of that many IDs. Furthermore, words like ‚Äúdog‚Äù are represented differently from words like ‚Äúdogs‚Äù, and the model will initially have no way of knowing that ‚Äúdog‚Äù and ‚Äúdogs‚Äù are similar: it will identify the two words as unrelated. The same applies to other similar words, like ‚Äúrun‚Äù and ‚Äúrunning‚Äù, which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the ‚Äúunknown‚Äù token, often represented as ‚Äù[UNK]‚Äù or",
    "metadata": {
      "title": "Tokenizers",
      "url": "https://huggingface.co/learn/llm-course/chapter2/4",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/4",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "map from each word to an input ID we‚Äôd need to keep track of that many IDs. Furthermore, words like ‚Äúdog‚Äù are represented differently from words like ‚Äúdogs‚Äù, and the model will initially have no way of knowing that ‚Äúdog‚Äù and ‚Äúdogs‚Äù are similar: it will identify the two words as unrelated. The same applies to other similar words, like ‚Äúrun‚Äù and ‚Äúrunning‚Äù, which the model will not see as being similar initially. Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the ‚Äúunknown‚Äù token, often represented as ‚Äù[UNK]‚Äù or ‚Äù<unk>‚Äù. It‚Äôs generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn‚Äôt able to retrieve a sensible representation of a word and you‚Äôre losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token. One way to reduce the amount of unknown tokens is to go one level deeper, using a *character-based* tokenizer. ## Character-based Character-based tokenizers split the text into characters, rather than words. This has two primary benefits: - The vocabulary is much smaller. - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters. But here too some questions arise concerning spaces and punctuation: ![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg) ![An example of character-based tokenization.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg) This approach isn‚Äôt perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it‚Äôs less meaningful: each character doesn‚Äôt mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language. Another thing to consider is that we‚Äôll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters. To get the best of both worlds, we can use a third technique that combines the two approaches: *subword tokenization*. ## Subword tokenization Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù. Here is an example showing how a subword tokenization algorithm would tokenize the sequence ‚ÄúLet‚Äôs do tokenization!‚Äú: ![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg) ![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg) These subwords end up providing a lot of semantic meaning: for instance, in the example above ‚Äútokenization‚Äù was split into ‚Äútoken‚Äù and ‚Äúization‚Äù,",
    "metadata": {
      "title": "Tokenizers",
      "url": "https://huggingface.co/learn/llm-course/chapter2/4",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/4",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "be decomposed into meaningful subwords. For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù. Here is an example showing how a subword tokenization algorithm would tokenize the sequence ‚ÄúLet‚Äôs do tokenization!‚Äú: ![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg) ![A subword tokenization algorithm.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg) These subwords end up providing a lot of semantic meaning: for instance, in the example above ‚Äútokenization‚Äù was split into ‚Äútoken‚Äù and ‚Äúization‚Äù, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens. This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords. ### And more! Unsurprisingly, there are many more techniques out there. To name a few: - Byte-level BPE, as used in GPT-2 - WordPiece, as used in BERT - SentencePiece or Unigram, as used in several multilingual models You should now have sufficient knowledge of how tokenizers work to get started with the API. ## Loading and saving Loading and saving tokenizers is as simple as it is with models. Actually, it‚Äôs based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the *architecture* of the model) as well as its vocabulary (a bit like the *weights* of the model). Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class: ``` from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\") ``` Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") ``` We can now use the tokenizer as shown in the previous section: ``` tokenizer(\"Using a Transformer network is simple\") ``` ``` {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} ``` Saving a tokenizer is identical to saving a model: ``` tokenizer.save_pretrained(\"directory_on_my_computer\") ``` We‚Äôll talk more about `token_type_ids` in [Chapter 3](/course/chapter3), and we‚Äôll explain the `attention_mask` key a little later. First, let‚Äôs see how the `input_ids` are generated. To do this, we‚Äôll need to look at the intermediate methods of the tokenizer. ## Encoding Translating text to numbers is known as *encoding*. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs. As we‚Äôve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called *tokens*. There",
    "metadata": {
      "title": "Tokenizers",
      "url": "https://huggingface.co/learn/llm-course/chapter2/4",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/4",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "a tokenizer is identical to saving a model: ``` tokenizer.save_pretrained(\"directory_on_my_computer\") ``` We‚Äôll talk more about `token_type_ids` in [Chapter 3](/course/chapter3), and we‚Äôll explain the `attention_mask` key a little later. First, let‚Äôs see how the `input_ids` are generated. To do this, we‚Äôll need to look at the intermediate methods of the tokenizer. ## Encoding Translating text to numbers is known as *encoding*. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs. As we‚Äôve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called *tokens*. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained. The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a *vocabulary*, which is the part we download when we instantiate it with the `from_pretrained()` method. Again, we need to use the same vocabulary used when the model was pretrained. To get a better understanding of the two steps, we‚Äôll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs (as shown in the section 2). ### Tokenization The tokenization process is done by the `tokenize()` method of the tokenizer: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") sequence = \"Using a Transformer network is simple\" tokens = tokenizer.tokenize(sequence) print(tokens) ``` The output of this method is a list of strings, or tokens: ``` ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple'] ``` This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That‚Äôs the case here with `transformer`, which is split into two tokens: `transform` and `##er`. ### From tokens to input IDs The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method: ``` ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) ``` ``` [7993, 170, 11303, 1200, 2443, 1110, 3014] ``` These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier in this chapter. > ‚úèÔ∏èTry it out!Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2 (‚ÄúI‚Äôve been waiting for a HuggingFace course my whole life.‚Äù and ‚ÄúI hate this so much!‚Äù). Check that you get the same input IDs we got earlier! ## Decoding *Decoding* is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows: ``` decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) ``` ``` 'Using a Transformer network is",
    "metadata": {
      "title": "Tokenizers",
      "url": "https://huggingface.co/learn/llm-course/chapter2/4",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/4",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "earlier in this chapter. > ‚úèÔ∏èTry it out!Replicate the two last steps (tokenization and conversion to input IDs) on the input sentences we used in section 2 (‚ÄúI‚Äôve been waiting for a HuggingFace course my whole life.‚Äù and ‚ÄúI hate this so much!‚Äù). Check that you get the same input IDs we got earlier! ## Decoding *Decoding* is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows: ``` decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) ``` ``` 'Using a Transformer network is simple' ``` Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization). By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string. However, we‚Äôve just scraped the tip of the iceberg. In the following section, we‚Äôll take our approach to its limits and take a look at how to overcome them.",
    "metadata": {
      "title": "Tokenizers",
      "url": "https://huggingface.co/learn/llm-course/chapter2/4",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/4",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Handling multiple sequences In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already: - How do we handle multiple sequences? - How do we handle multiple sequences *of different lengths*? - Are vocabulary indices the only inputs that allow a model to work well? - Is there such a thing as too long a sequence? Let‚Äôs see what kinds of problems these questions pose, and how we can solve them using the ü§ó Transformers API. ## Models expect a batch of inputs In the previous exercise you saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model: ``` import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence = \"I've been waiting for a HuggingFace course my whole life.\" tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.tensor(ids) # This line will fail. model(input_ids) ``` ``` IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1) ``` Oh no! Why did this fail? We followed the steps from the pipeline in section 2. The problem is that we sent a single sequence to the model, whereas ü§ó Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a `sequence`. But if you look closely, you‚Äôll see that the tokenizer didn‚Äôt just convert the list of input IDs into a tensor, it added a dimension on top of it: ``` tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\") print(tokenized_inputs[\"input_ids\"]) ``` ``` tensor([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]]) ``` Let‚Äôs try again and add a new dimension: ``` import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence = \"I've been waiting for a HuggingFace course my whole life.\" tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) input_ids = torch.tensor([ids]) print(\"Input IDs:\", input_ids) output = model(input_ids) print(\"Logits:\", output.logits) ``` We print the input IDs as well as the resulting logits ‚Äî here‚Äôs the output: ``` Input IDs: [[ 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]] Logits: [[-2.7276, 2.8789]] ``` *Batching* is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence: ``` batched_ids = [ids, ids] ``` This is a batch of two identical sequences! > ‚úèÔ∏èTry it out!Convert thisbatched_idslist into a tensor and pass it through your model. Check that you obtain the same logits as before (but twice)! Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There‚Äôs a second issue, though. When you‚Äôre trying to",
    "metadata": {
      "title": "Handling multiple sequences",
      "url": "https://huggingface.co/learn/llm-course/chapter2/5",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/5",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence: ``` batched_ids = [ids, ids] ``` This is a batch of two identical sequences! > ‚úèÔ∏èTry it out!Convert thisbatched_idslist into a tensor and pass it through your model. Check that you obtain the same logits as before (but twice)! Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. There‚Äôs a second issue, though. When you‚Äôre trying to batch together two (or more) sentences, they might be of different lengths. If you‚Äôve ever worked with tensors before, you know that they need to be of rectangular shape, so you won‚Äôt be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually *pad* the inputs. ## Padding the inputs The following list of lists cannot be converted to a tensor: ``` batched_ids = [ [200, 200, 200], [200, 200] ] ``` In order to work around this, we‚Äôll use *padding* to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the *padding token* to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this: ``` padding_id = 100 batched_ids = [ [200, 200, 200], [200, 200, padding_id], ] ``` The padding token ID can be found in `tokenizer.pad_token_id`. Let‚Äôs use it and send our two sentences through the model individually and batched together: ``` model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence1_ids = [[200, 200, 200]] sequence2_ids = [[200, 200]] batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] print(model(torch.tensor(sequence1_ids)).logits) print(model(torch.tensor(sequence2_ids)).logits) print(model(torch.tensor(batched_ids)).logits) ``` ``` tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>) tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>) tensor([[ 1.5694, -1.3895], [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>) ``` There‚Äôs something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we‚Äôve got completely different values! This is because the key feature of Transformer models is attention layers that *contextualize* each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask. ## Attention masks *Attention masks* are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by",
    "metadata": {
      "title": "Handling multiple sequences",
      "url": "https://huggingface.co/learn/llm-course/chapter2/5",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/5",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask. ## Attention masks *Attention masks* are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model). Let‚Äôs complete the previous example with an attention mask: ``` batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] attention_mask = [ [1, 1, 1], [1, 1, 0], ] outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)) print(outputs.logits) ``` ``` tensor([[ 1.5694, -1.3895], [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>) ``` Now we get the same logits for the second sentence in the batch. Notice how the last value of the second sequence is a padding ID, which is a 0 value in the attention mask. > ‚úèÔ∏èTry it out!Apply the tokenization manually on the two sentences used in section 2 (‚ÄúI‚Äôve been waiting for a HuggingFace course my whole life.‚Äù and ‚ÄúI hate this so much!‚Äù). Pass them through the model and check that you get the same logits as in section 2. Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model! ## Longer sequences With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem: - Use a model with a longer supported sequence length. - Truncate your sequences. Models have different supported sequence lengths, and some specialize in handling very long sequences. [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) is one example, and another is [LED](https://huggingface.co/docs/transformers/model_doc/led). If you‚Äôre working on a task that requires very long sequences, we recommend you take a look at those models. Otherwise, we recommend you truncate your sequences by specifying the `max_sequence_length` parameter: ``` sequence = sequence[:max_sequence_length] ```",
    "metadata": {
      "title": "Handling multiple sequences",
      "url": "https://huggingface.co/learn/llm-course/chapter2/5",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/5",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Putting it all together In the last few sections, we‚Äôve been trying our best to do most of the work by hand. We‚Äôve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks. However, as we saw in section 2, the ü§ó Transformers API can handle all of this for us with a high-level function that we‚Äôll dive into here. When you call your `tokenizer` directly on the sentence, you get back inputs that are ready to pass through your model: ``` from transformers import AutoTokenizer checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) ``` Here, the `model_inputs` variable contains everything that‚Äôs necessary for a model to operate well. For DistilBERT, that includes the input IDs as well as the attention mask. Other models that accept additional inputs will also have those output by the `tokenizer` object. As we‚Äôll see in some examples below, this method is very powerful. First, it can tokenize a single sequence: ``` sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) ``` It also handles multiple sequences at a time, with no change in the API: ``` sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] model_inputs = tokenizer(sequences) ``` It can pad according to several objectives: ``` # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer(sequences, padding=\"longest\") # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, padding=\"max_length\") # Will pad the sequences up to the specified max length model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8) ``` It can also truncate sequences: ``` sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, truncation=True) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer(sequences, max_length=8, truncation=True) ``` The `tokenizer` object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample we are prompting the tokenizer to return tensors from the different frameworks ‚Äî `\"pt\"` returns PyTorch tensors and `\"np\"` returns NumPy arrays: ``` sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Returns PyTorch tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # Returns NumPy arrays model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\") ``` ## Special tokens If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier: ``` sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) print(model_inputs[\"input_ids\"]) tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) ``` ``` [101, 1045, 1005, 2310, 2042, 3403, 2005,",
    "metadata": {
      "title": "Putting it all together",
      "url": "https://huggingface.co/learn/llm-course/chapter2/6",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/6",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "NumPy arrays: ``` sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Returns PyTorch tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # Returns NumPy arrays model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\") ``` ## Special tokens If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier: ``` sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) print(model_inputs[\"input_ids\"]) tokens = tokenizer.tokenize(sequence) ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) ``` ``` [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102] [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012] ``` One token ID was added at the beginning, and one at the end. Let‚Äôs decode the two sequences of IDs above to see what this is about: ``` print(tokenizer.decode(model_inputs[\"input_ids\"])) print(tokenizer.decode(ids)) ``` ``` \"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\" \"i've been waiting for a huggingface course my whole life.\" ``` The tokenizer added the special word `[CLS]` at the beginning and the special word `[SEP]` at the end. This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. Note that some models don‚Äôt add special words, or add different ones; models may also add these special words only at the beginning, or only at the end. In any case, the tokenizer knows which ones are expected and will deal with this for you. ## Wrapping up: From tokenizer to model Now that we‚Äôve seen all the individual steps the `tokenizer` object uses when applied on texts, let‚Äôs see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API: ``` import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\") output = model(**tokens) ```",
    "metadata": {
      "title": "Putting it all together",
      "url": "https://huggingface.co/learn/llm-course/chapter2/6",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/6",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Basic usage completed!\n\n   \nGreat job following the course up to here! To recap, in this chapter you:\n \n- Learned the basic building blocks of a Transformer model.\n- Learned what makes up a tokenization pipeline.\n- Saw how to use a Transformer model in practice.\n- Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.\n- Set up a tokenizer and a model together to get from text to predictions.\n- Learned the limitations of input IDs, and learned about attention masks.\n- Played around with versatile and configurable tokenizer methods.\n \nFrom now on, you should be able to freely navigate the ü§ó Transformers docs: the vocabulary will sound familiar, and you‚Äôve already seen the methods that you‚Äôll use the majority of the time.",
    "metadata": {
      "title": "Basic usage completed!",
      "url": "https://huggingface.co/learn/llm-course/chapter2/7",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Optimized Inference Deployment In this section, we‚Äôll explore advanced frameworks for optimizing LLM deployments: Text Generation Inference (TGI), vLLM, and llama.cpp. These applications are primarily used in production environments to serve LLMs to users. This section focuses on how to deploy these frameworks in production rather than how to use them for inference on a single machine. We‚Äôll cover how these tools maximize inference efficiency and simplify production deployments of Large Language Models. ## Framework Selection Guide TGI, vLLM, and llama.cpp serve similar purposes but have distinct characteristics that make them better suited for different use cases. Let‚Äôs look at the key differences between them, focusing on performance and integration. ### Memory Management and Performance **TGI** is designed to be stable and predictable in production, using fixed sequence lengths to keep memory usage consistent. TGI manages memory using Flash Attention 2 and continuous batching techniques. This means it can process attention calculations very efficiently and keep the GPU busy by constantly feeding it work. The system can move parts of the model between CPU and GPU when needed, which helps handle larger models. ![Flash Attention](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tgi/flash-attn.png) > Flash Attention is a technique that optimizes the attention mechanism in transformer models by addressing memory bandwidth bottlenecks. As discussed earlier inChapter 1.8, the attention mechanism has quadratic complexity and memory usage, making it inefficient for long sequences.The key innovation is in how it manages memory transfers between High Bandwidth Memory (HBM) and faster SRAM cache. Traditional attention repeatedly transfers data between HBM and SRAM, creating bottlenecks by leaving the GPU idle. Flash Attention loads data once into SRAM and performs all calculations there, minimizing expensive memory transfers.While the benefits are most significant during training, Flash Attention‚Äôs reduced VRAM usage and improved efficiency make it valuable for inference as well, enabling faster and more scalable LLM serving. **vLLM** takes a different approach by using PagedAttention. Just like how a computer manages its memory in pages, vLLM splits the model‚Äôs memory into smaller blocks. This clever system means it can handle different-sized requests more flexibly and doesn‚Äôt waste memory space. It‚Äôs particularly good at sharing memory between different requests and reduces memory fragmentation, which makes the whole system more efficient. > PagedAttention is a technique that addresses another critical bottleneck in LLM inference: KV cache memory management. As discussed inChapter 1.8, during text generation, the model stores attention keys and values (KV cache) for each generated token to reduce redundant computations. The KV cache can become enormous, especially with long sequences or multiple concurrent requests.vLLM‚Äôs key innovation lies in how it manages this cache:Memory Paging: Instead of treating the KV cache as one large block, it‚Äôs divided into fixed-size ‚Äúpages‚Äù (similar to virtual memory in operating systems).Non-contiguous Storage: Pages don‚Äôt need to be stored contiguously in GPU memory, allowing for more flexible memory allocation.Page Table Management: A page table tracks which pages belong to which sequence, enabling efficient lookup and access.Memory Sharing: For operations like parallel sampling, pages storing the KV cache for the",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "each generated token to reduce redundant computations. The KV cache can become enormous, especially with long sequences or multiple concurrent requests.vLLM‚Äôs key innovation lies in how it manages this cache:Memory Paging: Instead of treating the KV cache as one large block, it‚Äôs divided into fixed-size ‚Äúpages‚Äù (similar to virtual memory in operating systems).Non-contiguous Storage: Pages don‚Äôt need to be stored contiguously in GPU memory, allowing for more flexible memory allocation.Page Table Management: A page table tracks which pages belong to which sequence, enabling efficient lookup and access.Memory Sharing: For operations like parallel sampling, pages storing the KV cache for the prompt can be shared across multiple sequences.The PagedAttention approach can lead to up to 24x higher throughput compared to traditional methods, making it a game-changer for production LLM deployments. If you want to go really deep into how PagedAttention works, you can read thethe guide from the vLLM documentation. **llama.cpp** is a highly optimized C/C++ implementation originally designed for running LLaMA models on consumer hardware. It focuses on CPU efficiency with optional GPU acceleration and is ideal for resource-constrained environments. llama.cpp uses quantization techniques to reduce model size and memory requirements while maintaining good performance. It implements optimized kernels for various CPU architectures and supports basic KV cache management for efficient token generation. > Quantization in llama.cpp reduces the precision of model weights from 32-bit or 16-bit floating point to lower precision formats like 8-bit integers (INT8), 4-bit, or even lower. This significantly reduces memory usage and improves inference speed with minimal quality loss.Key quantization features in llama.cpp include:Multiple Quantization Levels: Supports 8-bit, 4-bit, 3-bit, and even 2-bit quantizationGGML/GGUF Format: Uses custom tensor formats optimized for quantized inferenceMixed Precision: Can apply different quantization levels to different parts of the modelHardware-Specific Optimizations: Includes optimized code paths for various CPU architectures (AVX2, AVX-512, NEON)This approach enables running billion-parameter models on consumer hardware with limited memory, making it perfect for local deployments and edge devices. ### Deployment and Integration Let‚Äôs move on to the deployment and integration differences between the frameworks. **TGI** excels in enterprise-level deployment with its production-ready features. It comes with built-in Kubernetes support and includes everything you need for running in production, like monitoring through Prometheus and Grafana, automatic scaling, and comprehensive safety features. The system also includes enterprise-grade logging and various protective measures like content filtering and rate limiting to keep your deployment secure and stable. **vLLM** takes a more flexible, developer-friendly approach to deployment. It‚Äôs built with Python at its core and can easily replace OpenAI‚Äôs API in your existing applications. The framework focuses on delivering raw performance and can be customized to fit your specific needs. It works particularly well with Ray for managing clusters, making it a great choice when you need high performance and adaptability. **llama.cpp** prioritizes simplicity and portability. Its server implementation is lightweight and can run on a wide range of hardware, from powerful servers to consumer laptops and even some high-end mobile devices. With minimal dependencies and a simple C/C++ core, it‚Äôs",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "more flexible, developer-friendly approach to deployment. It‚Äôs built with Python at its core and can easily replace OpenAI‚Äôs API in your existing applications. The framework focuses on delivering raw performance and can be customized to fit your specific needs. It works particularly well with Ray for managing clusters, making it a great choice when you need high performance and adaptability. **llama.cpp** prioritizes simplicity and portability. Its server implementation is lightweight and can run on a wide range of hardware, from powerful servers to consumer laptops and even some high-end mobile devices. With minimal dependencies and a simple C/C++ core, it‚Äôs easy to deploy in environments where installing Python frameworks would be challenging. The server provides an OpenAI-compatible API while maintaining a much smaller resource footprint than other solutions. ## Getting Started Let‚Äôs explore how to use these frameworks for deploying LLMs, starting with installation and basic setup. ### Installation and Basic Setup <hfoption value=\"tgi\" label=\"TGI\"> TGI is easy to install and use, with deep integration into the Hugging Face ecosystem. First, launch the TGI server using Docker: ``` docker run --gpus all \\ --shm-size 1g \\ -p 8080:80 \\ -v ~/.cache/huggingface:/data \\ ghcr.io/huggingface/text-generation-inference:latest \\ --model-id HuggingFaceTB/SmolLM2-360M-Instruct ``` Then interact with it using Hugging Face‚Äôs InferenceClient: ``` from huggingface_hub import InferenceClient # Initialize client pointing to TGI endpoint client = InferenceClient( model=\"http://localhost:8080\", # URL to the TGI server ) # Text generation response = client.text_generation( \"Tell me a story\", max_new_tokens=100, temperature=0.7, top_p=0.95, details=True, stop_sequences=[], ) print(response.generated_text) # For chat format response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` Alternatively, you can use the OpenAI client: ``` from openai import OpenAI # Initialize client pointing to TGI endpoint client = OpenAI( base_url=\"http://localhost:8080/v1\", # Make sure to include /v1 api_key=\"not-needed\", # TGI doesn't require an API key by default ) # Chat completion response = client.chat.completions.create( model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> llama.cpp is easy to install and use, requiring minimal dependencies and supporting both CPU and GPU inference. First, install and build llama.cpp: ``` # Clone the repository git clone https://github.com/ggerganov/llama.cpp cd llama.cpp # Build the project make # Download the SmolLM2-1.7B-Instruct-GGUF model curl -L -O https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct.Q4_K_M.gguf ``` Then, launch the server (with OpenAI API compatibility): ``` # Start the server ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --host 0.0.0.0 \\ --port 8080 \\ -c 4096 \\ --n-gpu-layers 0 # Set to a higher number to use GPU ``` Interact with the server using Hugging Face‚Äôs InferenceClient: ``` from huggingface_hub import InferenceClient # Initialize client pointing to llama.cpp server client = InferenceClient( model=\"http://localhost:8080/v1\", # URL to the llama.cpp server token=\"sk-no-key-required\", # llama.cpp server requires this placeholder ) # Text generation response = client.text_generation( \"Tell me a story\", max_new_tokens=100, temperature=0.7, top_p=0.95, details=True, ) print(response.generated_text) # For chat format response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\":",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(with OpenAI API compatibility): ``` # Start the server ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --host 0.0.0.0 \\ --port 8080 \\ -c 4096 \\ --n-gpu-layers 0 # Set to a higher number to use GPU ``` Interact with the server using Hugging Face‚Äôs InferenceClient: ``` from huggingface_hub import InferenceClient # Initialize client pointing to llama.cpp server client = InferenceClient( model=\"http://localhost:8080/v1\", # URL to the llama.cpp server token=\"sk-no-key-required\", # llama.cpp server requires this placeholder ) # Text generation response = client.text_generation( \"Tell me a story\", max_new_tokens=100, temperature=0.7, top_p=0.95, details=True, ) print(response.generated_text) # For chat format response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` Alternatively, you can use the OpenAI client: ``` from openai import OpenAI # Initialize client pointing to llama.cpp server client = OpenAI( base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\", # llama.cpp server requires this placeholder ) # Chat completion response = client.chat.completions.create( model=\"smollm2-1.7b-instruct\", # Model identifier can be anything as server only loads one model messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> vLLM is easy to install and use, with both OpenAI API compatibility and a native Python interface. First, launch the vLLM OpenAI-compatible server: ``` python -m vllm.entrypoints.openai.api_server \\ --model HuggingFaceTB/SmolLM2-360M-Instruct \\ --host 0.0.0.0 \\ --port 8000 ``` Then interact with it using Hugging Face‚Äôs InferenceClient: ``` from huggingface_hub import InferenceClient # Initialize client pointing to vLLM endpoint client = InferenceClient( model=\"http://localhost:8000/v1\", # URL to the vLLM server ) # Text generation response = client.text_generation( \"Tell me a story\", max_new_tokens=100, temperature=0.7, top_p=0.95, details=True, ) print(response.generated_text) # For chat format response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` Alternatively, you can use the OpenAI client: ``` from openai import OpenAI # Initialize client pointing to vLLM endpoint client = OpenAI( base_url=\"http://localhost:8000/v1\", api_key=\"not-needed\", # vLLM doesn't require an API key by default ) # Chat completion response = client.chat.completions.create( model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Tell me a story\"}, ], max_tokens=100, temperature=0.7, top_p=0.95, ) print(response.choices[0].message.content) ``` </hfoption> ### Basic Text Generation Let‚Äôs look at examples of text generation with the frameworks: <hfoption value=\"tgi\" label=\"TGI\"> First, deploy TGI with advanced parameters: ``` docker run --gpus all \\ --shm-size 1g \\ -p 8080:80 \\ -v ~/.cache/huggingface:/data \\ ghcr.io/huggingface/text-generation-inference:latest \\ --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\ --max-total-tokens 4096 \\ --max-input-length 3072 \\ --max-batch-total-tokens 8192 \\ --waiting-served-ratio 1.2 ``` Use the InferenceClient for flexible text generation: ``` from huggingface_hub import InferenceClient client = InferenceClient(model=\"http://localhost:8080\") # Advanced parameters example response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, max_tokens=200, top_p=0.95, ) print(response.choices[0].message.content) # Raw text generation response = client.text_generation( \"Write a creative story about space exploration\", max_new_tokens=200, temperature=0.8, top_p=0.95, repetition_penalty=1.1, do_sample=True, details=True, ) print(response.generated_text) ``` Or use the",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "run --gpus all \\ --shm-size 1g \\ -p 8080:80 \\ -v ~/.cache/huggingface:/data \\ ghcr.io/huggingface/text-generation-inference:latest \\ --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\ --max-total-tokens 4096 \\ --max-input-length 3072 \\ --max-batch-total-tokens 8192 \\ --waiting-served-ratio 1.2 ``` Use the InferenceClient for flexible text generation: ``` from huggingface_hub import InferenceClient client = InferenceClient(model=\"http://localhost:8080\") # Advanced parameters example response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, max_tokens=200, top_p=0.95, ) print(response.choices[0].message.content) # Raw text generation response = client.text_generation( \"Write a creative story about space exploration\", max_new_tokens=200, temperature=0.8, top_p=0.95, repetition_penalty=1.1, do_sample=True, details=True, ) print(response.generated_text) ``` Or use the OpenAI client: ``` from openai import OpenAI client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\") # Advanced parameters example response = client.chat.completions.create( model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, # Higher for more creativity ) print(response.choices[0].message.content) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> For llama.cpp, you can set advanced parameters when launching the server: ``` ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --host 0.0.0.0 \\ --port 8080 \\ -c 4096 \\ # Context size --threads 8 \\ # CPU threads to use --batch-size 512 \\ # Batch size for prompt evaluation --n-gpu-layers 0 # GPU layers (0 = CPU only) ``` Use the InferenceClient: ``` from huggingface_hub import InferenceClient client = InferenceClient(model=\"http://localhost:8080/v1\", token=\"sk-no-key-required\") # Advanced parameters example response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, max_tokens=200, top_p=0.95, ) print(response.choices[0].message.content) # For direct text generation response = client.text_generation( \"Write a creative story about space exploration\", max_new_tokens=200, temperature=0.8, top_p=0.95, repetition_penalty=1.1, details=True, ) print(response.generated_text) ``` Or use the OpenAI client for generation with control over the sampling parameters: ``` from openai import OpenAI client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\") # Advanced parameters example response = client.chat.completions.create( model=\"smollm2-1.7b-instruct\", messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, # Higher for more creativity top_p=0.95, # Nucleus sampling probability frequency_penalty=0.5, # Reduce repetition of frequent tokens presence_penalty=0.5, # Reduce repetition by penalizing tokens already present max_tokens=200, # Maximum generation length ) print(response.choices[0].message.content) ``` You can also use llama.cpp‚Äôs native library for even more control: ``` # Using llama-cpp-python package for direct model access from llama_cpp import Llama # Load the model llm = Llama( model_path=\"smollm2-1.7b-instruct.Q4_K_M.gguf\", n_ctx=4096, # Context window size n_threads=8, # CPU threads n_gpu_layers=0, # GPU layers (0 = CPU only) ) # Format prompt according to the model's expected format prompt = \"\"\"<|im_start|>system You are a creative storyteller. <|im_end|> <|im_start|>user Write a creative story <|im_end|> <|im_start|>assistant \"\"\" # Generate response with precise parameter control output = llm( prompt, max_tokens=200, temperature=0.8, top_p=0.95, frequency_penalty=0.5, presence_penalty=0.5, stop=[\"<|im_end|>\"], ) print(output[\"choices\"][0][\"text\"]) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> For advanced usage with vLLM, you can use the InferenceClient: ``` from huggingface_hub import InferenceClient client = InferenceClient(model=\"http://localhost:8000/v1\") # Advanced parameters example response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, max_tokens=200, top_p=0.95, ) print(response.choices[0].message.content) # For",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": ") # Format prompt according to the model's expected format prompt = \"\"\"<|im_start|>system You are a creative storyteller. <|im_end|> <|im_start|>user Write a creative story <|im_end|> <|im_start|>assistant \"\"\" # Generate response with precise parameter control output = llm( prompt, max_tokens=200, temperature=0.8, top_p=0.95, frequency_penalty=0.5, presence_penalty=0.5, stop=[\"<|im_end|>\"], ) print(output[\"choices\"][0][\"text\"]) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> For advanced usage with vLLM, you can use the InferenceClient: ``` from huggingface_hub import InferenceClient client = InferenceClient(model=\"http://localhost:8000/v1\") # Advanced parameters example response = client.chat_completion( messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, max_tokens=200, top_p=0.95, ) print(response.choices[0].message.content) # For direct text generation response = client.text_generation( \"Write a creative story about space exploration\", max_new_tokens=200, temperature=0.8, top_p=0.95, details=True, ) print(response.generated_text) ``` You can also use the OpenAI client: ``` from openai import OpenAI client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"not-needed\") # Advanced parameters example response = client.chat.completions.create( model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", messages=[ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ], temperature=0.8, top_p=0.95, max_tokens=200, ) print(response.choices[0].message.content) ``` vLLM also provides a native Python interface with fine-grained control: ``` from vllm import LLM, SamplingParams # Initialize the model with advanced parameters llm = LLM( model=\"HuggingFaceTB/SmolLM2-360M-Instruct\", gpu_memory_utilization=0.85, max_num_batched_tokens=8192, max_num_seqs=256, block_size=16, ) # Configure sampling parameters sampling_params = SamplingParams( temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95% probability mass max_tokens=100, # Maximum length presence_penalty=1.1, # Reduce repetition frequency_penalty=1.1, # Reduce repetition stop=[\"\\n\\n\", \"###\"], # Stop sequences ) # Generate text prompt = \"Write a creative story\" outputs = llm.generate(prompt, sampling_params) print(outputs[0].outputs[0].text) # For chat-style interactions chat_prompt = [ {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"}, {\"role\": \"user\", \"content\": \"Write a creative story\"}, ] formatted_prompt = llm.get_chat_template()(chat_prompt) # Uses model's chat template outputs = llm.generate(formatted_prompt, sampling_params) print(outputs[0].outputs[0].text) ``` </hfoption> ## Advanced Generation Control ### Token Selection and Sampling The process of generating text involves selecting the next token at each step. This selection process can be controlled through various parameters: 1. **Raw Logits**: The initial output probabilities for each token 2. **Temperature**: Controls randomness in selection (higher = more creative) 3. **Top-p (Nucleus) Sampling**: Filters to top tokens making up X% of probability mass 4. **Top-k Filtering**: Limits selection to k most likely tokens Here‚Äôs how to configure these parameters: <hfoption value=\"tgi\" label=\"TGI\"> ``` client.generate( \"Write a creative story\", temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95% probability mass top_k=50, # Consider top 50 tokens max_new_tokens=100, # Maximum length repetition_penalty=1.1, # Reduce repetition ) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> ``` # Via OpenAI API compatibility response = client.completions.create( model=\"smollm2-1.7b-instruct\", # Model name (can be any string for llama.cpp server) prompt=\"Write a creative story\", temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95% probability mass frequency_penalty=1.1, # Reduce repetition presence_penalty=0.1, # Reduce repetition max_tokens=100, # Maximum length ) # Via llama-cpp-python direct access output = llm( \"Write a creative story\", temperature=0.8, top_p=0.95, top_k=50, max_tokens=100, repeat_penalty=1.1, ) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> ``` params = SamplingParams( temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95%",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Maximum length repetition_penalty=1.1, # Reduce repetition ) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> ``` # Via OpenAI API compatibility response = client.completions.create( model=\"smollm2-1.7b-instruct\", # Model name (can be any string for llama.cpp server) prompt=\"Write a creative story\", temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95% probability mass frequency_penalty=1.1, # Reduce repetition presence_penalty=0.1, # Reduce repetition max_tokens=100, # Maximum length ) # Via llama-cpp-python direct access output = llm( \"Write a creative story\", temperature=0.8, top_p=0.95, top_k=50, max_tokens=100, repeat_penalty=1.1, ) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> ``` params = SamplingParams( temperature=0.8, # Higher for more creativity top_p=0.95, # Consider top 95% probability mass top_k=50, # Consider top 50 tokens max_tokens=100, # Maximum length presence_penalty=0.1, # Reduce repetition ) llm.generate(\"Write a creative story\", sampling_params=params) ``` </hfoption> ### Controlling Repetition Both frameworks provide ways to prevent repetitive text generation: <hfoption value=\"tgi\" label=\"TGI\"> ``` client.generate( \"Write a varied text\", repetition_penalty=1.1, # Penalize repeated tokens no_repeat_ngram_size=3, # Prevent 3-gram repetition ) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> ``` # Via OpenAI API response = client.completions.create( model=\"smollm2-1.7b-instruct\", prompt=\"Write a varied text\", frequency_penalty=1.1, # Penalize frequent tokens presence_penalty=0.8, # Penalize tokens already present ) # Via direct library output = llm( \"Write a varied text\", repeat_penalty=1.1, # Penalize repeated tokens frequency_penalty=0.5, # Additional frequency penalty presence_penalty=0.5, # Additional presence penalty ) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> ``` params = SamplingParams( presence_penalty=0.1, # Penalize token presence frequency_penalty=0.1, # Penalize token frequency ) ``` </hfoption> ### Length Control and Stop Sequences You can control generation length and specify when to stop: <hfoption value=\"tgi\" label=\"TGI\"> ``` client.generate( \"Generate a short paragraph\", max_new_tokens=100, min_new_tokens=10, stop_sequences=[\"\\n\\n\", \"###\"], ) ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> ``` # Via OpenAI API response = client.completions.create( model=\"smollm2-1.7b-instruct\", prompt=\"Generate a short paragraph\", max_tokens=100, stop=[\"\\n\\n\", \"###\"], ) # Via direct library output = llm(\"Generate a short paragraph\", max_tokens=100, stop=[\"\\n\\n\", \"###\"]) ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> ``` params = SamplingParams( max_tokens=100, min_tokens=10, stop=[\"###\", \"\\n\\n\"], ignore_eos=False, skip_special_tokens=True, ) ``` </hfoption> ## Memory Management Both frameworks implement advanced memory management techniques for efficient inference. <hfoption value=\"tgi\" label=\"TGI\"> TGI uses Flash Attention 2 and continuous batching: ``` # Docker deployment with memory optimization docker run --gpus all -p 8080:80 \\ --shm-size 1g \\ ghcr.io/huggingface/text-generation-inference:latest \\ --model-id HuggingFaceTB/SmolLM2-1.7B-Instruct \\ --max-batch-total-tokens 8192 \\ --max-input-length 4096 ``` </hfoption> <hfoption value=\"llama.cpp\" label=\"llama.cpp\"> llama.cpp uses quantization and optimized memory layout: ``` # Server with memory optimizations ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --host 0.0.0.0 \\ --port 8080 \\ -c 2048 \\ # Context size --threads 4 \\ # CPU threads --n-gpu-layers 32 \\ # Use more GPU layers for larger models --mlock \\ # Lock memory to prevent swapping --cont-batching # Enable continuous batching ``` For models too large for your GPU, you can use CPU offloading: ``` ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --n-gpu-layers 20 \\ # Keep first 20 layers on GPU --threads 8 # Use more CPU threads for CPU layers ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> vLLM uses PagedAttention for optimal memory management: ``` from vllm.engine.arg_utils import AsyncEngineArgs engine_args = AsyncEngineArgs( model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", gpu_memory_utilization=0.85, max_num_batched_tokens=8192, block_size=16, ) llm =",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "\\ # Context size --threads 4 \\ # CPU threads --n-gpu-layers 32 \\ # Use more GPU layers for larger models --mlock \\ # Lock memory to prevent swapping --cont-batching # Enable continuous batching ``` For models too large for your GPU, you can use CPU offloading: ``` ./server \\ -m smollm2-1.7b-instruct.Q4_K_M.gguf \\ --n-gpu-layers 20 \\ # Keep first 20 layers on GPU --threads 8 # Use more CPU threads for CPU layers ``` </hfoption> <hfoption value=\"vllm\" label=\"vLLM\"> vLLM uses PagedAttention for optimal memory management: ``` from vllm.engine.arg_utils import AsyncEngineArgs engine_args = AsyncEngineArgs( model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\", gpu_memory_utilization=0.85, max_num_batched_tokens=8192, block_size=16, ) llm = LLM(engine_args=engine_args) ``` </hfoption> ## Resources - [Text Generation Inference Documentation](https://huggingface.co/docs/text-generation-inference) - [TGI GitHub Repository](https://github.com/huggingface/text-generation-inference) - [vLLM Documentation](https://vllm.readthedocs.io/) - [vLLM GitHub Repository](https://github.com/vllm-project/vllm) - [PagedAttention Paper](https://arxiv.org/abs/2309.06180) - [llama.cpp GitHub Repository](https://github.com/ggerganov/llama.cpp) - [llama-cpp-python Repository](https://github.com/abetlen/llama-cpp-python)",
    "metadata": {
      "title": "Optimized Inference Deployment",
      "url": "https://huggingface.co/learn/llm-course/chapter2/8",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/8",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# End-of-chapter quiz\n\n   \n\n### 1. What is the order of the language modeling pipeline?\n\n  First, the model, which handles text and returns raw predictions. The tokenizer then makes sense of these predictions and converts them back to text when needed.  First, the tokenizer, which handles text and returns IDs. The model handles these IDs and outputs a prediction, which can be some text.  The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.   \n\n### 2. How many dimensions does the tensor output by the base Transformer model have, and what are they?\n\n  2: The sequence length and the batch size  2: The sequence length and the hidden size  3: The sequence length, the batch size, and the hidden size   \n\n### 3. Which of the following is an example of subword tokenization?\n\n  WordPiece  Character-based tokenization  Splitting on whitespace and punctuation  BPE  Unigram  None of the above   \n\n### 4. What is a model head?\n\n  A component of the base Transformer network that redirects tensors to their correct layers  Also known as the self-attention mechanism, it adapts the representation of a token according to the other tokens of the sequence  An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output   \n\n### 5. What is an AutoModel?\n\n  A model that automatically trains on your data  An object that returns the correct architecture based on the checkpoint  A model that automatically detects the language used for its inputs to load the correct weights   \n\n### 6. What are the techniques to be aware of when batching sequences of different lengths together?\n\n  Truncating  Returning tensors  Padding  Attention masking   \n\n### 7. What is the point of applying a SoftMax function to the logits output by a sequence classification model?\n\n  It softens the logits so that they're more reliable.  It applies a lower and upper bound so that they're understandable.  The total sum of the output is then 1, resulting in a possible probabilistic interpretation.   \n\n### 8. What method is most of the tokenizer API centered around?\n\n  `encode`, as it can encode text into IDs and IDs into predictions  Calling the tokenizer object directly.  `pad`  `tokenize`   \n\n### 9. What does the result variable contain in this code sample?\n\n  \n```\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nresult = tokenizer.tokenize(\"Hello!\")\n```\n  A list of strings, each string being a token  A list of IDs  A string containing all of the tokens   \n\n### 10. Is there something wrong with the following code?\n\n  \n```\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nmodel = AutoModel.from_pretrained(\"gpt2\")\n\nencoded = tokenizer(\"Hey!\", return_tensors=\"pt\")\nresult = model(**encoded)\n```\n  No, it seems correct.  The tokenizer and model should always be from the same checkpoint.  It's good practice to pad and truncate with the tokenizer as every input is a batch.",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter2/9",
      "course": "llm-course",
      "chapter": "2. Using ü§ó Transformers",
      "chapter_id": "chapter2/9",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter2/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# Introduction\n\n   \nIn [Chapter 2](/course/chapter2) we explored how to use tokenizers and pretrained models to make predictions. But what if you want to fine-tune a pretrained model to solve a specific task? That‚Äôs the topic of this chapter! You will learn:\n \n- How to prepare a large dataset from the Hub using the latest ü§ó Datasets features\n- How to use the high-level `Trainer` API to fine-tune a model with modern best practices\n- How to implement a custom training loop with optimization techniques\n- How to leverage the ü§ó Accelerate library to easily run distributed training on any setup\n- How to apply current fine-tuning best practices for maximum performance\n \n> üìöEssential Resources: Before starting, you might want to review theü§ó Datasets documentationfor data processing.\n \nThis chapter will also serve as an introduction to some Hugging Face libraries beyond the ü§ó Transformers library! We‚Äôll see how libraries like ü§ó Datasets, ü§ó Tokenizers, ü§ó Accelerate, and ü§ó Evaluate can help you train models more efficiently and effectively.\n \nEach of the main sections in this chapter will teach you something different:\n \n- **Section 2**: Learn modern data preprocessing techniques and efficient dataset handling\n- **Section 3**: Master the powerful Trainer API with all its latest features\n- **Section 4**: Implement training loops from scratch and understand distributed training with Accelerate\n \nBy the end of this chapter, you‚Äôll be able to fine-tune models on your own datasets using both high-level APIs and custom training loops, applying the latest best practices in the field.\n \n> üéØWhat You‚Äôll Build: By the end of this chapter, you‚Äôll have fine-tuned a BERT model for text classification and understand how to adapt the techniques to your own datasets and tasks.\n \nThis chapter focuses exclusively on **PyTorch**, as it has become the standard framework for modern deep learning research and production. We‚Äôll use the latest APIs and best practices from the Hugging Face ecosystem.\n \nTo upload your trained models to the Hugging Face Hub, you will need a Hugging Face account: [create an account](https://huggingface.co/join)",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter3/1",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Processing the data Continuing with the example from the [previous chapter](/course/chapter2), here is how we would train a sequence classifier on one batch: ``` import torch from torch.optim import AdamW from transformers import AutoTokenizer, AutoModelForSequenceClassification # Same as before checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequences = [ \"I've been waiting for a HuggingFace course my whole life.\", \"This course is amazing!\", ] batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\") # This is new batch[\"labels\"] = torch.tensor([1, 1]) optimizer = AdamW(model.parameters()) loss = model(**batch).loss loss.backward() optimizer.step() ``` Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset. In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a [paper](https://www.aclweb.org/anthology/I05-5002.pdf) by William B. Dolan and Chris Brockett. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). We‚Äôve selected it for this chapter because it‚Äôs a small dataset, so it‚Äôs easy to experiment with training on it. ### Loading a dataset from the Hub The Hub doesn‚Äôt just contain models; it also has multiple datasets in lots of different languages. You can browse the datasets [here](https://huggingface.co/datasets), and we recommend you try to load and process a new dataset once you have gone through this section (see the general documentation [here](https://huggingface.co/docs/datasets/loading)). But for now, let‚Äôs focus on the MRPC dataset! This is one of the 10 datasets composing the [GLUE benchmark](https://gluebenchmark.com/), which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks. The ü§ó Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this: > üí°Additional Resources: For more dataset loading techniques and examples, check out theü§ó Datasets documentation. ``` from datasets import load_dataset raw_datasets = load_dataset(\"glue\", \"mrpc\") raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 3668 }) validation: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 408 }) test: Dataset({ features: ['sentence1', 'sentence2', 'label', 'idx'], num_rows: 1725 }) }) ``` As you can see, we get a `DatasetDict` object which contains the training set, the validation set, and the test set. Each of those contains several columns (`sentence1`, `sentence2`, `label`, and `idx`) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). > This command downloads and caches the dataset, by default in~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting theHF_HOMEenvironment variable. We can access each pair of sentences in our `raw_datasets` object by indexing, like with a dictionary: ``` raw_train_dataset = raw_datasets[\"train\"] raw_train_dataset[0] ``` ``` {'idx': 0, 'label': 1, 'sentence1': 'Amrozi accused his brother",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 1,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "`sentence2`, `label`, and `idx`) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set). > This command downloads and caches the dataset, by default in~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting theHF_HOMEenvironment variable. We can access each pair of sentences in our `raw_datasets` object by indexing, like with a dictionary: ``` raw_train_dataset = raw_datasets[\"train\"] raw_train_dataset[0] ``` ``` {'idx': 0, 'label': 1, 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'} ``` We can see the labels are already integers, so we won‚Äôt have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the `features` of our `raw_train_dataset`. This will tell us the type of each column: ``` raw_train_dataset.features ``` ``` {'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)} ``` Behind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name is stored in the *names* folder. `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent`. > ‚úèÔ∏èTry it out!Look at element 15 of the training set and element 87 of the validation set. What are their labels? ### Preprocessing a dataset To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the [previous chapter](/course/chapter2), this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this: ``` from transformers import AutoTokenizer checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"]) tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"]) ``` > üí°Deep Dive: For more advanced tokenization techniques and understanding how different tokenizers work, explore theü§ó Tokenizers documentationand thetokenization guide in the cookbook. However, we can‚Äôt just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the two sequences as a pair, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects: ``` inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\") inputs ``` ``` { 'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] } ``` We discussed the `input_ids` and `attention_mask` keys in [Chapter 2](/course/chapter2), but we put off talking about `token_type_ids`.",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 2,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "can also take a pair of sequences and prepare it the way our BERT model expects: ``` inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\") inputs ``` ``` { 'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] } ``` We discussed the `input_ids` and `attention_mask` keys in [Chapter 2](/course/chapter2), but we put off talking about `token_type_ids`. In this example, this is what tells the model which part of the input is the first sentence and which is the second sentence. > ‚úèÔ∏èTry it out!Take element 15 of the training set and tokenize the two sentences separately and as a pair. What‚Äôs the difference between the two results? If we decode the IDs inside `input_ids` back to words: ``` tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]) ``` we will get: ``` ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]'] ``` So we see the model expects the inputs to be of the form `[CLS] sentence1 [SEP] sentence2 [SEP]` when there are two sentences. Aligning this with the `token_type_ids` gives us: ``` ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]'] [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] ``` As you can see, the parts of the input corresponding to `[CLS] sentence1 [SEP]` all have a token type ID of `0`, while the other parts, corresponding to `sentence2 [SEP]`, all have a token type ID of `1`. Note that if you select a different checkpoint, you won‚Äôt necessarily have the `token_type_ids` in your tokenized inputs (for instance, they‚Äôre not returned if you use a DistilBERT model). They are only returned when the model will know what to do with them, because it has seen them during its pretraining. Here, BERT is pretrained with token type IDs, and on top of the masked language modeling objective we talked about in [Chapter 1](/course/chapter1), it has an additional objective called *next sentence prediction*. The goal with this task is to model the relationship between pairs of sentences. With next sentence prediction, the model is provided pairs of sentences (with randomly masked tokens) and asked to predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents. In general, you don‚Äôt need to worry about whether or not there are `token_type_ids` in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model. Now that we have seen how our tokenizer can",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 3,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "predict whether the second sentence follows the first. To make the task non-trivial, half of the time the sentences follow each other in the original document they were extracted from, and the other half of the time the two sentences come from two different documents. In general, you don‚Äôt need to worry about whether or not there are `token_type_ids` in your tokenized inputs: as long as you use the same checkpoint for the tokenizer and the model, everything will be fine as the tokenizer knows what to provide to its model. Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the [previous chapter](/course/chapter2), we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in [Chapter 2](/course/chapter2). So, one way to preprocess the training dataset is: ``` tokenized_dataset = tokenizer( raw_datasets[\"train\"][\"sentence1\"], raw_datasets[\"train\"][\"sentence2\"], padding=True, truncation=True, ) ``` This works well, but it has the disadvantage of returning a dictionary (with our keys, `input_ids`, `attention_mask`, and `token_type_ids`, and values that are lists of lists). It will also only work if you have enough RAM to store your whole dataset during the tokenization (whereas the datasets from the ü§ó Datasets library are [Apache Arrow](https://arrow.apache.org/) files stored on the disk, so you only keep the samples you ask for loaded in memory). To keep the data as a dataset, we will use the [Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map) method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The `map()` method works by applying a function on each element of the dataset, so let‚Äôs define a function that tokenizes our inputs: ``` def tokenize_function(example): return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) ``` This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`. Note that it also works if the `example` dictionary contains several samples (each key as a list of sentences) since the `tokenizer` works on lists of pairs of sentences, as seen before. This will allow us to use the option `batched=True` in our call to `map()`, which will greatly speed up the tokenization. The `tokenizer` is backed by a tokenizer written in Rust from the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) library. This tokenizer can be very fast, but only if we give it lots of inputs at once. Note that we‚Äôve left the `padding` argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it‚Äôs better to pad the samples when we‚Äôre building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths! > üìöPerformance",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 4,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Tokenizers](https://github.com/huggingface/tokenizers) library. This tokenizer can be very fast, but only if we give it lots of inputs at once. Note that we‚Äôve left the `padding` argument out in our tokenization function for now. This is because padding all the samples to the maximum length is not efficient: it‚Äôs better to pad the samples when we‚Äôre building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length in the entire dataset. This can save a lot of time and processing power when the inputs have very variable lengths! > üìöPerformance Tips: Learn more about efficient data processing techniques in theü§ó Datasets performance guide. Here is how we apply the tokenization function on all our datasets at once. We‚Äôre using `batched=True` in our call to `map` so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing. ``` tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) tokenized_datasets ``` The way the ü§ó Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function: ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'], num_rows: 3668 }) validation: Dataset({ features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'], num_rows: 408 }) test: Dataset({ features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'], num_rows: 1725 }) }) ``` You can even use multiprocessing when applying your preprocessing function with `map()` by passing along a `num_proc` argument. We didn‚Äôt do this here because the ü§ó Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing. Our `tokenize_function` returns a dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`. The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together ‚Äî a technique we refer to as *dynamic padding*. ##### Dynamic padding The function that is responsible for putting together samples inside a batch is called a *collate function*. It‚Äôs an argument you can pass when you build a `DataLoader`, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won‚Äôt be possible in our case since the inputs we have won‚Äôt all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you‚Äôre training on a TPU",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 5,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "function*. It‚Äôs an argument you can pass when you build a `DataLoader`, the default being a function that will just convert your samples to PyTorch tensors and concatenate them (recursively if your elements are lists, tuples, or dictionaries). This won‚Äôt be possible in our case since the inputs we have won‚Äôt all be of the same size. We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding. This will speed up training by quite a bit, but note that if you‚Äôre training on a TPU it can cause problems ‚Äî TPUs prefer fixed shapes, even when that requires extra padding. > üöÄOptimization Guide: For more details on optimizing training performance, including padding strategies and TPU considerations, see theü§ó Transformers performance documentation. To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ü§ó Transformers library provides us with such a function via `DataCollatorWithPadding`. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need: ``` from transformers import DataCollatorWithPadding data_collator = DataCollatorWithPadding(tokenizer=tokenizer) ``` To test this new toy, let‚Äôs grab a few samples from our training set that we would like to batch together. Here, we remove the columns `idx`, `sentence1`, and `sentence2` as they won‚Äôt be needed and contain strings (and we can‚Äôt create tensors with strings) and have a look at the lengths of each entry in the batch: ``` samples = tokenized_datasets[\"train\"][:8] samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]} [len(x) for x in samples[\"input_ids\"]] ``` ``` [50, 59, 47, 67, 59, 50, 62, 32] ``` No surprise, we get samples of varying length, from 32 to 67. Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Let‚Äôs double-check that our `data_collator` is dynamically padding the batch properly: ``` batch = data_collator(samples) {k: v.shape for k, v in batch.items()} ``` ``` {'attention_mask': torch.Size([8, 67]), 'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'labels': torch.Size([8])} ``` Looking good! Now that we‚Äôve gone from raw text to batches our model can deal with, we‚Äôre ready to fine-tune it! > ‚úèÔ∏èTry it out!Replicate the preprocessing on the GLUE SST-2 dataset. It‚Äôs a little bit different since it‚Äôs composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.üìñAdditional Practice: Check out these",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 6,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "{k: v.shape for k, v in batch.items()} ``` ``` {'attention_mask': torch.Size([8, 67]), 'input_ids': torch.Size([8, 67]), 'token_type_ids': torch.Size([8, 67]), 'labels': torch.Size([8])} ``` Looking good! Now that we‚Äôve gone from raw text to batches our model can deal with, we‚Äôre ready to fine-tune it! > ‚úèÔ∏èTry it out!Replicate the preprocessing on the GLUE SST-2 dataset. It‚Äôs a little bit different since it‚Äôs composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks.üìñAdditional Practice: Check out these hands-on examples from theü§ó Transformers examples. Perfect! Now that we have preprocessed our data with the latest best practices from the ü§ó Datasets library, we‚Äôre ready to move on to training our model using the modern Trainer API. The next section will show you how to fine-tune your model effectively using the latest features and optimizations available in the Hugging Face ecosystem. ## Section Quiz Test your understanding of data processing concepts: ### 1. What is the main advantage of using Dataset.map() with batched=True ? It uses less memory. It processes multiple examples at once, making tokenization much faster. It automatically handles padding for you. It converts the data to PyTorch tensors. ### 2. Why do we use dynamic padding instead of padding all sequences to the maximum length in the dataset? Dynamic padding is required by the model architecture. It reduces computational overhead by only padding to the maximum length in each batch. It improves model accuracy. It's required when using the DataCollatorWithPadding. ### 3. What does the token_type_ids field represent in BERT tokenization? The position of each token in the sequence. Which sentence each token belongs to when processing sentence pairs. The attention mask for each token. The vocabulary ID of each token. ### 4. When loading a dataset with load_dataset('glue', 'mrpc') , what does the second argument specify? The version of the dataset to load. The specific task or subset within the GLUE benchmark. The split of the dataset (train/validation/test). The format to return the data in. ### 5. What is the purpose of removing columns like ‚Äòsentence1‚Äô and ‚Äòsentence2‚Äô before training? To save memory during training. The model doesn't expect these raw text columns and would throw an error. These columns are not needed for evaluation. It improves training speed significantly. > üí°Key Takeaways:Usebatched=TruewithDataset.map()for significantly faster preprocessingDynamic padding withDataCollatorWithPaddingis more efficient than fixed-length paddingAlways preprocess your data to match what your model expects (numerical tensors, correct column names)The ü§ó Datasets library provides powerful tools for efficient data processing at scale",
    "metadata": {
      "title": "Processing the data",
      "url": "https://huggingface.co/learn/llm-course/chapter3/2",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/2",
      "part": 7,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Fine-tuning a model with the Trainer API ü§ó Transformers provides a `Trainer` class to help you fine-tune any of the pretrained models it provides on your dataset with modern best practices. Once you‚Äôve done all the data preprocessing work in the last section, you have just a few steps left to define the `Trainer`. The hardest part is likely to be preparing the environment to run `Trainer.train()`, as it will run very slowly on a CPU. If you don‚Äôt have a GPU set up, you can get access to free GPUs or TPUs on [Google Colab](https://colab.research.google.com/). > üìöTraining Resources: Before diving into training, familiarize yourself with the comprehensiveü§ó Transformers training guideand explore practical examples in thefine-tuning cookbook. The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need: ``` from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding raw_datasets = load_dataset(\"glue\", \"mrpc\") checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) def tokenize_function(example): return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) ``` ### Training The first step before we can define our `Trainer` is to define a `TrainingArguments` class that will contain all the hyperparameters the `Trainer` will use for training and evaluation. The only argument you have to provide is a directory where the trained model will be saved, as well as the checkpoints along the way. For all the rest, you can leave the defaults, which should work pretty well for a basic fine-tuning. ``` from transformers import TrainingArguments training_args = TrainingArguments(\"test-trainer\") ``` If you want to automatically upload your model to the Hub during training, pass along `push_to_hub=True` in the `TrainingArguments`. We will learn more about this in [Chapter 4](/course/chapter4/3) > üöÄAdvanced Configuration: For detailed information on all available training arguments and optimization strategies, check out theTrainingArguments documentationand thetraining configuration cookbook. The second step is to define our model. As in the [previous chapter](/course/chapter2), we will use the `AutoModelForSequenceClassification` class, with two labels: ``` from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) ``` You will notice that unlike in [Chapter 2](/course/chapter2), you get a warning after instantiating this pretrained model. This is because BERT has not been pretrained on classifying pairs of sentences, so the head of the pretrained model has been discarded and a new head suitable for sequence classification has been added instead. The warnings indicate that some weights were not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now. Once we have our model, we can define a `Trainer` by passing it all the objects constructed up to now ‚Äî the `model`, the `training_args`, the training and validation datasets, our `data_collator`, and our `processing_class`. The `processing_class` parameter is a newer addition that tells the Trainer which tokenizer to use for processing: ``` from transformers import Trainer trainer",
    "metadata": {
      "title": "Fine-tuning a model with the Trainer API",
      "url": "https://huggingface.co/learn/llm-course/chapter3/3",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/3",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "not used (the ones corresponding to the dropped pretraining head) and that some others were randomly initialized (the ones for the new head). It concludes by encouraging you to train the model, which is exactly what we are going to do now. Once we have our model, we can define a `Trainer` by passing it all the objects constructed up to now ‚Äî the `model`, the `training_args`, the training and validation datasets, our `data_collator`, and our `processing_class`. The `processing_class` parameter is a newer addition that tells the Trainer which tokenizer to use for processing: ``` from transformers import Trainer trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, processing_class=tokenizer, ) ``` When you pass a tokenizer as the `processing_class`, the default `data_collator` used by the `Trainer` will be a `DataCollatorWithPadding`. You can skip the `data_collator=data_collator` line in this case, but we included it here to show you this important part of the processing pipeline. > üìñLearn More: For comprehensive details on the Trainer class and its parameters, visit theTrainer API documentationand explore advanced usage patterns in thetraining cookbook recipes. To fine-tune the model on our dataset, we just have to call the `train()` method of our `Trainer`: ``` trainer.train() ``` This will start the fine-tuning (which should take a couple of minutes on a GPU) and report the training loss every 500 steps. It won‚Äôt, however, tell you how well (or badly) your model is performing. This is because: 1. We didn‚Äôt tell the `Trainer` to evaluate during training by setting `eval_strategy` in `TrainingArguments` to either `\"steps\"` (evaluate every `eval_steps`) or `\"epoch\"` (evaluate at the end of each epoch). 2. We didn‚Äôt provide the `Trainer` with a `compute_metrics()` function to calculate a metric during said evaluation (otherwise the evaluation would just have printed the loss, which is not a very intuitive number). ### Evaluation Let‚Äôs see how we can build a useful `compute_metrics()` function and use it the next time we train. The function must take an `EvalPrediction` object (which is a named tuple with a `predictions` field and a `label_ids` field) and will return a dictionary mapping strings to floats (the strings being the names of the metrics returned, and the floats their values). To get some predictions from our model, we can use the `Trainer.predict()` command: ``` predictions = trainer.predict(tokenized_datasets[\"validation\"]) print(predictions.predictions.shape, predictions.label_ids.shape) ``` ``` (408, 2) (408,) ``` The output of the `predict()` method is another named tuple with three fields: `predictions`, `label_ids`, and `metrics`. The `metrics` field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our `compute_metrics()` function and pass it to the `Trainer`, that field will also contain the metrics returned by `compute_metrics()`. As you can see, `predictions` is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to `predict()` (as you saw in",
    "metadata": {
      "title": "Fine-tuning a model with the Trainer API",
      "url": "https://huggingface.co/learn/llm-course/chapter3/3",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/3",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "three fields: `predictions`, `label_ids`, and `metrics`. The `metrics` field will just contain the loss on the dataset passed, as well as some time metrics (how long it took to predict, in total and on average). Once we complete our `compute_metrics()` function and pass it to the `Trainer`, that field will also contain the metrics returned by `compute_metrics()`. As you can see, `predictions` is a two-dimensional array with shape 408 x 2 (408 being the number of elements in the dataset we used). Those are the logits for each element of the dataset we passed to `predict()` (as you saw in the [previous chapter](/course/chapter2), all Transformer models return logits). To transform them into predictions that we can compare to our labels, we need to take the index with the maximum value on the second axis: ``` import numpy as np preds = np.argmax(predictions.predictions, axis=-1) ``` We can now compare those `preds` to the labels. To build our `compute_metric()` function, we will rely on the metrics from the ü§ó [Evaluate](https://github.com/huggingface/evaluate/) library. We can load the metrics associated with the MRPC dataset as easily as we loaded the dataset, this time with the `evaluate.load()` function. The object returned has a `compute()` method we can use to do the metric calculation: ``` import evaluate metric = evaluate.load(\"glue\", \"mrpc\") metric.compute(predictions=preds, references=predictions.label_ids) ``` ``` {'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542} ``` > Learn about different evaluation metrics and strategies in theü§ó Evaluate documentation. The exact results you get may vary, as the random initialization of the model head might change the metrics it achieved. Here, we can see our model has an accuracy of 85.78% on the validation set and an F1 score of 89.97. Those are the two metrics used to evaluate results on the MRPC dataset for the GLUE benchmark. The table in the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf) reported an F1 score of 88.9 for the base model. That was the `uncased` model while we are currently using the `cased` model, which explains the better result. Wrapping everything together, we get our `compute_metrics()` function: ``` def compute_metrics(eval_preds): metric = evaluate.load(\"glue\", \"mrpc\") logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) ``` And to see it used in action to report metrics at the end of each epoch, here is how we define a new `Trainer` with this `compute_metrics()` function: ``` training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\") model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) trainer = Trainer( model, training_args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, processing_class=tokenizer, compute_metrics=compute_metrics, ) ``` Note that we create a new `TrainingArguments` with its `eval_strategy` set to `\"epoch\"` and a new model ‚Äî otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute: ``` trainer.train() ``` This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in",
    "metadata": {
      "title": "Fine-tuning a model with the Trainer API",
      "url": "https://huggingface.co/learn/llm-course/chapter3/3",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/3",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "processing_class=tokenizer, compute_metrics=compute_metrics, ) ``` Note that we create a new `TrainingArguments` with its `eval_strategy` set to `\"epoch\"` and a new model ‚Äî otherwise, we would just be continuing the training of the model we have already trained. To launch a new training run, we execute: ``` trainer.train() ``` This time, it will report the validation loss and metrics at the end of each epoch on top of the training loss. Again, the exact accuracy/F1 score you reach might be a bit different from what we found, because of the random head initialization of the model, but it should be in the same ballpark. ### Advanced Training Features The `Trainer` comes with many built-in features that make modern deep learning best practices accessible: **Mixed Precision Training**: Use `fp16=True` in your training arguments for faster training and reduced memory usage: ``` training_args = TrainingArguments( \"test-trainer\", eval_strategy=\"epoch\", fp16=True, # Enable mixed precision ) ``` **Gradient Accumulation**: For effective larger batch sizes when GPU memory is limited: ``` training_args = TrainingArguments( \"test-trainer\", eval_strategy=\"epoch\", per_device_train_batch_size=4, gradient_accumulation_steps=4, # Effective batch size = 4 * 4 = 16 ) ``` **Learning Rate Scheduling**: The Trainer uses linear decay by default, but you can customize this: ``` training_args = TrainingArguments( \"test-trainer\", eval_strategy=\"epoch\", learning_rate=2e-5, lr_scheduler_type=\"cosine\", # Try different schedulers ) ``` > üéØPerformance Optimization: For more advanced training techniques including distributed training, memory optimization, and hardware-specific optimizations, explore theü§ó Transformers performance guide. The `Trainer` will work out of the box on multiple GPUs or TPUs and provides lots of options for distributed training. We will go over everything it supports in Chapter 10. This concludes the introduction to fine-tuning using the `Trainer` API. An example of doing this for most common NLP tasks will be given in [Chapter 7](/course/chapter7), but for now let‚Äôs look at how to do the same thing with a pure PyTorch training loop. > üìùMore Examples: Check out the comprehensive collection ofü§ó Transformers notebooks. ## Section Quiz Test your understanding of the Trainer API and fine-tuning concepts: ### 1. What is the purpose of the <code> processing_class </code> parameter in the Trainer? It specifies which model architecture to use. It tells the Trainer which tokenizer to use for processing data. It determines the batch size for training. It controls the evaluation frequency. ### 2. Which TrainingArguments parameter controls how often evaluation occurs during training? eval_frequency eval_strategy evaluation_steps do_eval ### 3. What does <code> fp16=True </code> in TrainingArguments enable? 16-bit integer precision for faster training. Mixed precision training with 16-bit floating-point numbers for faster training and reduced memory usage. Training for exactly 16 epochs. Using 16 GPUs for distributed training. ### 4. What is the role of the <code> compute_metrics </code> function in the Trainer? It calculates the loss during training. It converts logits to predictions and calculates evaluation metrics like accuracy and F1. It determines which optimizer to use. It preprocesses the training data. ### 5. What happens when you don‚Äôt provide an <code> eval_dataset </code> to the Trainer? Training will fail with an error.",
    "metadata": {
      "title": "Fine-tuning a model with the Trainer API",
      "url": "https://huggingface.co/learn/llm-course/chapter3/3",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/3",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in TrainingArguments enable? 16-bit integer precision for faster training. Mixed precision training with 16-bit floating-point numbers for faster training and reduced memory usage. Training for exactly 16 epochs. Using 16 GPUs for distributed training. ### 4. What is the role of the <code> compute_metrics </code> function in the Trainer? It calculates the loss during training. It converts logits to predictions and calculates evaluation metrics like accuracy and F1. It determines which optimizer to use. It preprocesses the training data. ### 5. What happens when you don‚Äôt provide an <code> eval_dataset </code> to the Trainer? Training will fail with an error. The Trainer will automatically split the training data for evaluation. You won't get evaluation metrics during training, but training will still work. The model will use the training data for evaluation. ### 6. What is gradient accumulation and how do you enable it? It saves gradients to disk, enabled with save_gradients=True. It accumulates gradients over multiple batches before updating, enabled with gradient_accumulation_steps. It speeds up gradient computation, enabled automatically with fp16. It prevents gradient overflow, enabled with gradient_clipping=True. > üí°Key Takeaways:TheTrainerAPI provides a high-level interface that handles most training complexityUseprocessing_classto specify your tokenizer for proper data handlingTrainingArgumentscontrols all aspects of training: learning rate, batch size, evaluation strategy, and optimizationscompute_metricsenables custom evaluation metrics beyond just training lossModern features like mixed precision (fp16=True) and gradient accumulation can significantly improve training efficiency",
    "metadata": {
      "title": "Fine-tuning a model with the Trainer API",
      "url": "https://huggingface.co/learn/llm-course/chapter3/3",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/3",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# A full training loop Now we‚Äôll see how to achieve the same results as we did in the last section without using the `Trainer` class, implementing a training loop from scratch with modern PyTorch best practices. Again, we assume you have done the data processing in section 2. Here is a short summary covering everything you will need: > üèóÔ∏èTraining from Scratch: This section builds on the previous content. For comprehensive guidance on PyTorch training loops and best practices, check out theü§ó Transformers training documentationand thecustom training cookbook. ``` from datasets import load_dataset from transformers import AutoTokenizer, DataCollatorWithPadding raw_datasets = load_dataset(\"glue\", \"mrpc\") checkpoint = \"bert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(checkpoint) def tokenize_function(example): return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) ``` ### Prepare for training Before actually writing our training loop, we will need to define a few objects. The first ones are the dataloaders we will use to iterate over batches. But before we can define those dataloaders, we need to apply a bit of postprocessing to our `tokenized_datasets`, to take care of some things that the `Trainer` did for us automatically. Specifically, we need to: - Remove the columns corresponding to values the model does not expect (like the `sentence1` and `sentence2` columns). - Rename the column `label` to `labels` (because the model expects the argument to be named `labels`). - Set the format of the datasets so they return PyTorch tensors instead of lists. Our `tokenized_datasets` has one method for each of those steps: ``` tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"]) tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") tokenized_datasets.set_format(\"torch\") tokenized_datasets[\"train\"].column_names ``` We can then check that the result only has columns that our model will accept: ``` [\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"] ``` Now that this is done, we can easily define our dataloaders: ``` from torch.utils.data import DataLoader train_dataloader = DataLoader( tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator ) eval_dataloader = DataLoader( tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator ) ``` To quickly check there is no mistake in the data processing, we can inspect a batch like this: ``` for batch in train_dataloader: break {k: v.shape for k, v in batch.items()} ``` ``` {'attention_mask': torch.Size([8, 65]), 'input_ids': torch.Size([8, 65]), 'labels': torch.Size([8]), 'token_type_ids': torch.Size([8, 65])} ``` Note that the actual shapes will probably be slightly different for you since we set `shuffle=True` for the training dataloader and we are padding to the maximum length inside the batch. Now that we‚Äôre completely finished with data preprocessing (a satisfying yet elusive goal for any ML practitioner), let‚Äôs turn to the model. We instantiate it exactly as we did in the previous section: ``` from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) ``` To make sure that everything will go smoothly during training, we pass our batch to this model: ``` outputs = model(**batch) print(outputs.loss, outputs.logits.shape) ``` ``` tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2]) ``` All ü§ó Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2). We‚Äôre almost ready",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "satisfying yet elusive goal for any ML practitioner), let‚Äôs turn to the model. We instantiate it exactly as we did in the previous section: ``` from transformers import AutoModelForSequenceClassification model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) ``` To make sure that everything will go smoothly during training, we pass our batch to this model: ``` outputs = model(**batch) print(outputs.loss, outputs.logits.shape) ``` ``` tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2]) ``` All ü§ó Transformers models will return the loss when `labels` are provided, and we also get the logits (two for each input in our batch, so a tensor of size 8 x 2). We‚Äôre almost ready to write our training loop! We‚Äôre just missing two things: an optimizer and a learning rate scheduler. Since we are trying to replicate what the `Trainer` was doing by hand, we will use the same defaults. The optimizer used by the `Trainer` is `AdamW`, which is the same as Adam, but with a twist for weight decay regularization (see [‚ÄúDecoupled Weight Decay Regularization‚Äù](https://arxiv.org/abs/1711.05101) by Ilya Loshchilov and Frank Hutter): ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=5e-5) ``` > üí°Modern Optimization Tips: For even better performance, you can try:AdamW with weight decay:AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)8-bit Adam: Usebitsandbytesfor memory-efficient optimizationDifferent learning rates: Lower learning rates (1e-5 to 3e-5) often work better for large modelsüöÄOptimization Resources: Learn more about optimizers and training strategies in theü§ó Transformers optimization guide. Finally, the learning rate scheduler used by default is just a linear decay from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take, which is the number of epochs we want to run multiplied by the number of training batches (which is the length of our training dataloader). The `Trainer` uses three epochs by default, so we will follow that: ``` from transformers import get_scheduler num_epochs = 3 num_training_steps = num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) print(num_training_steps) ``` ``` 1377 ``` ### The training loop One last thing: we will want to use the GPU if we have access to one (on a CPU, training might take several hours instead of a couple of minutes). To do this, we define a `device` we will put our model and our batches on: ``` import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") model.to(device) device ``` ``` device(type='cuda') ``` We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the `tqdm` library: ``` from tqdm.auto import tqdm progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dataloader: batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) ``` > üí°Modern Training Optimizations: To make your training loop even more efficient, consider:Gradient Clipping: Addtorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)beforeoptimizer.step()Mixed Precision: Usetorch.cuda.amp.autocast()andGradScalerfor faster trainingGradient Accumulation: Accumulate gradients over multiple batches to simulate larger batch sizesCheckpointing: Save model checkpoints periodically to resume training if interruptedüîßImplementation",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "sense of when training will be finished, we add a progress bar over our number of training steps, using the `tqdm` library: ``` from tqdm.auto import tqdm progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dataloader: batch = {k: v.to(device) for k, v in batch.items()} outputs = model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) ``` > üí°Modern Training Optimizations: To make your training loop even more efficient, consider:Gradient Clipping: Addtorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)beforeoptimizer.step()Mixed Precision: Usetorch.cuda.amp.autocast()andGradScalerfor faster trainingGradient Accumulation: Accumulate gradients over multiple batches to simulate larger batch sizesCheckpointing: Save model checkpoints periodically to resume training if interruptedüîßImplementation Guide: For detailed examples of these optimizations, see theü§ó Transformers efficient training guideand therange of optimizers. You can see that the core of the training loop looks a lot like the one in the introduction. We didn‚Äôt ask for any reporting, so this training loop will not tell us anything about how the model fares. We need to add an evaluation loop for that. ### The evaluation loop As we did earlier, we will use a metric provided by the ü§ó Evaluate library. We‚Äôve already seen the `metric.compute()` method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method `add_batch()`. Once we have accumulated all the batches, we can get the final result with `metric.compute()`. Here‚Äôs how to implement all of this in an evaluation loop: > üìäEvaluation Best Practices: For more sophisticated evaluation strategies and metrics, explore theü§ó Evaluate documentationand thecomprehensive evaluation cookbook. ``` import evaluate metric = evaluate.load(\"glue\", \"mrpc\") model.eval() for batch in eval_dataloader: batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = model(**batch) logits = outputs.logits predictions = torch.argmax(logits, dim=-1) metric.add_batch(predictions=predictions, references=batch[\"labels\"]) metric.compute() ``` ``` {'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535} ``` Again, your results will be slightly different because of the randomness in the model head initialization and the data shuffling, but they should be in the same ballpark. > ‚úèÔ∏èTry it out!Modify the previous training loop to fine-tune your model on the SST-2 dataset. ### Supercharge your training loop with ü§ó Accelerate The training loop we defined earlier works fine on a single CPU or GPU. But using the [ü§ó Accelerate](https://github.com/huggingface/accelerate) library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs. ü§ó Accelerate handles the complexity of distributed training, mixed precision, and device placement automatically. Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like: > ‚ö°Accelerate Deep Dive: Learn everything about distributed training, mixed precision, and hardware optimization in theü§ó Accelerate documentationand explore practical examples in thetransformers documentation. ``` from accelerate import Accelerator from torch.optim import AdamW from transformers import AutoModelForSequenceClassification, get_scheduler accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) train_dl, eval_dl, model, optimizer = accelerator.prepare( train_dataloader, eval_dataloader, model, optimizer ) num_epochs = 3 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "of the training and validation dataloaders, here is what our manual training loop looks like: > ‚ö°Accelerate Deep Dive: Learn everything about distributed training, mixed precision, and hardware optimization in theü§ó Accelerate documentationand explore practical examples in thetransformers documentation. ``` from accelerate import Accelerator from torch.optim import AdamW from transformers import AutoModelForSequenceClassification, get_scheduler accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) train_dl, eval_dl, model, optimizer = accelerator.prepare( train_dataloader, eval_dataloader, model, optimizer ) num_epochs = 3 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dl: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) ``` The first line to add is the import line. The second line instantiates an `Accelerator` object that will look at the environment and initialize the proper distributed setup. ü§ó Accelerate handles the device placement for you, so you can remove the lines that put the model on the device (or, if you prefer, change them to use `accelerator.device` instead of `device`). Then the main bulk of the work is done in the line that sends the dataloaders, the model, and the optimizer to `accelerator.prepare()`. This will wrap those objects in the proper container to make sure your distributed training works as intended. The remaining changes to make are removing the line that puts the batch on the `device` (again, if you want to keep this you can just change it to use `accelerator.device`) and replacing `loss.backward()` with `accelerator.backward(loss)`. > ‚ö†Ô∏è In order to benefit from the speed-up offered by Cloud TPUs, we recommend padding your samples to a fixed length with thepadding=\"max_length\"andmax_lengtharguments of the tokenizer. If you‚Äôd like to copy and paste it to play around, here‚Äôs what the complete training loop looks like with ü§ó Accelerate: ``` from accelerate import Accelerator from torch.optim import AdamW from transformers import AutoModelForSequenceClassification, get_scheduler accelerator = Accelerator() model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2) optimizer = AdamW(model.parameters(), lr=3e-5) train_dl, eval_dl, model, optimizer = accelerator.prepare( train_dataloader, eval_dataloader, model, optimizer ) num_epochs = 3 num_training_steps = num_epochs * len(train_dl) lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) progress_bar = tqdm(range(num_training_steps)) model.train() for epoch in range(num_epochs): for batch in train_dl: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) ``` Putting this in a `train.py` script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command: ``` accelerate config ``` which will prompt you to answer a few questions and dump your answers in a configuration file used by this command: ``` accelerate launch train.py ``` which will launch the distributed training. If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a `training_function()` and run a last cell with: ``` from accelerate import notebook_launcher notebook_launcher(training_function) ``` You can find more examples in the [ü§ó Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples). > üåêDistributed Training: For comprehensive coverage of multi-GPU and multi-node",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your distributed setup, run the command: ``` accelerate config ``` which will prompt you to answer a few questions and dump your answers in a configuration file used by this command: ``` accelerate launch train.py ``` which will launch the distributed training. If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a `training_function()` and run a last cell with: ``` from accelerate import notebook_launcher notebook_launcher(training_function) ``` You can find more examples in the [ü§ó Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples). > üåêDistributed Training: For comprehensive coverage of multi-GPU and multi-node training, check out theü§ó Transformers distributed training guideand thescaling training cookbook. ### Next Steps and Best Practices Now that you‚Äôve learned how to implement training from scratch, here are some additional considerations for production use: **Model Evaluation**: Always evaluate your model on multiple metrics, not just accuracy. Use the ü§ó Evaluate library for comprehensive evaluation. **Hyperparameter Tuning**: Consider using libraries like Optuna or Ray Tune for systematic hyperparameter optimization. **Model Monitoring**: Track training metrics, learning curves, and validation performance throughout training. **Model Sharing**: Once trained, share your model on the Hugging Face Hub to make it available to the community. **Efficiency**: For large models, consider techniques like gradient checkpointing, parameter-efficient fine-tuning (LoRA, AdaLoRA), or quantization methods. This concludes our deep dive into fine-tuning with custom training loops. The skills you‚Äôve learned here will serve you well when you need full control over the training process or want to implement custom training logic that goes beyond what the `Trainer` API offers. ## Section Quiz Test your understanding of custom training loops and advanced training techniques: ### 1. What is the main difference between Adam and AdamW optimizers? AdamW uses a different learning rate schedule. AdamW includes decoupled weight decay regularization. AdamW only works with transformer models. AdamW requires less memory than Adam. ### 2. In a training loop, what is the correct order of operations? Forward pass ‚Üí Backward pass ‚Üí Optimizer step ‚Üí Zero gradients Forward pass ‚Üí Backward pass ‚Üí Optimizer step ‚Üí Scheduler step ‚Üí Zero gradients Zero gradients ‚Üí Forward pass ‚Üí Optimizer step ‚Üí Backward pass Forward pass ‚Üí Zero gradients ‚Üí Backward pass ‚Üí Optimizer step ### 3. What does the ü§ó Accelerate library primarily help with? Making your models train faster by optimizing the forward pass. Automatically selecting the best hyperparameters. Enabling distributed training across multiple GPUs/TPUs with minimal code changes. Converting models to different frameworks like TensorFlow. ### 4. Why do we move batches to the device in a training loop? To make the training faster. Because the model and data must be on the same device (CPU/GPU) for computation. To save memory. It's required by the DataLoader. ### 5. What does model.eval() do before evaluation? It freezes the model parameters so they can't be updated. It changes the behavior of layers like dropout and batch normalization for inference. It enables gradient computation for evaluation metrics. It automatically calculates evaluation metrics. ### 6.",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "across multiple GPUs/TPUs with minimal code changes. Converting models to different frameworks like TensorFlow. ### 4. Why do we move batches to the device in a training loop? To make the training faster. Because the model and data must be on the same device (CPU/GPU) for computation. To save memory. It's required by the DataLoader. ### 5. What does model.eval() do before evaluation? It freezes the model parameters so they can't be updated. It changes the behavior of layers like dropout and batch normalization for inference. It enables gradient computation for evaluation metrics. It automatically calculates evaluation metrics. ### 6. What is the purpose of torch.no_grad() during evaluation? To prevent the model from making predictions. To save memory and speed up computation by disabling gradient tracking. To enable evaluation mode for the model. To ensure consistent results across runs. ### 7. What changes when you use ü§ó Accelerate in your training loop? You must rewrite your entire training loop from scratch. You wrap key objects with accelerator.prepare() and use accelerator.backward() instead of loss.backward(). You need to specify the number of GPUs in your code. You must use a different optimizer and scheduler. > üí°Key Takeaways:Manual training loops give you complete control but require understanding of the proper sequence: forward ‚Üí backward ‚Üí optimizer step ‚Üí scheduler step ‚Üí zero gradientsAdamW with weight decay is the recommended optimizer for transformer modelsAlways usemodel.eval()andtorch.no_grad()during evaluation for correct behavior and efficiencyü§ó Accelerate makes distributed training accessible with minimal code changesDevice management (moving tensors to GPU/CPU) is crucial for PyTorch operationsModern techniques like mixed precision, gradient accumulation, and gradient clipping can significantly improve training efficiency",
    "metadata": {
      "title": "A full training loop",
      "url": "https://huggingface.co/learn/llm-course/chapter3/4",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/4",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Understanding Learning Curves Now that you‚Äôve learned how to implement fine-tuning using both the `Trainer` API and custom training loops, it‚Äôs crucial to understand how to interpret the results. Learning curves are invaluable tools that help you evaluate your model‚Äôs performance during training and identify potential issues before they reduce performance. In this section, we‚Äôll explore how to read and interpret accuracy and loss curves, understand what different curve shapes tell us about our model‚Äôs behavior, and learn how to address common training issues. ## What are Learning Curves? Learning curves are visual representations of your model‚Äôs performance metrics over time during training. The two most important curves to monitor are: - **Loss curves**: Show how the model‚Äôs error (loss) changes over training steps or epochs - **Accuracy curves**: Show the percentage of correct predictions over training steps or epochs These curves help us understand whether our model is learning effectively and can guide us in making adjustments to improve performance. In Transformers, these metrics are individually computed for each batch and then logged to the disk. We can then use libraries like [Weights & Biases](https://wandb.ai/) to visualize these curves and track our model‚Äôs performance over time. ### Loss Curves The loss curve shows how the model‚Äôs error decreases over time. In a typical successful training run, you‚Äôll see a curve similar to the one below: ![Loss Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/1.png) - **High initial loss**: The model starts without optimization, so predictions are initially poor - **Decreasing loss**: As training progresses, the loss should generally decrease - **Convergence**: Eventually, the loss stabilizes at a low value, indicating that the model has learned the patterns in the data As in previous chapters, we can use the `Trainer` API to track these metrics and visualize them in a dashboard. Below is an example of how to do this with Weights & Biases. ``` # Example of tracking loss during training with the Trainer from transformers import Trainer, TrainingArguments import wandb # Initialize Weights & Biases for experiment tracking wandb.init(project=\"transformer-fine-tuning\", name=\"bert-mrpc-analysis\") training_args = TrainingArguments( output_dir=\"./results\", eval_strategy=\"steps\", eval_steps=50, save_steps=100, logging_steps=10, # Log metrics every 10 steps num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=16, report_to=\"wandb\", # Send logs to Weights & Biases ) trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, processing_class=tokenizer, compute_metrics=compute_metrics, ) # Train and automatically log metrics trainer.train() ``` ### Accuracy Curves The accuracy curve shows the percentage of correct predictions over time. Unlike loss curves, accuracy curves should generally increase as the model learns and can typically include more steps than the loss curve. ![Accuracy Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/2.png) - **Start low**: Initial accuracy should be low, as the model has not yet learned the patterns in the data - **Increase with training**: Accuracy should generally improve as the model learns if it is able to learn the patterns in the data - **May show plateaus**: Accuracy often increases in discrete jumps rather than smoothly, as the model makes predictions that are close to the true labels > üí°Why Accuracy Curves Are ‚ÄúSteppy‚Äù: Unlike loss, which is continuous, accuracy is",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "should generally increase as the model learns and can typically include more steps than the loss curve. ![Accuracy Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/2.png) - **Start low**: Initial accuracy should be low, as the model has not yet learned the patterns in the data - **Increase with training**: Accuracy should generally improve as the model learns if it is able to learn the patterns in the data - **May show plateaus**: Accuracy often increases in discrete jumps rather than smoothly, as the model makes predictions that are close to the true labels > üí°Why Accuracy Curves Are ‚ÄúSteppy‚Äù: Unlike loss, which is continuous, accuracy is calculated by comparing discrete predictions to true labels. Small improvements in model confidence might not change the final prediction, causing accuracy to remain flat until a threshold is crossed. ### Convergence Convergence occurs when the model‚Äôs performance stabilizes and the loss and accuracy curves level off. This is a sign that the model has learned the patterns in the data and is ready to be used. In simple terms, we are aiming for the model to converge to a stable performance every time we train it. ![Convergence](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/4.png) Once models have converged, we can use them to make predictions on new data and refer to evaluation metrics to understand how well the model is performing. ## Interpreting Learning Curve Patterns Different curve shapes reveal different aspects of your model‚Äôs training. Let‚Äôs examine the most common patterns and what they mean. ### Healthy Learning Curves A well-behaved training run typically shows curve shapes similar to the one below: ![Healthy Loss Curve](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/5.png) Let‚Äôs look at the illustration above. It displays both the loss curve (on the left) and the corresponding accuracy curve (on the right). These curves have distinct characteristics. The loss curve shows the value of the model‚Äôs loss over time. Initially, the loss is high and then it gradually decreases, indicating that the model is improving. A decrease in the loss value suggests that the model is making better predictions, as the loss represents the error between the predicted output and the true output. Now let‚Äôs shift our focus to the accuracy curve. It represents the model‚Äôs accuracy over time. The accuracy curve begins at a low value and increases as training progresses. Accuracy measures the proportion of correctly classified instances. So, as the accuracy curve rises, it signifies that the model is making more correct predictions. One notable difference between the curves is the smoothness and the presence of ‚Äúplateaus‚Äù on the accuracy curve. While the loss decreases smoothly, the plateaus on the accuracy curve indicate discrete jumps in accuracy instead of a continuous increase. This behavior is attributed to how accuracy is measured. The loss can improve if the model‚Äôs output gets closer to the target, even if the final prediction is still incorrect. Accuracy, however, only improves when the prediction crosses the threshold to be correct. For example, in a binary classifier distinguishing cats (0) from dogs (1), if the model predicts 0.3 for an image of a",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the curves is the smoothness and the presence of ‚Äúplateaus‚Äù on the accuracy curve. While the loss decreases smoothly, the plateaus on the accuracy curve indicate discrete jumps in accuracy instead of a continuous increase. This behavior is attributed to how accuracy is measured. The loss can improve if the model‚Äôs output gets closer to the target, even if the final prediction is still incorrect. Accuracy, however, only improves when the prediction crosses the threshold to be correct. For example, in a binary classifier distinguishing cats (0) from dogs (1), if the model predicts 0.3 for an image of a dog (true value 1), this is rounded to 0 and is an incorrect classification. If in the next step it predicts 0.4, it‚Äôs still incorrect. The loss will have decreased because 0.4 is closer to 1 than 0.3, but the accuracy remains unchanged, creating a plateau. The accuracy will only jump up when the model predicts a value greater than 0.5 that gets rounded to 1. > Characteristics of healthy curves:Smooth decline in loss: Both training and validation loss decrease steadilyClose training/validation performance: Small gap between training and validation metricsConvergence: Curves level off, indicating the model has learned the patterns ### Practical Examples Let‚Äôs work through some practical examples of learning curves. First, we will highlight some approaches to monitor the learning curves during training. Below, we will break down the different patterns that can be observed in the learning curves. #### During Training During the training process (after you‚Äôve hit `trainer.train()`), you can monitor these key indicators: 1. **Loss convergence**: Is the loss still decreasing or has it plateaued? 2. **Overfitting signs**: Is validation loss starting to increase while training loss decreases? 3. **Learning rate**: Are the curves too erratic (LR too high) or too flat (LR too low)? 4. **Stability**: Are there sudden spikes or drops that indicate problems? #### After Training After the training process is complete, you can analyze the complete curves to understand the model‚Äôs performance. 1. **Final performance**: Did the model reach acceptable performance levels? 2. **Efficiency**: Could the same performance be achieved with fewer epochs? 3. **Generalization**: How close are training and validation performance? 4. **Trends**: Would additional training likely improve performance? > üîçW&B Dashboard Features: Weights & Biases automatically creates beautiful, interactive plots of your learning curves. You can:Compare multiple runs side by sideAdd custom metrics and visualizationsSet up alerts for anomalous behaviorShare results with your teamLearn more in theWeights & Biases documentation. #### Overfitting Overfitting occurs when the model learns too much from the training data and is unable to generalize to different data (represented by the validation set). ![Overfitting](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/10.png) **Symptoms:** - Training loss continues to decrease while validation loss increases or plateaus - Large gap between training and validation accuracy - Training accuracy much higher than validation accuracy **Solutions for overfitting:** - **Regularization**: Add dropout, weight decay, or other regularization techniques - **Early stopping**: Stop training when validation performance stops improving - **Data augmentation**: Increase training data diversity - **Reduce",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "behaviorShare results with your teamLearn more in theWeights & Biases documentation. #### Overfitting Overfitting occurs when the model learns too much from the training data and is unable to generalize to different data (represented by the validation set). ![Overfitting](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/10.png) **Symptoms:** - Training loss continues to decrease while validation loss increases or plateaus - Large gap between training and validation accuracy - Training accuracy much higher than validation accuracy **Solutions for overfitting:** - **Regularization**: Add dropout, weight decay, or other regularization techniques - **Early stopping**: Stop training when validation performance stops improving - **Data augmentation**: Increase training data diversity - **Reduce model complexity**: Use a smaller model or fewer parameters In the sample below, we use early stopping to prevent overfitting. We set the `early_stopping_patience` to 3, which means that if the validation loss does not improve for 3 consecutive epochs, the training will be stopped. ``` # Example of detecting overfitting with early stopping from transformers import EarlyStoppingCallback training_args = TrainingArguments( output_dir=\"./results\", eval_strategy=\"steps\", eval_steps=100, save_strategy=\"steps\", save_steps=100, load_best_model_at_end=True, metric_for_best_model=\"eval_loss\", greater_is_better=False, num_train_epochs=10, # Set high, but we'll stop early ) # Add early stopping to prevent overfitting trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, processing_class=tokenizer, compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], ) ``` #### 2. Underfitting Underfitting occurs when the model is too simple to capture the underlying patterns in the data. This can happen for several reasons: - The model is too small or lacks capacity to learn the patterns - The learning rate is too low, causing slow learning - The dataset is too small or not representative of the problem - The model is not properly regularized ![Underfitting](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/7.png) **Symptoms:** - Both training and validation loss remain high - Model performance plateaus early in training - Training accuracy is lower than expected **Solutions for underfitting:** - **Increase model capacity**: Use a larger model or more parameters - **Train longer**: Increase the number of epochs - **Adjust learning rate**: Try different learning rates - **Check data quality**: Ensure your data is properly preprocessed In the sample below, we train for more epochs to see if the model can learn the patterns in the data. ``` from transformers import TrainingArguments training_args = TrainingArguments( output_dir=\"./results\", -num_train_epochs=5, +num_train_epochs=10, ) ``` #### 3. Erratic Learning Curves Erratic learning curves occur when the model is not learning effectively. This can happen for several reasons: - The learning rate is too high, causing the model to overshoot the optimal parameters - The batch size is too small, causing the model to learn slowly - The model is not properly regularized, causing it to overfit to the training data - The dataset is not properly preprocessed, causing the model to learn from noise ![Erratic Learning Curves](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/3.png) **Symptoms:** - Frequent fluctuations in loss or accuracy - Curves show high variance or instability - Performance oscillates without clear trend Both training and validation curves show erratic behavior. ![Erratic Learning Curves](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/9.png) **Solutions for erratic curves:** - **Lower learning rate**: Reduce step size for more stable training - **Increase batch size**: Larger batches provide more",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "- The batch size is too small, causing the model to learn slowly - The model is not properly regularized, causing it to overfit to the training data - The dataset is not properly preprocessed, causing the model to learn from noise ![Erratic Learning Curves](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/3.png) **Symptoms:** - Frequent fluctuations in loss or accuracy - Curves show high variance or instability - Performance oscillates without clear trend Both training and validation curves show erratic behavior. ![Erratic Learning Curves](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter3/9.png) **Solutions for erratic curves:** - **Lower learning rate**: Reduce step size for more stable training - **Increase batch size**: Larger batches provide more stable gradients - **Gradient clipping**: Prevent exploding gradients - **Better data preprocessing**: Ensure consistent data quality In the sample below, we lower the learning rate and increase the batch size. ``` from transformers import TrainingArguments training_args = TrainingArguments( output_dir=\"./results\", -learning_rate=1e-5, +learning_rate=1e-4, -per_device_train_batch_size=16, +per_device_train_batch_size=32, ) ``` ## Key Takeaways Understanding learning curves is crucial for becoming an effective machine learning practitioner. These visual tools provide immediate feedback about your model‚Äôs training progress and help you make informed decisions about when to stop training, adjust hyperparameters, or try different approaches. With practice, you‚Äôll develop an intuitive understanding of what healthy learning curves look like and how to address issues when they arise. > üí°Key Takeaways:Learning curves are essential tools for understanding model training progressMonitor both loss and accuracy curves, but remember they have different characteristicsOverfitting shows as diverging training/validation performanceUnderfitting shows as poor performance on both training and validation dataTools like Weights & Biases make it easy to track and analyze learning curvesEarly stopping and proper regularization can address most common training issuesüî¨Next Steps: Practice analyzing learning curves on your own fine-tuning experiments. Try different hyperparameters and observe how they affect the curve shapes. This hands-on experience is the best way to develop intuition for reading training progress. ## Section Quiz Test your understanding of learning curves and training analysis: ### 1. What does it typically mean when training loss decreases but validation loss starts increasing? The model is learning successfully and will continue to improve. The model is overfitting to the training data. The learning rate is too low. The dataset is too small. ### 2. Why do accuracy curves often show a ‚Äústeppy‚Äù or plateau-like pattern rather than smooth increases? There's an error in the accuracy calculation. Accuracy is a discrete metric that only changes when predictions cross decision boundaries. The model is not learning effectively. The batch size is too small. ### 3. What is the best approach when you observe erratic, highly fluctuating learning curves? Increase the learning rate to speed up convergence. Reduce the learning rate and possibly increase the batch size. Stop training immediately as the model won't improve. Switch to a completely different model architecture. ### 4. When should you consider using early stopping? Always, as it prevents any form of overfitting. When validation performance stops improving or starts degrading. Only when training loss is still decreasing rapidly. Never, as it prevents the model from",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "The model is not learning effectively. The batch size is too small. ### 3. What is the best approach when you observe erratic, highly fluctuating learning curves? Increase the learning rate to speed up convergence. Reduce the learning rate and possibly increase the batch size. Stop training immediately as the model won't improve. Switch to a completely different model architecture. ### 4. When should you consider using early stopping? Always, as it prevents any form of overfitting. When validation performance stops improving or starts degrading. Only when training loss is still decreasing rapidly. Never, as it prevents the model from reaching its full potential. ### 5. What indicates that your model might be underfitting? Training accuracy is much higher than validation accuracy. Both training and validation performance are poor and plateau early. The learning curves are very smooth with no fluctuations. Validation loss is decreasing faster than training loss.",
    "metadata": {
      "title": "Understanding Learning Curves",
      "url": "https://huggingface.co/learn/llm-course/chapter3/5",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/5",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# Fine-tuning, Check!\n\n   \nThat was comprehensive! In the first two chapters you learned about models and tokenizers, and now you know how to fine-tune them for your own data using modern best practices. To recap, in this chapter you:\n \n- Learned about datasets on the [Hub](https://huggingface.co/datasets) and modern data processing techniques\n- Learned how to load and preprocess datasets efficiently, including using dynamic padding and data collators\n- Implemented fine-tuning and evaluation using the high-level `Trainer` API with the latest features\n- Implemented a complete custom training loop from scratch with PyTorch\n- Used ü§ó Accelerate to make your training code work seamlessly on multiple GPUs or TPUs\n- Applied modern optimization techniques like mixed precision training and gradient accumulation\n \n> üéâCongratulations!You‚Äôve mastered the fundamentals of fine-tuning transformer models. You‚Äôre now ready to tackle real-world ML projects!üìñContinue Learning: Explore these resources to deepen your knowledge:ü§ó Transformers task guidesfor specific NLP tasksü§ó Transformers examplesfor comprehensive notebooksüöÄNext Steps:Try fine-tuning on your own dataset using the techniques you‚Äôve learnedExperiment with different model architectures available on theHugging Face HubJoin theHugging Face communityto share your projects and get help\n \nThis is just the beginning of your journey with ü§ó Transformers. In the next chapter, we‚Äôll explore how to share your models and tokenizers with the community and contribute to the ever-growing ecosystem of pretrained models.\n \nThe skills you‚Äôve developed here - data preprocessing, training configuration, evaluation, and optimization - are fundamental to any machine learning project. Whether you‚Äôre working on text classification, named entity recognition, question answering, or any other NLP task, these techniques will serve you well.\n \n> üí°Pro Tips for Success:Always start with a strong baseline using theTrainerAPI before implementing custom training loopsUse the ü§ó Hub to find pretrained models that are close to your task for better starting pointsMonitor your training with proper evaluation metrics and don‚Äôt forget to save checkpointsLeverage the community - share your models and datasets to help others and get feedback on your work",
    "metadata": {
      "title": "Fine-tuning, Check!",
      "url": "https://huggingface.co/learn/llm-course/chapter3/6",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/6",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter Certificate\n\n   \nCongratulations on completing the course! You‚Äôve learned how to fine-tune pretrained models, understand learning curves, and share your models with the community. Now it‚Äôs time to take the quiz to test your knowledge and get your certificate.\n \nTo take the quiz, you will need to follow these steps:\n \n1. Sign in to your Hugging Face account.\n2. Answer the questions in the quiz.\n3. Submit your answers.\n \n\n## Multiple Choice Quiz\n\n \nIn this quiz, you will be asked to select the correct answer from a list of options. We‚Äôll test you on the fundamentals of supervised finetuning.",
    "metadata": {
      "title": "End-of-chapter Certificate",
      "url": "https://huggingface.co/learn/llm-course/chapter3/7",
      "course": "llm-course",
      "chapter": "3. Fine-tuning a pretrained model",
      "chapter_id": "chapter3/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter3/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# The Hugging Face Hub\n\n   \nThe [Hugging Face Hub](https://huggingface.co/) ‚Äì- our main website ‚Äì- is a central platform that enables anyone to discover, use, and contribute new state-of-the-art models and datasets. It hosts a wide variety of models, with more than 10,000 publicly available. We‚Äôll focus on the models in this chapter, and take a look at the datasets in Chapter 5.\n \nThe models in the Hub are not limited to ü§ó Transformers or even NLP. There are models from [Flair](https://github.com/flairNLP/flair) and [AllenNLP](https://github.com/allenai/allennlp) for NLP, [Asteroid](https://github.com/asteroid-team/asteroid) and [pyannote](https://github.com/pyannote/pyannote-audio) for speech, and [timm](https://github.com/rwightman/pytorch-image-models) for vision, to name a few.\n \nEach of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage.\n \nAdditionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model‚Äôs page, with custom inputs and appropriate widgets.\n \nThe best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.\n \nThe video below shows how to navigate the Hub.\n  \nHaving a huggingface.co account is required to follow along this part, as we‚Äôll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)",
    "metadata": {
      "title": "The Hugging Face Hub",
      "url": "https://huggingface.co/learn/llm-course/chapter4/1",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# Using pretrained models\n\n    \nThe Model Hub makes selecting the appropriate model simple, so that using it in any downstream library can be done in a few lines of code. Let‚Äôs take a look at how to actually use one of these models, and how to contribute back to the community.\n \nLet‚Äôs say we‚Äôre looking for a French-based model that can perform mask filling.\n \n![Selecting the Camembert model.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/camembert.gif)\n \nWe select the `camembert-base` checkpoint to try it out. The identifier `camembert-base` is all we need to start using it! As you‚Äôve seen in previous chapters, we can instantiate it using the `pipeline()` function:\n  \n```\nfrom transformers import pipeline\n\ncamembert_fill_mask = pipeline(\"fill-mask\", model=\"camembert-base\")\nresults = camembert_fill_mask(\"Le camembert est <mask> :)\")\n```\n  \n```\n[\n  {'sequence': 'Le camembert est d√©licieux :)', 'score': 0.49091005325317383, 'token': 7200, 'token_str': 'd√©licieux'}, \n  {'sequence': 'Le camembert est excellent :)', 'score': 0.1055697426199913, 'token': 2183, 'token_str': 'excellent'}, \n  {'sequence': 'Le camembert est succulent :)', 'score': 0.03453313186764717, 'token': 26202, 'token_str': 'succulent'}, \n  {'sequence': 'Le camembert est meilleur :)', 'score': 0.0330314114689827, 'token': 528, 'token_str': 'meilleur'}, \n  {'sequence': 'Le camembert est parfait :)', 'score': 0.03007650189101696, 'token': 1654, 'token_str': 'parfait'}\n]\n```\n \nAs you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it‚Äôs going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:\n \n![The task selector on the web interface.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png)\n \nYou can also instantiate the checkpoint using the model architecture directly:\n  \n```\nfrom transformers import CamembertTokenizer, CamembertForMaskedLM\n\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n```\n \nHowever, we recommend using the [Auto*classes](https://huggingface.co/transformers/model_doc/auto?highlight=auto#auto-classes) instead, as these are by design architecture-agnostic. While the previous code sample limits users to checkpoints loadable in the CamemBERT architecture, using the `Auto*` classes makes switching checkpoints simple:\n  \n```\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"camembert-base\")\n```\n \n> When using a pretrained model, make sure to check how it was trained, on which datasets, its limits, and its biases. All of this information should be indicated on its model card.",
    "metadata": {
      "title": "Using pretrained models",
      "url": "https://huggingface.co/learn/llm-course/chapter4/2",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/2",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "",
    "metadata": {
      "title": "huggingface.co",
      "url": "https://huggingface.co/learn/llm-course/chapter4/3",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/3",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "",
    "metadata": {
      "title": "huggingface.co",
      "url": "https://huggingface.co/learn/llm-course/chapter4/4",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/4",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Part 1 completed!\n\n   \nThis is the end of the first part of the course! Part 2 will be released on November 15th with a big community event, see more information [here](https://huggingface.co/blog/course-launch-event).\n \nYou should now be able to fine-tune a pretrained model on a text classification problem (single or pairs of sentences) and upload the result to the Model Hub. To make sure you mastered this first section, you should do exactly that on a problem that interests you (and not necessarily in English if you speak another language)! You can find help in the [Hugging Face forums](https://discuss.huggingface.co/) and share your project in [this topic](https://discuss.huggingface.co/t/share-your-projects/6803) once you‚Äôre finished.\n \nWe can‚Äôt wait to see what you will build with this!",
    "metadata": {
      "title": "Part 1 completed!",
      "url": "https://huggingface.co/learn/llm-course/chapter4/5",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/5",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# End-of-chapter quiz\n\n   \nLet‚Äôs test what you learned in this chapter!\n \n\n### 1. What are models on the Hub limited to?\n\n  Models from the ü§ó Transformers library.  All models with a similar interface to ü§ó Transformers.  There are no limits.  Models that are in some way related to NLP.   \n\n### 2. How can you manage models on the Hub?\n\n  Through a GCP account.  Through peer-to-peer distribution.  Through git and git-lfs.   \n\n### 3. What can you do using the Hugging Face Hub web interface?\n\n  Fork an existing repository.  Create a new model repository.  Manage and edit files.  Upload files.  See diffs across versions.   \n\n### 4. What is a model card?\n\n  A rough description of the model, therefore less important than the model and tokenizer files.  A way to ensure reproducibility, reusability, and fairness.  A Python file that can be run to retrieve information about the model.   \n\n### 5. Which of these objects of the ü§ó Transformers library can be directly shared on the Hub with push_to_hub() ?\n\n  A tokenizer  A model configuration  A model  A Trainer   \n\n### 6. What is the first step when using the push_to_hub() method or the CLI tools?\n\n  Log in on the website.  Run 'huggingface-cli login' in a terminal.  Run 'notebook_login()' in a notebook.   \n\n### 7. You‚Äôre using a model and a tokenizer ‚Äî how can you upload them to the Hub?\n\n  By calling the push_to_hub method directly on the model and the tokenizer.  Within the Python runtime, by wrapping them in a `huggingface_hub` utility.  By saving them to disk and calling `transformers-cli upload-model`   \n\n### 8. Which git operations can you do with the Repository class?\n\n  A commit.  A pull  A push  A merge",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter4/6",
      "course": "llm-course",
      "chapter": "4. Sharing models and tokenizers",
      "chapter_id": "chapter4/6",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter4/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n   \nIn [Chapter 3](/course/chapter3) you got your first taste of the ü§ó Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n \n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n \nBut this is just scratching the surface of what ü§ó Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we‚Äôll find answers to the following questions:\n \n- What do you do when your dataset is not on the Hub?\n- How can you slice and dice a dataset? (And what if you *really* need to use Pandas?)\n- What do you do when your dataset is huge and will melt your laptop‚Äôs RAM?\n- What the heck are ‚Äúmemory mapping‚Äù and Apache Arrow?\n- How can you create your own dataset and push it to the Hub?\n \nThe techniques you learn here will prepare you for the advanced tokenization and fine-tuning tasks in [Chapter 6](/course/chapter6) and [Chapter 7](/course/chapter7) ‚Äî so grab a coffee and let‚Äôs get started!",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter5/1",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What if my dataset isn‚Äôt on the Hub? You know how to use the [Hugging Face Hub](https://huggingface.co/datasets) to download datasets, but you‚Äôll often find yourself working with data that is stored either on your laptop or on a remote server. In this section we‚Äôll show you how ü§ó Datasets can be used to load datasets that aren‚Äôt available on the Hugging Face Hub. ## Working with local and remote datasets ü§ó Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as: Data format Loading script Example CSV & TSV `csv` `load_dataset(\"csv\", data_files=\"my_file.csv\")` Text files `text` `load_dataset(\"text\", data_files=\"my_file.txt\")` JSON & JSON Lines `json` `load_dataset(\"json\", data_files=\"my_file.jsonl\")` Pickled DataFrames `pandas` `load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")` As shown in the table, for each data format we just need to specify the type of loading script in the `load_dataset()` function, along with a `data_files` argument that specifies the path to one or more files. Let‚Äôs start by loading a dataset from local files; later we‚Äôll see how to do the same with remote files. ## Loading a local dataset For this example we‚Äôll use the [SQuAD-it dataset](https://github.com/crux82/squad-it/), which is a large-scale dataset for question answering in Italian. The training and test splits are hosted on GitHub, so we can download them with a simple `wget` command: ``` !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz !wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz ``` This will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json.gz*, which we can decompress with the Linux `gzip` command: ``` !gzip -dkv SQuAD_it-*.json.gz ``` ``` SQuAD_it-test.json.gz: 87.4% -- replaced with SQuAD_it-test.json SQuAD_it-train.json.gz: 82.2% -- replaced with SQuAD_it-train.json ``` We can see that the compressed files have been replaced with *SQuAD_it-train.json* and *SQuAD_it-test.json*, and that the data is stored in the JSON format. > ‚úé If you‚Äôre wondering why there‚Äôs a!character in the above shell commands, that‚Äôs because we‚Äôre running them within a Jupyter notebook. Simply remove the prefix if you want to download and unzip the dataset within a terminal. To load a JSON file with the `load_dataset()` function, we just need to know if we‚Äôre dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a `data` field. This means we can load the dataset by specifying the `field` argument as follows: ``` from datasets import load_dataset squad_it_dataset = load_dataset(\"json\", data_files=\"SQuAD_it-train.json\", field=\"data\") ``` By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object: ``` squad_it_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 }) }) ``` This shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the `train` split as follows: ``` squad_it_dataset[\"train\"][0] ``` ``` { \"title\": \"Terremoto del Sichuan del 2008\", \"paragraphs\": [ { \"context\": \"Il terremoto del Sichuan del 2008 o il terremoto...\", \"qas\": [ { \"answers\": [{\"answer_start\": 29, \"text\": \"2008\"}],",
    "metadata": {
      "title": "What if my dataset isn‚Äôt on the Hub?",
      "url": "https://huggingface.co/learn/llm-course/chapter5/2",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/2",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "By default, loading local files creates a `DatasetDict` object with a `train` split. We can see this by inspecting the `squad_it_dataset` object: ``` squad_it_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 }) }) ``` This shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the `train` split as follows: ``` squad_it_dataset[\"train\"][0] ``` ``` { \"title\": \"Terremoto del Sichuan del 2008\", \"paragraphs\": [ { \"context\": \"Il terremoto del Sichuan del 2008 o il terremoto...\", \"qas\": [ { \"answers\": [{\"answer_start\": 29, \"text\": \"2008\"}], \"id\": \"56cdca7862d2951400fa6826\", \"question\": \"In quale anno si √® verificato il terremoto nel Sichuan?\", }, ... ], }, ... ], } ``` Great, we‚Äôve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the `train` and `test` splits in a single `DatasetDict` object so we can apply `Dataset.map()` functions across both splits at once. To do this, we can provide a dictionary to the `data_files` argument that maps each split name to a file associated with that split: ``` data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"} squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\") squad_it_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['title', 'paragraphs'], num_rows: 442 }) test: Dataset({ features: ['title', 'paragraphs'], num_rows: 48 }) }) ``` This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on. > Thedata_filesargument of theload_dataset()function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by settingdata_files=\"*.json\"). See the ü§ó Datasetsdocumentationfor more details. The loading scripts in ü§ó Datasets actually support automatic decompression of the input files, so we could have skipped the use of `gzip` by pointing the `data_files` argument directly to the compressed files: ``` data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"} squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\") ``` This can be useful if you don‚Äôt want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point `data_files` to the compressed files and you‚Äôre good to go! Now that you know how to load local files on your laptop or desktop, let‚Äôs take a look at loading remote files. ## Loading a remote dataset If you‚Äôre working as a data scientist or coder in a company, there‚Äôs a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the `data_files` argument of `load_dataset()` to one or more URLs where the remote files are",
    "metadata": {
      "title": "What if my dataset isn‚Äôt on the Hub?",
      "url": "https://huggingface.co/learn/llm-course/chapter5/2",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/2",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the compressed files and you‚Äôre good to go! Now that you know how to load local files on your laptop or desktop, let‚Äôs take a look at loading remote files. ## Loading a remote dataset If you‚Äôre working as a data scientist or coder in a company, there‚Äôs a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the `data_files` argument of `load_dataset()` to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point `data_files` to the *SQuAD_it-*.json.gz* URLs as follows: ``` url = \"https://github.com/crux82/squad-it/raw/master/\" data_files = { \"train\": url + \"SQuAD_it-train.json.gz\", \"test\": url + \"SQuAD_it-test.json.gz\", } squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\") ``` This returns the same `DatasetDict` object obtained above, but saves us the step of manually downloading and decompressing the *SQuAD_it-*.json.gz* files. This wraps up our foray into the various ways to load datasets that aren‚Äôt hosted on the Hugging Face Hub. Now that we‚Äôve got a dataset to play with, let‚Äôs get our hands dirty with various data-wrangling techniques! > ‚úèÔ∏èTry it out!Pick another dataset hosted on GitHub or theUCI Machine Learning Repositoryand try loading it both locally and remotely using the techniques introduced above. For bonus points, try loading a dataset that‚Äôs stored in a CSV or text format (see thedocumentationfor more information on these formats).",
    "metadata": {
      "title": "What if my dataset isn‚Äôt on the Hub?",
      "url": "https://huggingface.co/learn/llm-course/chapter5/2",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/2",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Time to slice and dice Most of the time, the data you work with won‚Äôt be perfectly prepared for training models. In this section we‚Äôll explore the various features that ü§ó Datasets provides to clean up your datasets. ## Slicing and dicing our data Similar to Pandas, ü§ó Datasets provides several functions to manipulate the contents of `Dataset` and `DatasetDict` objects. We already encountered the `Dataset.map()` method in [Chapter 3](/course/chapter3), and in this section we‚Äôll explore some of the other functions at our disposal. For this example we‚Äôll use the [Drug Review Dataset](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) that‚Äôs hosted on the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), which contains patient reviews on various drugs, along with the condition being treated and a 10-star rating of the patient‚Äôs satisfaction. First we need to download and extract the data, which can be done with the `wget` and `unzip` commands: ``` !wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\" !unzip drugsCom_raw.zip ``` Since TSV is just a variant of CSV that uses tabs instead of commas as the separator, we can load these files by using the `csv` loading script and specifying the `delimiter` argument in the `load_dataset()` function as follows: ``` from datasets import load_dataset data_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"} # \\t is the tab character in Python drug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\") ``` A good practice when doing any sort of data analysis is to grab a small random sample to get a quick feel for the type of data you‚Äôre working with. In ü§ó Datasets, we can create a random sample by chaining the `Dataset.shuffle()` and `Dataset.select()` functions together: ``` drug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000)) # Peek at the first few examples drug_sample[:3] ``` ``` {'Unnamed: 0': [87571, 178045, 80482], 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'], 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'], 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"', '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"', '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure. I had severe knee and ankle pain which completely went away after taking Mobic. I attempted to stop the medication however pain returned after a few days.\"'], 'rating': [9.0, 3.0, 10.0], 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'], 'usefulCount': [36, 13, 128]} ``` Note that we‚Äôve fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.select()` expects an iterable of indices,",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 1,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"', '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure. I had severe knee and ankle pain which completely went away after taking Mobic. I attempted to stop the medication however pain returned after a few days.\"'], 'rating': [9.0, 3.0, 10.0], 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'], 'usefulCount': [36, 13, 128]} ``` Note that we‚Äôve fixed the seed in `Dataset.shuffle()` for reproducibility purposes. `Dataset.select()` expects an iterable of indices, so we‚Äôve passed `range(1000)` to grab the first 1,000 examples from the shuffled dataset. From this sample we can already see a few quirks in our dataset: - The `Unnamed: 0` column looks suspiciously like an anonymized ID for each patient. - The `condition` column includes a mix of uppercase and lowercase labels. - The reviews are of varying length and contain a mix of Python line separators (`\\r\\n`) as well as HTML character codes like `&\\#039;`. Let‚Äôs see how we can use ü§ó Datasets to deal with each of these issues. To test the patient ID hypothesis for the `Unnamed: 0` column, we can use the `Dataset.unique()` function to verify that the number of IDs matches the number of rows in each split: ``` for split in drug_dataset.keys(): assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\")) ``` This seems to confirm our hypothesis, so let‚Äôs clean up the dataset a bit by renaming the `Unnamed: 0` column to something a bit more interpretable. We can use the `DatasetDict.rename_column()` function to rename the column across both splits in one go: ``` drug_dataset = drug_dataset.rename_column( original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\" ) drug_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'], num_rows: 161297 }) test: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'], num_rows: 53766 }) }) ``` > ‚úèÔ∏èTry it out!Use theDataset.unique()function to find the number of unique drugs and conditions in the training and test sets. Next, let‚Äôs normalize all the `condition` labels using `Dataset.map()`. As we did with tokenization in [Chapter 3](/course/chapter3), we can define a simple function that can be applied across all the rows of each split in `drug_dataset`: ``` def lowercase_condition(example): return {\"condition\": example[\"condition\"].lower()} drug_dataset.map(lowercase_condition) ``` ``` AttributeError: 'NoneType' object has no attribute 'lower' ``` Oh no, we‚Äôve run into a problem with our map function! From the error we can infer that some of the entries in the `condition` column are `None`, which cannot be lowercased as they‚Äôre not strings. Let‚Äôs drop these rows using `Dataset.filter()`, which works in a similar way to `Dataset.map()` and expects a function that receives a single example of the dataset. Instead of writing an explicit function like: ``` def filter_nones(x): return x[\"condition\"] is not None ``` and then running `drug_dataset.filter(filter_nones)`, we can do this in one line using a *lambda function*. In Python, lambda functions are small functions that you can define without explicitly naming them. They take",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 2,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "function! From the error we can infer that some of the entries in the `condition` column are `None`, which cannot be lowercased as they‚Äôre not strings. Let‚Äôs drop these rows using `Dataset.filter()`, which works in a similar way to `Dataset.map()` and expects a function that receives a single example of the dataset. Instead of writing an explicit function like: ``` def filter_nones(x): return x[\"condition\"] is not None ``` and then running `drug_dataset.filter(filter_nones)`, we can do this in one line using a *lambda function*. In Python, lambda functions are small functions that you can define without explicitly naming them. They take the general form: ``` lambda <arguments> : <expression> ``` where `lambda` is one of Python‚Äôs special [keywords](https://docs.python.org/3/reference/lexical_analysis.html#keywords), `<arguments>` is a list/set of comma-separated values that define the inputs to the function, and `<expression>` represents the operations you wish to execute. For example, we can define a simple lambda function that squares a number as follows: ``` lambda x : x * x ``` To apply this function to an input, we need to wrap it and the input in parentheses: ``` (lambda x: x * x)(3) ``` ``` 9 ``` Similarly, we can define lambda functions with multiple arguments by separating them with commas. For example, we can compute the area of a triangle as follows: ``` (lambda base, height: 0.5 * base * height)(4, 8) ``` ``` 16.0 ``` Lambda functions are handy when you want to define small, single-use functions (for more information about them, we recommend reading the excellent [Real Python tutorial](https://realpython.com/python-lambda/) by Andre Burgaud). In the ü§ó Datasets context, we can use lambda functions to define simple map and filter operations, so let‚Äôs use this trick to eliminate the `None` entries in our dataset: ``` drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None) ``` With the `None` entries removed, we can normalize our `condition` column: ``` drug_dataset = drug_dataset.map(lowercase_condition) # Check that lowercasing worked drug_dataset[\"train\"][\"condition\"][:3] ``` ``` ['left ventricular dysfunction', 'adhd', 'birth control'] ``` It works! Now that we‚Äôve cleaned up the labels, let‚Äôs take a look at cleaning up the reviews themselves. ## Creating new columns Whenever you‚Äôre dealing with customer reviews, a good practice is to check the number of words in each review. A review might be just a single word like ‚ÄúGreat!‚Äù or a full-blown essay with thousands of words, and depending on the use case you‚Äôll need to handle these extremes differently. To compute the number of words in each review, we‚Äôll use a rough heuristic based on splitting each text by whitespace. Let‚Äôs define a simple function that counts the number of words in each review: ``` def compute_review_length(example): return {\"review_length\": len(example[\"review\"].split())} ``` Unlike our `lowercase_condition()` function, `compute_review_length()` returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when `compute_review_length()` is passed to `Dataset.map()`, it will be applied to all the rows in the dataset to create a new `review_length` column: ``` drug_dataset = drug_dataset.map(compute_review_length) # Inspect the first training",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 3,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "differently. To compute the number of words in each review, we‚Äôll use a rough heuristic based on splitting each text by whitespace. Let‚Äôs define a simple function that counts the number of words in each review: ``` def compute_review_length(example): return {\"review_length\": len(example[\"review\"].split())} ``` Unlike our `lowercase_condition()` function, `compute_review_length()` returns a dictionary whose key does not correspond to one of the column names in the dataset. In this case, when `compute_review_length()` is passed to `Dataset.map()`, it will be applied to all the rows in the dataset to create a new `review_length` column: ``` drug_dataset = drug_dataset.map(compute_review_length) # Inspect the first training example drug_dataset[\"train\"][0] ``` ``` {'patient_id': 206461, 'drugName': 'Valsartan', 'condition': 'left ventricular dysfunction', 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"', 'rating': 9.0, 'date': 'May 20, 2012', 'usefulCount': 27, 'review_length': 17} ``` As expected, we can see a `review_length` column has been added to our training set. We can sort this new column with `Dataset.sort()` to see what the extreme values look like: ``` drug_dataset[\"train\"].sort(\"review_length\")[:3] ``` ``` {'patient_id': [103488, 23627, 20558], 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'], 'condition': ['birth control', 'muscle spasm', 'pain'], 'review': ['\"Excellent.\"', '\"useless\"', '\"ok\"'], 'rating': [10.0, 1.0, 6.0], 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'], 'usefulCount': [5, 2, 10], 'review_length': [1, 1, 1]} ``` As we suspected, some reviews contain just a single word, which, although it may be okay for sentiment analysis, would not be informative if we want to predict the condition. > üôã An alternative way to add new columns to a dataset is with theDataset.add_column()function. This allows you to provide the column as a Python list or NumPy array and can be handy in situations whereDataset.map()is not well suited for your analysis. Let‚Äôs use the `Dataset.filter()` function to remove reviews that contain fewer than 30 words. Similarly to what we did with the `condition` column, we can filter out the very short reviews by requiring that the reviews have a length above this threshold: ``` drug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30) print(drug_dataset.num_rows) ``` ``` {'train': 138514, 'test': 46108} ``` As you can see, this has removed around 15% of the reviews from our original training and test sets. > ‚úèÔ∏èTry it out!Use theDataset.sort()function to inspect the reviews with the largest numbers of words. See thedocumentationto see which argument you need to use sort the reviews by length in descending order. The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python‚Äôs `html` module to unescape these characters, like so: ``` import html text = \"I&#039;m a transformer called BERT\" html.unescape(text) ``` ``` \"I'm a transformer called BERT\" ``` We‚Äôll use `Dataset.map()` to unescape all the HTML characters in our corpus: ``` drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])}) ``` As you can see, the `Dataset.map()` method is quite useful for processing data ‚Äî and we haven‚Äôt even scratched the surface of everything it can do! ## The map()",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 4,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "descending order. The last thing we need to deal with is the presence of HTML character codes in our reviews. We can use Python‚Äôs `html` module to unescape these characters, like so: ``` import html text = \"I&#039;m a transformer called BERT\" html.unescape(text) ``` ``` \"I'm a transformer called BERT\" ``` We‚Äôll use `Dataset.map()` to unescape all the HTML characters in our corpus: ``` drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])}) ``` As you can see, the `Dataset.map()` method is quite useful for processing data ‚Äî and we haven‚Äôt even scratched the surface of everything it can do! ## The map() method‚Äôs superpowers The `Dataset.map()` method takes a `batched` argument that, if set to `True`, causes it to send a batch of examples to the map function at once (the batch size is configurable but defaults to 1,000). For instance, the previous map function that unescaped all the HTML took a bit of time to run (you can read the time taken from the progress bars). We can speed this up by processing several elements at the same time using a list comprehension. When you specify `batched=True` the function receives a dictionary with the fields of the dataset, but each value is now a *list of values*, and not just a single value. The return value of `Dataset.map()` should be the same: a dictionary with the fields we want to update or add to our dataset, and a list of values. For example, here is another way to unescape all HTML characters, but using `batched=True`: ``` new_drug_dataset = drug_dataset.map( lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True ) ``` If you‚Äôre running this code in a notebook, you‚Äôll see that this command executes way faster than the previous one. And it‚Äôs not because our reviews have already been HTML-unescaped ‚Äî if you re-execute the instruction from the previous section (without `batched=True`), it will take the same amount of time as before. This is because list comprehensions are usually faster than executing the same code in a `for` loop, and we also gain some performance by accessing lots of elements at the same time instead of one by one. Using `Dataset.map()` with `batched=True` will be essential to unlock the speed of the ‚Äúfast‚Äù tokenizers that we‚Äôll encounter in [Chapter 6](/course/chapter6), which can quickly tokenize big lists of texts. For instance, to tokenize all the drug reviews with a fast tokenizer, we could use a function like this: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") def tokenize_function(examples): return tokenizer(examples[\"review\"], truncation=True) ``` As you saw in [Chapter 3](/course/chapter3), we can pass one or several examples to the tokenizer, so we can use this function with or without `batched=True`. Let‚Äôs take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding `%time` before the line of code you wish to measure: ``` %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True) ``` You can also time a whole cell by putting `%%time` at the beginning of",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 5,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "could use a function like this: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") def tokenize_function(examples): return tokenizer(examples[\"review\"], truncation=True) ``` As you saw in [Chapter 3](/course/chapter3), we can pass one or several examples to the tokenizer, so we can use this function with or without `batched=True`. Let‚Äôs take this opportunity to compare the performance of the different options. In a notebook, you can time a one-line instruction by adding `%time` before the line of code you wish to measure: ``` %time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True) ``` You can also time a whole cell by putting `%%time` at the beginning of the cell. On the hardware we executed this on, it showed 10.8s for this instruction (it‚Äôs the number written after ‚ÄúWall time‚Äù). > ‚úèÔ∏èTry it out!Execute the same instruction with and withoutbatched=True, then try it with a slow tokenizer (adduse_fast=Falsein theAutoTokenizer.from_pretrained()method) so you can see what numbers you get on your hardware. Here are the results we obtained with and without batching, with a fast and a slow tokenizer: Options Fast tokenizer Slow tokenizer `batched=True` 10.8s 4min41s `batched=False` 59.2s 5min3s This means that using a fast tokenizer with the `batched=True` option is 30 times faster than its slow counterpart with no batching ‚Äî this is truly amazing! That‚Äôs the main reason why fast tokenizers are the default when using `AutoTokenizer` (and why they are called ‚Äúfast‚Äù). They‚Äôre able to achieve such a speedup because behind the scenes the tokenization code is executed in Rust, which is a language that makes it easy to parallelize code execution. Parallelization is also the reason for the nearly 6x speedup the fast tokenizer achieves with batching: you can‚Äôt parallelize a single tokenization operation, but when you want to tokenize lots of texts at the same time you can just split the execution across several processes, each responsible for its own texts. `Dataset.map()` also has some parallelization capabilities of its own. Since they are not backed by Rust, they won‚Äôt let a slow tokenizer catch up with a fast one, but they can still be helpful (especially if you‚Äôre using a tokenizer that doesn‚Äôt have a fast version). To enable multiprocessing, use the `num_proc` argument and specify the number of processes to use in your call to `Dataset.map()`: ``` slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False) def slow_tokenize_function(examples): return slow_tokenizer(examples[\"review\"], truncation=True) tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8) ``` You can experiment a little with timing to determine the optimal number of processes to use; in our case 8 seemed to produce the best speed gain. Here are the numbers we got with and without multiprocessing: Options Fast tokenizer Slow tokenizer `batched=True` 10.8s 4min41s `batched=False` 59.2s 5min3s `batched=True`, `num_proc=8` 6.52s 41.3s `batched=False`, `num_proc=8` 9.49s 45.2s Those are much more reasonable results for the slow tokenizer, but the performance of the fast tokenizer was also substantially improved. Note, however, that won‚Äôt always be the case ‚Äî for values of `num_proc` other than 8, our tests showed that it was faster to use `batched=True` without that option. In general, we don‚Äôt recommend",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 6,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "number of processes to use; in our case 8 seemed to produce the best speed gain. Here are the numbers we got with and without multiprocessing: Options Fast tokenizer Slow tokenizer `batched=True` 10.8s 4min41s `batched=False` 59.2s 5min3s `batched=True`, `num_proc=8` 6.52s 41.3s `batched=False`, `num_proc=8` 9.49s 45.2s Those are much more reasonable results for the slow tokenizer, but the performance of the fast tokenizer was also substantially improved. Note, however, that won‚Äôt always be the case ‚Äî for values of `num_proc` other than 8, our tests showed that it was faster to use `batched=True` without that option. In general, we don‚Äôt recommend using Python multiprocessing for fast tokenizers with `batched=True`. > Usingnum_procto speed up your processing is usually a great idea, as long as the function you are using is not already doing some kind of multiprocessing of its own. All of this functionality condensed into a single method is already pretty amazing, but there‚Äôs more! With `Dataset.map()` and `batched=True` you can change the number of elements in your dataset. This is super useful in many situations where you want to create several training features from one example, and we will need to do this as part of the preprocessing for several of the NLP tasks we‚Äôll undertake in [Chapter 7](/course/chapter7). > üí° In machine learning, anexampleis usually defined as the set offeaturesthat we feed to the model. In some contexts, these features will be the set of columns in aDataset, but in others (like here and for question answering), multiple features can be extracted from a single example and belong to a single column. Let‚Äôs have a look at how it works! Here we will tokenize our examples and truncate them to a maximum length of 128, but we will ask the tokenizer to return *all* the chunks of the texts instead of just the first one. This can be done with `return_overflowing_tokens=True`: ``` def tokenize_and_split(examples): return tokenizer( examples[\"review\"], truncation=True, max_length=128, return_overflowing_tokens=True, ) ``` Let‚Äôs test this on one example before using `Dataset.map()` on the whole dataset: ``` result = tokenize_and_split(drug_dataset[\"train\"][0]) [len(inp) for inp in result[\"input_ids\"]] ``` ``` [128, 49] ``` So, our first example in the training set became two features because it was tokenized to more than the maximum number of tokens we specified: the first one of length 128 and the second one of length 49. Now let‚Äôs do this for all elements of the dataset! ``` tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True) ``` ``` ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000 ``` Oh no! That didn‚Äôt work! Why not? Looking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. If you‚Äôve looked at the `Dataset.map()` [documentation](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map), you may recall that it‚Äôs the number of samples passed to the function that we are mapping; here those 1,000 examples gave 1,463 new features, resulting in a shape error. The problem is that we‚Äôre",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 7,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "= drug_dataset.map(tokenize_and_split, batched=True) ``` ``` ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000 ``` Oh no! That didn‚Äôt work! Why not? Looking at the error message will give us a clue: there is a mismatch in the lengths of one of the columns, one being of length 1,463 and the other of length 1,000. If you‚Äôve looked at the `Dataset.map()` [documentation](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map), you may recall that it‚Äôs the number of samples passed to the function that we are mapping; here those 1,000 examples gave 1,463 new features, resulting in a shape error. The problem is that we‚Äôre trying to mix two different datasets of different sizes: the `drug_dataset` columns will have a certain number of examples (the 1,000 in our error), but the `tokenized_dataset` we are building will have more (the 1,463 in the error message; it is more than 1,000 because we are tokenizing long reviews into more than one example by using `return_overflowing_tokens=True`). That doesn‚Äôt work for a `Dataset`, so we need to either remove the columns from the old dataset or make them the same size as they are in the new dataset. We can do the former with the `remove_columns` argument: ``` tokenized_dataset = drug_dataset.map( tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names ) ``` Now this works without error. We can check that our new dataset has many more elements than the original dataset by comparing the lengths: ``` len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"]) ``` ``` (206772, 138514) ``` We mentioned that we can also deal with the mismatched length problem by making the old columns the same size as the new ones. To do this, we will need the `overflow_to_sample_mapping` field the tokenizer returns when we set `return_overflowing_tokens=True`. It gives us a mapping from a new feature index to the index of the sample it originated from. Using this, we can associate each key present in our original dataset with a list of values of the right size by repeating the values of each example as many times as it generates new features: ``` def tokenize_and_split(examples): result = tokenizer( examples[\"review\"], truncation=True, max_length=128, return_overflowing_tokens=True, ) # Extract mapping between new and old indices sample_map = result.pop(\"overflow_to_sample_mapping\") for key, values in examples.items(): result[key] = [values[i] for i in sample_map] return result ``` We can see it works with `Dataset.map()` without us needing to remove the old columns: ``` tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True) tokenized_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'], num_rows: 206772 }) test: Dataset({ features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'], num_rows: 68876 }) }) ``` We get the same number of training features as before, but here we‚Äôve kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach. You‚Äôve now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs, there may",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 8,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'], num_rows: 206772 }) test: Dataset({ features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'], num_rows: 68876 }) }) ``` We get the same number of training features as before, but here we‚Äôve kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach. You‚Äôve now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs, there may be times when you‚Äôll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let‚Äôs take a look at how this works. ## From Dataset s to DataFrame s and back To enable the conversion between various third-party libraries, ü§ó Datasets provides a `Dataset.set_format()` function. This function only changes the *output format* of the dataset, so you can easily switch to another format without affecting the underlying *data format*, which is Apache Arrow. The formatting is done in place. To demonstrate, let‚Äôs convert our dataset to Pandas: ``` drug_dataset.set_format(\"pandas\") ``` Now when we access elements of the dataset we get a `pandas.DataFrame` instead of a dictionary: ``` drug_dataset[\"train\"][:3] ``` patient_id drugName condition review rating date usefulCount review_length 0 95260 Guanfacine adhd \"My son is halfway through his fourth week of Intuniv...\" 8.0 April 27, 2010 192 141 1 92703 Lybrel birth control \"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects...\" 5.0 December 14, 2009 17 134 2 138000 Ortho Evra birth control \"This is my first time using any form of birth control...\" 8.0 November 3, 2015 10 89 Let‚Äôs create a `pandas.DataFrame` for the whole training set by selecting all the elements of `drug_dataset[\"train\"]`: ``` train_df = drug_dataset[\"train\"][:] ``` > üö® Under the hood,Dataset.set_format()changes the return format for the dataset‚Äôs__getitem__()dunder method. This means that when we want to create a new object liketrain_dffrom aDatasetin the\"pandas\"format, we need to slice the whole dataset to obtain apandas.DataFrame. You can verify for yourself that the type ofdrug_dataset[\"train\"]isDataset, irrespective of the output format. From here we can use all the Pandas functionality that we want. For example, we can do fancy chaining to compute the class distribution among the `condition` entries: ``` frequencies = ( train_df[\"condition\"] .value_counts() .to_frame() .reset_index() .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"}) ) frequencies.head() ``` condition frequency 0 birth control 27655 1 depression 8023 2 acne 5209 3 anxiety 4991 4 pain 4744 And once we‚Äôre done with our Pandas analysis, we can always create a new `Dataset` object by using the `Dataset.from_pandas()` function as follows: ``` from datasets import Dataset freq_dataset = Dataset.from_pandas(frequencies) freq_dataset ``` ``` Dataset({ features: ['condition', 'frequency'], num_rows: 819 }) ``` > ‚úèÔ∏èTry it out!Compute",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 9,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Pandas functionality that we want. For example, we can do fancy chaining to compute the class distribution among the `condition` entries: ``` frequencies = ( train_df[\"condition\"] .value_counts() .to_frame() .reset_index() .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"}) ) frequencies.head() ``` condition frequency 0 birth control 27655 1 depression 8023 2 acne 5209 3 anxiety 4991 4 pain 4744 And once we‚Äôre done with our Pandas analysis, we can always create a new `Dataset` object by using the `Dataset.from_pandas()` function as follows: ``` from datasets import Dataset freq_dataset = Dataset.from_pandas(frequencies) freq_dataset ``` ``` Dataset({ features: ['condition', 'frequency'], num_rows: 819 }) ``` > ‚úèÔ∏èTry it out!Compute the average rating per drug and store the result in a newDataset. This wraps up our tour of the various preprocessing techniques available in ü§ó Datasets. To round out the section, let‚Äôs create a validation set to prepare the dataset for training a classifier on. Before doing so, we‚Äôll reset the output format of `drug_dataset` from `\"pandas\"` to `\"arrow\"`: ``` drug_dataset.reset_format() ``` ## Creating a validation set Although we have a test set we could use for evaluation, it‚Äôs a good practice to leave the test set untouched and create a separate validation set during development. Once you are happy with the performance of your models on the validation set, you can do a final sanity check on the test set. This process helps mitigate the risk that you‚Äôll overfit to the test set and deploy a model that fails on real-world data. ü§ó Datasets provides a `Dataset.train_test_split()` function that is based on the famous functionality from `scikit-learn`. Let‚Äôs use it to split our training set into `train` and `validation` splits (we set the `seed` argument for reproducibility): ``` drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42) # Rename the default \"test\" split to \"validation\" drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\") # Add the \"test\" set to our `DatasetDict` drug_dataset_clean[\"test\"] = drug_dataset[\"test\"] drug_dataset_clean ``` ``` DatasetDict({ train: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'], num_rows: 110811 }) validation: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'], num_rows: 27703 }) test: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'], num_rows: 46108 }) }) ``` Great, we‚Äôve now prepared a dataset that‚Äôs ready for training some models on! In [section 5](/course/chapter5/5) we‚Äôll show you how to upload datasets to the Hugging Face Hub, but for now let‚Äôs cap off our analysis by looking at a few ways you can save datasets on your local machine. ## Saving a dataset Although ü§ó Datasets will cache every downloaded dataset and the operations performed on it, there are times when you‚Äôll want to save a dataset to disk (e.g., in case the cache gets deleted). As shown in the table below, ü§ó Datasets provides three main functions to save your dataset in different formats: Data format Function Arrow `Dataset.save_to_disk()` CSV `Dataset.to_csv()` JSON `Dataset.to_json()` For example, let‚Äôs save our cleaned dataset in the Arrow format: ``` drug_dataset_clean.save_to_disk(\"drug-reviews\") ``` This will create a directory with the following structure: ``` drug-reviews/ ‚îú‚îÄ‚îÄ dataset_dict.json ‚îú‚îÄ‚îÄ test",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 10,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "save datasets on your local machine. ## Saving a dataset Although ü§ó Datasets will cache every downloaded dataset and the operations performed on it, there are times when you‚Äôll want to save a dataset to disk (e.g., in case the cache gets deleted). As shown in the table below, ü§ó Datasets provides three main functions to save your dataset in different formats: Data format Function Arrow `Dataset.save_to_disk()` CSV `Dataset.to_csv()` JSON `Dataset.to_json()` For example, let‚Äôs save our cleaned dataset in the Arrow format: ``` drug_dataset_clean.save_to_disk(\"drug-reviews\") ``` This will create a directory with the following structure: ``` drug-reviews/ ‚îú‚îÄ‚îÄ dataset_dict.json ‚îú‚îÄ‚îÄ test ‚îÇ ‚îú‚îÄ‚îÄ dataset.arrow ‚îÇ ‚îú‚îÄ‚îÄ dataset_info.json ‚îÇ ‚îî‚îÄ‚îÄ state.json ‚îú‚îÄ‚îÄ train ‚îÇ ‚îú‚îÄ‚îÄ dataset.arrow ‚îÇ ‚îú‚îÄ‚îÄ dataset_info.json ‚îÇ ‚îú‚îÄ‚îÄ indices.arrow ‚îÇ ‚îî‚îÄ‚îÄ state.json ‚îî‚îÄ‚îÄ validation ‚îú‚îÄ‚îÄ dataset.arrow ‚îú‚îÄ‚îÄ dataset_info.json ‚îú‚îÄ‚îÄ indices.arrow ‚îî‚îÄ‚îÄ state.json ``` where we can see that each split is associated with its own *dataset.arrow* table, and some metadata in *dataset_info.json* and *state.json*. You can think of the Arrow format as a fancy table of columns and rows that is optimized for building high-performance applications that process and transport large datasets. Once the dataset is saved, we can load it by using the `load_from_disk()` function as follows: ``` from datasets import load_from_disk drug_dataset_reloaded = load_from_disk(\"drug-reviews\") drug_dataset_reloaded ``` ``` DatasetDict({ train: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'], num_rows: 110811 }) validation: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'], num_rows: 27703 }) test: Dataset({ features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'], num_rows: 46108 }) }) ``` For the CSV and JSON formats, we have to store each split as a separate file. One way to do this is by iterating over the keys and values in the `DatasetDict` object: ``` for split, dataset in drug_dataset_clean.items(): dataset.to_json(f\"drug-reviews-{split}.jsonl\") ``` This saves each split in [JSON Lines format](https://jsonlines.org), where each row in the dataset is stored as a single line of JSON. Here‚Äôs what the first example looks like: ``` !head -n 1 drug-reviews-train.jsonl ``` ``` {\"patient_id\":141780,\"drugName\":\"Escitalopram\",\"condition\":\"depression\",\"review\":\"\\\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\\\"\",\"rating\":9.0,\"date\":\"May 29, 2011\",\"usefulCount\":10,\"review_length\":125} ``` We can then use the techniques from [section 2](/course/chapter5/2) to load the JSON files as follows: ``` data_files = { \"train\": \"drug-reviews-train.jsonl\", \"validation\": \"drug-reviews-validation.jsonl\", \"test\": \"drug-reviews-test.jsonl\", } drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files) ``` And that‚Äôs it for our excursion into data",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 11,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\\\"\",\"rating\":9.0,\"date\":\"May 29, 2011\",\"usefulCount\":10,\"review_length\":125} ``` We can then use the techniques from [section 2](/course/chapter5/2) to load the JSON files as follows: ``` data_files = { \"train\": \"drug-reviews-train.jsonl\", \"validation\": \"drug-reviews-validation.jsonl\", \"test\": \"drug-reviews-test.jsonl\", } drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files) ``` And that‚Äôs it for our excursion into data wrangling with ü§ó Datasets! Now that we have a cleaned dataset for training a model on, here are a few ideas that you could try out: 1. Use the techniques from [Chapter 3](/course/chapter3) to train a classifier that can predict the patient condition based on the drug review. 2. Use the `summarization` pipeline from [Chapter 1](/course/chapter1) to generate summaries of the reviews. Next, we‚Äôll take a look at how ü§ó Datasets can enable you to work with huge datasets without blowing up your laptop!",
    "metadata": {
      "title": "Time to slice and dice",
      "url": "https://huggingface.co/learn/llm-course/chapter5/3",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/3",
      "part": 12,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Big data? ü§ó Datasets to the rescue! Nowadays it is not uncommon to find yourself working with multi-gigabyte datasets, especially if you‚Äôre planning to pretrain a transformer like BERT or GPT-2 from scratch. In these cases, even *loading* the data can be a challenge. For example, the WebText corpus used to pretrain GPT-2 consists of over 8 million documents and 40 GB of text ‚Äî loading this into your laptop‚Äôs RAM is likely to give it a heart attack! Fortunately, ü§ó Datasets has been designed to overcome these limitations. It frees you from memory management problems by treating datasets as *memory-mapped* files, and from hard drive limits by *streaming* the entries in a corpus. In this section we‚Äôll explore these features of ü§ó Datasets with a huge 825 GB corpus known as [the Pile](https://pile.eleuther.ai). Let‚Äôs get started! ## What is the Pile? The Pile is an English text corpus that was created by [EleutherAI](https://www.eleuther.ai) for training large-scale language models. It includes a diverse range of datasets, spanning scientific articles, GitHub code repositories, and filtered web text. The training corpus is available in [14 GB chunks](https://the-eye.eu/public/AI/pile/), and you can also download several of the [individual components](https://the-eye.eu/public/AI/pile_preliminary_components/). Let‚Äôs start by taking a look at the PubMed Abstracts dataset, which is a corpus of abstracts from 15 million biomedical publications on [PubMed](https://pubmed.ncbi.nlm.nih.gov/). The dataset is in [JSON Lines format](https://jsonlines.org) and is compressed using the `zstandard` library, so first we need to install that: ``` !pip install zstandard ``` Next, we can load the dataset using the method for remote files that we learned in [section 2](/course/chapter5/2): ``` from datasets import load_dataset # This takes a few minutes to run, so go grab a tea or coffee while you wait :) data_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\" pubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\") pubmed_dataset ``` ``` Dataset({ features: ['meta', 'text'], num_rows: 15518009 }) ``` We can see that there are 15,518,009 rows and 2 columns in our dataset ‚Äî that‚Äôs a lot! > ‚úé By default, ü§ó Datasets will decompress the files needed to load a dataset. If you want to preserve hard drive space, you can passDownloadConfig(delete_extracted=True)to thedownload_configargument ofload_dataset(). See thedocumentationfor more details. Let‚Äôs inspect the contents of the first example: ``` pubmed_dataset[0] ``` ``` {'meta': {'pmid': 11409574, 'language': 'eng'}, 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'} ``` Okay, this looks like the abstract from a medical article. Now let‚Äôs see how much RAM we‚Äôve used to load the dataset! ## The magic of memory mapping A simple way to measure memory usage in Python is with the [psutil](https://psutil.readthedocs.io/en/latest/) library, which can be installed with `pip` as follows: ``` !pip install psutil ``` It provides a `Process` class that allows us to check",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'} ``` Okay, this looks like the abstract from a medical article. Now let‚Äôs see how much RAM we‚Äôve used to load the dataset! ## The magic of memory mapping A simple way to measure memory usage in Python is with the [psutil](https://psutil.readthedocs.io/en/latest/) library, which can be installed with `pip` as follows: ``` !pip install psutil ``` It provides a `Process` class that allows us to check the memory usage of the current process as follows: ``` import psutil # Process.memory_info is expressed in bytes, so convert to megabytes print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\") ``` ``` RAM used: 5678.33 MB ``` Here the `rss` attribute refers to the *resident set size*, which is the fraction of memory that a process occupies in RAM. This measurement also includes the memory used by the Python interpreter and the libraries we‚Äôve loaded, so the actual amount of memory used to load the dataset is a bit smaller. For comparison, let‚Äôs see how large the dataset is on disk, using the `dataset_size` attribute. Since the result is expressed in bytes like before, we need to manually convert it to gigabytes: ``` print(f\"Dataset size in bytes: {pubmed_dataset.dataset_size}\") size_gb = pubmed_dataset.dataset_size / (1024**3) print(f\"Dataset size (cache file) : {size_gb:.2f} GB\") ``` ``` Dataset size in bytes : 20979437051 Dataset size (cache file) : 19.54 GB ``` Nice ‚Äî despite it being almost 20 GB large, we‚Äôre able to load and access the dataset with much less RAM! > ‚úèÔ∏èTry it out!Pick one of thesubsetsfrom the Pile that is larger than your laptop or desktop‚Äôs RAM, load it with ü§ó Datasets, and measure the amount of RAM used. Note that to get an accurate measurement, you‚Äôll want to do this in a new process. You can find the decompressed sizes of each subset in Table 1 ofthe Pile paper. If you‚Äôre familiar with Pandas, this result might come as a surprise because of Wes Kinney‚Äôs famous [rule of thumb](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) that you typically need 5 to 10 times as much RAM as the size of your dataset. So how does ü§ó Datasets solve this memory management problem? ü§ó Datasets treats each dataset as a [memory-mapped file](https://en.wikipedia.org/wiki/Memory-mapped_file), which provides a mapping between RAM and filesystem storage that allows the library to access and operate on elements of the dataset without needing to fully load it into memory. Memory-mapped files can also be shared across multiple processes, which enables methods like `Dataset.map()` to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the [Apache Arrow](https://arrow.apache.org) memory format and [pyarrow](https://arrow.apache.org/docs/python/index.html) library, which make the data loading and processing lightning fast. (For more details about Apache Arrow and comparisons to Pandas, check out [Dejan Simic‚Äôs blog post](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) To see this in action, let‚Äôs run a little",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and filesystem storage that allows the library to access and operate on elements of the dataset without needing to fully load it into memory. Memory-mapped files can also be shared across multiple processes, which enables methods like `Dataset.map()` to be parallelized without needing to move or copy the dataset. Under the hood, these capabilities are all realized by the [Apache Arrow](https://arrow.apache.org) memory format and [pyarrow](https://arrow.apache.org/docs/python/index.html) library, which make the data loading and processing lightning fast. (For more details about Apache Arrow and comparisons to Pandas, check out [Dejan Simic‚Äôs blog post](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) To see this in action, let‚Äôs run a little speed test by iterating over all the elements in the PubMed Abstracts dataset: ``` import timeit code_snippet = \"\"\"batch_size = 1000 for idx in range(0, len(pubmed_dataset), batch_size): _ = pubmed_dataset[idx:idx + batch_size] \"\"\" time = timeit.timeit(stmt=code_snippet, number=1, globals=globals()) print( f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \" f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\" ) ``` ``` 'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s' ``` Here we‚Äôve used Python‚Äôs `timeit` module to measure the execution time taken by `code_snippet`. You‚Äôll typically be able to iterate over a dataset at speed of a few tenths of a GB/s to several GB/s. This works great for the vast majority of applications, but sometimes you‚Äôll have to work with a dataset that is too large to even store on your laptop‚Äôs hard drive. For example, if we tried to download the Pile in its entirety, we‚Äôd need 825 GB of free disk space! To handle these cases, ü§ó Datasets provides a streaming feature that allows us to download and access elements on the fly, without needing to download the whole dataset. Let‚Äôs take a look at how this works. > üí° In Jupyter notebooks you can also time cells using the%%timeitmagic function. ## Streaming datasets To enable dataset streaming you just need to pass the `streaming=True` argument to the `load_dataset()` function. For example, let‚Äôs load the PubMed Abstracts dataset again, but in streaming mode: ``` pubmed_dataset_streamed = load_dataset( \"json\", data_files=data_files, split=\"train\", streaming=True ) ``` Instead of the familiar `Dataset` that we‚Äôve encountered elsewhere in this chapter, the object returned with `streaming=True` is an `IterableDataset`. As the name suggests, to access the elements of an `IterableDataset` we need to iterate over it. We can access the first element of our streamed dataset as follows: ``` next(iter(pubmed_dataset_streamed)) ``` ``` {'meta': {'pmid': 11409574, 'language': 'eng'}, 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'} ``` The elements from a streamed dataset can be processed on the fly using `IterableDataset.map()`, which is useful during training if you need to tokenize the inputs. The process is exactly the same as the one we used",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'language': 'eng'}, 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'} ``` The elements from a streamed dataset can be processed on the fly using `IterableDataset.map()`, which is useful during training if you need to tokenize the inputs. The process is exactly the same as the one we used to tokenize our dataset in [Chapter 3](/course/chapter3), with the only difference being that outputs are returned one by one: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"])) next(iter(tokenized_dataset)) ``` ``` {'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]} ``` > üí° To speed up tokenization with streaming you can passbatched=True, as we saw in the last section. It will process the examples batch by batch; the default batch size is 1,000 and can be specified with thebatch_sizeargument. You can also shuffle a streamed dataset using `IterableDataset.shuffle()`, but unlike `Dataset.shuffle()` this only shuffles the elements in a predefined `buffer_size`: ``` shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42) next(iter(shuffled_dataset)) ``` ``` {'meta': {'pmid': 11410799, 'language': 'eng'}, 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'} ``` In this example, we selected a random example from the first 10,000 examples in the buffer. Once an example is accessed, its spot in the buffer is filled with the next example in the corpus (i.e., the 10,001st example in the case above). You can also select elements from a streamed dataset using the `IterableDataset.take()` and `IterableDataset.skip()` functions, which act in a similar way to `Dataset.select()`. For example, to select the first 5 examples in the PubMed Abstracts dataset we can do the following: ``` dataset_head = pubmed_dataset_streamed.take(5) list(dataset_head) ``` ``` [{'meta': {'pmid': 11409574, 'language': 'eng'}, 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'}, {'meta': {'pmid': 11409575, 'language': 'eng'}, 'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'}, {'meta': {'pmid': 11409576, 'language': 'eng'}, 'text': \"Hypoxaemia in children with severe pneumonia in Papua New Guinea ...\"}, {'meta': {'pmid': 11409577, 'language': 'eng'}, 'text': 'Oxygen concentrators and cylinders ...'}, {'meta': {'pmid': 11409578, 'language': 'eng'}, 'text': 'Oxygen supply in rural africa: a personal experience ...'}] ``` Similarly, you can use the `IterableDataset.skip()` function to create training and validation splits from a shuffled dataset as follows: ``` # Skip the first 1,000 examples and include the rest in the training set train_dataset = shuffled_dataset.skip(1000) # Take the first 1,000 examples for the validation set validation_dataset = shuffled_dataset.take(1000) ``` Let‚Äôs round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus. ü§ó Datasets provides an `interleave_datasets()` function that converts a",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and cylinders ...'}, {'meta': {'pmid': 11409578, 'language': 'eng'}, 'text': 'Oxygen supply in rural africa: a personal experience ...'}] ``` Similarly, you can use the `IterableDataset.skip()` function to create training and validation splits from a shuffled dataset as follows: ``` # Skip the first 1,000 examples and include the rest in the training set train_dataset = shuffled_dataset.skip(1000) # Take the first 1,000 examples for the validation set validation_dataset = shuffled_dataset.take(1000) ``` Let‚Äôs round out our exploration of dataset streaming with a common application: combining multiple datasets together to create a single corpus. ü§ó Datasets provides an `interleave_datasets()` function that converts a list of `IterableDataset` objects into a single `IterableDataset`, where the elements of the new dataset are obtained by alternating among the source examples. This function is especially useful when you‚Äôre trying to combine large datasets, so as an example let‚Äôs stream the FreeLaw subset of the Pile, which is a 51 GB dataset of legal opinions from US courts: ``` law_dataset_streamed = load_dataset( \"json\", data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\", split=\"train\", streaming=True, ) next(iter(law_dataset_streamed)) ``` ``` {'meta': {'case_ID': '110921.json', 'case_jurisdiction': 'scotus.tar.gz', 'date_created': '2010-04-28T17:12:49Z'}, 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'} ``` This dataset is large enough to stress the RAM of most laptops, yet we‚Äôve been able to load and access it without breaking a sweat! Let‚Äôs now combine the examples from the FreeLaw and PubMed Abstracts datasets with the `interleave_datasets()` function: ``` from itertools import islice from datasets import interleave_datasets combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed]) list(islice(combined_dataset, 2)) ``` ``` [{'meta': {'pmid': 11409574, 'language': 'eng'}, 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'}, {'meta': {'case_ID': '110921.json', 'case_jurisdiction': 'scotus.tar.gz', 'date_created': '2010-04-28T17:12:49Z'}, 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}] ``` Here we‚Äôve used the `islice()` function from Python‚Äôs `itertools` module to select the first two examples from the combined dataset, and we can see that they match the first examples from each of the two source datasets. Finally, if you want to stream the Pile in its 825 GB entirety, you can grab all the prepared files as follows: ``` base_url = \"https://the-eye.eu/public/AI/pile/\" data_files = { \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)], \"validation\": base_url + \"val.jsonl.zst\", \"test\": base_url + \"test.jsonl.zst\", } pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True) next(iter(pile_dataset[\"train\"])) ``` ``` {'meta': {'pile_set_name': 'Pile-CC'}, 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'} ``` > ‚úèÔ∏èTry it out!Use one of the large Common Crawl corpora likemc4oroscarto",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "first examples from each of the two source datasets. Finally, if you want to stream the Pile in its 825 GB entirety, you can grab all the prepared files as follows: ``` base_url = \"https://the-eye.eu/public/AI/pile/\" data_files = { \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)], \"validation\": base_url + \"val.jsonl.zst\", \"test\": base_url + \"test.jsonl.zst\", } pile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True) next(iter(pile_dataset[\"train\"])) ``` ``` {'meta': {'pile_set_name': 'Pile-CC'}, 'text': 'It is done, and submitted. You can play ‚ÄúSurvival of the Tastiest‚Äù on Android, and on the web...'} ``` > ‚úèÔ∏èTry it out!Use one of the large Common Crawl corpora likemc4oroscarto create a streaming multilingual dataset that represents the spoken proportions of languages in a country of your choice. For example, the four national languages in Switzerland are German, French, Italian, and Romansh, so you could try creating a Swiss corpus by sampling the Oscar subsets according to their spoken proportion. You now have all the tools you need to load and process datasets of all shapes and sizes ‚Äî but unless you‚Äôre exceptionally lucky, there will come a point in your NLP journey where you‚Äôll have to actually create a dataset to solve the problem at hand. That‚Äôs the topic of the next section!",
    "metadata": {
      "title": "Big data? ü§ó Datasets to the rescue!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/4",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/4",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "",
    "metadata": {
      "title": "huggingface.co",
      "url": "https://huggingface.co/learn/llm-course/chapter5/5",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/5",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Semantic search with FAISS In [section 5](/course/chapter5/5), we created a dataset of GitHub issues and comments from the ü§ó Datasets repository. In this section we‚Äôll use this information to build a search engine that can help us find answers to our most pressing questions about the library! ## Using embeddings for semantic search As we saw in [Chapter 1](/course/chapter1), Transformer-based language models represent each token in a span of text as an *embedding vector*. It turns out that one can ‚Äúpool‚Äù the individual embeddings to create a vector representation for whole sentences, paragraphs, or (in some cases) documents. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and returning the documents with the greatest overlap. In this section we‚Äôll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents. ![Semantic search.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg) ![Semantic search.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg) ## Loading and preparing the dataset The first thing we need to do is download our dataset of GitHub issues, so let‚Äôs use `load_dataset()` function as usual: ``` from datasets import load_dataset issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\") issues_dataset ``` ``` Dataset({ features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'], num_rows: 2855 }) ``` Here we‚Äôve specified the default `train` split in `load_dataset()`, so it returns a `Dataset` instead of a `DatasetDict`. The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. As should be familiar by now, we can use the `Dataset.filter()` function to exclude these rows in our dataset. While we‚Äôre at it, let‚Äôs also filter out rows with no comments, since these provide no answers to user queries: ``` issues_dataset = issues_dataset.filter( lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0) ) issues_dataset ``` ``` Dataset({ features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'], num_rows: 771 }) ``` We can see that there are a lot of columns in our dataset, most of which we don‚Äôt need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let‚Äôs use the `Dataset.remove_columns()` function to drop the rest: ``` columns = issues_dataset.column_names columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"] columns_to_remove = set(columns_to_keep).symmetric_difference(columns) issues_dataset = issues_dataset.remove_columns(columns_to_remove) issues_dataset ``` ``` Dataset({ features: ['html_url', 'title', 'comments', 'body'], num_rows: 771 }) ``` To create our embeddings we‚Äôll augment each comment with the issue‚Äôs title and body, since these fields often include useful contextual information. Because our `comments` column is currently a list of comments",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let‚Äôs use the `Dataset.remove_columns()` function to drop the rest: ``` columns = issues_dataset.column_names columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"] columns_to_remove = set(columns_to_keep).symmetric_difference(columns) issues_dataset = issues_dataset.remove_columns(columns_to_remove) issues_dataset ``` ``` Dataset({ features: ['html_url', 'title', 'comments', 'body'], num_rows: 771 }) ``` To create our embeddings we‚Äôll augment each comment with the issue‚Äôs title and body, since these fields often include useful contextual information. Because our `comments` column is currently a list of comments for each issue, we need to ‚Äúexplode‚Äù the column so that each row consists of an `(html_url, title, body, comment)` tuple. In Pandas we can do this with the [DataFrame.explode()function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html), which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let‚Äôs first switch to the Pandas `DataFrame` format: ``` issues_dataset.set_format(\"pandas\") df = issues_dataset[:] ``` If we inspect the first row in this `DataFrame` we can see there are four comments associated with this issue: ``` df[\"comments\"][0].tolist() ``` ``` ['the bug code locate in Ôºö\\r\\n if data_args.task_name is not None:\\r\\n # Downloading and loading a dataset from the hub.\\r\\n datasets = load_dataset(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)', 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?', 'cannot connectÔºåeven by Web browserÔºåplease check that there is some problems„ÄÇ', 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...'] ``` When we explode `df`, we expect to get one row for each of these comments. Let‚Äôs check if that‚Äôs the case: ``` comments_df = df.explode(\"comments\", ignore_index=True) comments_df.head(4) ``` html_url title comments body 0 https://github.com/huggingface/datasets/issues/2787 ConnectionError: Couldn't reach https://raw.githubusercontent.com the bug code locate in Ôºö\\r\\n if data_args.task_name is not None... Hello,\\r\\nI am trying to run run_glue.py and it gives me this error... 1 https://github.com/huggingface/datasets/issues/2787 ConnectionError: Couldn't reach https://raw.githubusercontent.com Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com... Hello,\\r\\nI am trying to run run_glue.py and it gives me this error... 2 https://github.com/huggingface/datasets/issues/2787 ConnectionError: Couldn't reach https://raw.githubusercontent.com cannot connectÔºåeven by Web browserÔºåplease check that there is some problems„ÄÇ Hello,\\r\\nI am trying to run run_glue.py and it gives me this error... 3 https://github.com/huggingface/datasets/issues/2787 ConnectionError: Couldn't reach https://raw.githubusercontent.com I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem... Hello,\\r\\nI am trying to run run_glue.py and it gives me this error... Great, we can see the rows have been replicated, with the `comments` column containing the individual comments! Now that we‚Äôre finished with Pandas, we can quickly switch back to a `Dataset` by loading the `DataFrame` in memory: ``` from datasets import Dataset comments_dataset = Dataset.from_pandas(comments_df) comments_dataset ``` ``` Dataset({ features: ['html_url', 'title', 'comments', 'body'], num_rows: 2842 }) ``` Okay, this has given us a few thousand comments to work with! > ‚úèÔ∏èTry it out!See if you can useDataset.map()to explode thecommentscolumn ofissues_datasetwithoutresorting",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "https://raw.githubusercontent.com I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem... Hello,\\r\\nI am trying to run run_glue.py and it gives me this error... Great, we can see the rows have been replicated, with the `comments` column containing the individual comments! Now that we‚Äôre finished with Pandas, we can quickly switch back to a `Dataset` by loading the `DataFrame` in memory: ``` from datasets import Dataset comments_dataset = Dataset.from_pandas(comments_df) comments_dataset ``` ``` Dataset({ features: ['html_url', 'title', 'comments', 'body'], num_rows: 2842 }) ``` Okay, this has given us a few thousand comments to work with! > ‚úèÔ∏èTry it out!See if you can useDataset.map()to explode thecommentscolumn ofissues_datasetwithoutresorting to the use of Pandas. This is a little tricky; you might find the‚ÄúBatch mapping‚Äùsection of the ü§ó Datasets documentation useful for this task. Now that we have one comment per row, let‚Äôs create a new `comments_length` column that contains the number of words per comment: ``` comments_dataset = comments_dataset.map( lambda x: {\"comment_length\": len(x[\"comments\"].split())} ) ``` We can use this new column to filter out short comments, which typically include things like ‚Äúcc @lewtun‚Äù or ‚ÄúThanks!‚Äù that are not relevant for our search engine. There‚Äôs no precise number to select for the filter, but around 15 words seems like a good start: ``` comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15) comments_dataset ``` ``` Dataset({ features: ['html_url', 'title', 'comments', 'body', 'comment_length'], num_rows: 2098 }) ``` Having cleaned up our dataset a bit, let‚Äôs concatenate the issue title, description, and comments together in a new `text` column. As usual, we‚Äôll write a simple function that we can pass to `Dataset.map()`: ``` def concatenate_text(examples): return { \"text\": examples[\"title\"] + \" \\n \" + examples[\"body\"] + \" \\n \" + examples[\"comments\"] } comments_dataset = comments_dataset.map(concatenate_text) ``` We‚Äôre finally ready to create some embeddings! Let‚Äôs take a look. ## Creating text embeddings We saw in [Chapter 2](/course/chapter2) that we can obtain token embeddings by using the `AutoModel` class. All we need to do is pick a suitable checkpoint to load the model from. Fortunately, there‚Äôs a library called `sentence-transformers` that is dedicated to creating embeddings. As described in the library‚Äôs [documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search), our use case is an example of *asymmetric semantic search* because we have a short query whose answer we‚Äôd like to find in a longer document, like a an issue comment. The handy [model overview table](https://www.sbert.net/docs/pretrained_models.html#model-overview) in the documentation indicates that the `multi-qa-mpnet-base-dot-v1` checkpoint has the best performance for semantic search, so we‚Äôll use that for our application. We‚Äôll also load the tokenizer using the same checkpoint: ``` from transformers import AutoTokenizer, AutoModel model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" tokenizer = AutoTokenizer.from_pretrained(model_ckpt) model = AutoModel.from_pretrained(model_ckpt) ``` To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let‚Äôs do that now: ``` import torch device = torch.device(\"cuda\") model.to(device) ``` As we mentioned earlier, we‚Äôd like to represent each entry in our GitHub issues corpus as a single vector, so we need to ‚Äúpool‚Äù or average our token embeddings in some way. One popular approach is to perform *CLS pooling*",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "that for our application. We‚Äôll also load the tokenizer using the same checkpoint: ``` from transformers import AutoTokenizer, AutoModel model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" tokenizer = AutoTokenizer.from_pretrained(model_ckpt) model = AutoModel.from_pretrained(model_ckpt) ``` To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let‚Äôs do that now: ``` import torch device = torch.device(\"cuda\") model.to(device) ``` As we mentioned earlier, we‚Äôd like to represent each entry in our GitHub issues corpus as a single vector, so we need to ‚Äúpool‚Äù or average our token embeddings in some way. One popular approach is to perform *CLS pooling* on our model‚Äôs outputs, where we simply collect the last hidden state for the special `[CLS]` token. The following function does the trick for us: ``` def cls_pooling(model_output): return model_output.last_hidden_state[:, 0] ``` Next, we‚Äôll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs: ``` def get_embeddings(text_list): encoded_input = tokenizer( text_list, padding=True, truncation=True, return_tensors=\"pt\" ) encoded_input = {k: v.to(device) for k, v in encoded_input.items()} model_output = model(**encoded_input) return cls_pooling(model_output) ``` We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape: ``` embedding = get_embeddings(comments_dataset[\"text\"][0]) embedding.shape ``` ``` torch.Size([1, 768]) ``` Great, we‚Äôve converted the first entry in our corpus into a 768-dimensional vector! We can use `Dataset.map()` to apply our `get_embeddings()` function to each row in our corpus, so let‚Äôs create a new `embeddings` column as follows: ``` embeddings_dataset = comments_dataset.map( lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]} ) ``` Notice that we‚Äôve converted the embeddings to NumPy arrays ‚Äî that‚Äôs because ü§ó Datasets requires this format when we try to index them with FAISS, which we‚Äôll do next. ## Using FAISS for efficient similarity search Now that we have a dataset of embeddings, we need some way to search over them. To do this, we‚Äôll use a special data structure in ü§ó Datasets called a *FAISS index*. [FAISS](https://faiss.ai/) (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors. The basic idea behind FAISS is to create a special data structure called an *index* that allows one to find which embeddings are similar to an input embedding. Creating a FAISS index in ü§ó Datasets is simple ‚Äî we use the `Dataset.add_faiss_index()` function and specify which column of our dataset we‚Äôd like to index: ``` embeddings_dataset.add_faiss_index(column=\"embeddings\") ``` We can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.get_nearest_examples()` function. Let‚Äôs test this out by first embedding a question as follows: ``` question = \"How can I load a dataset offline?\" question_embedding = get_embeddings([question]).cpu().detach().numpy() question_embedding.shape ``` ``` torch.Size([1, 768]) ``` Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings: ``` scores, samples = embeddings_dataset.get_nearest_examples( \"embeddings\", question_embedding, k=5 ) ```",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and specify which column of our dataset we‚Äôd like to index: ``` embeddings_dataset.add_faiss_index(column=\"embeddings\") ``` We can now perform queries on this index by doing a nearest neighbor lookup with the `Dataset.get_nearest_examples()` function. Let‚Äôs test this out by first embedding a question as follows: ``` question = \"How can I load a dataset offline?\" question_embedding = get_embeddings([question]).cpu().detach().numpy() question_embedding.shape ``` ``` torch.Size([1, 768]) ``` Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings: ``` scores, samples = embeddings_dataset.get_nearest_examples( \"embeddings\", question_embedding, k=5 ) ``` The `Dataset.get_nearest_examples()` function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let‚Äôs collect these in a `pandas.DataFrame` so we can easily sort them: ``` import pandas as pd samples_df = pd.DataFrame.from_dict(samples) samples_df[\"scores\"] = scores samples_df.sort_values(\"scores\", ascending=False, inplace=True) ``` Now we can iterate over the first few rows to see how well our query matched the available comments: ``` for _, row in samples_df.iterrows(): print(f\"COMMENT: {row.comments}\") print(f\"SCORE: {row.scores}\") print(f\"TITLE: {row.title}\") print(f\"URL: {row.html_url}\") print(\"=\" * 50) print() ``` ``` \"\"\" COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine. @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like? SCORE: 25.505046844482422 TITLE: Discussion using datasets in offline mode URL: https://github.com/huggingface/datasets/issues/824 ================================================== COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :) You can now use them offline \\`\\`\\`python datasets = load_dataset(\"text\", data_files=data_files) \\`\\`\\` We'll do a new release soon SCORE: 24.555509567260742 TITLE: Discussion using datasets in offline mode URL: https://github.com/huggingface/datasets/issues/824 ================================================== COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet. Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature. ---------- > @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like? Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones. For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do \\`\\`\\`python load_dataset(\"./my_dataset\") \\`\\`\\` and the dataset script will generate your dataset once and for all. ---------- About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded. cf #1724 SCORE: 24.14896583557129 TITLE:",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Could you please elaborate on how that should look like? Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones. For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do \\`\\`\\`python load_dataset(\"./my_dataset\") \\`\\`\\` and the dataset script will generate your dataset once and for all. ---------- About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded. cf #1724 SCORE: 24.14896583557129 TITLE: Discussion using datasets in offline mode URL: https://github.com/huggingface/datasets/issues/824 ================================================== COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine > > 1. (online machine) > > ``` > > import datasets > > data = datasets.load_dataset(...) > > data.save_to_disk(/YOUR/DATASET/DIR) > > ``` > > 2. copy the dir from online to the offline machine > > 3. (offline machine) > > ``` > > import datasets > > data = datasets.load_from_disk(/SAVED/DATA/DIR) > > ``` > > > > HTH. SCORE: 22.893993377685547 TITLE: Discussion using datasets in offline mode URL: https://github.com/huggingface/datasets/issues/824 ================================================== COMMENT: here is my way to load a dataset offline, but it **requires** an online machine 1. (online machine) \\`\\`\\` import datasets data = datasets.load_dataset(...) data.save_to_disk(/YOUR/DATASET/DIR) \\`\\`\\` 2. copy the dir from online to the offline machine 3. (offline machine) \\`\\`\\` import datasets data = datasets.load_from_disk(/SAVED/DATA/DIR) \\`\\`\\` HTH. SCORE: 22.406635284423828 TITLE: Discussion using datasets in offline mode URL: https://github.com/huggingface/datasets/issues/824 ================================================== \"\"\" ``` Not bad! Our second hit seems to match the query. > ‚úèÔ∏èTry it out!Create your own query and see whether you can find an answer in the retrieved documents. You might have to increase thekparameter inDataset.get_nearest_examples()to broaden the search.",
    "metadata": {
      "title": "Semantic search with FAISS",
      "url": "https://huggingface.co/learn/llm-course/chapter5/6",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/6",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# ü§ó Datasets, check!\n\n   \nWell, that was quite a tour through the ü§ó Datasets library ‚Äî congratulations on making it this far! With the knowledge that you‚Äôve gained from this chapter, you should be able to:\n \n- Load datasets from anywhere, be it the Hugging Face Hub, your laptop, or a remote server at your company.\n- Wrangle your data using a mix of the `Dataset.map()` and `Dataset.filter()` functions.\n- Quickly switch between data formats like Pandas and NumPy using `Dataset.set_format()`.\n- Create your very own dataset and push it to the Hugging Face Hub.\n- Embed your documents using a Transformer model and build a semantic search engine using FAISS.\n \nIn [Chapter 7](/course/chapter7), we‚Äôll put all of this to good use as we take a deep dive into the core NLP tasks that Transformer models are great for. Before jumping ahead, though, put your knowledge of ü§ó Datasets to the test with a quick quiz!",
    "metadata": {
      "title": "ü§ó Datasets, check!",
      "url": "https://huggingface.co/learn/llm-course/chapter5/7",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter quiz\n\n   \nThis chapter covered a lot of ground! Don‚Äôt worry if you didn‚Äôt grasp all the details; the next chapters will help you understand how things work under the hood.\n \nBefore moving on, though, let‚Äôs test what you learned in this chapter.\n \n\n### 1. The load_dataset() function in ü§ó Datasets allows you to load a dataset from which of the following locations?\n\n  Locally, e.g. on your laptop  The Hugging Face Hub  A remote server   \n\n### 2. Suppose you load one of the GLUE tasks as follows:\n\n  \n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n```\n \nWhich of the following commands will produce a random sample of 50 elements from `dataset`?\n  `dataset.sample(50)`  `dataset.shuffle().select(range(50))`  `dataset.select(range(50)).shuffle()`   \n\n### 3. Suppose you have a dataset about household pets called pets_dataset , which has a name column that denotes the name of each pet. Which of the following approaches would allow you to filter the dataset for all pets whose names start with the letter ‚ÄúL‚Äù?\n\n  `pets_dataset.filter(lambda x : x['name'].startswith('L'))`  `pets_dataset.filter(lambda x['name'].startswith('L'))`  Create a function like `def filter_names(x): return x['name'].startswith('L')` and run `pets_dataset.filter(filter_names)`.   \n\n### 4. What is memory mapping?\n\n  A mapping between CPU and GPU RAM  A mapping between RAM and filesystem storage  A mapping between two files in the ü§ó Datasets cache   \n\n### 5. Which of the following are the main benefits of memory mapping?\n\n  Accessing memory-mapped files is faster than reading from or writing to disk.  Applications can access segments of data in an extremely large file without having to read the whole file into RAM first.  It consumes less energy, so your battery lasts longer.   \n\n### 6. Why does the following code fail?\n\n  \n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"allocine\", streaming=True, split=\"train\")\ndataset[0]\n```\n  It tries to stream a dataset that's too large to fit in RAM.  It tries to access an `IterableDataset`.  The `allocine` dataset doesn't have a `train` split.   \n\n### 7. Which of the following are the main benefits of creating a dataset card?\n\n  It provides information about the intended use and supported tasks of the dataset so others in the community can make an informed decision about using it.  It helps draw attention to the biases that are present in a corpus.  It improves the chances that others in the community will use my dataset.   \n\n### 8. What is semantic search?\n\n  A way to search for exact matches between the words in a query and the documents in a corpus  A way to search for matching documents by understanding the contextual meaning of a query  A way to improve search accuracy   \n\n### 9. For asymmetric semantic search, you usually have:\n\n  A short query and a longer paragraph that answers the query  Queries and paragraphs that are of about the same length  A long query and a shorter paragraph that answers the query   \n\n### 10. Can I use ü§ó Datasets to load data for use in other domains, like speech processing?\n\n  No  Yes",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter5/8",
      "course": "llm-course",
      "chapter": "5. The ü§ó Datasets library",
      "chapter_id": "chapter5/8",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter5/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n   \nIn [Chapter 3](/course/chapter3), we looked at how to fine-tune a model on a given task. When we do that, we use the same tokenizer that the model was pretrained with ‚Äî but what do we do when we want to train a model from scratch? In these cases, using a tokenizer that was pretrained on a corpus from another domain or language is typically suboptimal. For example, a tokenizer that‚Äôs trained on an English corpus will perform poorly on a corpus of Japanese texts because the use of spaces and punctuation is very different in the two languages.\n \nIn this chapter, you will learn how to train a brand new tokenizer on a corpus of texts, so it can then be used to pretrain a language model. This will all be done with the help of the [ü§ó Tokenizers](https://github.com/huggingface/tokenizers) library, which provides the ‚Äúfast‚Äù tokenizers in the [ü§ó Transformers](https://github.com/huggingface/transformers) library. We‚Äôll take a close look at the features that this library provides, and explore how the fast tokenizers differ from the ‚Äúslow‚Äù versions.\n \nTopics we will cover include:\n \n- How to train a new tokenizer similar to the one used by a given checkpoint on a new corpus of texts\n- The special features of fast tokenizers\n- The differences between the three main subword tokenization algorithms used in NLP today\n- How to build a tokenizer from scratch with the ü§ó Tokenizers library and train it on some data\n \nThe techniques introduced in this chapter will prepare you for the section in [Chapter 7](/course/chapter7/6) where we look at creating a language model for Python source code. Let‚Äôs start by looking at what it means to ‚Äútrain‚Äù a tokenizer in the first place.",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter6/1",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter quiz Let‚Äôs test what you learned in this chapter! ### 1. When should you train a new tokenizer? When your dataset is similar to that used by an existing pretrained model, and you want to pretrain a new model When your dataset is similar to that used by an existing pretrained model, and you want to fine-tune a new model using this pretrained model When your dataset is different from the one used by an existing pretrained model, and you want to pretrain a new model When your dataset is different from the one used by an existing pretrained model, but you want to fine-tune a new model using this pretrained model ### 2. What is the advantage of using a generator of lists of texts compared to a list of lists of texts when using train_new_from_iterator() ? That's the only type the method `train_new_from_iterator()` accepts. You will avoid loading the whole dataset into memory at once. This will allow the ü§ó Tokenizers library to use multiprocessing. The tokenizer you train will generate better texts. ### 3. What are the advantages of using a ‚Äúfast‚Äù tokenizer? It can process inputs faster than a slow tokenizer when you batch lots of inputs together. Fast tokenizers always tokenize faster than their slow counterparts. It can apply padding and truncation. It has some additional features allowing you to map tokens to the span of text that created them. ### 4. How does the token-classification pipeline handle entities that span over several tokens? The entities with the same label are merged into one entity. There is a label for the beginning of an entity and a label for the continuation of an entity. In a given word, as long as the first token has the label of the entity, the whole word is considered labeled with that entity. When a token has the label of a given entity, any other following token with the same label is considered part of the same entity, unless it's labeled as the start of a new entity. ### 5. How does the question-answering pipeline handle long contexts? It doesn't really, as it truncates the long context at the maximum length accepted by the model. It splits the context into several parts and averages the results obtained. It splits the context into several parts (with overlap) and finds the maximum score for an answer in each part. It splits the context into several parts (without overlap, for efficiency) and finds the maximum score for an answer in each part. ### 6. What is normalization? It's any cleanup the tokenizer performs on the texts in the initial stages. It's a data augmentation technique that involves making the text more normal by removing rare words. It's the final post-processing step where the tokenizer adds the special tokens. It's when the embeddings are made with mean 0 and standard deviation 1, by subtracting the mean and dividing by the std. ### 7. What is pre-tokenization for a subword tokenizer? It's",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter6/10",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/10",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/10.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "It splits the context into several parts (without overlap, for efficiency) and finds the maximum score for an answer in each part. ### 6. What is normalization? It's any cleanup the tokenizer performs on the texts in the initial stages. It's a data augmentation technique that involves making the text more normal by removing rare words. It's the final post-processing step where the tokenizer adds the special tokens. It's when the embeddings are made with mean 0 and standard deviation 1, by subtracting the mean and dividing by the std. ### 7. What is pre-tokenization for a subword tokenizer? It's the step before the tokenization, where data augmentation (like random masking) is applied. It's the step before the tokenization, where the desired cleanup operations are applied to the text. It's the step before the tokenizer model is applied, to split the input into words. It's the step before the tokenizer model is applied, to split the input into tokens. ### 8. Select the sentences that apply to the BPE model of tokenization. BPE is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules. BPE is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it. BPE tokenizers learn merge rules by merging the pair of tokens that is the most frequent. A BPE tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts. BPE tokenizes words into subwords by splitting them into characters and then applying the merge rules. BPE tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text. ### 9. Select the sentences that apply to the WordPiece model of tokenization. WordPiece is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules. WordPiece is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it. WordPiece tokenizers learn merge rules by merging the pair of tokens that is the most frequent. A WordPiece tokenizer learns a merge rule by merging the pair of tokens that maximizes a score that privileges frequent pairs with less frequent individual parts. WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model. WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text. ### 10. Select the sentences that apply to the Unigram model of tokenization. Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules. Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it. Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus. Unigram adapts its vocabulary by keeping the most frequent subwords. Unigram tokenizes",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter6/10",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/10",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/10.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text. ### 10. Select the sentences that apply to the Unigram model of tokenization. Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules. Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it. Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus. Unigram adapts its vocabulary by keeping the most frequent subwords. Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model. Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules.",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter6/10",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/10",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/10.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Training a new tokenizer from an old one If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in [Chapter 2](/course/chapter2), we saw that most Transformer models use a *subword tokenization algorithm*. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus ‚Äî a process we call *training*. The exact rules that govern this training depend on the type of tokenizer used, and we‚Äôll go over the three main algorithms later in this chapter. > ‚ö†Ô∏è Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It‚Äôs randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It‚Äôs deterministic, meaning you always get the same results when training with the same algorithm on the same corpus. ## Assembling a corpus There‚Äôs a very simple API in ü§ó Transformers that you can use to train a new tokenizer with the same characteristics as an existing one: `AutoTokenizer.train_new_from_iterator()`. To see this in action, let‚Äôs say we want to train GPT-2 from scratch, but in a language other than English. Our first task will be to gather lots of data in that language in a training corpus. To provide examples everyone will be able to understand, we won‚Äôt use a language like Russian or Chinese here, but rather a specialized English language: Python code. The [ü§ó Datasets](https://github.com/huggingface/datasets) library can help us assemble a corpus of Python source code. We‚Äôll use the usual `load_dataset()` function to download and cache the [CodeSearchNet](https://huggingface.co/datasets/code_search_net) dataset. This dataset was created for the [CodeSearchNet challenge](https://wandb.ai/github/CodeSearchNet/benchmark) and contains millions of functions from open source libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset: ``` from datasets import load_dataset # This can take a few minutes to load, so grab a coffee or tea while you wait! raw_datasets = load_dataset(\"code_search_net\", \"python\") ``` We can have a look at the training split to see which columns we have access to: ``` raw_datasets[\"train\"] ``` ``` Dataset({ features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url' ], num_rows: 412178 }) ``` We can see the dataset separates docstrings from code and suggests a tokenization of both. Here.",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "libraries on GitHub in several programming languages. Here, we will load the Python part of this dataset: ``` from datasets import load_dataset # This can take a few minutes to load, so grab a coffee or tea while you wait! raw_datasets = load_dataset(\"code_search_net\", \"python\") ``` We can have a look at the training split to see which columns we have access to: ``` raw_datasets[\"train\"] ``` ``` Dataset({ features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url' ], num_rows: 412178 }) ``` We can see the dataset separates docstrings from code and suggests a tokenization of both. Here. we‚Äôll just use the `whole_func_string` column to train our tokenizer. We can look at an example of one these functions by indexing into the `train` split: ``` print(raw_datasets[\"train\"][123456][\"whole_func_string\"]) ``` which should print the following: ``` def handle_simple_responses( self, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK): \"\"\"Accepts normal responses from the device. Args: timeout_ms: Timeout in milliseconds to wait for each response. info_cb: Optional callback for text sent from the bootloader. Returns: OKAY packet's message. \"\"\" return self._accept_responses('OKAY', info_cb, timeout_ms=timeout_ms) ``` The first thing we need to do is transform the dataset into an *iterator* of lists of texts ‚Äî for instance, a list of list of texts. Using lists of texts will enable our tokenizer to go faster (training on batches of texts instead of processing individual texts one by one), and it should be an iterator if we want to avoid having everything in memory at once. If your corpus is huge, you will want to take advantage of the fact that ü§ó Datasets does not load everything into RAM but stores the elements of the dataset on disk. Doing the following would create a list of lists of 1,000 texts each, but would load everything in memory: ``` # Don't uncomment the following line unless your dataset is small! # training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)] ``` Using a Python generator, we can avoid Python loading anything into memory until it‚Äôs actually necessary. To create such a generator, you just to need to replace the brackets with parentheses: ``` training_corpus = ( raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000) ) ``` This line of code doesn‚Äôt fetch any elements of the dataset; it just creates an object you can use in a Python `for` loop. The texts will only be loaded when you need them (that is, when you‚Äôre at the step of the `for` loop that requires them), and only 1,000 texts at a time will be loaded. This way you won‚Äôt exhaust all your memory even if you are processing a huge dataset. The problem with a generator object is that it can only be used once. So, instead of this giving us the list of the first 10 digits twice: ``` gen = (i for i in range(10)) print(list(gen)) print(list(gen)) ``` we get them once and then an empty list: ``` [0, 1, 2, 3, 4, 5, 6, 7, 8,",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "need them (that is, when you‚Äôre at the step of the `for` loop that requires them), and only 1,000 texts at a time will be loaded. This way you won‚Äôt exhaust all your memory even if you are processing a huge dataset. The problem with a generator object is that it can only be used once. So, instead of this giving us the list of the first 10 digits twice: ``` gen = (i for i in range(10)) print(list(gen)) print(list(gen)) ``` we get them once and then an empty list: ``` [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [] ``` That‚Äôs why we define a function that returns a generator instead: ``` def get_training_corpus(): return ( raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000) ) training_corpus = get_training_corpus() ``` You can also define your generator inside a `for` loop by using the `yield` statement: ``` def get_training_corpus(): dataset = raw_datasets[\"train\"] for start_idx in range(0, len(dataset), 1000): samples = dataset[start_idx : start_idx + 1000] yield samples[\"whole_func_string\"] ``` which will produce the exact same generator as before, but allows you to use more complex logic than you can in a list comprehension. ## Training a new tokenizer Now that we have our corpus in the form of an iterator of batches of texts, we are ready to train a new tokenizer. To do this, we first need to load the tokenizer we want to pair with our model (here, GPT-2): ``` from transformers import AutoTokenizer old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") ``` Even though we are going to train a new tokenizer, it‚Äôs a good idea to do this to avoid starting entirely from scratch. This way, we won‚Äôt have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus. First let‚Äôs have a look at how this tokenizer would treat an example function: ``` example = '''def add_numbers(a, b): \"\"\"Add the two numbers `a` and `b`.\"\"\" return a + b''' tokens = old_tokenizer.tokenize(example) tokens ``` ``` ['def', 'ƒ†add', '_', 'n', 'umbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†\"\"\"', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`', '.\"', '\"\"', 'ƒä', 'ƒ†', 'ƒ†', 'ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b'] ``` This tokenizer has a few special symbols, like `ƒ†` and `ƒä`, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the `_` character. Let‚Äôs train a new tokenizer and see if it solves those issues. For this, we‚Äôll use the method `train_new_from_iterator()`: ``` tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "This tokenizer has a few special symbols, like `ƒ†` and `ƒä`, which denote spaces and newlines, respectively. As we can see, this is not too efficient: the tokenizer returns individual tokens for each space, when it could group together indentation levels (since having sets of four or eight spaces is going to be very common in code). It also split the function name a bit weirdly, not being used to seeing words with the `_` character. Let‚Äôs train a new tokenizer and see if it solves those issues. For this, we‚Äôll use the method `train_new_from_iterator()`: ``` tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000) ``` This command might take a bit of time if your corpus is very large, but for this dataset of 1.6 GB of texts it‚Äôs blazing fast (1 minute 16 seconds on an AMD Ryzen 9 3900X CPU with 12 cores). Note that `AutoTokenizer.train_new_from_iterator()` only works if the tokenizer you are using is a ‚Äúfast‚Äù tokenizer. As you‚Äôll see in the next section, the ü§ó Transformers library contains two types of tokenizers: some are written purely in Python and others (the fast ones) are backed by the ü§ó Tokenizers library, which is written in the [Rust](https://www.rust-lang.org) programming language. Python is the language most often used for data science and deep learning applications, but when anything needs to be parallelized to be fast, it has to be written in another language. For instance, the matrix multiplications that are at the core of the model computation are written in CUDA, an optimized C library for GPUs. Training a brand new tokenizer in pure Python would be excruciatingly slow, which is why we developed the ü§ó Tokenizers library. Note that just as you didn‚Äôt have to learn the CUDA language to be able to execute your model on a batch of inputs on a GPU, you won‚Äôt need to learn Rust to use a fast tokenizer. The ü§ó Tokenizers library provides Python bindings for many methods that internally call some piece of code in Rust; for example, to parallelize the training of your new tokenizer or, as we saw in [Chapter 3](/course/chapter3), the tokenization of a batch of inputs. Most of the Transformer models have a fast tokenizer available (there are some exceptions that you can check [here](https://huggingface.co/transformers/#supported-frameworks)), and the `AutoTokenizer` API always selects the fast tokenizer for you if it‚Äôs available. In the next section we‚Äôll take a look at some of the other special features fast tokenizers have, which will be really useful for tasks like token classification and question answering. Before diving into that, however, let‚Äôs try our brand new tokenizer on the previous example: ``` tokens = tokenizer.tokenize(example) tokens ``` ``` ['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†\"\"\"', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`.\"\"\"', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b'] ``` Here we again see the special symbols `ƒ†` and `ƒä` that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the other special features fast tokenizers have, which will be really useful for tasks like token classification and question answering. Before diving into that, however, let‚Äôs try our brand new tokenizer on the previous example: ``` tokens = tokenizer.tokenize(example) tokens ``` ``` ['def', 'ƒ†add', '_', 'numbers', '(', 'a', ',', 'ƒ†b', '):', 'ƒäƒ†ƒ†ƒ†', 'ƒ†\"\"\"', 'Add', 'ƒ†the', 'ƒ†two', 'ƒ†numbers', 'ƒ†`', 'a', '`', 'ƒ†and', 'ƒ†`', 'b', '`.\"\"\"', 'ƒäƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†a', 'ƒ†+', 'ƒ†b'] ``` Here we again see the special symbols `ƒ†` and `ƒä` that denote spaces and newlines, but we can also see that our tokenizer learned some tokens that are highly specific to a corpus of Python functions: for example, there is a `ƒäƒ†ƒ†ƒ†` token that represents an indentation, and a `ƒ†\"\"\"` token that represents the three quotes that start a docstring. The tokenizer also correctly split the function name on `_`. This is quite a compact representation; comparatively, using the plain English tokenizer on the same example will give us a longer sentence: ``` print(len(tokens)) print(len(old_tokenizer.tokenize(example))) ``` ``` 27 36 ``` Let‚Äôs look at another example: ``` example = \"\"\"class LinearLayer(): def __init__(self, input_size, output_size): self.weight = torch.randn(input_size, output_size) self.bias = torch.zeros(output_size) def __call__(self, x): return x @ self.weights + self.bias \"\"\" tokenizer.tokenize(example) ``` ``` ['class', 'ƒ†Linear', 'Layer', '():', 'ƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'init', '__(', 'self', ',', 'ƒ†input', '_', 'size', ',', 'ƒ†output', '_', 'size', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'weight', 'ƒ†=', 'ƒ†torch', '.', 'randn', '(', 'input', '_', 'size', ',', 'ƒ†output', '_', 'size', ')', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†self', '.', 'bias', 'ƒ†=', 'ƒ†torch', '.', 'zeros', '(', 'output', '_', 'size', ')', 'ƒäƒäƒ†ƒ†ƒ†', 'ƒ†def', 'ƒ†__', 'call', '__(', 'self', ',', 'ƒ†x', '):', 'ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†', 'ƒ†return', 'ƒ†x', 'ƒ†@', 'ƒ†self', '.', 'weights', 'ƒ†+', 'ƒ†self', '.', 'bias', 'ƒäƒ†ƒ†ƒ†ƒ†'] ``` In addition to the token corresponding to an indentation, here we can also see a token for a double indentation: `ƒäƒ†ƒ†ƒ†ƒ†ƒ†ƒ†ƒ†`. The special Python words like `class`, `init`, `call`, `self`, and `return` are each tokenized as one token, and we can see that as well as splitting on `_` and `.` the tokenizer correctly splits even camel-cased names: `LinearLayer` is tokenized as `[\"ƒ†Linear\", \"Layer\"]`. ## Saving the tokenizer To make sure we can use it later, we need to save our new tokenizer. Like for models, this is done with the `save_pretrained()` method: ``` tokenizer.save_pretrained(\"code-search-net-tokenizer\") ``` This will create a new folder named *code-search-net-tokenizer*, which will contain all the files the tokenizer needs to be reloaded. If you want to share this tokenizer with your colleagues and friends, you can upload it to the Hub by logging into your account. If you‚Äôre working in a notebook, there‚Äôs a convenience function to help you with this: ``` from huggingface_hub import notebook_login notebook_login() ``` This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` Once you‚Äôve logged in, you can push your tokenizer by executing the following command: ``` tokenizer.push_to_hub(\"code-search-net-tokenizer\") ``` This will create a new repository in your namespace with the name `code-search-net-tokenizer`,",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "colleagues and friends, you can upload it to the Hub by logging into your account. If you‚Äôre working in a notebook, there‚Äôs a convenience function to help you with this: ``` from huggingface_hub import notebook_login notebook_login() ``` This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` Once you‚Äôve logged in, you can push your tokenizer by executing the following command: ``` tokenizer.push_to_hub(\"code-search-net-tokenizer\") ``` This will create a new repository in your namespace with the name `code-search-net-tokenizer`, containing the tokenizer file. You can then load the tokenizer from anywhere with the `from_pretrained()` method: ``` # Replace \"huggingface-course\" below with your actual namespace to use your own tokenizer tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\") ``` You‚Äôre now all set for training a language model from scratch and fine-tuning it on your task at hand! We‚Äôll get to that in [Chapter 7](/course/chapter7), but first, in the rest of this chapter we‚Äôll take a closer look at fast tokenizers and explore in detail what actually happens when we call the method `train_new_from_iterator()`.",
    "metadata": {
      "title": "Training a new tokenizer from an old one",
      "url": "https://huggingface.co/learn/llm-course/chapter6/2",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/2",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Fast tokenizers‚Äô special powers In this section we will take a closer look at the capabilities of the tokenizers in ü§ó Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers ‚Äî especially those backed by the ü§ó Tokenizers library ‚Äî can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](/course/chapter1). In the following discussion, we will often make the distinction between ‚Äúslow‚Äù and ‚Äúfast‚Äù tokenizers. Slow tokenizers are those written in Python inside the ü§ó Transformers library, while the fast versions are the ones provided by ü§ó Tokenizers, which are written in Rust. If you remember the table from [Chapter 5](/course/chapter5/3) that reported how long it took a fast and a slow tokenizer to tokenize the Drug Review Dataset, you should have an idea of why we call them fast and slow: Fast tokenizer Slow tokenizer `batched=True` 10.8s 4min41s `batched=False` 59.2s 5min3s > ‚ö†Ô∏è When tokenizing a single sentence, you won‚Äôt always see a difference in speed between the slow and fast versions of the same tokenizer. In fact, the fast version might actually be slower! It‚Äôs only when tokenizing lots of texts in parallel at the same time that you will be able to clearly see the difference. ## Batch encoding The output of a tokenizer isn‚Äôt a simple Python dictionary; what we get is actually a special `BatchEncoding` object. It‚Äôs a subclass of a dictionary (which is why we were able to index into that result without any problem before), but with additional methods that are mostly used by fast tokenizers. Besides their parallelization capabilities, the key functionality of fast tokenizers is that they always keep track of the original span of texts the final tokens come from ‚Äî a feature we call *offset mapping*. This in turn unlocks features like mapping each word to the tokens it generated or mapping each character of the original text to the token it‚Äôs inside, and vice versa. Let‚Äôs take a look at an example: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" encoding = tokenizer(example) print(type(encoding)) ``` As mentioned previously, we get a `BatchEncoding` object in the tokenizer‚Äôs output: ``` <class 'transformers.tokenization_utils_base.BatchEncoding'> ``` Since the `AutoTokenizer` class picks a fast tokenizer by default, we can use the additional methods this `BatchEncoding` object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`: ``` tokenizer.is_fast ``` ``` True ``` or check the same attribute of our `encoding`: ``` encoding.is_fast ``` ``` True ``` Let‚Äôs see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens: ``` encoding.tokens() ``` ```",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "output: ``` <class 'transformers.tokenization_utils_base.BatchEncoding'> ``` Since the `AutoTokenizer` class picks a fast tokenizer by default, we can use the additional methods this `BatchEncoding` object provides. We have two ways to check if our tokenizer is a fast or a slow one. We can either check the attribute `is_fast` of the `tokenizer`: ``` tokenizer.is_fast ``` ``` True ``` or check the same attribute of our `encoding`: ``` encoding.is_fast ``` ``` True ``` Let‚Äôs see what a fast tokenizer enables us to do. First, we can access the tokens without having to convert the IDs back to tokens: ``` encoding.tokens() ``` ``` ['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]'] ``` In this case the token at index 5 is `##yl`, which is part of the word ‚ÄúSylvain‚Äù in the original sentence. We can also use the `word_ids()` method to get the index of the word each token comes from: ``` encoding.word_ids() ``` ``` [None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None] ``` We can see that the tokenizer‚Äôs special tokens `[CLS]` and `[SEP]` are mapped to `None`, and then each token is mapped to the word it originates from. This is especially useful to determine if a token is at the start of a word or if two tokens are in the same word. We could rely on the `##` prefix for that, but it only works for BERT-like tokenizers; this method works for any type of tokenizer as long as it‚Äôs a fast one. In the next chapter, we‚Äôll see how we can use this capability to apply the labels we have for each word properly to the tokens in tasks like named entity recognition (NER) and part-of-speech (POS) tagging. We can also use it to mask all the tokens coming from the same word in masked language modeling (a technique called *whole word masking*). > The notion of what a word is complicated. For instance, does ‚ÄúI‚Äôll‚Äù (a contraction of ‚ÄúI will‚Äù) count as one or two words? It actually depends on the tokenizer and the pre-tokenization operation it applies. Some tokenizers just split on spaces, so they will consider this as one word. Others use punctuation on top of spaces, so will consider it two words.‚úèÔ∏èTry it out!Create a tokenizer from thebert-base-casedandroberta-basecheckpoints and tokenize ‚Äù81s‚Äù with them. What do you observe? What are the word IDs? Similarly, there is a `sentence_ids()` method that we can use to map a token to the sentence it came from (though in this case, the `token_type_ids` returned by the tokenizer can give us the same information). Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the `word_ids()` method told us that `##yl` is part of the word at index 3, but which word is it in the sentence? We can",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "them. What do you observe? What are the word IDs? Similarly, there is a `sentence_ids()` method that we can use to map a token to the sentence it came from (though in this case, the `token_type_ids` returned by the tokenizer can give us the same information). Lastly, we can map any word or token to characters in the original text, and vice versa, via the `word_to_chars()` or `token_to_chars()` and `char_to_word()` or `char_to_token()` methods. For instance, the `word_ids()` method told us that `##yl` is part of the word at index 3, but which word is it in the sentence? We can find out like this: ``` start, end = encoding.word_to_chars(3) example[start:end] ``` ``` Sylvain ``` As we mentioned previously, this is all powered by the fact the fast tokenizer keeps track of the span of text each token comes from in a list of *offsets*. To illustrate their use, next we‚Äôll show you how to replicate the results of the `token-classification` pipeline manually. > ‚úèÔ∏èTry it out!Create your own example text and see if you can understand which tokens are associated with word ID, and also how to extract the character spans for a single word. For bonus points, try using two sentences as input and see if the sentence IDs make sense to you. ## Inside the token-classification pipeline In [Chapter 1](/course/chapter1) we got our first taste of applying NER ‚Äî where the task is to identify which parts of the text correspond to entities like persons, locations, or organizations ‚Äî with the ü§ó Transformers `pipeline()` function. Then, in [Chapter 2](/course/chapter2), we saw how a pipeline groups together the three stages necessary to get the predictions from a raw text: tokenization, passing the inputs through the model, and post-processing. The first two steps in the `token-classification` pipeline are the same as in any other pipeline, but the post-processing is a little more complex ‚Äî let‚Äôs see how! ### Getting the base results with the pipeline First, let‚Äôs grab a token classification pipeline so we can get some results to compare manually. The model used by default is [dbmdz/bert-large-cased-finetuned-conll03-english](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english); it performs NER on sentences: ``` from transformers import pipeline token_classifier = pipeline(\"token-classification\") token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` ``` [{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` The model properly identified each token generated by ‚ÄúSylvain‚Äù as a person, each token generated by ‚ÄúHugging Face‚Äù as an organization, and the token ‚ÄúBrooklyn‚Äù as a location.",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` The model properly identified each token generated by ‚ÄúSylvain‚Äù as a person, each token generated by ‚ÄúHugging Face‚Äù as an organization, and the token ‚ÄúBrooklyn‚Äù as a location. We can also ask the pipeline to group together the tokens that correspond to the same entity: ``` from transformers import pipeline token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\") token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` ``` [{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` The `aggregation_strategy` picked will change the scores computed for each grouped entity. With `\"simple\"` the score is just the mean of the scores of each token in the given entity: for instance, the score of ‚ÄúSylvain‚Äù is the mean of the scores we saw in the previous example for the tokens `S`, `##yl`, `##va`, and `##in`. Other strategies available are: - `\"first\"`, where the score of each entity is the score of the first token of that entity (so for ‚ÄúSylvain‚Äù it would be 0.993828, the score of the token `S`) - `\"max\"`, where the score of each entity is the maximum score of the tokens in that entity (so for ‚ÄúHugging Face‚Äù it would be 0.98879766, the score of ‚ÄúFace‚Äù) - `\"average\"`, where the score of each entity is the average of the scores of the words composing that entity (so for ‚ÄúSylvain‚Äù there would be no difference from the `\"simple\"` strategy, but ‚ÄúHugging Face‚Äù would have a score of 0.9819, the average of the scores for ‚ÄúHugging‚Äù, 0.975, and ‚ÄúFace‚Äù, 0.98879) Now let‚Äôs see how to obtain these results without using the `pipeline()` function! ### From inputs to predictions First we need to tokenize our input and pass it through the model. This is done exactly as in [Chapter 2](/course/chapter2); we instantiate the tokenizer and the model using the `AutoXxx` classes and then use them on our example: ``` from transformers import AutoTokenizer, AutoModelForTokenClassification model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) model = AutoModelForTokenClassification.from_pretrained(model_checkpoint) example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" inputs = tokenizer(example, return_tensors=\"pt\") outputs = model(**inputs) ``` Since we‚Äôre using `AutoModelForTokenClassification` here, we get one set of logits for each token in the input sequence: ``` print(inputs[\"input_ids\"].shape) print(outputs.logits.shape) ``` ``` torch.Size([1, 19]) torch.Size([1, 19, 9]) ``` We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9.",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "our example: ``` from transformers import AutoTokenizer, AutoModelForTokenClassification model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) model = AutoModelForTokenClassification.from_pretrained(model_checkpoint) example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\" inputs = tokenizer(example, return_tensors=\"pt\") outputs = model(**inputs) ``` Since we‚Äôre using `AutoModelForTokenClassification` here, we get one set of logits for each token in the input sequence: ``` print(inputs[\"input_ids\"].shape) print(outputs.logits.shape) ``` ``` torch.Size([1, 19]) torch.Size([1, 19, 9]) ``` We have a batch with 1 sequence of 19 tokens and the model has 9 different labels, so the output of the model has a shape of 1 x 19 x 9. Like for the text classification pipeline, we use a softmax function to convert those logits to probabilities, and we take the argmax to get predictions (note that we can take the argmax on the logits because the softmax does not change the order): ``` import torch probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist() predictions = outputs.logits.argmax(dim=-1)[0].tolist() print(predictions) ``` ``` [0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0] ``` The `model.config.id2label` attribute contains the mapping of indexes to labels that we can use to make sense of the predictions: ``` model.config.id2label ``` ``` {0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'} ``` As we saw earlier, there are 9 labels: `O` is the label for the tokens that are not in any named entity (it stands for ‚Äúoutside‚Äù), and we then have two labels for each type of entity (miscellaneous, person, organization, and location). The label `B-XXX` indicates the token is at the beginning of an entity `XXX` and the label `I-XXX` indicates the token is inside the entity `XXX`. For instance, in the current example we would expect our model to classify the token `S` as `B-PER` (beginning of a person entity) and the tokens `##yl`, `##va` and `##in` as `I-PER` (inside a person entity). You might think the model was wrong in this case as it gave the label `I-PER` to all four of these tokens, but that‚Äôs not entirely true. There are actually two formats for those `B-` and `I-` labels: *IOB1* and *IOB2*. The IOB2 format (in pink below), is the one we introduced whereas in the IOB1 format (in blue), the labels beginning with `B-` are only ever used to separate two adjacent entities of the same type. The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the `S` token. ![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg) ![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg) With this map, we are ready to reproduce (almost entirely) the results of the first pipeline ‚Äî we can just grab the score and label of each token that was not classified as `O`: ``` results = [] tokens = inputs.tokens() for idx, pred in enumerate(predictions): label = model.config.id2label[pred] if label != \"O\": results.append( {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]} ) print(results) ``` ``` [{'entity': 'I-PER', 'score':",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "The model we are using was fine-tuned on a dataset using that format, which is why it assigns the label `I-PER` to the `S` token. ![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg) ![IOB1 vs IOB2 format](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg) With this map, we are ready to reproduce (almost entirely) the results of the first pipeline ‚Äî we can just grab the score and label of each token that was not classified as `O`: ``` results = [] tokens = inputs.tokens() for idx, pred in enumerate(predictions): label = model.config.id2label[pred] if label != \"O\": results.append( {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]} ) print(results) ``` ``` [{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}] ``` This is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs: ``` inputs_with_offsets = tokenizer(example, return_offsets_mapping=True) inputs_with_offsets[\"offset_mapping\"] ``` ``` [(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32), (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)] ``` Each tuple is the span of text corresponding to each token, where `(0, 0)` is reserved for the special tokens. We saw before that the token at index 5 is `##yl`, which has `(12, 14)` as offsets here. If we grab the corresponding slice in our example: ``` example[12:14] ``` we get the proper span of text without the `##`: ``` yl ``` Using this, we can now complete the previous results: ``` results = [] inputs_with_offsets = tokenizer(example, return_offsets_mapping=True) tokens = inputs_with_offsets.tokens() offsets = inputs_with_offsets[\"offset_mapping\"] for idx, pred in enumerate(predictions): label = model.config.id2label[pred] if label != \"O\": start, end = offsets[idx] results.append( { \"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx], \"start\": start, \"end\": end, } ) print(results) ``` ``` [{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` This is the same as what we got from the first pipeline! ### Grouping entities Using the",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` This is the same as what we got from the first pipeline! ### Grouping entities Using the offsets to determine the start and end keys for each entity is handy, but that information isn‚Äôt strictly necessary. When we want to group the entities together, however, the offsets will save us a lot of messy code. For example, if we wanted to group together the tokens `Hu`, `##gging`, and `Face`, we could make special rules that say the first two should be attached while removing the `##`, and the `Face` should be added with a space since it does not begin with `##` ‚Äî but that would only work for this particular type of tokenizer. We would have to write another set of rules for a SentencePiece or a Byte-Pair-Encoding tokenizer (discussed later in this chapter). With the offsets, all that custom code goes away: we just can take the span in the original text that begins with the first token and ends with the last token. So, in the case of the tokens `Hu`, `##gging`, and `Face`, we should start at character 33 (the beginning of `Hu`) and end before character 45 (the end of `Face`): ``` example[33:45] ``` ``` Hugging Face ``` To write the code that post-processes the predictions while grouping entities, we will group together entities that are consecutive and labeled with `I-XXX`, except for the first one, which can be labeled as `B-XXX` or `I-XXX` (so, we stop grouping an entity when we get a `O`, a new type of entity, or a `B-XXX` that tells us an entity of the same type is starting): ``` import numpy as np results = [] inputs_with_offsets = tokenizer(example, return_offsets_mapping=True) tokens = inputs_with_offsets.tokens() offsets = inputs_with_offsets[\"offset_mapping\"] idx = 0 while idx < len(predictions): pred = predictions[idx] label = model.config.id2label[pred] if label != \"O\": # Remove the B- or I- label = label[2:] start, _ = offsets[idx] # Grab all the tokens labeled with I-label all_scores = [] while ( idx < len(predictions) and model.config.id2label[predictions[idx]] == f\"I-{label}\" ): all_scores.append(probabilities[idx][pred]) _, end = offsets[idx] idx += 1 # The score is the mean of all the scores of the tokens in that grouped entity score = np.mean(all_scores).item() word = example[start:end] results.append( { \"entity_group\": label, \"score\": score, \"word\": word, \"start\": start, \"end\": end, } ) idx += 1 print(results) ``` And we get the same results as with our second pipeline! ``` [{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face',",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Grab all the tokens labeled with I-label all_scores = [] while ( idx < len(predictions) and model.config.id2label[predictions[idx]] == f\"I-{label}\" ): all_scores.append(probabilities[idx][pred]) _, end = offsets[idx] idx += 1 # The score is the mean of all the scores of the tokens in that grouped entity score = np.mean(all_scores).item() word = example[start:end] results.append( { \"entity_group\": label, \"score\": score, \"word\": word, \"start\": start, \"end\": end, } ) idx += 1 print(results) ``` And we get the same results as with our second pipeline! ``` [{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` Another example of a task where these offsets are extremely useful is question answering. Diving into that pipeline, which we‚Äôll do in the next section, will also enable us to take a look at one last feature of the tokenizers in the ü§ó Transformers library: dealing with overflowing tokens when we truncate an input to a given length.",
    "metadata": {
      "title": "Fast tokenizers‚Äô special powers",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Fast tokenizers in the QA pipeline We will now dive into the `question-answering` pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you‚Äôre not interested in the question answering task. ## Using the question-answering pipeline As we saw in [Chapter 1](/course/chapter1), we can use the `question-answering` pipeline like this to get the answer to a question: ``` from transformers import pipeline question_answerer = pipeline(\"question-answering\") context = \"\"\" ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question = \"Which deep learning libraries back ü§ó Transformers?\" question_answerer(question=question, context=context) ``` ``` {'score': 0.97773, 'start': 78, 'end': 105, 'answer': 'Jax, PyTorch and TensorFlow'} ``` Unlike the other pipelines, which can‚Äôt truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it‚Äôs at the end: ``` long_context = \"\"\" ü§ó Transformers: State of the Art NLP ü§ó Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models: - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint: 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime: - Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs: - We provide examples for each architecture to reproduce the results published by its original authors. -",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime: - Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs: - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as consistently as possible. - Model files can be used independently of the library for quick experiments. ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question_answerer(question=question, context=long_context) ``` ``` {'score': 0.97149, 'start': 1892, 'end': 1919, 'answer': 'Jax, PyTorch and TensorFlow'} ``` Let‚Äôs see how it does all of this! ## Using a model for question answering Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the `question-answering` pipeline is [distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad) (the ‚Äúsquad‚Äù in the name comes from the dataset on which the model was fine-tuned; we‚Äôll talk more about the SQuAD dataset in [Chapter 7](/course/chapter7/7)): ``` from transformers import AutoTokenizer, AutoModelForQuestionAnswering model_checkpoint = \"distilbert-base-cased-distilled-squad\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) inputs = tokenizer(question, context, return_tensors=\"pt\") outputs = model(**inputs) ``` Note that we tokenize the question and the context as a pair, with the question first. ![An example of tokenization of question and context](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg) ![An example of tokenization of question and context](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg) Models for question answering work a little differently from the models we‚Äôve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don‚Äôt return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get: ``` start_logits = outputs.start_logits end_logits = outputs.end_logits print(start_logits.shape, end_logits.shape) ``` ``` torch.Size([1, 66]) torch.Size([1, 66]) ``` To convert those logits into probabilities, we will apply a softmax function ‚Äî but before that, we need to make sure we mask the indices that are not part of the context. Our input is `[CLS] question [SEP] context [SEP]`, so we need to mask the tokens of the question as well as the `[SEP]` token. We‚Äôll keep the `[CLS]` token, however, as some models use it to indicate that the answer is not in the context.",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "one input containing 66 tokens, we get: ``` start_logits = outputs.start_logits end_logits = outputs.end_logits print(start_logits.shape, end_logits.shape) ``` ``` torch.Size([1, 66]) torch.Size([1, 66]) ``` To convert those logits into probabilities, we will apply a softmax function ‚Äî but before that, we need to make sure we mask the indices that are not part of the context. Our input is `[CLS] question [SEP] context [SEP]`, so we need to mask the tokens of the question as well as the `[SEP]` token. We‚Äôll keep the `[CLS]` token, however, as some models use it to indicate that the answer is not in the context. Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use `-10000`: ``` import torch sequence_ids = inputs.sequence_ids() # Mask everything apart from the tokens of the context mask = [i != 1 for i in sequence_ids] # Unmask the [CLS] token mask[0] = False mask = torch.tensor(mask)[None] start_logits[mask] = -10000 end_logits[mask] = -10000 ``` Now that we have properly masked the logits corresponding to positions we don‚Äôt want to predict, we can apply the softmax: ``` start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0] end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0] ``` At this stage, we could take the argmax of the start and end probabilities ‚Äî but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible `start_index` and `end_index` where `start_index <= end_index`, then take the tuple `(start_index, end_index)` with the highest probability. Assuming the events ‚ÄúThe answer starts at `start_index`‚Äù and ‚ÄúThe answer ends at `end_index`‚Äù to be independent, the probability that the answer starts at `start_index` and ends at `end_index` is: start_probabilities[start_index]√óend_probabilities[end_index]\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]start_probabilities[start_index]√óend_probabilities[end_index] So, to compute all the scores, we just need to compute all the productsstart_probabilities[start_index]√óend_probabilities[end_index]\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]start_probabilities[start_index]√óend_probabilities[end_index] where `start_index <= end_index`. First let‚Äôs compute all the possible products: ``` scores = start_probabilities[:, None] * end_probabilities[None, :] ``` Then we‚Äôll mask the values where `start_index > end_index` by setting them to `0` (the other probabilities are all positive numbers). The `torch.triu()` function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us: ``` scores = torch.triu(scores) ``` Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division `//` and modulus `%` operations to get the `start_index` and `end_index`: ``` max_index = scores.argmax().item() start_index = max_index // scores.shape[1] end_index = max_index % scores.shape[1] print(scores[start_index, end_index]) ``` We‚Äôre not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section): ``` 0.97773 ``` > ‚úèÔ∏èTry it out!Compute the start and end indices for the five most likely answers. We have the `start_index` and `end_index` of the answer",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the index in the flattened tensor, we need to use the floor division `//` and modulus `%` operations to get the `start_index` and `end_index`: ``` max_index = scores.argmax().item() start_index = max_index // scores.shape[1] end_index = max_index % scores.shape[1] print(scores[start_index, end_index]) ``` We‚Äôre not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section): ``` 0.97773 ``` > ‚úèÔ∏èTry it out!Compute the start and end indices for the five most likely answers. We have the `start_index` and `end_index` of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task: ``` inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True) offsets = inputs_with_offsets[\"offset_mapping\"] start_char, _ = offsets[start_index] _, end_char = offsets[end_index] answer = context[start_char:end_char] ``` Now we just have to format everything to get our result: ``` result = { \"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": scores[start_index, end_index], } print(result) ``` ``` {'answer': 'Jax, PyTorch and TensorFlow', 'start': 78, 'end': 105, 'score': 0.97773} ``` Great! That‚Äôs the same as in our first example! > ‚úèÔ∏èTry it out!Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass intop_k=5when calling it. ## Handling long contexts If we try to tokenize the question and long context we used as an example previously, we‚Äôll get a number of tokens higher than the maximum length used in the `question-answering` pipeline (which is 384): ``` inputs = tokenizer(question, long_context) print(len(inputs[\"input_ids\"])) ``` ``` 461 ``` So, we‚Äôll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don‚Äôt want to truncate the question, only the context. Since the context is the second sentence, we‚Äôll use the `\"only_second\"` truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present: ``` inputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\") print(tokenizer.decode(inputs[\"input_ids\"])) ``` ``` \"\"\" [CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Transformers? [SEP] [UNK] Transformers : State of the Art NLP [UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting-edge NLP easier to use for everyone. [UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy-to-use state-of-the-art models: - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user-facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint: 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model's lifetime: - Train state-of-the-art models in 3 lines of code. - Move a single model between TF2.0/PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs: - We provide examples for each architecture to reproduce the results published by its original authors. - Model internal [SEP] \"\"\" ``` This means the model will have a hard time picking the correct answer. To fix this, the `question-answering` pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don‚Äôt split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks. We can have the tokenizer (fast or slow) do this for us by adding `return_overflowing_tokens=True`, and we can specify the overlap we want with the `stride` argument. Here is an example, using a smaller sentence: ``` sentence = \"This sentence is not too long but we are going to split it anyway.\" inputs = tokenizer( sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2 ) for ids in inputs[\"input_ids\"]: print(tokenizer.decode(ids)) ``` ``` '[CLS] This sentence is not [SEP]' '[CLS] is not too long [SEP]' '[CLS] too long but we [SEP]' '[CLS] but we are going [SEP]' '[CLS] are going to split [SEP]' '[CLS] to split it anyway [SEP]' '[CLS] it anyway. [SEP]' ``` As we can see, the sentence has been split into chunks in such a way that each entry in `inputs[\"input_ids\"]` has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries. Let‚Äôs take a closer look at",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is not [SEP]' '[CLS] is not too long [SEP]' '[CLS] too long but we [SEP]' '[CLS] but we are going [SEP]' '[CLS] are going to split [SEP]' '[CLS] to split it anyway [SEP]' '[CLS] it anyway. [SEP]' ``` As we can see, the sentence has been split into chunks in such a way that each entry in `inputs[\"input_ids\"]` has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries. Let‚Äôs take a closer look at the result of the tokenization: ``` print(inputs.keys()) ``` ``` dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping']) ``` As expected, we get input IDs and an attention mask. The last key, `overflow_to_sample_mapping`, is a map that tells us which sentence each of the results corresponds to ‚Äî here we have 7 results that all come from the (only) sentence we passed the tokenizer: ``` print(inputs[\"overflow_to_sample_mapping\"]) ``` ``` [0, 0, 0, 0, 0, 0, 0] ``` This is more useful when we tokenize several sentences together. For instance, this: ``` sentences = [ \"This sentence is not too long but we are going to split it anyway.\", \"This sentence is shorter but will still get split.\", ] inputs = tokenizer( sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2 ) print(inputs[\"overflow_to_sample_mapping\"]) ``` gets us: ``` [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] ``` which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence. Now let‚Äôs go back to our long context. By default the `question-answering` pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing `max_seq_len` and `stride` arguments when calling the pipeline). We will thus use those parameters when tokenizing. We‚Äôll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets: ``` inputs = tokenizer( question, long_context, stride=128, max_length=384, padding=\"longest\", truncation=\"only_second\", return_overflowing_tokens=True, return_offsets_mapping=True, ) ``` Those `inputs` will contain the input IDs and attention masks the model expects, as well as the offsets and the `overflow_to_sample_mapping` we just talked about. Since those two are not parameters used by the model, we‚Äôll pop them out of the `inputs` (and we won‚Äôt store the map, since it‚Äôs not useful here) before converting it to a tensor: ``` _ = inputs.pop(\"overflow_to_sample_mapping\") offsets = inputs.pop(\"offset_mapping\") inputs = inputs.convert_to_tensors(\"pt\") print(inputs[\"input_ids\"].shape) ``` ``` torch.Size([2, 384]) ``` Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits: ``` outputs = model(**inputs) start_logits = outputs.start_logits end_logits = outputs.end_logits print(start_logits.shape, end_logits.shape) ``` ``` torch.Size([2, 384]) torch.Size([2, 384]) ``` Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "won‚Äôt store the map, since it‚Äôs not useful here) before converting it to a tensor: ``` _ = inputs.pop(\"overflow_to_sample_mapping\") offsets = inputs.pop(\"offset_mapping\") inputs = inputs.convert_to_tensors(\"pt\") print(inputs[\"input_ids\"].shape) ``` ``` torch.Size([2, 384]) ``` Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits: ``` outputs = model(**inputs) start_logits = outputs.start_logits end_logits = outputs.end_logits print(start_logits.shape, end_logits.shape) ``` ``` torch.Size([2, 384]) torch.Size([2, 384]) ``` Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask): ``` sequence_ids = inputs.sequence_ids() # Mask everything apart from the tokens of the context mask = [i != 1 for i in sequence_ids] # Unmask the [CLS] token mask[0] = False # Mask all the [PAD] tokens mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0)) start_logits[mask] = -10000 end_logits[mask] = -10000 ``` Then we can use the softmax to convert our logits to probabilities: ``` start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1) end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1) ``` The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score: ``` candidates = [] for start_probs, end_probs in zip(start_probabilities, end_probabilities): scores = start_probs[:, None] * end_probs[None, :] idx = torch.triu(scores).argmax().item() start_idx = idx // scores.shape[1] end_idx = idx % scores.shape[1] score = scores[start_idx, end_idx].item() candidates.append((start_idx, end_idx, score)) print(candidates) ``` ``` [(0, 18, 0.33867), (173, 184, 0.97149)] ``` Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it‚Äôs interesting to see what the model has picked in the first chunk). > ‚úèÔ∏èTry it out!Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk). The `offsets` we grabbed earlier is actually a list of offsets, with one list per chunk of text: ``` for candidate, offset in zip(candidates, offsets): start_token, end_token, score = candidate start_char, _ = offset[start_token] _, end_char = offset[end_token] answer = long_context[start_char:end_char] result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score} print(result) ``` ``` {'answer': '\\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867} {'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149} ``` If we ignore the first result, we get the same result as our pipeline for this long context ‚Äî yay! > ‚úèÔ∏èTry it out!Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "start_char, _ = offset[start_token] _, end_char = offset[end_token] answer = long_context[start_char:end_char] result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score} print(result) ``` ``` {'answer': '\\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867} {'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149} ``` If we ignore the first result, we get the same result as our pipeline for this long context ‚Äî yay! > ‚úèÔ∏èTry it out!Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass intop_k=5when calling it. This concludes our deep dive into the tokenizer‚Äôs capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks.",
    "metadata": {
      "title": "Fast tokenizers in the QA pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter6/3b",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/3b",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/3b.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Normalization and pre-tokenization Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we‚Äôll first take a look at the preprocessing that each tokenizer applies to text. Here‚Äôs a high-level overview of the steps in the tokenization pipeline: ![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg) ![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg) Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: *normalization* and *pre-tokenization*. ## Normalization The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you‚Äôre familiar with [Unicode normalization](http://www.unicode.org/reports/tr15/) (such as NFC or NFKC), this is also something the tokenizer may apply. The ü§ó Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the ü§ó Tokenizers library: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") print(type(tokenizer.backend_tokenizer)) ``` ``` <class 'tokenizers.Tokenizer'> ``` The `normalizer` attribute of the `tokenizer` object has a `normalize_str()` method that we can use to see how the normalization is performed: ``` print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")) ``` ``` 'hello how are u?' ``` In this example, since we picked the `bert-base-uncased` checkpoint, the normalization applied lowercasing and removed the accents. > ‚úèÔ∏èTry it out!Load a tokenizer from thebert-base-casedcheckpoint and pass the same example to it. What are the main differences you can see between the cased and uncased versions of the tokenizer? ## Pre-tokenization As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That‚Äôs where the pre-tokenization step comes in. As we saw in [Chapter 2](/course/chapter2), a word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training. To see how a fast tokenizer performs pre-tokenization, we can use the `pre_tokenize_str()` method of the `pre_tokenizer` attribute of the `tokenizer` object: ``` tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are you?\") ``` ``` [('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))] ``` Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between `are` and `you` to account for that. Since we‚Äôre using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer: ``` tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are you?\") ``` it will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `ƒ†` symbol, enabling it to recover the original spaces if we decode the tokens: ``` [('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),",
    "metadata": {
      "title": "Normalization and pre-tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/4",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/4",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "`are` and `you` to account for that. Since we‚Äôre using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer: ``` tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are you?\") ``` it will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a `ƒ†` symbol, enabling it to recover the original spaces if we decode the tokens: ``` [('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)), ('?', (19, 20))] ``` Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space. For a last example, let‚Äôs have a look at the T5 tokenizer, which is based on the SentencePiece algorithm: ``` tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are you?\") ``` ``` [('‚ñÅHello,', (0, 6)), ('‚ñÅhow', (7, 10)), ('‚ñÅare', (11, 14)), ('‚ñÅyou?', (16, 20))] ``` Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before `Hello`) and ignored the double space between `are` and `you`. Now that we‚Äôve seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We‚Äôll begin with a quick look at the broadly widely applicable SentencePiece; then, over the next three sections, we‚Äôll examine how the three main algorithms used for subword tokenization work. ## SentencePiece [SentencePiece](https://github.com/google/sentencepiece) is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `‚ñÅ`. Used in conjunction with the Unigram algorithm (see [section 7](/course/chapter6/7)), it doesn‚Äôt even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese). The other main feature of SentencePiece is *reversible tokenization*: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the `_`s with spaces ‚Äî this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible. ## Algorithm overview In the following sections, we‚Äôll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here‚Äôs a quick overview of how they each work. Don‚Äôt hesitate to come back to this table after reading each of the next sections if it doesn‚Äôt make sense to you yet. Model BPE WordPiece Unigram Training Starts from a small vocabulary and learns rules to merge tokens Starts from a small vocabulary and learns rules to merge tokens Starts",
    "metadata": {
      "title": "Normalization and pre-tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/4",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/4",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "reversible. ## Algorithm overview In the following sections, we‚Äôll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here‚Äôs a quick overview of how they each work. Don‚Äôt hesitate to come back to this table after reading each of the next sections if it doesn‚Äôt make sense to you yet. Model BPE WordPiece Unigram Training Starts from a small vocabulary and learns rules to merge tokens Starts from a small vocabulary and learns rules to merge tokens Starts from a large vocabulary and learns rules to remove tokens Training step Merges the tokens corresponding to the most common pair Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus Learns Merge rules and a vocabulary Just a vocabulary A vocabulary with a score for each token Encoding Splits a word into characters and applies the merges learned during training Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word Finds the most likely split into tokens, using the scores learned during training Now let‚Äôs dive into BPE!",
    "metadata": {
      "title": "Normalization and pre-tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/4",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/4",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Byte-Pair Encoding tokenization Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It‚Äôs used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa. > üí° This section covers BPE in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let‚Äôs say our corpus uses these five words: ``` \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\" ``` The base vocabulary will then be `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token. That‚Äôs one reason why lots of NLP models are very bad at analyzing content with emojis, for instance. > The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don‚Äôt look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is calledbyte-level BPE. After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning *merges*, which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords. At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by ‚Äúpair,‚Äù here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step. Going back to our previous example, let‚Äôs assume the words had the following frequencies: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` meaning `\"hug\"` was present 10 times in the corpus, `\"pug\"` 5 times, `\"pun\"` 12 times, `\"bun\"` 4 times, and `\"hugs\"` 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens: ``` (\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5) ``` Then we look at pairs. The pair `(\"h\", \"u\")` is present in the words",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "frequencies: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` meaning `\"hug\"` was present 10 times in the corpus, `\"pug\"` 5 times, `\"pun\"` 12 times, `\"bun\"` 4 times, and `\"hugs\"` 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens: ``` (\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5) ``` Then we look at pairs. The pair `(\"h\", \"u\")` is present in the words `\"hug\"` and `\"hugs\"`, so 15 times total in the corpus. It‚Äôs not the most frequent pair, though: that honor belongs to `(\"u\", \"g\")`, which is present in `\"hug\"`, `\"pug\"`, and `\"hugs\"`, for a grand total of 20 times in the vocabulary. Thus, the first merge rule learned by the tokenizer is `(\"u\", \"g\") -> \"ug\"`, which means that `\"ug\"` will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this: ``` Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"] Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5) ``` Now we have some pairs that result in a token longer than two characters: the pair `(\"h\", \"ug\")`, for instance (present 15 times in the corpus). The most frequent pair at this stage is `(\"u\", \"n\")`, however, present 16 times in the corpus, so the second merge rule learned is `(\"u\", \"n\") -> \"un\"`. Adding that to the vocabulary and merging all existing occurrences leads us to: ``` Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"] Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5) ``` Now the most frequent pair is `(\"h\", \"ug\")`, so we learn the merge rule `(\"h\", \"ug\") -> \"hug\"`, which gives us our first three-letter token. After the merge, the corpus looks like this: ``` Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"] Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5) ``` And we continue like this until we reach the desired vocabulary size. > ‚úèÔ∏èNow your turn!What do you think the next merge rule will be? ## Tokenization algorithm Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps: 1. Normalization 2. Pre-tokenization 3. Splitting the words into individual characters 4. Applying the merge rules learned in order on those splits Let‚Äôs take the example we used during training, with the three merge rules learned: ``` (\"u\", \"g\") -> \"ug\" (\"u\", \"n\") -> \"un\" (\"h\", \"ug\") -> \"hug\" ``` The word `\"bug\"` will be tokenized as `[\"b\", \"ug\"]`. `\"mug\"`, however, will be tokenized as `[\"[UNK]\", \"ug\"]` since the letter `\"m\"` was not in the",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "rule will be? ## Tokenization algorithm Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps: 1. Normalization 2. Pre-tokenization 3. Splitting the words into individual characters 4. Applying the merge rules learned in order on those splits Let‚Äôs take the example we used during training, with the three merge rules learned: ``` (\"u\", \"g\") -> \"ug\" (\"u\", \"n\") -> \"un\" (\"h\", \"ug\") -> \"hug\" ``` The word `\"bug\"` will be tokenized as `[\"b\", \"ug\"]`. `\"mug\"`, however, will be tokenized as `[\"[UNK]\", \"ug\"]` since the letter `\"m\"` was not in the base vocabulary. Likewise, the word `\"thug\"` will be tokenized as `[\"[UNK]\", \"hug\"]`: the letter `\"t\"` is not in the base vocabulary, and applying the merge rules results first in `\"u\"` and `\"g\"` being merged and then `\"h\"` and `\"ug\"` being merged. > ‚úèÔ∏èNow your turn!How do you think the word\"unhug\"will be tokenized? ## Implementing BPE Now let‚Äôs take a look at an implementation of the BPE algorithm. This won‚Äôt be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better. First we need a corpus, so let‚Äôs create a simple one with a few sentences: ``` corpus = [ \"This is the Hugging Face Course.\", \"This chapter is about tokenization.\", \"This section shows several tokenizer algorithms.\", \"Hopefully, you will be able to understand how they are trained and generate tokens.\", ] ``` Next, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the `gpt2` tokenizer for the pre-tokenization: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") ``` Then we compute the frequencies of each word in the corpus as we do the pre-tokenization: ``` from collections import defaultdict word_freqs = defaultdict(int) for text in corpus: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) new_words = [word for word, offset in words_with_offsets] for word in new_words: word_freqs[word] += 1 print(word_freqs) ``` ``` defaultdict(int, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1, 'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1, 'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1, 'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1}) ``` The next step is to compute the base vocabulary, formed by all the characters used in the corpus: ``` alphabet = [] for word in word_freqs.keys(): for letter in word: if letter not in alphabet: alphabet.append(letter) alphabet.sort() print(alphabet) ``` ``` [ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†'] ``` We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is `\"<|endoftext|>\"`: ``` vocab =",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "step is to compute the base vocabulary, formed by all the characters used in the corpus: ``` alphabet = [] for word in word_freqs.keys(): for letter in word: if letter not in alphabet: alphabet.append(letter) alphabet.sort() print(alphabet) ``` ``` [ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†'] ``` We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is `\"<|endoftext|>\"`: ``` vocab = [\"<|endoftext|>\"] + alphabet.copy() ``` We now need to split each word into individual characters, to be able to start training: ``` splits = {word: [c for c in word] for word in word_freqs.keys()} ``` Now that we are ready for training, let‚Äôs write a function that computes the frequency of each pair. We‚Äôll need to use this at each step of the training: ``` def compute_pair_freqs(splits): pair_freqs = defaultdict(int) for word, freq in word_freqs.items(): split = splits[word] if len(split) == 1: continue for i in range(len(split) - 1): pair = (split[i], split[i + 1]) pair_freqs[pair] += freq return pair_freqs ``` Let‚Äôs have a look at a part of this dictionary after the initial splits: ``` pair_freqs = compute_pair_freqs(splits) for i, key in enumerate(pair_freqs.keys()): print(f\"{key}: {pair_freqs[key]}\") if i >= 5: break ``` ``` ('T', 'h'): 3 ('h', 'i'): 3 ('i', 's'): 5 ('ƒ†', 'i'): 2 ('ƒ†', 't'): 7 ('t', 'h'): 3 ``` Now, finding the most frequent pair only takes a quick loop: ``` best_pair = \"\" max_freq = None for pair, freq in pair_freqs.items(): if max_freq is None or max_freq < freq: best_pair = pair max_freq = freq print(best_pair, max_freq) ``` ``` ('ƒ†', 't') 7 ``` So the first merge to learn is `('ƒ†', 't') -> 'ƒ†t'`, and we add `'ƒ†t'` to the vocabulary: ``` merges = {(\"ƒ†\", \"t\"): \"ƒ†t\"} vocab.append(\"ƒ†t\") ``` To continue, we need to apply that merge in our `splits` dictionary. Let‚Äôs write another function for this: ``` def merge_pair(a, b, splits): for word in word_freqs: split = splits[word] if len(split) == 1: continue i = 0 while i < len(split) - 1: if split[i] == a and split[i + 1] == b: split = split[:i] + [a + b] + split[i + 2 :] else: i += 1 splits[word] = split return splits ``` And we can have a look at the result of the first merge: ``` splits = merge_pair(\"ƒ†\", \"t\", splits) print(splits[\"ƒ†trained\"]) ``` ``` ['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd'] ``` Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 50: ``` vocab_size = 50 while len(vocab) < vocab_size: pair_freqs = compute_pair_freqs(splits) best_pair = \"\" max_freq = None for pair, freq in pair_freqs.items(): if max_freq is None or max_freq < freq: best_pair = pair max_freq = freq splits = merge_pair(*best_pair, splits) merges[best_pair] = best_pair[0] + best_pair[1] vocab.append(best_pair[0] +",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "can have a look at the result of the first merge: ``` splits = merge_pair(\"ƒ†\", \"t\", splits) print(splits[\"ƒ†trained\"]) ``` ``` ['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd'] ``` Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 50: ``` vocab_size = 50 while len(vocab) < vocab_size: pair_freqs = compute_pair_freqs(splits) best_pair = \"\" max_freq = None for pair, freq in pair_freqs.items(): if max_freq is None or max_freq < freq: best_pair = pair max_freq = freq splits = merge_pair(*best_pair, splits) merges[best_pair] = best_pair[0] + best_pair[1] vocab.append(best_pair[0] + best_pair[1]) ``` As a result, we‚Äôve learned 19 merge rules (the initial vocabulary had a size of 31 ‚Äî 30 characters in the alphabet, plus the special token): ``` print(merges) ``` ``` {('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok', ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the', ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'} ``` And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges: ``` print(vocab) ``` ``` ['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se', 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni'] ``` > üí° Usingtrain_new_from_iterator()on the same corpus won‚Äôt result in the exact same vocabulary. This is because when there is a choice of the most frequent pair, we selected the first one encountered, while the ü§ó Tokenizers library selects the first one based on its inner IDs. To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned: ``` def tokenize(text): pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text) pre_tokenized_text = [word for word, offset in pre_tokenize_result] splits = [[l for l in word] for word in pre_tokenized_text] for pair, merge in merges.items(): for idx, split in enumerate(splits): i = 0 while i < len(split) - 1: if split[i] == pair[0] and split[i + 1] == pair[1]: split = split[:i] + [merge] + split[i + 2 :] else: i += 1 splits[idx] = split return sum(splits, []) ``` We can try this on any text composed of characters in the alphabet: ``` tokenize(\"This is not a token.\") ``` ``` ['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.'] ``` > ‚ö†Ô∏è Our implementation will throw an error if there is an unknown character since we didn‚Äôt do anything to handle them. GPT-2 doesn‚Äôt actually have an unknown token (it‚Äôs impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[]) ``` We can try this on any text composed of characters in the alphabet: ``` tokenize(\"This is not a token.\") ``` ``` ['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.'] ``` > ‚ö†Ô∏è Our implementation will throw an error if there is an unknown character since we didn‚Äôt do anything to handle them. GPT-2 doesn‚Äôt actually have an unknown token (it‚Äôs impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we‚Äôve left the details out. That‚Äôs it for the BPE algorithm! Next, we‚Äôll have a look at WordPiece.",
    "metadata": {
      "title": "Byte-Pair Encoding tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/5",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/5",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# WordPiece tokenization WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It‚Äôs very similar to BPE in terms of the training, but the actual tokenization is done differently. > üí° This section covers WordPiece in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm > ‚ö†Ô∏è Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate. Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this: ``` w ##o ##r ##d ``` Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula: score=(freq_of_pair)/(freq_of_first_element√ófreq_of_second_element)\\mathrm{score} = (\\mathrm{freq\\_of\\_pair}) / (\\mathrm{freq\\_of\\_first\\_element} \\times \\mathrm{freq\\_of\\_second\\_element})score=(freq_of_pair)/(freq_of_first_element√ófreq_of_second_element) By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won‚Äôt necessarily merge `(\"un\", \"##able\")` even if that pair occurs very frequently in the vocabulary, because the two pairs `\"un\"` and `\"##able\"` will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like `(\"hu\", \"##gging\")` will probably be merged faster (assuming the word ‚Äúhugging‚Äù appears often in the vocabulary) since `\"hu\"` and `\"##gging\"` are likely to be less frequent individually. Let‚Äôs look at the same vocabulary we used in the BPE training example: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` The splits here will be: ``` (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5) ``` so the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget about special tokens for now). The most frequent pair is `(\"##u\", \"##g\")` (present 20 times), but the individual frequency of `\"##u\"` is very high, so its score is not the highest (it‚Äôs 1 / 36). All pairs with a `\"##u\"` actually have that same score (1 / 36), so the best score goes to the pair `(\"##g\", \"##s\")` ‚Äî the only one without a `\"##u\"` ‚Äî at 1 / 20,",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5) ``` so the initial vocabulary will be `[\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"]` (if we forget about special tokens for now). The most frequent pair is `(\"##u\", \"##g\")` (present 20 times), but the individual frequency of `\"##u\"` is very high, so its score is not the highest (it‚Äôs 1 / 36). All pairs with a `\"##u\"` actually have that same score (1 / 36), so the best score goes to the pair `(\"##g\", \"##s\")` ‚Äî the only one without a `\"##u\"` ‚Äî at 1 / 20, and the first merge learned is `(\"##g\", \"##s\") -> (\"##gs\")`. Note that when we merge, we remove the `##` between the two tokens, so we add `\"##gs\"` to the vocabulary and apply the merge in the words of the corpus: ``` Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"] Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5) ``` At this point, `\"##u\"` is in all the possible pairs, so they all end up with the same score. Let‚Äôs say that in this case, the first pair is merged, so `(\"h\", \"##u\") -> \"hu\"`. This takes us to: ``` Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"] Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5) ``` Then the next best score is shared by `(\"hu\", \"##g\")` and `(\"hu\", \"##gs\")` (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged: ``` Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"] Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5) ``` and we continue like this until we reach the desired vocabulary size. > ‚úèÔ∏èNow your turn!What will the next merge rule be? ## Tokenization algorithm Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`. With BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different. As another example, let‚Äôs see how the word `\"bugs\"` would be tokenized. `\"b\"` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\"b\", \"##ugs\"]`. Then `\"##u\"` is the longest subword starting at the beginning of `\"##ugs\"` that is in",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`. With BPE, we would have applied the merges learned in order and tokenized this as `[\"hu\", \"##gs\"]`, so the encoding is different. As another example, let‚Äôs see how the word `\"bugs\"` would be tokenized. `\"b\"` is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get `[\"b\", \"##ugs\"]`. Then `\"##u\"` is the longest subword starting at the beginning of `\"##ugs\"` that is in the vocabulary, so we split there and get `[\"b\", \"##u, \"##gs\"]`. Finally, `\"##gs\"` is in the vocabulary, so this last list is the tokenization of `\"bugs\"`. When the tokenization gets to a stage where it‚Äôs not possible to find a subword in the vocabulary, the whole word is tokenized as unknown ‚Äî so, for instance, `\"mug\"` would be tokenized as `[\"[UNK]\"]`, as would `\"bum\"` (even if we can begin with `\"b\"` and `\"##u\"`, `\"##m\"` is not the vocabulary, and the resulting tokenization will just be `[\"[UNK]\"]`, not `[\"b\", \"##u\", \"[UNK]\"]`). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown. > ‚úèÔ∏èNow your turn!How will the word\"pugs\"be tokenized? ## Implementing WordPiece Now let‚Äôs take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won‚Äôt able to use this on a big corpus. We will use the same corpus as in the BPE example: ``` corpus = [ \"This is the Hugging Face Course.\", \"This chapter is about tokenization.\", \"This section shows several tokenizer algorithms.\", \"Hopefully, you will be able to understand how they are trained and generate tokens.\", ] ``` First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the `bert-base-cased` tokenizer for the pre-tokenization: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") ``` Then we compute the frequencies of each word in the corpus as we do the pre-tokenization: ``` from collections import defaultdict word_freqs = defaultdict(int) for text in corpus: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) new_words = [word for word, offset in words_with_offsets] for word in new_words: word_freqs[word] += 1 word_freqs ``` ``` defaultdict( int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1}) ``` As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by `##`: ``` alphabet = [] for word in word_freqs.keys(): if word[0] not in alphabet: alphabet.append(word[0]) for letter in word[1:]: if f\"##{letter}\" not in alphabet: alphabet.append(f\"##{letter}\")",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "1, 'about': 1, 'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1, ',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1, 'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1}) ``` As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by `##`: ``` alphabet = [] for word in word_freqs.keys(): if word[0] not in alphabet: alphabet.append(word[0]) for letter in word[1:]: if f\"##{letter}\" not in alphabet: alphabet.append(f\"##{letter}\") alphabet.sort() alphabet print(alphabet) ``` ``` ['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y'] ``` We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it‚Äôs the list `[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]`: ``` vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy() ``` Next we need to split each word, with all the letters that are not the first prefixed by `##`: ``` splits = { word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys() } ``` Now that we are ready for training, let‚Äôs write a function that computes the score of each pair. We‚Äôll need to use this at each step of the training: ``` def compute_pair_scores(splits): letter_freqs = defaultdict(int) pair_freqs = defaultdict(int) for word, freq in word_freqs.items(): split = splits[word] if len(split) == 1: letter_freqs[split[0]] += freq continue for i in range(len(split) - 1): pair = (split[i], split[i + 1]) letter_freqs[split[i]] += freq pair_freqs[pair] += freq letter_freqs[split[-1]] += freq scores = { pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) for pair, freq in pair_freqs.items() } return scores ``` Let‚Äôs have a look at a part of this dictionary after the initial splits: ``` pair_scores = compute_pair_scores(splits) for i, key in enumerate(pair_scores.keys()): print(f\"{key}: {pair_scores[key]}\") if i >= 5: break ``` ``` ('T', '##h'): 0.125 ('##h', '##i'): 0.03409090909090909 ('##i', '##s'): 0.02727272727272727 ('i', '##s'): 0.1 ('t', '##h'): 0.03571428571428571 ('##h', '##e'): 0.011904761904761904 ``` Now, finding the pair with the best score only takes a quick loop: ``` best_pair = \"\" max_score = None for pair, score in pair_scores.items(): if max_score is None or max_score < score: best_pair = pair max_score = score print(best_pair, max_score) ``` ``` ('a', '##b') 0.2 ``` So the first merge to learn is `('a', '##b') -> 'ab'`, and we add `'ab'` to the vocabulary: ``` vocab.append(\"ab\") ``` To continue, we need to apply that merge in our `splits` dictionary. Let‚Äôs write another function for this: ``` def merge_pair(a, b, splits): for word in word_freqs: split = splits[word] if len(split) == 1: continue i = 0 while i < len(split) - 1: if split[i] == a and split[i + 1] == b: merge = a + b[2:] if",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "None or max_score < score: best_pair = pair max_score = score print(best_pair, max_score) ``` ``` ('a', '##b') 0.2 ``` So the first merge to learn is `('a', '##b') -> 'ab'`, and we add `'ab'` to the vocabulary: ``` vocab.append(\"ab\") ``` To continue, we need to apply that merge in our `splits` dictionary. Let‚Äôs write another function for this: ``` def merge_pair(a, b, splits): for word in word_freqs: split = splits[word] if len(split) == 1: continue i = 0 while i < len(split) - 1: if split[i] == a and split[i + 1] == b: merge = a + b[2:] if b.startswith(\"##\") else a + b split = split[:i] + [merge] + split[i + 2 :] else: i += 1 splits[word] = split return splits ``` And we can have a look at the result of the first merge: ``` splits = merge_pair(\"a\", \"##b\", splits) splits[\"about\"] ``` ``` ['ab', '##o', '##u', '##t'] ``` Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 70: ``` vocab_size = 70 while len(vocab) < vocab_size: scores = compute_pair_scores(splits) best_pair, max_score = \"\", None for pair, score in scores.items(): if max_score is None or max_score < score: best_pair = pair max_score = score splits = merge_pair(*best_pair, splits) new_token = ( best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1] ) vocab.append(new_token) ``` We can then look at the generated vocabulary: ``` print(vocab) ``` ``` ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut'] ``` As we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster. > üí° Usingtrain_new_from_iterator()on the same corpus won‚Äôt result in the exact same vocabulary. This is because the ü§ó Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead. To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text: ``` def encode_word(word): tokens = [] while len(word) > 0: i = len(word) while i > 0 and word[:i] not in vocab: i -= 1 if i == 0: return [\"[UNK]\"] tokens.append(word[:i]) word = word[i:] if len(word) > 0: word = f\"##{word}\" return tokens ``` Let‚Äôs test it on one word that‚Äôs in the vocabulary, and another that isn‚Äôt: ``` print(encode_word(\"Hugging\")) print(encode_word(\"HOgging\")) ``` ``` ['Hugg', '##i',",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text: ``` def encode_word(word): tokens = [] while len(word) > 0: i = len(word) while i > 0 and word[:i] not in vocab: i -= 1 if i == 0: return [\"[UNK]\"] tokens.append(word[:i]) word = word[i:] if len(word) > 0: word = f\"##{word}\" return tokens ``` Let‚Äôs test it on one word that‚Äôs in the vocabulary, and another that isn‚Äôt: ``` print(encode_word(\"Hugging\")) print(encode_word(\"HOgging\")) ``` ``` ['Hugg', '##i', '##n', '##g'] ['[UNK]'] ``` Now, let‚Äôs write a function that tokenizes a text: ``` def tokenize(text): pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text) pre_tokenized_text = [word for word, offset in pre_tokenize_result] encoded_words = [encode_word(word) for word in pre_tokenized_text] return sum(encoded_words, []) ``` We can try it on any text: ``` tokenize(\"This is the Hugging Face course!\") ``` ``` ['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]'] ``` That‚Äôs it for the WordPiece algorithm! Now let‚Äôs take a look at Unigram.",
    "metadata": {
      "title": "WordPiece tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/6",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/6",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Unigram tokenization The Unigram algorithm is used in combination with [SentencePiece](https://huggingface.co/papers/1808.06226), which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet. SentencePiece addresses the fact that not all languages use spaces to separate words. Instead, SentencePiece treats the input as a raw input stream which includes the space in the set of characters to use. Then it can use the Unigram algorithm to construct the appropriate vocabulary. > üí° This section covers Unigram in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm. ## Training algorithm Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size. At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are ‚Äúless needed‚Äù and are the best candidates for removal. This is all a very costly operation, so we don‚Äôt just remove the single symbol associated with the lowest loss increase, but theppp (\\(p\\) being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the desired size. Note that we never remove the base characters, to make sure any word can be tokenized. Now, this is still a bit vague: the main part of the algorithm is to compute a loss over the corpus and see how it changes when we remove some tokens from the vocabulary, but we haven‚Äôt explained how to do this yet. This step relies on the tokenization algorithm of a Unigram model, so we‚Äôll dive into this next. We‚Äôll reuse the corpus from the previous examples: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` and for this example, we will take all strict substrings for the initial vocabulary : ``` [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"] ``` ## Tokenization algorithm A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It‚Äôs the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 1,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` and for this example, we will take all strict substrings for the initial vocabulary : ``` [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"] ``` ## Tokenization algorithm A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It‚Äôs the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we would always predict the most common token. The probability of a given token is its frequency (the number of times we find it) in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary (to make sure the probabilities sum up to 1). For instance, `\"ug\"` is present in `\"hug\"`, `\"pug\"`, and `\"hugs\"`, so it has a frequency of 20 in our corpus. Here are the frequencies of all the possible subwords in the vocabulary: ``` (\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16) (\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5) ``` So, the sum of all frequencies is 210, and the probability of the subword `\"ug\"` is thus 20/210. > ‚úèÔ∏èNow your turn!Write the code to compute the frequencies above and double-check that the results shown are correct, as well as the total sum. Now, to tokenize a given word, we look at all the possible segmentations into tokens and compute the probability of each according to the Unigram model. Since all tokens are considered independent, this probability is just the product of the probability of each token. For instance, the tokenization `[\"p\", \"u\", \"g\"]` of `\"pug\"` has the probability: P([‚Äò‚Äòp\",‚Äò‚Äòu\",‚Äò‚Äòg\"])=P(‚Äò‚Äòp\")√óP(‚Äò‚Äòu\")√óP(‚Äò‚Äòg\")=5210√ó36210√ó20210=0.000389P([``p\", ``u\", ``g\"]) = P(``p\") \\times P(``u\") \\times P(``g\") = \\frac{5}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.000389P([‚Äò‚Äòp\",‚Äò‚Äòu\",‚Äò‚Äòg\"])=P(‚Äò‚Äòp\")√óP(‚Äò‚Äòu\")√óP(‚Äò‚Äòg\")=2105‚Äã√ó21036‚Äã√ó21020‚Äã=0.000389 Comparatively, the tokenization `[\"pu\", \"g\"]` has the probability: P([‚Äò‚Äòpu\",‚Äò‚Äòg\"])=P(‚Äò‚Äòpu\")√óP(‚Äò‚Äòg\")=5210√ó20210=0.0022676P([``pu\", ``g\"]) = P(``pu\") \\times P(``g\") = \\frac{5}{210} \\times \\frac{20}{210} = 0.0022676P([‚Äò‚Äòpu\",‚Äò‚Äòg\"])=P(‚Äò‚Äòpu\")√óP(‚Äò‚Äòg\")=2105‚Äã√ó21020‚Äã=0.0022676 so that one is way more likely. In general, tokenizations with the least tokens possible will have the highest probability (because of that division by 210 repeated for each token), which corresponds to what we want intuitively: to split a word into the least number of tokens possible. The tokenization of a word with the Unigram model is then the tokenization with the highest probability. In the example of `\"pug\"`, here are the probabilities we would get for each possible segmentation: ``` [\"p\", \"u\", \"g\"] : 0.000389 [\"p\", \"ug\"] : 0.0022676 [\"pu\", \"g\"] : 0.0022676 ``` So, `\"pug\"` would be tokenized as `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]`, depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare). In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it‚Äôs going to",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 2,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "a word with the Unigram model is then the tokenization with the highest probability. In the example of `\"pug\"`, here are the probabilities we would get for each possible segmentation: ``` [\"p\", \"u\", \"g\"] : 0.000389 [\"p\", \"ug\"] : 0.0022676 [\"pu\", \"g\"] : 0.0022676 ``` So, `\"pug\"` would be tokenized as `[\"p\", \"ug\"]` or `[\"pu\", \"g\"]`, depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare). In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it‚Äôs going to be a bit harder. There is a classic algorithm used for this, called the *Viterbi algorithm*. Essentially, we can build a graph to detect the possible segmentations of a given word by saying there is a branch from character *a* to character *b* if the subword from *a* to *b* is in the vocabulary, and attribute to that branch the probability of the subword. To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end. Let‚Äôs take a look at an example using our vocabulary and the word `\"unhug\"`. For each position, the subwords with the best scores ending there are the following: ``` Character 0 (u): \"u\" (score 0.171429) Character 1 (n): \"un\" (score 0.076191) Character 2 (h): \"un\" \"h\" (score 0.005442) Character 3 (u): \"un\" \"hu\" (score 0.005442) Character 4 (g): \"un\" \"hug\" (score 0.005442) ``` Thus `\"unhug\"` would be tokenized as `[\"un\", \"hug\"]`. > ‚úèÔ∏èNow your turn!Determine the tokenization of the word\"huggun\", and its score. ## Back to training Now that we have seen how the tokenization works, we can dive a little more deeply into the loss used during training. At any given stage, this loss is computed by tokenizing every word in the corpus, using the current vocabulary and the Unigram model determined by the frequencies of each token in the corpus (as seen before). Each word in the corpus has a score, and the loss is the negative log likelihood of those scores ‚Äî that is, the sum for all the words in the corpus of all the `-log(P(word))`. Let‚Äôs go back to our example with the following corpus: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` The tokenization of each word with their respective scores is: ``` \"hug\": [\"hug\"] (score 0.071428) \"pug\": [\"pu\", \"g\"] (score 0.007710) \"pun\": [\"pu\", \"n\"] (score 0.006168) \"bun\": [\"bu\", \"n\"] (score 0.001451) \"hugs\": [\"hug\", \"s\"] (score 0.001701) ``` So the loss is: ``` 10 * (-log(0.071428)) +",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 3,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in the corpus has a score, and the loss is the negative log likelihood of those scores ‚Äî that is, the sum for all the words in the corpus of all the `-log(P(word))`. Let‚Äôs go back to our example with the following corpus: ``` (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) ``` The tokenization of each word with their respective scores is: ``` \"hug\": [\"hug\"] (score 0.071428) \"pug\": [\"pu\", \"g\"] (score 0.007710) \"pun\": [\"pu\", \"n\"] (score 0.006168) \"bun\": [\"bu\", \"n\"] (score 0.001451) \"hugs\": [\"hug\", \"s\"] (score 0.001701) ``` So the loss is: ``` 10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8 ``` Now we need to compute how removing each token affects the loss. This is rather tedious, so we‚Äôll just do it for two tokens here and save the whole process for when we have code to help us. In this (very) particular case, we had two equivalent tokenizations of all the words: as we saw earlier, for example, `\"pug\"` could be tokenized `[\"p\", \"ug\"]` with the same score. Thus, removing the `\"pu\"` token from the vocabulary will give the exact same loss. On the other hand, removing `\"hug\"` will make the loss worse, because the tokenization of `\"hug\"` and `\"hugs\"` will become: ``` \"hug\": [\"hu\", \"g\"] (score 0.006802) \"hugs\": [\"hu\", \"gs\"] (score 0.001701) ``` These changes will cause the loss to rise by: ``` - 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5 ``` Therefore, the token `\"pu\"` will probably be removed from the vocabulary, but not `\"hug\"`. ## Implementing Unigram Now let‚Äôs implement everything we‚Äôve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better. We will use the same corpus as before as an example: ``` corpus = [ \"This is the Hugging Face Course.\", \"This chapter is about tokenization.\", \"This section shows several tokenizer algorithms.\", \"Hopefully, you will be able to understand how they are trained and generate tokens.\", ] ``` This time, we will use `xlnet-base-cased` as our model: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\") ``` Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus: ``` from collections import defaultdict word_freqs = defaultdict(int) for text in corpus: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) new_words = [word for word, offset in words_with_offsets] for word in new_words: word_freqs[word] += 1 word_freqs ``` Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won‚Äôt be able to tokenize every word), but for the bigger substrings we‚Äôll only keep the most common ones, so we sort them by frequency: ``` char_freqs = defaultdict(int) subwords_freqs = defaultdict(int) for word, freq in word_freqs.items(): for i in range(len(word)): char_freqs[word[i]] += freq #",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 4,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "defaultdict(int) for text in corpus: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) new_words = [word for word, offset in words_with_offsets] for word in new_words: word_freqs[word] += 1 word_freqs ``` Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won‚Äôt be able to tokenize every word), but for the bigger substrings we‚Äôll only keep the most common ones, so we sort them by frequency: ``` char_freqs = defaultdict(int) subwords_freqs = defaultdict(int) for word, freq in word_freqs.items(): for i in range(len(word)): char_freqs[word[i]] += freq # Loop through the subwords of length at least 2 for j in range(i + 2, len(word) + 1): subwords_freqs[word[i:j]] += freq # Sort subwords by frequency sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True) sorted_subwords[:10] ``` ``` [('‚ñÅt', 7), ('is', 5), ('er', 5), ('‚ñÅa', 5), ('‚ñÅto', 4), ('to', 4), ('en', 4), ('‚ñÅT', 3), ('‚ñÅTh', 3), ('‚ñÅThi', 3)] ``` We group the characters with the best subwords to arrive at an initial vocabulary of size 300: ``` token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)] token_freqs = {token: freq for token, freq in token_freqs} ``` > üí° SentencePiece uses a more efficient algorithm called Enhanced Suffix Array (ESA) to create the initial vocabulary. Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it‚Äôs more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model: ``` from math import log total_sum = sum([freq for token, freq in token_freqs.items()]) model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()} ``` Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named `best_segmentations`. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated. Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`. Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word: ``` def encode_word(word, model): best_segmentations = [{\"start\": 0, \"score\": 1}] + [ {\"start\": None, \"score\": None} for _ in",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 5,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`. Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word: ``` def encode_word(word, model): best_segmentations = [{\"start\": 0, \"score\": 1}] + [ {\"start\": None, \"score\": None} for _ in range(len(word)) ] for start_idx in range(len(word)): # This should be properly filled by the previous steps of the loop best_score_at_start = best_segmentations[start_idx][\"score\"] for end_idx in range(start_idx + 1, len(word) + 1): token = word[start_idx:end_idx] if token in model and best_score_at_start is not None: score = model[token] + best_score_at_start # If we have found a better segmentation ending at end_idx, we update if ( best_segmentations[end_idx][\"score\"] is None or best_segmentations[end_idx][\"score\"] > score ): best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score} segmentation = best_segmentations[-1] if segmentation[\"score\"] is None: # We did not find a tokenization of the word -> unknown return [\"<unk>\"], None score = segmentation[\"score\"] start = segmentation[\"start\"] end = len(word) tokens = [] while start != 0: tokens.insert(0, word[start:end]) next_start = best_segmentations[start][\"start\"] end = start start = next_start tokens.insert(0, word[start:end]) return tokens, score ``` We can already try our initial model on some words: ``` print(encode_word(\"Hopefully\", model)) print(encode_word(\"This\", model)) ``` ``` (['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402) (['This'], 6.288267030694535) ``` Now it‚Äôs easy to compute the loss of the model on the corpus! ``` def compute_loss(model): loss = 0 for word, freq in word_freqs.items(): _, word_loss = encode_word(word, model) loss += freq * word_loss return loss ``` We can check it works on the model we have: ``` compute_loss(model) ``` ``` 413.10377642940875 ``` Computing the scores for each token is not very hard either; we just have to compute the loss for the models obtained by deleting each token: ``` import copy def compute_scores(model): scores = {} model_loss = compute_loss(model) for token, score in model.items(): # We always keep tokens of length 1 if len(token) == 1: continue model_without_token = copy.deepcopy(model) _ = model_without_token.pop(token) scores[token] = compute_loss(model_without_token) - model_loss return scores ``` We can try it on a given token: ``` scores = compute_scores(model) print(scores[\"ll\"]) print(scores[\"his\"]) ``` Since `\"ll\"` is used in the tokenization of `\"Hopefully\"`, and removing it will probably make us use the token `\"l\"` twice instead, we expect it will have a positive loss. `\"his\"` is only used inside the word `\"This\"`, which is tokenized as itself, so we expect it to have a zero loss. Here are the results: ``` 6.376412403623874 0.0 ``` > üí° This approach is very inefficient, so SentencePiece uses an approximation of the loss of the model without token X: instead of starting from scratch, it just replaces token X by its segmentation in the vocabulary that is left. This way,",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 6,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is used in the tokenization of `\"Hopefully\"`, and removing it will probably make us use the token `\"l\"` twice instead, we expect it will have a positive loss. `\"his\"` is only used inside the word `\"This\"`, which is tokenized as itself, so we expect it to have a zero loss. Here are the results: ``` 6.376412403623874 0.0 ``` > üí° This approach is very inefficient, so SentencePiece uses an approximation of the loss of the model without token X: instead of starting from scratch, it just replaces token X by its segmentation in the vocabulary that is left. This way, all the scores can be computed at once at the same time as the model loss. With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size: ``` percent_to_remove = 0.1 while len(model) > 100: scores = compute_scores(model) sorted_scores = sorted(scores.items(), key=lambda x: x[1]) # Remove percent_to_remove tokens with the lowest scores. for i in range(int(len(model) * percent_to_remove)): _ = token_freqs.pop(sorted_scores[i][0]) total_sum = sum([freq for token, freq in token_freqs.items()]) model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()} ``` Then, to tokenize some text, we just need to apply the pre-tokenization and then use our `encode_word()` function: ``` def tokenize(text, model): words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text) pre_tokenized_text = [word for word, offset in words_with_offsets] encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text] return sum(encoded_words, []) tokenize(\"This is the Hugging Face course.\", model) ``` ``` ['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅHugging', '‚ñÅFace', '‚ñÅ', 'c', 'ou', 'r', 's', 'e', '.'] ``` > The XLNetTokenizer uses SentencePiece which is why the\"_\"character is included. To decode with SentencePiece, concatenate all the tokens and replace\"_\"with a space. That‚Äôs it for Unigram! Hopefully by now you‚Äôre feeling like an expert in all things tokenizer. In the next section, we will delve into the building blocks of the ü§ó Tokenizers library, and show you how you can use them to build your own tokenizer.",
    "metadata": {
      "title": "Unigram tokenization",
      "url": "https://huggingface.co/learn/llm-course/chapter6/7",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/7",
      "part": 7,
      "total_parts": 7,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building a tokenizer, block by block As we‚Äôve seen in the previous sections, tokenization comprises several steps: - Normalization (any cleanup of the text that is deemed necessary, such as removing spaces or accents, Unicode normalization, etc.) - Pre-tokenization (splitting the input into words) - Running the input through the model (using the pre-tokenized words to produce a sequence of tokens) - Post-processing (adding the special tokens of the tokenizer, generating the attention mask and token type IDs) As a reminder, here‚Äôs another look at the overall process: ![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg) ![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg) The ü§ó Tokenizers library has been built to provide several options for each of those steps, which you can mix and match together. In this section we‚Äôll see how we can build a tokenizer from scratch, as opposed to training a new tokenizer from an old one as we did in [section 2](/course/chapter6/2). You‚Äôll then be able to build any kind of tokenizer you can think of! More precisely, the library is built around a central `Tokenizer` class with the building blocks regrouped in submodules: - `normalizers` contains all the possible types of `Normalizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/normalizers)). - `pre_tokenizers` contains all the possible types of `PreTokenizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers)). - `models` contains the various types of `Model` you can use, like `BPE`, `WordPiece`, and `Unigram` (complete list [here](https://huggingface.co/docs/tokenizers/api/models)). - `trainers` contains all the different types of `Trainer` you can use to train your model on a corpus (one per type of model; complete list [here](https://huggingface.co/docs/tokenizers/api/trainers)). - `post_processors` contains the various types of `PostProcessor` you can use (complete list [here](https://huggingface.co/docs/tokenizers/api/post-processors)). - `decoders` contains the various types of `Decoder` you can use to decode the outputs of tokenization (complete list [here](https://huggingface.co/docs/tokenizers/components#decoders)). You can find the whole list of building blocks [here](https://huggingface.co/docs/tokenizers/components). ## Acquiring a corpus To train our new tokenizer, we will use a small corpus of text (so the examples run fast). The steps for acquiring the corpus are similar to the ones we took at the [beginning of this chapter](/course/chapter6/2), but this time we‚Äôll use the [WikiText-2](https://huggingface.co/datasets/wikitext) dataset: ``` from datasets import load_dataset dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\") def get_training_corpus(): for i in range(0, len(dataset), 1000): yield dataset[i : i + 1000][\"text\"] ``` The function `get_training_corpus()` is a generator that will yield batches of 1,000 texts, which we will use to train the tokenizer. ü§ó Tokenizers can also be trained on text files directly. Here‚Äôs how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally: ``` with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f: for i in range(len(dataset)): f.write(dataset[i][\"text\"] + \"\\n\") ``` Next we‚Äôll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Let‚Äôs start with BERT! ## Building a WordPiece tokenizer from scratch To build a tokenizer with the ü§ó Tokenizers library, we start by instantiating a `Tokenizer`",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 1,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "trained on text files directly. Here‚Äôs how we can generate a text file containing all the texts/inputs from WikiText-2 that we can use locally: ``` with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f: for i in range(len(dataset)): f.write(dataset[i][\"text\"] + \"\\n\") ``` Next we‚Äôll show you how to build your own BERT, GPT-2, and XLNet tokenizers, block by block. That will give us an example of each of the three main tokenization algorithms: WordPiece, BPE, and Unigram. Let‚Äôs start with BERT! ## Building a WordPiece tokenizer from scratch To build a tokenizer with the ü§ó Tokenizers library, we start by instantiating a `Tokenizer` object with a `model`, then set its `normalizer`, `pre_tokenizer`, `post_processor`, and `decoder` attributes to the values we want. For this example, we‚Äôll create a `Tokenizer` with a WordPiece model: ``` from tokenizers import ( decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer, ) tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\")) ``` We have to specify the `unk_token` so the model knows what to return when it encounters characters it hasn‚Äôt seen before. Other arguments we can set here include the `vocab` of our model (we‚Äôre going to train the model, so we don‚Äôt need to set this) and `max_input_chars_per_word`, which specifies a maximum length for each word (words longer than the value passed will be split). The first step of tokenization is normalization, so let‚Äôs begin with that. Since BERT is widely used, there is a `BertNormalizer` with the classic options we can set for BERT: `lowercase` and `strip_accents`, which are self-explanatory; `clean_text` to remove all control characters and replace repeating spaces with a single one; and `handle_chinese_chars`, which places spaces around Chinese characters. To replicate the `bert-base-uncased` tokenizer, we can just set this normalizer: ``` tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True) ``` Generally speaking, however, when building a new tokenizer you won‚Äôt have access to such a handy normalizer already implemented in the ü§ó Tokenizers library ‚Äî so let‚Äôs see how to create the BERT normalizer by hand. The library provides a `Lowercase` normalizer and a `StripAccents` normalizer, and you can compose several normalizers using a `Sequence`: ``` tokenizer.normalizer = normalizers.Sequence( [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()] ) ``` We‚Äôre also using an `NFD` Unicode normalizer, as otherwise the `StripAccents` normalizer won‚Äôt properly recognize the accented characters and thus won‚Äôt strip them out. As we‚Äôve seen before, we can use the `normalize_str()` method of the `normalizer` to check out the effects it has on a given text: ``` print(tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")) ``` ``` hello how are u? ``` > To go furtherIf you test the two versions of the previous normalizers on a string containing the unicode characteru\"\\u0085\"you will surely notice that these two normalizers are not exactly equivalent. > To not over-complicate the version withnormalizers.Sequencetoo much , we haven‚Äôt included the Regex replacements that theBertNormalizerrequires when theclean_textargument is set toTrue- which is the default behavior. But don‚Äôt worry: it is possible to get exactly the same normalization without using the handyBertNormalizerby adding twonormalizers.Replace‚Äôs to the normalizers sequence. Next is the pre-tokenization step. Again, there is a prebuilt `BertPreTokenizer`",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 2,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "are √º?\")) ``` ``` hello how are u? ``` > To go furtherIf you test the two versions of the previous normalizers on a string containing the unicode characteru\"\\u0085\"you will surely notice that these two normalizers are not exactly equivalent. > To not over-complicate the version withnormalizers.Sequencetoo much , we haven‚Äôt included the Regex replacements that theBertNormalizerrequires when theclean_textargument is set toTrue- which is the default behavior. But don‚Äôt worry: it is possible to get exactly the same normalization without using the handyBertNormalizerby adding twonormalizers.Replace‚Äôs to the normalizers sequence. Next is the pre-tokenization step. Again, there is a prebuilt `BertPreTokenizer` that we can use: ``` tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer() ``` Or we can build it from scratch: ``` tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() ``` Note that the `Whitespace` pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation: ``` tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\") ``` ``` [('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)), ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))] ``` If you only want to split on whitespace, you should use the `WhitespaceSplit` pre-tokenizer instead: ``` pre_tokenizer = pre_tokenizers.WhitespaceSplit() pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\") ``` ``` [(\"Let's\", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))] ``` Like with normalizers, you can use a `Sequence` to compose several pre-tokenizers: ``` pre_tokenizer = pre_tokenizers.Sequence( [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()] ) pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\") ``` ``` [('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)), ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))] ``` The next step in the tokenization pipeline is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a `WordPieceTrainer`. The main thing to remember when instantiating a trainer in ü§ó Tokenizers is that you need to pass it all the special tokens you intend to use ‚Äî otherwise it won‚Äôt add them to the vocabulary, since they are not in the training corpus: ``` special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens) ``` As well as specifying the `vocab_size` and `special_tokens`, we can set the `min_frequency` (the number of times a token must appear to be included in the vocabulary) or change the `continuing_subword_prefix` (if we want to use something different from `##`). To train our model using the iterator we defined earlier, we just have to execute this command: ``` tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer) ``` We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with an empty `WordPiece` beforehand): ``` tokenizer.model = models.WordPiece(unk_token=\"[UNK]\") tokenizer.train([\"wikitext-2.txt\"], trainer=trainer) ``` In both cases, we can then test the tokenizer on a text by calling the `encode()` method: ``` encoding = tokenizer.encode(\"Let's test this tokenizer.\") print(encoding.tokens) ``` ``` ['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.'] ``` The `encoding` obtained is an",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 3,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to use something different from `##`). To train our model using the iterator we defined earlier, we just have to execute this command: ``` tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer) ``` We can also use text files to train our tokenizer, which would look like this (we reinitialize the model with an empty `WordPiece` beforehand): ``` tokenizer.model = models.WordPiece(unk_token=\"[UNK]\") tokenizer.train([\"wikitext-2.txt\"], trainer=trainer) ``` In both cases, we can then test the tokenizer on a text by calling the `encode()` method: ``` encoding = tokenizer.encode(\"Let's test this tokenizer.\") print(encoding.tokens) ``` ``` ['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.'] ``` The `encoding` obtained is an `Encoding`, which contains all the necessary outputs of the tokenizer in its various attributes: `ids`, `type_ids`, `tokens`, `offsets`, `attention_mask`, `special_tokens_mask`, and `overflowing`. The last step in the tokenization pipeline is post-processing. We need to add the `[CLS]` token at the beginning and the `[SEP]` token at the end (or after each sentence, if we have a pair of sentences). We will use a `TemplateProcessor` for this, but first we need to know the IDs of the `[CLS]` and `[SEP]` tokens in the vocabulary: ``` cls_token_id = tokenizer.token_to_id(\"[CLS]\") sep_token_id = tokenizer.token_to_id(\"[SEP]\") print(cls_token_id, sep_token_id) ``` ``` (2, 3) ``` To write the template for the `TemplateProcessor`, we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon. The classic BERT template is thus defined as follows: ``` tokenizer.post_processor = processors.TemplateProcessing( single=f\"[CLS]:0 $A:0 [SEP]:0\", pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\", special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)], ) ``` Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs. Once this is added, going back to our previous example will give: ``` encoding = tokenizer.encode(\"Let's test this tokenizer.\") print(encoding.tokens) ``` ``` ['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]'] ``` And on a pair of sentences, we get the proper result: ``` encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\") print(encoding.tokens) print(encoding.type_ids) ``` ``` ['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]'] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] ``` We‚Äôve almost finished building this tokenizer from scratch ‚Äî the last step is to include a decoder: ``` tokenizer.decoder = decoders.WordPiece(prefix=\"##\") ``` Let‚Äôs test it on our previous `encoding`: ``` tokenizer.decode(encoding.ids) ``` ``` \"let's test this tokenizer... on a pair of sentences.\" ``` Great! We can save our tokenizer in a single JSON file like this: ``` tokenizer.save(\"tokenizer.json\") ``` We can then reload that file in a `Tokenizer` object with the `from_file()` method: ``` new_tokenizer = Tokenizer.from_file(\"tokenizer.json\") ``` To use this tokenizer",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 4,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] ``` We‚Äôve almost finished building this tokenizer from scratch ‚Äî the last step is to include a decoder: ``` tokenizer.decoder = decoders.WordPiece(prefix=\"##\") ``` Let‚Äôs test it on our previous `encoding`: ``` tokenizer.decode(encoding.ids) ``` ``` \"let's test this tokenizer... on a pair of sentences.\" ``` Great! We can save our tokenizer in a single JSON file like this: ``` tokenizer.save(\"tokenizer.json\") ``` We can then reload that file in a `Tokenizer` object with the `from_file()` method: ``` new_tokenizer = Tokenizer.from_file(\"tokenizer.json\") ``` To use this tokenizer in ü§ó Transformers, we have to wrap it in a `PreTrainedTokenizerFast`. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, `BertTokenizerFast`). If you apply this lesson to build a brand new tokenizer, you will have to use the first option. To wrap the tokenizer in a `PreTrainedTokenizerFast`, we can either pass the tokenizer we built as a `tokenizer_object` or pass the tokenizer file we saved as `tokenizer_file`. The key thing to remember is that we have to manually set all the special tokens, since that class can‚Äôt infer from the `tokenizer` object which token is the mask token, the `[CLS]` token, etc.: ``` from transformers import PreTrainedTokenizerFast wrapped_tokenizer = PreTrainedTokenizerFast( tokenizer_object=tokenizer, # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively unk_token=\"[UNK]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\", ) ``` If you are using a specific tokenizer class (like `BertTokenizerFast`), you will only need to specify the special tokens that are different from the default ones (here, none): ``` from transformers import BertTokenizerFast wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer) ``` You can then use this tokenizer like any other ü§ó Transformers tokenizer. You can save it with the `save_pretrained()` method, or upload it to the Hub with the `push_to_hub()` method. Now that we‚Äôve seen how to build a WordPiece tokenizer, let‚Äôs do the same for a BPE tokenizer. We‚Äôll go a bit faster since you know all the steps, and only highlight the differences. ## Building a BPE tokenizer from scratch Let‚Äôs now build a GPT-2 tokenizer. Like for the BERT tokenizer, we start by initializing a `Tokenizer` with a BPE model: ``` tokenizer = Tokenizer(models.BPE()) ``` Also like for BERT, we could initialize this model with a vocabulary if we had one (we would need to pass the `vocab` and `merges` in this case), but since we will train from scratch, we don‚Äôt need to do that. We also don‚Äôt need to specify an `unk_token` because GPT-2 uses byte-level BPE, which doesn‚Äôt require it. GPT-2 does not use a normalizer, so we skip that step and go directly to the pre-tokenization: ``` tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) ``` The option we added to `ByteLevel` here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before: ``` tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\") ``` ``` [('Let', (0,",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 5,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "this case), but since we will train from scratch, we don‚Äôt need to do that. We also don‚Äôt need to specify an `unk_token` because GPT-2 uses byte-level BPE, which doesn‚Äôt require it. GPT-2 does not use a normalizer, so we skip that step and go directly to the pre-tokenization: ``` tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) ``` The option we added to `ByteLevel` here is to not add a space at the beginning of a sentence (which is the default otherwise). We can have a look at the pre-tokenization of an example text like before: ``` tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\") ``` ``` [('Let', (0, 3)), (\"'s\", (3, 5)), ('ƒ†test', (5, 10)), ('ƒ†pre', (10, 14)), ('-', (14, 15)), ('tokenization', (15, 27)), ('!', (27, 28))] ``` Next is the model, which needs training. For GPT-2, the only special token is the end-of-text token: ``` trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"]) tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer) ``` Like with the `WordPieceTrainer`, as well as the `vocab_size` and `special_tokens`, we can specify the `min_frequency` if we want to, or if we have an end-of-word suffix (like `</w>`), we can set it with `end_of_word_suffix`. This tokenizer can also be trained on text files: ``` tokenizer.model = models.BPE() tokenizer.train([\"wikitext-2.txt\"], trainer=trainer) ``` Let‚Äôs have a look at the tokenization of a sample text: ``` encoding = tokenizer.encode(\"Let's test this tokenizer.\") print(encoding.tokens) ``` ``` ['L', 'et', \"'\", 's', 'ƒ†test', 'ƒ†this', 'ƒ†to', 'ken', 'izer', '.'] ``` We apply the byte-level post-processing for the GPT-2 tokenizer as follows: ``` tokenizer.post_processor = processors.ByteLevel(trim_offsets=False) ``` The `trim_offsets = False` option indicates to the post-processor that we should leave the offsets of tokens that begin with ‚Äòƒ†‚Äô as they are: this way the start of the offsets will point to the space before the word, not the first character of the word (since the space is technically part of the token). Let‚Äôs have a look at the result with the text we just encoded, where `'ƒ†test'` is the token at index 4: ``` sentence = \"Let's test this tokenizer.\" encoding = tokenizer.encode(sentence) start, end = encoding.offsets[4] sentence[start:end] ``` ``` ' test' ``` Finally, we add a byte-level decoder: ``` tokenizer.decoder = decoders.ByteLevel() ``` and we can double-check it works properly: ``` tokenizer.decode(encoding.ids) ``` ``` \"Let's test this tokenizer.\" ``` Great! Now that we‚Äôre done, we can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast` or `GPT2TokenizerFast` if we want to use it in ü§ó Transformers: ``` from transformers import PreTrainedTokenizerFast wrapped_tokenizer = PreTrainedTokenizerFast( tokenizer_object=tokenizer, bos_token=\"<|endoftext|>\", eos_token=\"<|endoftext|>\", ) ``` or: ``` from transformers import GPT2TokenizerFast wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer) ``` As the last example, we‚Äôll show you how to build a Unigram tokenizer from scratch. ## Building a Unigram tokenizer from scratch Let‚Äôs now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a `Tokenizer` with a Unigram model: ``` tokenizer = Tokenizer(models.Unigram()) ``` Again, we could initialize this model with a vocabulary if we had one. For the normalization, XLNet uses a few replacements (which come from SentencePiece): ``` from tokenizers import Regex tokenizer.normalizer = normalizers.Sequence(",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 6,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "PreTrainedTokenizerFast wrapped_tokenizer = PreTrainedTokenizerFast( tokenizer_object=tokenizer, bos_token=\"<|endoftext|>\", eos_token=\"<|endoftext|>\", ) ``` or: ``` from transformers import GPT2TokenizerFast wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer) ``` As the last example, we‚Äôll show you how to build a Unigram tokenizer from scratch. ## Building a Unigram tokenizer from scratch Let‚Äôs now build an XLNet tokenizer. Like for the previous tokenizers, we start by initializing a `Tokenizer` with a Unigram model: ``` tokenizer = Tokenizer(models.Unigram()) ``` Again, we could initialize this model with a vocabulary if we had one. For the normalization, XLNet uses a few replacements (which come from SentencePiece): ``` from tokenizers import Regex tokenizer.normalizer = normalizers.Sequence( [ normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"'), normalizers.NFKD(), normalizers.StripAccents(), normalizers.Replace(Regex(\" {2,}\"), \" \"), ] ) ``` This replaces `‚Äú` and `‚Äù` with `‚Äù` and any sequence of two or more spaces with a single space, as well as removing the accents in the texts to tokenize. The pre-tokenizer to use for any SentencePiece tokenizer is `Metaspace`: ``` tokenizer.pre_tokenizer = pre_tokenizers.Metaspace() ``` We can have a look at the pre-tokenization of an example text like before: ``` tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\") ``` ``` [(\"‚ñÅLet's\", (0, 5)), ('‚ñÅtest', (5, 10)), ('‚ñÅthe', (10, 14)), ('‚ñÅpre-tokenizer!', (14, 29))] ``` Next is the model, which needs training. XLNet has quite a few special tokens: ``` special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"] trainer = trainers.UnigramTrainer( vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\" ) tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer) ``` A very important argument not to forget for the `UnigramTrainer` is the `unk_token`. We can also pass along other arguments specific to the Unigram algorithm, such as the `shrinking_factor` for each step where we remove tokens (defaults to 0.75) or the `max_piece_length` to specify the maximum length of a given token (defaults to 16). This tokenizer can also be trained on text files: ``` tokenizer.model = models.Unigram() tokenizer.train([\"wikitext-2.txt\"], trainer=trainer) ``` Let‚Äôs have a look at the tokenization of a sample text: ``` encoding = tokenizer.encode(\"Let's test this tokenizer.\") print(encoding.tokens) ``` ``` ['‚ñÅLet', \"'\", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.'] ``` A peculiarity of XLNet is that it puts the `<cls>` token at the end of the sentence, with a type ID of 2 (to distinguish it from the other tokens). It‚Äôs padding on the left, as a result. We can deal with all the special tokens and token type IDs with a template, like for BERT, but first we have to get the IDs of the `<cls>` and `<sep>` tokens: ``` cls_token_id = tokenizer.token_to_id(\"<cls>\") sep_token_id = tokenizer.token_to_id(\"<sep>\") print(cls_token_id, sep_token_id) ``` ``` 0 1 ``` The template looks like this: ``` tokenizer.post_processor = processors.TemplateProcessing( single=\"$A:0 <sep>:0 <cls>:2\", pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\", special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)], ) ``` And we can test it works by encoding a pair of sentences: ``` encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\") print(encoding.tokens) print(encoding.type_ids) ``` ``` ['‚ñÅLet', \"'\", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.', '.', '.', '<sep>', '‚ñÅ', 'on', '‚ñÅ', 'a', '‚ñÅpair', '‚ñÅof', '‚ñÅsentence', 's', '!', '<sep>', '<cls>'] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 7,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "= tokenizer.token_to_id(\"<cls>\") sep_token_id = tokenizer.token_to_id(\"<sep>\") print(cls_token_id, sep_token_id) ``` ``` 0 1 ``` The template looks like this: ``` tokenizer.post_processor = processors.TemplateProcessing( single=\"$A:0 <sep>:0 <cls>:2\", pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\", special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)], ) ``` And we can test it works by encoding a pair of sentences: ``` encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\") print(encoding.tokens) print(encoding.type_ids) ``` ``` ['‚ñÅLet', \"'\", 's', '‚ñÅtest', '‚ñÅthis', '‚ñÅto', 'ken', 'izer', '.', '.', '.', '<sep>', '‚ñÅ', 'on', '‚ñÅ', 'a', '‚ñÅpair', '‚ñÅof', '‚ñÅsentence', 's', '!', '<sep>', '<cls>'] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2] ``` Finally, we add a `Metaspace` decoder: ``` tokenizer.decoder = decoders.Metaspace() ``` and we‚Äôre done with this tokenizer! We can save the tokenizer like before, and wrap it in a `PreTrainedTokenizerFast` or `XLNetTokenizerFast` if we want to use it in ü§ó Transformers. One thing to note when using `PreTrainedTokenizerFast` is that on top of the special tokens, we need to tell the ü§ó Transformers library to pad on the left: ``` from transformers import PreTrainedTokenizerFast wrapped_tokenizer = PreTrainedTokenizerFast( tokenizer_object=tokenizer, bos_token=\"<s>\", eos_token=\"</s>\", unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"<cls>\", sep_token=\"<sep>\", mask_token=\"<mask>\", padding_side=\"left\", ) ``` Or alternatively: ``` from transformers import XLNetTokenizerFast wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer) ``` Now that you have seen how the various building blocks are used to build existing tokenizers, you should be able to write any tokenizer you want with the ü§ó Tokenizers library and be able to use it in ü§ó Transformers.",
    "metadata": {
      "title": "Building a tokenizer, block by block",
      "url": "https://huggingface.co/learn/llm-course/chapter6/8",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/8",
      "part": 8,
      "total_parts": 8,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Tokenizers, check!\n\n   \nGreat job finishing this chapter!\n \nAfter this deep dive into tokenizers, you should:\n \n- Be able to train a new tokenizer using an old one as a template\n- Understand how to use offsets to map tokens‚Äô positions to their original span of text\n- Know the differences between BPE, WordPiece, and Unigram\n- Be able to mix and match the blocks provided by the ü§ó Tokenizers library to build your own tokenizer\n- Be able to use that tokenizer inside the ü§ó Transformers library",
    "metadata": {
      "title": "Tokenizers, check!",
      "url": "https://huggingface.co/learn/llm-course/chapter6/9",
      "course": "llm-course",
      "chapter": "6. The ü§ó Tokenizers library",
      "chapter_id": "chapter6/9",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter6/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf)  \n\n# Introduction\n\n   \nIn [Chapter 3](/course/chapter3), you saw how to fine-tune a model for text classification. In this chapter, we will tackle the following common language tasks that are essential for working with both traditional NLP models and modern LLMs:\n \n- Token classification\n- Masked language modeling (like BERT)\n- Summarization\n- Translation\n- Causal language modeling pretraining (like GPT-2)\n- Question answering\n \nThese fundamental tasks form the foundation of how Large Language Models (LLMs) work and understanding them is crucial for effectively working with today‚Äôs most advanced language models.\n \nTo do this, you‚Äôll need to leverage everything you learned about the `Trainer` API and the ü§ó Accelerate library in [Chapter 3](/course/chapter3), the ü§ó Datasets library in [Chapter 5](/course/chapter5), and the ü§ó Tokenizers library in [Chapter 6](/course/chapter6). We‚Äôll also upload our results to the Model Hub, like we did in [Chapter 4](/course/chapter4), so this is really the chapter where everything comes together!\n \nEach section can be read independently and will show you how to train a model with the `Trainer` API or with your own training loop, using ü§ó Accelerate. Feel free to skip either part and focus on the one that interests you the most: the `Trainer` API is great for fine-tuning or training your model without worrying about what‚Äôs going on behind the scenes, while the training loop with `Accelerate` will let you customize any part you want more easily.\n \n> If you read the sections in sequence, you will notice that they have quite a bit of code and prose in common. The repetition is intentional, to allow you to dip in (or come back later) to any task that interests you and find a complete working example.",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter7/1",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Token classification The first application we‚Äôll explore is token classification. This generic task encompasses any problem that can be formulated as ‚Äúattributing a label to each token in a sentence,‚Äù such as: - **Named entity recognition (NER)**: Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for ‚Äúno entity.‚Äù - **Part-of-speech tagging (POS)**: Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.). - **Chunking**: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually `B-`) to any tokens that are at the beginning of a chunk, another label (usually `I-`) to tokens that are inside a chunk, and a third label (usually `O`) to tokens that don‚Äôt belong to any chunk. Of course, there are many other types of token classification problem; those are just a few representative examples. In this section, we will fine-tune a model (BERT) on a NER task, which will then be able to compute predictions like this one: You can find the model we‚Äôll train and upload to the Hub and double-check its predictions [here](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn). ## Preparing the data First things first, we need a dataset suitable for token classification. In this section we will use the [CoNLL-2003 dataset](https://huggingface.co/datasets/conll2003), which contains news stories from Reuters. > üí° As long as your dataset consists of texts split into words with their corresponding labels, you will be able to adapt the data processing procedures described here to your own dataset. Refer back toChapter 5if you need a refresher on how to load your own custom data in aDataset. ### The CoNLL-2003 dataset To load the CoNLL-2003 dataset, we use the `load_dataset()` method from the ü§ó Datasets library: ``` from datasets import load_dataset raw_datasets = load_dataset(\"conll2003\") ``` This will download and cache the dataset, like we saw in [Chapter 3](/course/chapter3) for the GLUE MRPC dataset. Inspecting this object shows us the columns present and the split between the training, validation, and test sets: ``` raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'], num_rows: 14041 }) validation: Dataset({ features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'], num_rows: 3250 }) test: Dataset({ features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'], num_rows: 3453 }) }) ``` In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called `tokens`, but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization). Let‚Äôs have a look at the first element of the training set: ``` raw_datasets[\"train\"][0][\"tokens\"] ``` ``` ['EU', 'rejects', 'German', 'call', 'to',",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 1,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'ner_tags', 'pos_tags', 'tokens'], num_rows: 3453 }) }) ``` In particular, we can see the dataset contains labels for the three tasks we mentioned earlier: NER, POS, and chunking. A big difference from other datasets is that the input texts are not presented as sentences or documents, but lists of words (the last column is called `tokens`, but it contains words in the sense that these are pre-tokenized inputs that still need to go through the tokenizer for subword tokenization). Let‚Äôs have a look at the first element of the training set: ``` raw_datasets[\"train\"][0][\"tokens\"] ``` ``` ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'] ``` Since we want to perform named entity recognition, we will look at the NER tags: ``` raw_datasets[\"train\"][0][\"ner_tags\"] ``` ``` [3, 0, 7, 0, 0, 0, 7, 0, 0] ``` Those are the labels as integers ready for training, but they‚Äôre not necessarily useful when we want to inspect the data. Like for text classification, we can access the correspondence between those integers and the label names by looking at the `features` attribute of our dataset: ``` ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"] ner_feature ``` ``` Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None) ``` So this column contains elements that are sequences of `ClassLabel`s. The type of the elements of the sequence is in the `feature` attribute of this `ner_feature`, and we can access the list of names by looking at the `names` attribute of that `feature`: ``` label_names = ner_feature.feature.names label_names ``` ``` ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'] ``` We already saw these labels when digging into the `token-classification` pipeline in [Chapter 6](/course/chapter6/3), but for a quick refresher: - `O` means the word doesn‚Äôt correspond to any entity. - `B-PER`/`I-PER` means the word corresponds to the beginning of/is inside a *person* entity. - `B-ORG`/`I-ORG` means the word corresponds to the beginning of/is inside an *organization* entity. - `B-LOC`/`I-LOC` means the word corresponds to the beginning of/is inside a *location* entity. - `B-MISC`/`I-MISC` means the word corresponds to the beginning of/is inside a *miscellaneous* entity. Now decoding the labels we saw earlier gives us this: ``` words = raw_datasets[\"train\"][0][\"tokens\"] labels = raw_datasets[\"train\"][0][\"ner_tags\"] line1 = \"\" line2 = \"\" for word, label in zip(words, labels): full_label = label_names[label] max_length = max(len(word), len(full_label)) line1 += word + \" \" * (max_length - len(word) + 1) line2 += full_label + \" \" * (max_length - len(full_label) + 1) print(line1) print(line2) ``` ``` 'EU rejects German call to boycott British lamb .' 'B-ORG O B-MISC O O O B-MISC O O' ``` And for an example mixing `B-` and `I-` labels, here‚Äôs what the same code gives us on the element of the training set at index 4: ``` 'Germany \\'s representative to the European Union \\'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .' 'B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 2,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "\" * (max_length - len(full_label) + 1) print(line1) print(line2) ``` ``` 'EU rejects German call to boycott British lamb .' 'B-ORG O B-MISC O O O B-MISC O O' ``` And for an example mixing `B-` and `I-` labels, here‚Äôs what the same code gives us on the element of the training set at index 4: ``` 'Germany \\'s representative to the European Union \\'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .' 'B-LOC O O O O B-ORG I-ORG O O O B-PER I-PER O O O O O O O O O O O B-LOC O O O O O O O' ``` As we can see, entities spanning two words, like ‚ÄúEuropean Union‚Äù and ‚ÄúWerner Zwingmann,‚Äù are attributed a `B-` label for the first word and an `I-` label for the second. > ‚úèÔ∏èYour turn!Print the same two sentences with their POS or chunking labels. ### Processing the data As usual, our texts need to be converted to token IDs before the model can make sense of them. As we saw in [Chapter 6](/course/chapter6/), a big difference in the case of token classification tasks is that we have pre-tokenized inputs. Fortunately, the tokenizer API can deal with that pretty easily; we just need to warn the `tokenizer` with a special flag. To begin, let‚Äôs create our `tokenizer` object. As we said before, we will be using a BERT pretrained model, so we‚Äôll start by downloading and caching the associated tokenizer: ``` from transformers import AutoTokenizer model_checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` You can replace the `model_checkpoint` with any other model you prefer from the [Hub](https://huggingface.co/models), or with a local folder in which you‚Äôve saved a pretrained model and a tokenizer. The only constraint is that the tokenizer needs to be backed by the ü§ó Tokenizers library, so there‚Äôs a ‚Äúfast‚Äù version available. You can see all the architectures that come with a fast version in [this big table](https://huggingface.co/transformers/#supported-frameworks), and to check that the `tokenizer` object you‚Äôre using is indeed backed by ü§ó Tokenizers you can look at its `is_fast` attribute: ``` tokenizer.is_fast ``` ``` True ``` To tokenize a pre-tokenized input, we can use our `tokenizer` as usual and just add `is_split_into_words=True`: ``` inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True) inputs.tokens() ``` ``` ['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]'] ``` As we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginning and `[SEP]` at the end) and left most of the words untouched. The word `lamb`, however, was tokenized into two subwords, `la` and `##mb`. This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 12 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 3,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "'[SEP]'] ``` As we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginning and `[SEP]` at the end) and left most of the words untouched. The word `lamb`, however, was tokenized into two subwords, `la` and `##mb`. This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 12 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words. Fortunately, because we‚Äôre using a fast tokenizer we have access to the ü§ó Tokenizers superpowers, which means we can easily map each token to its corresponding word (as seen in [Chapter 6](/course/chapter6/3)): ``` inputs.word_ids() ``` ``` [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None] ``` With a tiny bit of work, we can then expand our label list to match the tokens. The first rule we‚Äôll apply is that special tokens get a label of `-100`. This is because by default `-100` is an index that is ignored in the loss function we will use (cross entropy). Then, each token gets the same label as the token that started the word it‚Äôs inside, since they are part of the same entity. For tokens inside a word but not at the beginning, we replace the `B-` with `I-` (since the token does not begin the entity): ``` def align_labels_with_tokens(labels, word_ids): new_labels = [] current_word = None for word_id in word_ids: if word_id != current_word: # Start of a new word! current_word = word_id label = -100 if word_id is None else labels[word_id] new_labels.append(label) elif word_id is None: # Special token new_labels.append(-100) else: # Same word as previous token label = labels[word_id] # If the label is B-XXX we change it to I-XXX if label % 2 == 1: label += 1 new_labels.append(label) return new_labels ``` Let‚Äôs try it out on our first sentence: ``` labels = raw_datasets[\"train\"][0][\"ner_tags\"] word_ids = inputs.word_ids() print(labels) print(align_labels_with_tokens(labels, word_ids)) ``` ``` [3, 0, 7, 0, 0, 0, 7, 0, 0] [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100] ``` As we can see, our function added the `-100` for the two special tokens at the beginning and the end, and a new `0` for our word that was split into two tokens. > ‚úèÔ∏èYour turn!Some researchers prefer to attribute only one label per word, and assign-100to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule. To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our fast tokenizer, it‚Äôs best to tokenize lots of texts at the same time, so we‚Äôll write",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 4,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "was split into two tokens. > ‚úèÔ∏èYour turn!Some researchers prefer to attribute only one label per word, and assign-100to the other subtokens in a given word. This is to avoid long words that split into lots of subtokens contributing heavily to the loss. Change the previous function to align labels with input IDs by following this rule. To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our fast tokenizer, it‚Äôs best to tokenize lots of texts at the same time, so we‚Äôll write a function that processes a list of examples and use the `Dataset.map()` method with the option `batched=True`. The only thing that is different from our previous example is that the `word_ids()` function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too: ``` def tokenize_and_align_labels(examples): tokenized_inputs = tokenizer( examples[\"tokens\"], truncation=True, is_split_into_words=True ) all_labels = examples[\"ner_tags\"] new_labels = [] for i, labels in enumerate(all_labels): word_ids = tokenized_inputs.word_ids(i) new_labels.append(align_labels_with_tokens(labels, word_ids)) tokenized_inputs[\"labels\"] = new_labels return tokenized_inputs ``` Note that we haven‚Äôt padded our inputs yet; we‚Äôll do that later, when creating the batches with a data collator. We can now apply all that preprocessing in one go on the other splits of our dataset: ``` tokenized_datasets = raw_datasets.map( tokenize_and_align_labels, batched=True, remove_columns=raw_datasets[\"train\"].column_names, ) ``` We‚Äôve done the hardest part! Now that the data has been preprocessed, the actual training will look a lot like what we did in [Chapter 3](/course/chapter3). ## Fine-tuning the model with the Trainer API The actual code using the `Trainer` will be the same as before; the only changes are the way the data is collated into a batch and the metric computation function. ### Data collation We can‚Äôt just use a `DataCollatorWithPadding` like in [Chapter 3](/course/chapter3) because that only pads the inputs (input IDs, attention mask, and token type IDs). Here our labels should be padded the exact same way as the inputs so that they stay the same size, using `-100` as a value so that the corresponding predictions are ignored in the loss computation. This is all done by a [DataCollatorForTokenClassification](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification). Like the `DataCollatorWithPadding`, it takes the `tokenizer` used to preprocess the inputs: ``` from transformers import DataCollatorForTokenClassification data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer) ``` To test this on a few samples, we can just call it on a list of examples from our tokenized training set: ``` batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)]) batch[\"labels\"] ``` ``` tensor([[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100]]) ``` Let‚Äôs compare this to the labels for the first and second elements in our dataset: ``` for i in range(2): print(tokenized_datasets[\"train\"][i][\"labels\"]) ``` ``` [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100] [-100, 1, 2, -100]",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 5,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "= DataCollatorForTokenClassification(tokenizer=tokenizer) ``` To test this on a few samples, we can just call it on a list of examples from our tokenized training set: ``` batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)]) batch[\"labels\"] ``` ``` tensor([[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100]]) ``` Let‚Äôs compare this to the labels for the first and second elements in our dataset: ``` for i in range(2): print(tokenized_datasets[\"train\"][i][\"labels\"]) ``` ``` [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100] [-100, 1, 2, -100] ``` As we can see, the second set of labels has been padded to the length of the first one using `-100`s. ### Metrics To have the `Trainer` compute a metric every epoch, we will need to define a `compute_metrics()` function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values. The traditional framework used to evaluate token classification prediction is [seqeval](https://github.com/chakki-works/seqeval). To use this metric, we first need to install the *seqeval* library: ``` !pip install seqeval ``` We can then load it via the `evaluate.load()` function like we did in [Chapter 3](/course/chapter3): ``` import evaluate metric = evaluate.load(\"seqeval\") ``` This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let‚Äôs see how it works. First, we‚Äôll get the labels for our first training example: ``` labels = raw_datasets[\"train\"][0][\"ner_tags\"] labels = [label_names[i] for i in labels] labels ``` ``` ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'] ``` We can then create fake predictions for those by just changing the value at index 2: ``` predictions = labels.copy() predictions[2] = \"O\" metric.compute(predictions=[predictions], references=[labels]) ``` Note that the metric takes a list of predictions (not just one) and a list of labels. Here‚Äôs the output: ``` {'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2}, 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1}, 'overall_precision': 1.0, 'overall_recall': 0.67, 'overall_f1': 0.8, 'overall_accuracy': 0.89} ``` This is sending back a lot of information! We get the precision, recall, and F1 score for each separate entity, as well as overall. For our metric computation we will only keep the overall score, but feel free to tweak the `compute_metrics()` function to return all the metrics you would like reported. This `compute_metrics()` function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don‚Äôt need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is `-100`, then pass the results to the `metric.compute()` method: ``` import numpy as np def compute_metrics(eval_preds): logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) # Remove ignored index (special tokens) and",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 6,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the `compute_metrics()` function to return all the metrics you would like reported. This `compute_metrics()` function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don‚Äôt need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is `-100`, then pass the results to the `metric.compute()` method: ``` import numpy as np def compute_metrics(eval_preds): logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) # Remove ignored index (special tokens) and convert to labels true_labels = [[label_names[l] for l in label if l != -100] for label in labels] true_predictions = [ [label_names[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] all_metrics = metric.compute(predictions=true_predictions, references=true_labels) return { \"precision\": all_metrics[\"overall_precision\"], \"recall\": all_metrics[\"overall_recall\"], \"f1\": all_metrics[\"overall_f1\"], \"accuracy\": all_metrics[\"overall_accuracy\"], } ``` Now that this is done, we are almost ready to define our `Trainer`. We just need a `model` to fine-tune! ### Defining the model Since we are working on a token classification problem, we will use the `AutoModelForTokenClassification` class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the `num_labels` argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it‚Äôs better to set the correct label correspondences instead. They should be set by two dictionaries, `id2label` and `label2id`, which contain the mappings from ID to label and vice versa: ``` id2label = {i: label for i, label in enumerate(label_names)} label2id = {v: k for k, v in id2label.items()} ``` Now we can just pass them to the `AutoModelForTokenClassification.from_pretrained()` method, and they will be set in the model‚Äôs configuration and then properly saved and uploaded to the Hub: ``` from transformers import AutoModelForTokenClassification model = AutoModelForTokenClassification.from_pretrained( model_checkpoint, id2label=id2label, label2id=label2id, ) ``` Like when we defined our `AutoModelForSequenceClassification` in [Chapter 3](/course/chapter3), creating the model issues a warning that some weights were not used (the ones from the pretraining head) and some other weights are randomly initialized (the ones from the new token classification head), and that this model should be trained. We will do that in a minute, but first let‚Äôs double-check that our model has the right number of labels: ``` model.config.num_labels ``` ``` 9 ``` > ‚ö†Ô∏è If you have a model with the wrong number of labels, you will get an obscure error when calling theTrainer.train()method later on (something like ‚ÄúCUDA error: device-side assert triggered‚Äù). This is the number one cause of bugs reported by users for such errors, so make sure you do this check to confirm that you have the expected number of labels. ### Fine-tuning the model We are now ready to train our model! We just need to do two last things before",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 7,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "let‚Äôs double-check that our model has the right number of labels: ``` model.config.num_labels ``` ``` 9 ``` > ‚ö†Ô∏è If you have a model with the wrong number of labels, you will get an obscure error when calling theTrainer.train()method later on (something like ‚ÄúCUDA error: device-side assert triggered‚Äù). This is the number one cause of bugs reported by users for such errors, so make sure you do this check to confirm that you have the expected number of labels. ### Fine-tuning the model We are now ready to train our model! We just need to do two last things before we define our `Trainer`: log in to Hugging Face and define our training arguments. If you‚Äôre working in a notebook, there‚Äôs a convenience function to help you with this: ``` from huggingface_hub import notebook_login notebook_login() ``` This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` Once this is done, we can define our `TrainingArguments`: ``` from transformers import TrainingArguments args = TrainingArguments( \"bert-finetuned-ner\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, push_to_hub=True, ) ``` You‚Äôve seen most of those before: we set some hyperparameters (like the learning rate, the number of epochs to train for, and the weight decay), and we specify `push_to_hub=True` to indicate that we want to save the model and evaluate it at the end of every epoch, and that we want to upload our results to the Model Hub. Note that you can specify the name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the [huggingface-courseorganization](https://huggingface.co/huggingface-course), we added `hub_model_id=\"huggingface-course/bert-finetuned-ner\"` to `TrainingArguments`. By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be `\"sgugger/bert-finetuned-ner\"`. > üí° If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn‚Äôt, you‚Äôll get an error when defining yourTrainerand will need to set a new name. Finally, we just pass everything to the `Trainer` and launch the training: ``` from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, compute_metrics=compute_metrics, processing_class=tokenizer, ) trainer.train() ``` Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. Once the training is complete, we use the `push_to_hub()` method to make sure we upload the most recent version of the model: ``` trainer.push_to_hub(commit_message=\"Training complete\") ``` This command returns the URL of the commit it just did, if you want to inspect it: ``` 'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed' ``` The `Trainer` also drafts a model card with all the evaluation results and",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 8,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. Once the training is complete, we use the `push_to_hub()` method to make sure we upload the most recent version of the model: ``` trainer.push_to_hub(commit_message=\"Training complete\") ``` This command returns the URL of the commit it just did, if you want to inspect it: ``` 'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed' ``` The `Trainer` also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a token classification task ‚Äî congratulations! If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using ü§ó Accelerate. ## A custom training loop Let‚Äôs now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in [Chapter 3](/course/chapter3/4), with a few changes for the evaluation. ### Preparing everything for training First we need to build the `DataLoader`s from our datasets. We‚Äôll reuse our `data_collator` as a `collate_fn` and shuffle the training set, but not the validation set: ``` from torch.utils.data import DataLoader train_dataloader = DataLoader( tokenized_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8, ) eval_dataloader = DataLoader( tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8 ) ``` Next we reinstantiate our model, to make sure we‚Äôre not continuing the fine-tuning from before but starting from the BERT pretrained model again: ``` model = AutoModelForTokenClassification.from_pretrained( model_checkpoint, id2label=id2label, label2id=label2id, ) ``` Then we will need an optimizer. We‚Äôll use the classic `AdamW`, which is like `Adam`, but with a fix in the way weight decay is applied: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=2e-5) ``` Once we have all those objects, we can send them to the `accelerator.prepare()` method: ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` > üö® If you‚Äôre training on a TPU, you‚Äôll need to move all the code starting from the cell above into a dedicated training function. SeeChapter 3for more details. Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to Hugging Face, if you‚Äôre not logged in already.",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 9,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "`accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to Hugging Face, if you‚Äôre not logged in already. We‚Äôll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does): ``` from huggingface_hub import Repository, get_full_repo_name model_name = \"bert-finetuned-ner-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'sgugger/bert-finetuned-ner-accelerate' ``` Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with: ``` output_dir = \"bert-finetuned-ner-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch. ### Training loop We are now ready to write the full training loop. To simplify its evaluation part, we define this `postprocess()` function that takes predictions and labels and converts them to lists of strings, like our `metric` object expects: ``` def postprocess(predictions, labels): predictions = predictions.detach().cpu().clone().numpy() labels = labels.detach().cpu().clone().numpy() # Remove ignored index (special tokens) and convert to labels true_labels = [[label_names[l] for l in label if l != -100] for label in labels] true_predictions = [ [label_names[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels) ] return true_labels, true_predictions ``` Then we can write the training loop. After defining a progress bar to follow how training goes, the loop has three parts: - The training in itself, which is the classic iteration over the `train_dataloader`, forward pass through the model, then backward pass and optimizer step. - The evaluation, in which there is a novelty after getting the outputs of our model on a batch: since two processes may have padded the inputs and labels to different shapes, we need to use `accelerator.pad_across_processes()` to make the predictions and labels the same shape before calling the `gather()` method. If we don‚Äôt do this, the evaluation will either error out or hang forever. Then we send the results to `metric.add_batch()` and call `metric.compute()` once the evaluation loop is over. - Saving and uploading, where we first save the model and the tokenizer, then call `repo.push_to_hub()`. Notice that we use the argument `blocking=False` to tell the ü§ó Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background. Here‚Äôs the",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 10,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "we need to use `accelerator.pad_across_processes()` to make the predictions and labels the same shape before calling the `gather()` method. If we don‚Äôt do this, the evaluation will either error out or hang forever. Then we send the results to `metric.add_batch()` and call `metric.compute()` once the evaluation loop is over. - Saving and uploading, where we first save the model and the tokenizer, then call `repo.push_to_hub()`. Notice that we use the argument `blocking=False` to tell the ü§ó Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background. Here‚Äôs the complete code for the training loop: ``` from tqdm.auto import tqdm import torch progress_bar = tqdm(range(num_training_steps)) for epoch in range(num_train_epochs): # Training model.train() for batch in train_dataloader: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # Evaluation model.eval() for batch in eval_dataloader: with torch.no_grad(): outputs = model(**batch) predictions = outputs.logits.argmax(dim=-1) labels = batch[\"labels\"] # Necessary to pad predictions and labels for being gathered predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100) labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100) predictions_gathered = accelerator.gather(predictions) labels_gathered = accelerator.gather(labels) true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered) metric.add_batch(predictions=true_predictions, references=true_labels) results = metric.compute() print( f\"epoch {epoch}:\", { key: results[f\"overall_{key}\"] for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"] }, ) # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` In case this is the first time you‚Äôre seeing a model saved with ü§ó Accelerate, let‚Äôs take a moment to inspect the three lines of code that go with it: ``` accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) ``` The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the `unwrapped_model`, which is the base model we defined. The `accelerator.prepare()` method changes the model to work in distributed training, so it won‚Äôt have the `save_pretrained()` method anymore; the `accelerator.unwrap_model()` method undoes that step. Lastly, we call `save_pretrained()` but tell that method to use `accelerator.save()` instead of `torch.save()`. Once this is done, you should have a model that produces results pretty similar to the one trained with the `Trainer`. You can check the model we trained using this code at [huggingface-course/bert-finetuned-ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate). And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above! ## Using the fine-tuned model We‚Äôve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a `pipeline`, you just have to specify the proper model identifier: ``` from transformers import pipeline # Replace this with your own checkpoint model_checkpoint = \"huggingface-course/bert-finetuned-ner\" token_classifier = pipeline( \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\" ) token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` ``` [{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score':",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 11,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "can directly implement them by editing the code shown above! ## Using the fine-tuned model We‚Äôve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a `pipeline`, you just have to specify the proper model identifier: ``` from transformers import pipeline # Replace this with your own checkpoint model_checkpoint = \"huggingface-course/bert-finetuned-ner\" token_classifier = pipeline( \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\" ) token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\") ``` ``` [{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}] ``` Great! Our model is working as well as the default one for this pipeline!",
    "metadata": {
      "title": "Token classification",
      "url": "https://huggingface.co/learn/llm-course/chapter7/2",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/2",
      "part": 12,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Fine-tuning a masked language model For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. However, there are a few cases where you‚Äôll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once! This process of fine-tuning a pretrained language model on in-domain data is usually called *domain adaptation*. It was popularized in 2018 by [ULMFiT](https://arxiv.org/abs/1801.06146), which was one of the first neural architectures (based on LSTMs) to make transfer learning really work for NLP. An example of domain adaptation with ULMFiT is shown in the image below; in this section we‚Äôll do something similar, but with a Transformer instead of an LSTM! ![ULMFiT.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit.svg) ![ULMFiT.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/ulmfit-dark.svg) By the end of this section you‚Äôll have a [masked language model](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) on the Hub that can autocomplete sentences as shown below: Let‚Äôs dive in! > üôã If the terms ‚Äúmasked language modeling‚Äù and ‚Äúpretrained model‚Äù sound unfamiliar to you, go check outChapter 1, where we explain all these core concepts, complete with videos! ## Picking a pretrained model for masked language modeling To get started, let‚Äôs pick a suitable pretrained model for masked language modeling. As shown in the following screenshot, you can find a list of candidates by applying the ‚ÄúFill-Mask‚Äù filter on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads): ![Hub models.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/mlm-models.png) Although the BERT and RoBERTa family of models are the most downloaded, we‚Äôll use a model called [DistilBERT](https://huggingface.co/distilbert-base-uncased) that can be trained much faster with little to no loss in downstream performance. This model was trained using a special technique called [knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation), where a large ‚Äúteacher model‚Äù like BERT is used to guide the training of a ‚Äústudent model‚Äù that has far fewer parameters. An explanation of the details of knowledge distillation would take us too far afield in this section, but if you‚Äôre interested you can read all about it in [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) (colloquially known as the Transformers textbook). Let‚Äôs go ahead and download DistilBERT using the `AutoModelForMaskedLM` class: ``` from transformers import AutoModelForMaskedLM model_checkpoint = \"distilbert-base-uncased\" model = AutoModelForMaskedLM.from_pretrained(model_checkpoint) ``` We can see how many parameters this model has by calling the `num_parameters()` method: ``` distilbert_num_parameters = model.num_parameters() / 1_000_000 print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\") print(f\"'>>> BERT number of parameters: 110M'\") ``` ``` '>>> DistilBERT number of parameters: 67M' '>>> BERT number of parameters: 110M' ``` With around 67",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 1,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "too far afield in this section, but if you‚Äôre interested you can read all about it in [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) (colloquially known as the Transformers textbook). Let‚Äôs go ahead and download DistilBERT using the `AutoModelForMaskedLM` class: ``` from transformers import AutoModelForMaskedLM model_checkpoint = \"distilbert-base-uncased\" model = AutoModelForMaskedLM.from_pretrained(model_checkpoint) ``` We can see how many parameters this model has by calling the `num_parameters()` method: ``` distilbert_num_parameters = model.num_parameters() / 1_000_000 print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\") print(f\"'>>> BERT number of parameters: 110M'\") ``` ``` '>>> DistilBERT number of parameters: 67M' '>>> BERT number of parameters: 110M' ``` With around 67 million parameters, DistilBERT is approximately two times smaller than the BERT base model, which roughly translates into a two-fold speedup in training ‚Äî nice! Let‚Äôs now see what kinds of tokens this model predicts are the most likely completions of a small sample of text: ``` text = \"This is a great [MASK].\" ``` As humans, we can imagine many possibilities for the `[MASK]` token, such as ‚Äúday‚Äù, ‚Äúride‚Äù, or ‚Äúpainting‚Äù. For pretrained models, the predictions depend on the corpus the model was trained on, since it learns to pick up the statistical patterns present in the data. Like BERT, DistilBERT was pretrained on the [English Wikipedia](https://huggingface.co/datasets/wikipedia) and [BookCorpus](https://huggingface.co/datasets/bookcorpus) datasets, so we expect the predictions for `[MASK]` to reflect these domains. To predict the mask we need DistilBERT‚Äôs tokenizer to produce the inputs for the model, so let‚Äôs download that from the Hub as well: ``` from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` With a tokenizer and a model, we can now pass our text example to the model, extract the logits, and print out the top 5 candidates: ``` import torch inputs = tokenizer(text, return_tensors=\"pt\") token_logits = model(**inputs).logits # Find the location of [MASK] and extract its logits mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1] mask_token_logits = token_logits[0, mask_token_index, :] # Pick the [MASK] candidates with the highest logits top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist() for token in top_5_tokens: print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\") ``` ``` '>>> This is a great deal.' '>>> This is a great success.' '>>> This is a great adventure.' '>>> This is a great idea.' '>>> This is a great feat.' ``` We can see from the outputs that the model‚Äôs predictions refer to everyday terms, which is perhaps not surprising given the foundation of English Wikipedia. Let‚Äôs see how we can change this domain to something a bit more niche ‚Äî highly polarized movie reviews! ## The dataset To showcase domain adaptation, we‚Äôll use the famous [Large Movie Review Dataset](https://huggingface.co/datasets/imdb) (or IMDb for short), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models. By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews. We can get the data from the Hugging Face Hub with the `load_dataset()` function from ü§ó Datasets: ``` from datasets import load_dataset imdb_dataset",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 2,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "bit more niche ‚Äî highly polarized movie reviews! ## The dataset To showcase domain adaptation, we‚Äôll use the famous [Large Movie Review Dataset](https://huggingface.co/datasets/imdb) (or IMDb for short), which is a corpus of movie reviews that is often used to benchmark sentiment analysis models. By fine-tuning DistilBERT on this corpus, we expect the language model will adapt its vocabulary from the factual data of Wikipedia that it was pretrained on to the more subjective elements of movie reviews. We can get the data from the Hugging Face Hub with the `load_dataset()` function from ü§ó Datasets: ``` from datasets import load_dataset imdb_dataset = load_dataset(\"imdb\") imdb_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['text', 'label'], num_rows: 25000 }) test: Dataset({ features: ['text', 'label'], num_rows: 25000 }) unsupervised: Dataset({ features: ['text', 'label'], num_rows: 50000 }) }) ``` We can see that the `train` and `test` splits each consist of 25,000 reviews, while there is an unlabeled split called `unsupervised` that contains 50,000 reviews. Let‚Äôs take a look at a few samples to get an idea of what kind of text we‚Äôre dealing with. As we‚Äôve done in previous chapters of the course, we‚Äôll chain the `Dataset.shuffle()` and `Dataset.select()` functions to create a random sample: ``` sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3)) for row in sample: print(f\"\\n'>>> Review: {row['text']}'\") print(f\"'>>> Label: {row['label']}'\") ``` ``` '>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, \"kodokoo\" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\\'t get me wrong; as clich√©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.' '>>> Label: 0' '>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam\" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.' '>>> Label: 0' '>>> Review: I saw",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 3,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "is dismal, and only editing saves a bit of the muddle, but Sam\" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.' '>>> Label: 0' '>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.' '>>> Label: 1' ``` Yep, these are certainly movie reviews, and if you‚Äôre old enough you may even understand the comment in the last review about owning a VHS version üòú! Although we won‚Äôt need the labels for language modeling, we can already see that a `0` denotes a negative review, while a `1` corresponds to a positive one. > ‚úèÔ∏èTry it out!Create a random sample of theunsupervisedsplit and verify that the labels are neither0nor1. While you‚Äôre at it, you could also check that the labels in thetrainandtestsplits are indeed0or1‚Äî this is a useful sanity check that every NLP practitioner should perform at the start of a new project! Now that we‚Äôve had a quick look at the data, let‚Äôs dive into preparing it for masked language modeling. As we‚Äôll see, there are some additional steps that one needs to take compared to the sequence classification tasks we saw in [Chapter 3](/course/chapter3). Let‚Äôs go! ## Preprocessing the data For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they‚Äôre too long, and that would result in losing information that might be useful for the language modeling task! So to get started, we‚Äôll first tokenize our corpus as usual, but *without* setting the `truncation=True` option in our tokenizer. We‚Äôll also grab the word IDs if they are available ((which they will be if we‚Äôre using a fast tokenizer, as described in [Chapter 6](/course/chapter6/3)), as we will need them later on to do whole word masking. We‚Äôll",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 4,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they‚Äôre too long, and that would result in losing information that might be useful for the language modeling task! So to get started, we‚Äôll first tokenize our corpus as usual, but *without* setting the `truncation=True` option in our tokenizer. We‚Äôll also grab the word IDs if they are available ((which they will be if we‚Äôre using a fast tokenizer, as described in [Chapter 6](/course/chapter6/3)), as we will need them later on to do whole word masking. We‚Äôll wrap this in a simple function, and while we‚Äôre at it we‚Äôll remove the `text` and `label` columns since we don‚Äôt need them any longer: ``` def tokenize_function(examples): result = tokenizer(examples[\"text\"]) if tokenizer.is_fast: result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))] return result # Use batched=True to activate fast multithreading! tokenized_datasets = imdb_dataset.map( tokenize_function, batched=True, remove_columns=[\"text\", \"label\"] ) tokenized_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'input_ids', 'word_ids'], num_rows: 25000 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'word_ids'], num_rows: 25000 }) unsupervised: Dataset({ features: ['attention_mask', 'input_ids', 'word_ids'], num_rows: 50000 }) }) ``` Since DistilBERT is a BERT-like model, we can see that the encoded texts consist of the `input_ids` and `attention_mask` that we‚Äôve seen in other chapters, as well as the `word_ids` we added. Now that we‚Äôve tokenized our movie reviews, the next step is to group them all together and split the result into chunks. But how big should these chunks be? This will ultimately be determined by the amount of GPU memory that you have available, but a good starting point is to see what the model‚Äôs maximum context size is. This can be inferred by inspecting the `model_max_length` attribute of the tokenizer: ``` tokenizer.model_max_length ``` ``` 512 ``` This value is derived from the *tokenizer_config.json* file associated with a checkpoint; in this case we can see that the context size is 512 tokens, just like with BERT. > ‚úèÔ∏èTry it out!Some Transformer models, likeBigBirdandLongformer, have a much longer context length than BERT and other early Transformer models. Instantiate the tokenizer for one of these checkpoints and verify that themodel_max_lengthagrees with what‚Äôs quoted on its model card. So, in order to run our experiments on GPUs like those found on Google Colab, we‚Äôll pick something a bit smaller that can fit in memory: ``` chunk_size = 128 ``` > Note that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to. Now comes the fun part. To show how the concatenation works, let‚Äôs take a few reviews from our tokenized training set and print out the number of tokens per review: ``` # Slicing produces a list of lists for each feature tokenized_samples = tokenized_datasets[\"train\"][:3] for idx, sample in enumerate(tokenized_samples[\"input_ids\"]): print(f\"'>>> Review {idx} length: {len(sample)}'\") ``` ``` '>>> Review 0 length: 200' '>>> Review 1 length: 559' '>>> Review 2 length:",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 5,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "that using a small chunk size can be detrimental in real-world scenarios, so you should use a size that corresponds to the use case you will apply your model to. Now comes the fun part. To show how the concatenation works, let‚Äôs take a few reviews from our tokenized training set and print out the number of tokens per review: ``` # Slicing produces a list of lists for each feature tokenized_samples = tokenized_datasets[\"train\"][:3] for idx, sample in enumerate(tokenized_samples[\"input_ids\"]): print(f\"'>>> Review {idx} length: {len(sample)}'\") ``` ``` '>>> Review 0 length: 200' '>>> Review 1 length: 559' '>>> Review 2 length: 192' ``` We can then concatenate all these examples with a simple dictionary comprehension, as follows: ``` concatenated_examples = { k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys() } total_length = len(concatenated_examples[\"input_ids\"]) print(f\"'>>> Concatenated reviews length: {total_length}'\") ``` ``` '>>> Concatenated reviews length: 951' ``` Great, the total length checks out ‚Äî so now let‚Äôs split the concatenated reviews into chunks of the size given by `chunk_size`. To do so, we iterate over the features in `concatenated_examples` and use a list comprehension to create slices of each feature. The result is a dictionary of chunks for each feature: ``` chunks = { k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)] for k, t in concatenated_examples.items() } for chunk in chunks[\"input_ids\"]: print(f\"'>>> Chunk length: {len(chunk)}'\") ``` ``` '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 128' '>>> Chunk length: 55' ``` As you can see in this example, the last chunk will generally be smaller than the maximum chunk size. There are two main strategies for dealing with this: - Drop the last chunk if it‚Äôs smaller than `chunk_size`. - Pad the last chunk until its length equals `chunk_size`. We‚Äôll take the first approach here, so let‚Äôs wrap all of the above logic in a single function that we can apply to our tokenized datasets: ``` def group_texts(examples): # Concatenate all texts concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()} # Compute length of concatenated texts total_length = len(concatenated_examples[list(examples.keys())[0]]) # We drop the last chunk if it's smaller than chunk_size total_length = (total_length // chunk_size) * chunk_size # Split by chunks of max_len result = { k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)] for k, t in concatenated_examples.items() } # Create a new labels column result[\"labels\"] = result[\"input_ids\"].copy() return result ``` Note that in the last step of `group_texts()` we create a new `labels` column which is a copy of the `input_ids` one. As we‚Äôll see shortly, that‚Äôs because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a `labels` column we provide the ground truth for our language model to learn from. Let‚Äôs now apply `group_texts()` to our tokenized datasets using our trusty `Dataset.map()` function: ``` lm_datasets = tokenized_datasets.map(group_texts, batched=True) lm_datasets ```",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 6,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for k, t in concatenated_examples.items() } # Create a new labels column result[\"labels\"] = result[\"input_ids\"].copy() return result ``` Note that in the last step of `group_texts()` we create a new `labels` column which is a copy of the `input_ids` one. As we‚Äôll see shortly, that‚Äôs because in masked language modeling the objective is to predict randomly masked tokens in the input batch, and by creating a `labels` column we provide the ground truth for our language model to learn from. Let‚Äôs now apply `group_texts()` to our tokenized datasets using our trusty `Dataset.map()` function: ``` lm_datasets = tokenized_datasets.map(group_texts, batched=True) lm_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 61289 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 59905 }) unsupervised: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 122963 }) }) ``` You can see that grouping and then chunking the texts has produced many more examples than our original 25,000 for the `train` and `test` splits. That‚Äôs because we now have examples involving *contiguous tokens* that span across multiple examples from the original corpus. You can see this explicitly by looking for the special `[SEP]` and `[CLS]` tokens in one of the chunks: ``` tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"]) ``` ``` \".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless\" ``` In this example you can see two overlapping movie reviews, one about a high school movie and the other about homelessness. Let‚Äôs also check out what the labels look like for masked language modeling: ``` tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"]) ``` ``` \".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless\" ``` As expected from our `group_texts()` function above, this looks identical to the decoded `input_ids` ‚Äî but then how can our model possibly learn anything? We‚Äôre missing a key step: inserting `[MASK]` tokens at random positions in the inputs! Let‚Äôs see how we can do this on the fly during fine-tuning using a special data collator. ## Fine-tuning DistilBERT with the Trainer API Fine-tuning a masked language model is almost identical",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 7,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless\" ``` As expected from our `group_texts()` function above, this looks identical to the decoded `input_ids` ‚Äî but then how can our model possibly learn anything? We‚Äôre missing a key step: inserting `[MASK]` tokens at random positions in the inputs! Let‚Äôs see how we can do this on the fly during fine-tuning using a special data collator. ## Fine-tuning DistilBERT with the Trainer API Fine-tuning a masked language model is almost identical to fine-tuning a sequence classification model, like we did in [Chapter 3](/course/chapter3). The only difference is that we need a special data collator that can randomly mask some of the tokens in each batch of texts. Fortunately, ü§ó Transformers comes prepared with a dedicated `DataCollatorForLanguageModeling` for just this task. We just have to pass it the tokenizer and an `mlm_probability` argument that specifies what fraction of the tokens to mask. We‚Äôll pick 15%, which is the amount used for BERT and a common choice in the literature: ``` from transformers import DataCollatorForLanguageModeling data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) ``` To see how the random masking works, let‚Äôs feed a few examples to the data collator. Since it expects a list of `dict`s, where each `dict` represents a single chunk of contiguous text, we first iterate over the dataset before feeding the batch to the collator. We remove the `\"word_ids\"` key for this data collator as it does not expect it: ``` samples = [lm_datasets[\"train\"][i] for i in range(2)] for sample in samples: _ = sample.pop(\"word_ids\") for chunk in data_collator(samples)[\"input_ids\"]: print(f\"\\n'>>> {tokenizer.decode(chunk)}'\") ``` ``` '>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as \" teachers \". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\\'[MASK] satire is much closer to reality than is \" teachers \". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...' '>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george ÂÆáin stated )ÂÖ¨ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless' ``` Nice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locations in",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 8,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george ÂÆáin stated )ÂÖ¨ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless' ``` Nice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locations in our text. These will be the tokens which our model will have to predict during training ‚Äî and the beauty of the data collator is that it will randomize the `[MASK]` insertion with every batch! > ‚úèÔ∏èTry it out!Run the code snippet above several times to see the random masking happen in front of your very eyes! Also replace thetokenizer.decode()method withtokenizer.convert_ids_to_tokens()to see that sometimes a single token from a given word is masked, and not the others. One side effect of random masking is that our evaluation metrics will not be deterministic when using the `Trainer`, since we use the same data collator for the training and test sets. We‚Äôll see later, when we look at fine-tuning with ü§ó Accelerate, how we can use the flexibility of a custom evaluation loop to freeze the randomness. When training models for masked language modeling, one technique that can be used is to mask whole words together, not just individual tokens. This approach is called *whole word masking*. If we want to use whole word masking, we will need to build a data collator ourselves. A data collator is just a function that takes a list of samples and converts them into a batch, so let‚Äôs do this now! We‚Äôll use the word IDs computed earlier to make a map between word indices and the corresponding tokens, then randomly decide which words to mask and apply that mask on the inputs. Note that the labels are all `-100` except for the ones corresponding to mask words. ``` import collections import numpy as np from transformers import default_data_collator wwm_probability = 0.2 def whole_word_masking_data_collator(features): for feature in features: word_ids = feature.pop(\"word_ids\") # Create a map between words and corresponding token indices mapping = collections.defaultdict(list) current_word_index = -1 current_word = None for idx, word_id in enumerate(word_ids): if word_id is not None: if word_id != current_word: current_word = word_id current_word_index += 1 mapping[current_word_index].append(idx) # Randomly mask words mask = np.random.binomial(1, wwm_probability, (len(mapping),)) input_ids = feature[\"input_ids\"] labels = feature[\"labels\"] new_labels = [-100] * len(labels) for word_id in np.where(mask)[0]: word_id = word_id.item() for idx in mapping[word_id]: new_labels[idx] = labels[idx] input_ids[idx] = tokenizer.mask_token_id feature[\"labels\"] = new_labels return default_data_collator(features) ``` Next, we can try it on the same samples as before: ``` samples = [lm_datasets[\"train\"][i] for i in range(2)] batch = whole_word_masking_data_collator(samples) for chunk in batch[\"input_ids\"]: print(f\"\\n'>>> {tokenizer.decode(chunk)}'\") ``` ``` '>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 9,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "if word_id is not None: if word_id != current_word: current_word = word_id current_word_index += 1 mapping[current_word_index].append(idx) # Randomly mask words mask = np.random.binomial(1, wwm_probability, (len(mapping),)) input_ids = feature[\"input_ids\"] labels = feature[\"labels\"] new_labels = [-100] * len(labels) for word_id in np.where(mask)[0]: word_id = word_id.item() for idx in mapping[word_id]: new_labels[idx] = labels[idx] input_ids[idx] = tokenizer.mask_token_id feature[\"labels\"] = new_labels return default_data_collator(features) ``` Next, we can try it on the same samples as before: ``` samples = [lm_datasets[\"train\"][i] for i in range(2)] batch = whole_word_masking_data_collator(samples) for chunk in batch[\"input_ids\"]: print(f\"\\n'>>> {tokenizer.decode(chunk)}'\") ``` ``` '>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as \" teachers \". my 35 years in the teaching profession lead me to believe that bromwell high\\'s satire is much closer to reality than is \" teachers \". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....' '>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless' ``` > ‚úèÔ∏èTry it out!Run the code snippet above several times to see the random masking happen in front of your very eyes! Also replace thetokenizer.decode()method withtokenizer.convert_ids_to_tokens()to see that the tokens from a given word are always masked together. Now that we have two data collators, the rest of the fine-tuning steps are standard. Training can take a while on Google Colab if you‚Äôre not lucky enough to score a mythical P100 GPU üò≠, so we‚Äôll first downsample the size of the training set to a few thousand examples. Don‚Äôt worry, we‚Äôll still get a pretty decent language model! A quick way to downsample a dataset in ü§ó Datasets is via the `Dataset.train_test_split()` function that we saw in [Chapter 5](/course/chapter5): ``` train_size = 10_000 test_size = int(0.1 * train_size) downsampled_dataset = lm_datasets[\"train\"].train_test_split( train_size=train_size, test_size=test_size, seed=42 ) downsampled_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 10000 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 1000 }) }) ``` This has automatically created new `train` and `test` splits, with the training set size set to 10,000 examples and the validation set to 10% of that ‚Äî feel free to increase this if you have a beefy GPU! The next thing we need to do is",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 10,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Datasets is via the `Dataset.train_test_split()` function that we saw in [Chapter 5](/course/chapter5): ``` train_size = 10_000 test_size = int(0.1 * train_size) downsampled_dataset = lm_datasets[\"train\"].train_test_split( train_size=train_size, test_size=test_size, seed=42 ) downsampled_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 10000 }) test: Dataset({ features: ['attention_mask', 'input_ids', 'labels', 'word_ids'], num_rows: 1000 }) }) ``` This has automatically created new `train` and `test` splits, with the training set size set to 10,000 examples and the validation set to 10% of that ‚Äî feel free to increase this if you have a beefy GPU! The next thing we need to do is log in to the Hugging Face Hub. If you‚Äôre running this code in a notebook, you can do so with the following utility function: ``` from huggingface_hub import notebook_login notebook_login() ``` which will display a widget where you can enter your credentials. Alternatively, you can run: ``` huggingface-cli login ``` in your favorite terminal and log in there. Once we‚Äôre logged in, we can specify the arguments for the `Trainer`: ``` from transformers import TrainingArguments batch_size = 64 # Show the training loss with every epoch logging_steps = len(downsampled_dataset[\"train\"]) // batch_size model_name = model_checkpoint.split(\"/\")[-1] training_args = TrainingArguments( output_dir=f\"{model_name}-finetuned-imdb\", overwrite_output_dir=True, evaluation_strategy=\"epoch\", learning_rate=2e-5, weight_decay=0.01, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, push_to_hub=True, fp16=True, logging_steps=logging_steps, ) ``` Here we tweaked a few of the default options, including `logging_steps` to ensure we track the training loss with each epoch. We‚Äôve also used `fp16=True` to enable mixed-precision training, which gives us another boost in speed. By default, the `Trainer` will remove any columns that are not part of the model‚Äôs `forward()` method. This means that if you‚Äôre using the whole word masking collator, you‚Äôll also need to set `remove_unused_columns=False` to ensure we don‚Äôt lose the `word_ids` column during training. Note that you can specify the name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the [huggingface-courseorganization](https://huggingface.co/huggingface-course), we added `hub_model_id=\"huggingface-course/distilbert-finetuned-imdb\"` to `TrainingArguments`. By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be `\"lewtun/distilbert-finetuned-imdb\"`. We now have all the ingredients to instantiate the `Trainer`. Here we just use the standard `data_collator`, but you can try the whole word masking collator and compare the results as an exercise: ``` from transformers import Trainer trainer = Trainer( model=model, args=training_args, train_dataset=downsampled_dataset[\"train\"], eval_dataset=downsampled_dataset[\"test\"], data_collator=data_collator, tokenizer=tokenizer, ) ``` We‚Äôre now ready to run `trainer.train()` ‚Äî but before doing so let‚Äôs briefly look at *perplexity*, which is a common metric to evaluate the performance of language models. ### Perplexity for language models Unlike other tasks like text classification or question answering where we‚Äôre given a labeled corpus to train on, with language modeling we don‚Äôt have any explicit labels. So how do we determine what makes a good language model? Like with the autocorrect feature in your phone, a good language model is one that assigns high",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 11,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Trainer trainer = Trainer( model=model, args=training_args, train_dataset=downsampled_dataset[\"train\"], eval_dataset=downsampled_dataset[\"test\"], data_collator=data_collator, tokenizer=tokenizer, ) ``` We‚Äôre now ready to run `trainer.train()` ‚Äî but before doing so let‚Äôs briefly look at *perplexity*, which is a common metric to evaluate the performance of language models. ### Perplexity for language models Unlike other tasks like text classification or question answering where we‚Äôre given a labeled corpus to train on, with language modeling we don‚Äôt have any explicit labels. So how do we determine what makes a good language model? Like with the autocorrect feature in your phone, a good language model is one that assigns high probabilities to sentences that are grammatically correct, and low probabilities to nonsense sentences. To give you a better idea of what this looks like, you can find whole sets of ‚Äúautocorrect fails‚Äù online, where the model in a person‚Äôs phone has produced some rather funny (and often inappropriate) completions! Assuming our test set consists mostly of sentences that are grammatically correct, then one way to measure the quality of our language model is to calculate the probabilities it assigns to the next word in all the sentences of the test set. High probabilities indicates that the model is not ‚Äúsurprised‚Äù or ‚Äúperplexed‚Äù by the unseen examples, and suggests it has learned the basic patterns of grammar in the language. There are various mathematical definitions of perplexity, but the one we‚Äôll use defines it as the exponential of the cross-entropy loss. Thus, we can calculate the perplexity of our pretrained model by using the `Trainer.evaluate()` function to compute the cross-entropy loss on the test set and then taking the exponential of the result: ``` import math eval_results = trainer.evaluate() print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") ``` ``` >>> Perplexity: 21.75 ``` A lower perplexity score means a better language model, and we can see here that our starting model has a somewhat large value. Let‚Äôs see if we can lower it by fine-tuning! To do that, we first run the training loop: ``` trainer.train() ``` and then compute the resulting perplexity on the test set as before: ``` eval_results = trainer.evaluate() print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") ``` ``` >>> Perplexity: 11.32 ``` Nice ‚Äî this is quite a reduction in perplexity, which tells us the model has learned something about the domain of movie reviews! Once training is finished, we can push the model card with the training information to the Hub (the checkpoints are saved during training itself): ``` trainer.push_to_hub() ``` > ‚úèÔ∏èYour turn!Run the training above after changing the data collator to the whole word masking collator. Do you get better results? In our use case we didn‚Äôt need to do anything special with the training loop, but in some cases you might need to implement some custom logic. For these applications, you can use ü§ó Accelerate ‚Äî let‚Äôs take a look! ## Fine-tuning DistilBERT with ü§ó Accelerate As we saw with the `Trainer`, fine-tuning a masked language model is very similar to the text classification example from [Chapter 3](/course/chapter3). In fact, the only",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 12,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "during training itself): ``` trainer.push_to_hub() ``` > ‚úèÔ∏èYour turn!Run the training above after changing the data collator to the whole word masking collator. Do you get better results? In our use case we didn‚Äôt need to do anything special with the training loop, but in some cases you might need to implement some custom logic. For these applications, you can use ü§ó Accelerate ‚Äî let‚Äôs take a look! ## Fine-tuning DistilBERT with ü§ó Accelerate As we saw with the `Trainer`, fine-tuning a masked language model is very similar to the text classification example from [Chapter 3](/course/chapter3). In fact, the only subtlety is the use of a special data collator, and we‚Äôve already covered that earlier in this section! However, we saw that `DataCollatorForLanguageModeling` also applies random masking with each evaluation, so we‚Äôll see some fluctuations in our perplexity scores with each training run. One way to eliminate this source of randomness is to apply the masking *once* on the whole test set, and then use the default data collator in ü§ó Transformers to collect the batches during evaluation. To see how this works, let‚Äôs implement a simple function that applies the masking on a batch, similar to our first encounter with `DataCollatorForLanguageModeling`: ``` def insert_random_mask(batch): features = [dict(zip(batch, t)) for t in zip(*batch.values())] masked_inputs = data_collator(features) # Create a new \"masked\" column for each column in the dataset return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()} ``` Next, we‚Äôll apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones. You can use whole word masking by replacing the `data_collator` above with the appropriate one, in which case you should remove the first line here: ``` downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"]) eval_dataset = downsampled_dataset[\"test\"].map( insert_random_mask, batched=True, remove_columns=downsampled_dataset[\"test\"].column_names, ) eval_dataset = eval_dataset.rename_columns( { \"masked_input_ids\": \"input_ids\", \"masked_attention_mask\": \"attention_mask\", \"masked_labels\": \"labels\", } ) ``` We can then set up the dataloaders as usual, but we‚Äôll use the `default_data_collator` from ü§ó Transformers for the evaluation set: ``` from torch.utils.data import DataLoader from transformers import default_data_collator batch_size = 64 train_dataloader = DataLoader( downsampled_dataset[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator, ) eval_dataloader = DataLoader( eval_dataset, batch_size=batch_size, collate_fn=default_data_collator ) ``` Form here, we follow the standard steps with ü§ó Accelerate. The first order of business is to load a fresh version of the pretrained model: ``` model = AutoModelForMaskedLM.from_pretrained(model_checkpoint) ``` Then we need to specify the optimizer; we‚Äôll use the standard `AdamW`: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=5e-5) ``` With these objects, we can now prepare everything for training with the `Accelerator` object: ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` Now that our model, optimizer, and dataloaders are configured, we can specify the learning rate scheduler as follows: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` There is just one last thing to do before training: create",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 13,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "standard `AdamW`: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=5e-5) ``` With these objects, we can now prepare everything for training with the `Accelerator` object: ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` Now that our model, optimizer, and dataloaders are configured, we can specify the learning rate scheduler as follows: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` There is just one last thing to do before training: create a model repository on the Hugging Face Hub! We can use the ü§ó Hub library to first generate the full name of our repo: ``` from huggingface_hub import get_full_repo_name model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate' ``` then create and clone the repository using the `Repository` class from ü§ó Hub: ``` from huggingface_hub import Repository output_dir = model_name repo = Repository(output_dir, clone_from=repo_name) ``` With that done, it‚Äôs just a simple matter of writing out the full training and evaluation loop: ``` from tqdm.auto import tqdm import torch import math progress_bar = tqdm(range(num_training_steps)) for epoch in range(num_train_epochs): # Training model.train() for batch in train_dataloader: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # Evaluation model.eval() losses = [] for step, batch in enumerate(eval_dataloader): with torch.no_grad(): outputs = model(**batch) loss = outputs.loss losses.append(accelerator.gather(loss.repeat(batch_size))) losses = torch.cat(losses) losses = losses[: len(eval_dataset)] try: perplexity = math.exp(torch.mean(losses)) except OverflowError: perplexity = float(\"inf\") print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\") # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` ``` >>> Epoch 0: Perplexity: 11.397545307900472 >>> Epoch 1: Perplexity: 10.904909330983092 >>> Epoch 2: Perplexity: 10.729503505340409 ``` Cool, we‚Äôve been able to evaluate perplexity with each epoch and ensure that multiple training runs are reproducible! ## Using our fine-tuned model You can interact with your fine-tuned model either by using its widget on the Hub or locally with the `pipeline` from ü§ó Transformers. Let‚Äôs use the latter to download our model using the `fill-mask` pipeline: ``` from transformers import pipeline mask_filler = pipeline( \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\" ) ``` We can then feed the pipeline our sample text of ‚ÄúThis is a great [MASK]‚Äù and see what the top 5 predictions are: ``` preds = mask_filler(text) for pred in preds: print(f\">>> {pred['sequence']}\") ``` ``` '>>> this is a great movie.' '>>> this is a great film.' '>>> this is a great story.' '>>> this is a great movies.' '>>> this is a great character.' ``` Neat ‚Äî our model has clearly adapted its weights to predict words that are more strongly associated with movies! This wraps up our first experiment with training a language model. In [section 6](/course/en/chapter7/6) you‚Äôll learn how to train an auto-regressive model like GPT-2 from scratch; head over there if you‚Äôd like to see how you can pretrain your very own Transformer model! > ‚úèÔ∏èTry it out!To quantify",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 14,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` ``` '>>> this is a great movie.' '>>> this is a great film.' '>>> this is a great story.' '>>> this is a great movies.' '>>> this is a great character.' ``` Neat ‚Äî our model has clearly adapted its weights to predict words that are more strongly associated with movies! This wraps up our first experiment with training a language model. In [section 6](/course/en/chapter7/6) you‚Äôll learn how to train an auto-regressive model like GPT-2 from scratch; head over there if you‚Äôd like to see how you can pretrain your very own Transformer model! > ‚úèÔ∏èTry it out!To quantify the benefits of domain adaptation, fine-tune a classifier on the IMDb labels for both the pretrained and fine-tuned DistilBERT checkpoints. If you need a refresher on text classification, check outChapter 3.",
    "metadata": {
      "title": "Fine-tuning a masked language model",
      "url": "https://huggingface.co/learn/llm-course/chapter7/3",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/3",
      "part": 15,
      "total_parts": 15,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Translation Let‚Äôs now dive into translation. This is another [sequence-to-sequence task](/course/chapter1/7), which means it‚Äôs a problem that can be formulated as going from one sequence to another. In that sense the problem is pretty close to [summarization](/course/chapter7/6), and you could adapt what we will see here to other sequence-to-sequence problems such as: - **Style transfer**: Creating a model that *translates* texts written in a certain style to another (e.g., formal to casual or Shakespearean English to modern English) - **Generative question answering**: Creating a model that generates answers to questions, given a context If you have a big enough corpus of texts in two (or more) languages, you can train a new translation model from scratch like we will in the section on [causal language modeling](/course/chapter7/6). It will be faster, however, to fine-tune an existing translation model, be it a multilingual one like mT5 or mBART that you want to fine-tune to a specific language pair, or even a model specialized for translation from one language to another that you want to fine-tune to your specific corpus. In this section, we will fine-tune a Marian model pretrained to translate from English to French (since a lot of Hugging Face employees speak both those languages) on the [KDE4 dataset](https://huggingface.co/datasets/kde4), which is a dataset of localized files for the [KDE apps](https://apps.kde.org/). The model we will use has been pretrained on a large corpus of French and English texts taken from the [Opus dataset](https://opus.nlpl.eu/), which actually contains the KDE4 dataset. But even if the pretrained model we use has seen that data during its pretraining, we will see that we can get a better version of it after fine-tuning. Once we‚Äôre finished, we will have a model able to make predictions like this one: As in the previous sections, you can find the actual model that we‚Äôll train and upload to the Hub using the code below and double-check its predictions [here](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.). ## Preparing the data To fine-tune or train a translation model from scratch, we will need a dataset suitable for the task. As mentioned previously, we‚Äôll use the [KDE4 dataset](https://huggingface.co/datasets/kde4) in this section, but you can adapt the code to use your own data quite easily, as long as you have pairs of sentences in the two languages you want to translate from and into. Refer back to [Chapter 5](/course/chapter5) if you need a reminder of how to load your custom data in a `Dataset`. ### The KDE4 dataset As usual, we download our dataset using the `load_dataset()` function: ``` from datasets import load_dataset raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\") ``` If you want to work with a different pair of languages, you can specify them by their codes. A total of 92 languages are available for this dataset; you can see them all by expanding the language tags on its [dataset card](https://huggingface.co/datasets/kde4). ![Language available for the KDE4 dataset.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png) Let‚Äôs have a look at the dataset: ``` raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['id', 'translation'], num_rows: 210173 }) })",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 1,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your custom data in a `Dataset`. ### The KDE4 dataset As usual, we download our dataset using the `load_dataset()` function: ``` from datasets import load_dataset raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\") ``` If you want to work with a different pair of languages, you can specify them by their codes. A total of 92 languages are available for this dataset; you can see them all by expanding the language tags on its [dataset card](https://huggingface.co/datasets/kde4). ![Language available for the KDE4 dataset.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png) Let‚Äôs have a look at the dataset: ``` raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['id', 'translation'], num_rows: 210173 }) }) ``` We have 210,173 pairs of sentences, but in one single split, so we will need to create our own validation set. As we saw in [Chapter 5](/course/chapter5), a `Dataset` has a `train_test_split()` method that can help us. We‚Äôll provide a seed for reproducibility: ``` split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20) split_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['id', 'translation'], num_rows: 189155 }) test: Dataset({ features: ['id', 'translation'], num_rows: 21018 }) }) ``` We can rename the `\"test\"` key to `\"validation\"` like this: ``` split_datasets[\"validation\"] = split_datasets.pop(\"test\") ``` Now let‚Äôs take a look at one element of the dataset: ``` split_datasets[\"train\"][1][\"translation\"] ``` ``` {'en': 'Default to expanded threads', 'fr': 'Par d√©faut, d√©velopper les fils de discussion'} ``` We get a dictionary with two sentences in the pair of languages we requested. One particularity of this dataset full of technical computer science terms is that they are all fully translated in French. However, French engineers leave most computer science-specific words in English when they talk. Here, for instance, the word ‚Äúthreads‚Äù might well appear in a French sentence, especially in a technical conversation; but in this dataset it has been translated into the more correct ‚Äúfils de discussion.‚Äù The pretrained model we use, which has been pretrained on a larger corpus of French and English sentences, takes the easier option of leaving the word as is: ``` from transformers import pipeline model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\" translator = pipeline(\"translation\", model=model_checkpoint) translator(\"Default to expanded threads\") ``` ``` [{'translation_text': 'Par d√©faut pour les threads √©largis'}] ``` Another example of this behavior can be seen with the word ‚Äúplugin,‚Äù which isn‚Äôt officially a French word but which most native speakers will understand and not bother to translate. In the KDE4 dataset this word has been translated in French into the more official ‚Äúmodule d‚Äôextension‚Äù: ``` split_datasets[\"train\"][172][\"translation\"] ``` ``` {'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.', 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"} ``` Our pretrained model, however, sticks with the compact and familiar English word: ``` translator( \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\" ) ``` ``` [{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}] ``` It will be interesting to see if our fine-tuned model picks up on",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 2,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.', 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"} ``` Our pretrained model, however, sticks with the compact and familiar English word: ``` translator( \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\" ) ``` ``` [{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}] ``` It will be interesting to see if our fine-tuned model picks up on those particularities of the dataset (spoiler alert: it will). > ‚úèÔ∏èYour turn!Another English word that is often used in French is ‚Äúemail.‚Äù Find the first sample in the training dataset that uses this word. How is it translated? How does the pretrained model translate the same English sentence? ### Processing the data You should know the drill by now: the texts all need to be converted into sets of token IDs so the model can make sense of them. For this task, we‚Äôll need to tokenize both the inputs and the targets. Our first task is to create our `tokenizer` object. As noted earlier, we‚Äôll be using a Marian English to French pretrained model. If you are trying this code with another pair of languages, make sure to adapt the model checkpoint. The [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) organization provides more than a thousand models in multiple languages. ``` from transformers import AutoTokenizer model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\") ``` You can also replace the `model_checkpoint` with any other model you prefer from the [Hub](https://huggingface.co/models), or a local folder where you‚Äôve saved a pretrained model and a tokenizer. > üí° If you are using a multilingual tokenizer such as mBART, mBART-50, or M2M100, you will need to set the language codes of your inputs and targets in the tokenizer by settingtokenizer.src_langandtokenizer.tgt_langto the right values. The preparation of our data is pretty straightforward. There‚Äôs just one thing to remember; you need to ensure that the tokenizer processes the targets in the output language (here, French). You can do this by passing the targets to the `text_targets` argument of the tokenizer‚Äôs `__call__` method. To see how this works, let‚Äôs process one sample of each language in the training set: ``` en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"] fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"] inputs = tokenizer(en_sentence, text_target=fr_sentence) inputs ``` ``` {'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]} ``` As we can see, the output contains the input IDs associated with the English sentence, while the IDs associated with the French one are stored in the `labels` field. If you forget to indicate that you are tokenizing labels, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all: ``` wrong_targets = tokenizer(fr_sentence) print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"])) ``` ``` ['‚ñÅPar', '‚ñÅd√©', 'f', 'aut', ',', '‚ñÅd√©',",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 3,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]} ``` As we can see, the output contains the input IDs associated with the English sentence, while the IDs associated with the French one are stored in the `labels` field. If you forget to indicate that you are tokenizing labels, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all: ``` wrong_targets = tokenizer(fr_sentence) print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"])) print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"])) ``` ``` ['‚ñÅPar', '‚ñÅd√©', 'f', 'aut', ',', '‚ñÅd√©', 've', 'lop', 'per', '‚ñÅles', '‚ñÅfil', 's', '‚ñÅde', '‚ñÅdiscussion', '</s>'] ['‚ñÅPar', '‚ñÅd√©faut', ',', '‚ñÅd√©velopper', '‚ñÅles', '‚ñÅfils', '‚ñÅde', '‚ñÅdiscussion', '</s>'] ``` As we can see, using the English tokenizer to preprocess a French sentence results in a lot more tokens, since the tokenizer doesn‚Äôt know any French words (except those that also appear in the English language, like ‚Äúdiscussion‚Äù). Since `inputs` is a dictionary with our usual keys (input IDs, attention mask, etc.), the last step is to define the preprocessing function we will apply on the datasets: ``` max_length = 128 def preprocess_function(examples): inputs = [ex[\"en\"] for ex in examples[\"translation\"]] targets = [ex[\"fr\"] for ex in examples[\"translation\"]] model_inputs = tokenizer( inputs, text_target=targets, max_length=max_length, truncation=True ) return model_inputs ``` Note that we set the same maximum length for our inputs and outputs. Since the texts we‚Äôre dealing with seem pretty short, we use 128. > üí° If you are using a T5 model (more specifically, one of thet5-xxxcheckpoints), the model will expect the text inputs to have a prefix indicating the task at hand, such astranslate: English to French:. > ‚ö†Ô∏è We don‚Äôt pay attention to the attention mask of the targets, as the model won‚Äôt expect it. Instead, the labels corresponding to a padding token should be set to-100so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to-100. We can now apply that preprocessing in one go on all the splits of our dataset: ``` tokenized_datasets = split_datasets.map( preprocess_function, batched=True, remove_columns=split_datasets[\"train\"].column_names, ) ``` Now that the data has been preprocessed, we are ready to fine-tune our pretrained model! ## Fine-tuning the model with the Trainer API The actual code using the `Trainer` will be the same as before, with just one little change: we use a [Seq2SeqTrainer](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) here, which is a subclass of `Trainer` that will allow us to properly deal with the evaluation, using the `generate()` method to predict outputs from the inputs. We‚Äôll dive into that in more detail when we talk about the metric computation. First things first, we need an actual model to fine-tune. We‚Äôll use the usual `AutoModel` API: ``` from transformers import AutoModelForSeq2SeqLM model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` Note that this time we are using a model that was",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 4,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Trainer API The actual code using the `Trainer` will be the same as before, with just one little change: we use a [Seq2SeqTrainer](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) here, which is a subclass of `Trainer` that will allow us to properly deal with the evaluation, using the `generate()` method to predict outputs from the inputs. We‚Äôll dive into that in more detail when we talk about the metric computation. First things first, we need an actual model to fine-tune. We‚Äôll use the usual `AutoModel` API: ``` from transformers import AutoModelForSeq2SeqLM model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` Note that this time we are using a model that was trained on a translation task and can actually be used already, so there is no warning about missing weights or newly initialized ones. ### Data collation We‚Äôll need a data collator to deal with the padding for dynamic batching. We can‚Äôt just use a `DataCollatorWithPadding` like in [Chapter 3](/course/chapter3) in this case, because that only pads the inputs (input IDs, attention mask, and token type IDs). Our labels should also be padded to the maximum length encountered in the labels. And, as mentioned previously, the padding value used to pad the labels should be `-100` and not the padding token of the tokenizer, to make sure those padded values are ignored in the loss computation. This is all done by a [DataCollatorForSeq2Seq](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq). Like the `DataCollatorWithPadding`, it takes the `tokenizer` used to preprocess the inputs, but it also takes the `model`. This is because this data collator will also be responsible for preparing the decoder input IDs, which are shifted versions of the labels with a special token at the beginning. Since this shift is done slightly differently for different architectures, the `DataCollatorForSeq2Seq` needs to know the `model` object: ``` from transformers import DataCollatorForSeq2Seq data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) ``` To test this on a few samples, we just call it on a list of examples from our tokenized training set: ``` batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)]) batch.keys() ``` ``` dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids']) ``` We can check our labels have been padded to the maximum length of the batch, using `-100`: ``` batch[\"labels\"] ``` ``` tensor([[ 577, 5891, 2, 3184, 16, 2542, 5, 1710, 0, -100, -100, -100, -100, -100, -100, -100], [ 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]]) ``` And we can also have a look at the decoder input IDs, to see that they are shifted versions of the labels: ``` batch[\"decoder_input_ids\"] ``` ``` tensor([[59513, 577, 5891, 2, 3184, 16, 2542, 5, 1710, 0, 59513, 59513, 59513, 59513, 59513, 59513], [59513, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649]]) ``` Here are the labels for the first and second elements in our dataset: ``` for i in range(1, 3): print(tokenized_datasets[\"train\"][i][\"labels\"]) ``` ``` [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0] [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0] ``` We",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 5,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the decoder input IDs, to see that they are shifted versions of the labels: ``` batch[\"decoder_input_ids\"] ``` ``` tensor([[59513, 577, 5891, 2, 3184, 16, 2542, 5, 1710, 0, 59513, 59513, 59513, 59513, 59513, 59513], [59513, 1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649]]) ``` Here are the labels for the first and second elements in our dataset: ``` for i in range(1, 3): print(tokenized_datasets[\"train\"][i][\"labels\"]) ``` ``` [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0] [1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0] ``` We will pass this `data_collator` along to the `Seq2SeqTrainer`. Next, let‚Äôs have a look at the metric. ### Metrics The feature that `Seq2SeqTrainer` adds to its superclass `Trainer` is the ability to use the `generate()` method during evaluation or prediction. During training, the model will use the `decoder_input_ids` with an attention mask ensuring it does not use the tokens after the token it‚Äôs trying to predict, to speed up training. During inference we won‚Äôt be able to use those since we won‚Äôt have labels, so it‚Äôs a good idea to evaluate our model with the same setup. As we saw in [Chapter 1](/course/chapter1/6), the decoder performs inference by predicting tokens one by one ‚Äî something that‚Äôs implemented behind the scenes in ü§ó Transformers by the `generate()` method. The `Seq2SeqTrainer` will let us use that method for evaluation if we set `predict_with_generate=True`. The traditional metric used for translation is the [BLEU score](https://en.wikipedia.org/wiki/BLEU), introduced in [a 2002 article](https://aclanthology.org/P02-1040.pdf) by Kishore Papineni et al. The BLEU score evaluates how close the translations are to their labels. It does not measure the intelligibility or grammatical correctness of the model‚Äôs generated outputs, but uses statistical rules to ensure that all the words in the generated outputs also appear in the targets. In addition, there are rules that penalize repetitions of the same words if they are not also repeated in the targets (to avoid the model outputting sentences like `\"the the the the the\"`) and output sentences that are shorter than those in the targets (to avoid the model outputting sentences like `\"the\"`). One weakness with BLEU is that it expects the text to already be tokenized, which makes it difficult to compare scores between models that use different tokenizers. So instead, the most commonly used metric for benchmarking translation models today is [SacreBLEU](https://github.com/mjpost/sacrebleu), which addresses this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the SacreBLEU library: ``` !pip install sacrebleu ``` We can then load it via `evaluate.load()` like we did in [Chapter 3](/course/chapter3): ``` import evaluate metric = evaluate.load(\"sacrebleu\") ``` This metric will take texts as inputs and targets. It is designed to accept several acceptable targets, as there are often multiple acceptable translations of the same sentence ‚Äî the dataset we‚Äôre using only provides one, but it‚Äôs not uncommon in NLP to find datasets that give several sentences as labels. So, the predictions",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 6,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "this weakness (and others) by standardizing the tokenization step. To use this metric, we first need to install the SacreBLEU library: ``` !pip install sacrebleu ``` We can then load it via `evaluate.load()` like we did in [Chapter 3](/course/chapter3): ``` import evaluate metric = evaluate.load(\"sacrebleu\") ``` This metric will take texts as inputs and targets. It is designed to accept several acceptable targets, as there are often multiple acceptable translations of the same sentence ‚Äî the dataset we‚Äôre using only provides one, but it‚Äôs not uncommon in NLP to find datasets that give several sentences as labels. So, the predictions should be a list of sentences, but the references should be a list of lists of sentences. Let‚Äôs try an example: ``` predictions = [ \"This plugin lets you translate web pages between several languages automatically.\" ] references = [ [ \"This plugin allows you to automatically translate web pages between several languages.\" ] ] metric.compute(predictions=predictions, references=references) ``` ``` {'score': 46.750469682990165, 'counts': [11, 6, 4, 3], 'totals': [12, 11, 10, 9], 'precisions': [91.67, 54.54, 40.0, 33.33], 'bp': 0.9200444146293233, 'sys_len': 12, 'ref_len': 13} ``` This gets a BLEU score of 46.75, which is rather good ‚Äî for reference, the original Transformer model in the [‚ÄúAttention Is All You Need‚Äù paper](https://arxiv.org/pdf/1706.03762.pdf) achieved a BLEU score of 41.8 on a similar translation task between English and French! (For more information about the individual metrics, like `counts` and `bp`, see the [SacreBLEU repository](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74).) On the other hand, if we try with the two bad types of predictions (lots of repetitions or too short) that often come out of translation models, we will get rather bad BLEU scores: ``` predictions = [\"This This This This\"] references = [ [ \"This plugin allows you to automatically translate web pages between several languages.\" ] ] metric.compute(predictions=predictions, references=references) ``` ``` {'score': 1.683602693167689, 'counts': [1, 0, 0, 0], 'totals': [4, 3, 2, 1], 'precisions': [25.0, 16.67, 12.5, 12.5], 'bp': 0.10539922456186433, 'sys_len': 4, 'ref_len': 13} ``` ``` predictions = [\"This plugin\"] references = [ [ \"This plugin allows you to automatically translate web pages between several languages.\" ] ] metric.compute(predictions=predictions, references=references) ``` ``` {'score': 0.0, 'counts': [2, 1, 0, 0], 'totals': [2, 1, 0, 0], 'precisions': [100.0, 100.0, 0.0, 0.0], 'bp': 0.004086771438464067, 'sys_len': 2, 'ref_len': 13} ``` The score can go from 0 to 100, and higher is better. To get from the model outputs to texts the metric can use, we will use the `tokenizer.batch_decode()` method. We just have to clean up all the `-100`s in the labels (the tokenizer will automatically do the same for the padding token): ``` import numpy as np def compute_metrics(eval_preds): preds, labels = eval_preds # In case the model returns more than the prediction logits if isinstance(preds, tuple): preds = preds[0] decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # Replace -100s in the labels as we can't decode them labels = np.where(labels != -100, labels, tokenizer.pad_token_id) decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Some simple post-processing decoded_preds = [pred.strip() for pred in decoded_preds] decoded_labels = [[label.strip()] for label",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 7,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "use, we will use the `tokenizer.batch_decode()` method. We just have to clean up all the `-100`s in the labels (the tokenizer will automatically do the same for the padding token): ``` import numpy as np def compute_metrics(eval_preds): preds, labels = eval_preds # In case the model returns more than the prediction logits if isinstance(preds, tuple): preds = preds[0] decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # Replace -100s in the labels as we can't decode them labels = np.where(labels != -100, labels, tokenizer.pad_token_id) decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Some simple post-processing decoded_preds = [pred.strip() for pred in decoded_preds] decoded_labels = [[label.strip()] for label in decoded_labels] result = metric.compute(predictions=decoded_preds, references=decoded_labels) return {\"bleu\": result[\"score\"]} ``` Now that this is done, we are ready to fine-tune our model! ### Fine-tuning the model The first step is to log in to Hugging Face, so you‚Äôre able to upload your results to the Model Hub. There‚Äôs a convenience function to help you with this in a notebook: ``` from huggingface_hub import notebook_login notebook_login() ``` This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` Once this is done, we can define our `Seq2SeqTrainingArguments`. Like for the `Trainer`, we use a subclass of `TrainingArguments` that contains a few more fields: ``` from transformers import Seq2SeqTrainingArguments args = Seq2SeqTrainingArguments( f\"marian-finetuned-kde4-en-to-fr\", evaluation_strategy=\"no\", save_strategy=\"epoch\", learning_rate=2e-5, per_device_train_batch_size=32, per_device_eval_batch_size=64, weight_decay=0.01, save_total_limit=3, num_train_epochs=3, predict_with_generate=True, fp16=True, push_to_hub=True, ) ``` Apart from the usual hyperparameters (like learning rate, number of epochs, batch size, and some weight decay), here are a few changes compared to what we saw in the previous sections: - We don‚Äôt set any regular evaluation, as evaluation takes a while; we will just evaluate our model once before training and after. - We set `fp16=True`, which speeds up training on modern GPUs. - We set `predict_with_generate=True`, as discussed above. - We use `push_to_hub=True` to upload the model to the Hub at the end of each epoch. Note that you can specify the full name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the [huggingface-courseorganization](https://huggingface.co/huggingface-course), we added `hub_model_id=\"huggingface-course/marian-finetuned-kde4-en-to-fr\"` to `Seq2SeqTrainingArguments`. By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be `\"sgugger/marian-finetuned-kde4-en-to-fr\"` (which is the model we linked to at the beginning of this section). > üí° If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn‚Äôt, you‚Äôll get an error when defining yourSeq2SeqTrainerand will need to set a new name. Finally, we just pass everything to the `Seq2SeqTrainer`: ``` from transformers import Seq2SeqTrainer trainer = Seq2SeqTrainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics, ) ``` Before training, we‚Äôll first look at the score our",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 8,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "you set, so in our case it will be `\"sgugger/marian-finetuned-kde4-en-to-fr\"` (which is the model we linked to at the beginning of this section). > üí° If the output directory you are using already exists, it needs to be a local clone of the repository you want to push to. If it isn‚Äôt, you‚Äôll get an error when defining yourSeq2SeqTrainerand will need to set a new name. Finally, we just pass everything to the `Seq2SeqTrainer`: ``` from transformers import Seq2SeqTrainer trainer = Seq2SeqTrainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics, ) ``` Before training, we‚Äôll first look at the score our model gets, to double-check that we‚Äôre not making things worse with our fine-tuning. This command will take a bit of time, so you can grab a coffee while it executes: ``` trainer.evaluate(max_length=max_length) ``` ``` {'eval_loss': 1.6964408159255981, 'eval_bleu': 39.26865061007616, 'eval_runtime': 965.8884, 'eval_samples_per_second': 21.76, 'eval_steps_per_second': 0.341} ``` A BLEU score of 39 is not too bad, which reflects the fact that our model is already good at translating English sentences to French ones. Next is the training, which will also take a bit of time: ``` trainer.train() ``` Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. Once training is done, we evaluate our model again ‚Äî hopefully we will see some amelioration in the BLEU score! ``` trainer.evaluate(max_length=max_length) ``` ``` {'eval_loss': 0.8558505773544312, 'eval_bleu': 52.94161337775576, 'eval_runtime': 714.2576, 'eval_samples_per_second': 29.426, 'eval_steps_per_second': 0.461, 'epoch': 3.0} ``` That‚Äôs a nearly 14-point improvement, which is great. Finally, we use the `push_to_hub()` method to make sure we upload the latest version of the model. The `Trainer` also drafts a model card with all the evaluation results and uploads it. This model card contains metadata that helps the Model Hub pick the widget for the inference demo. Usually, there is no need to say anything as it can infer the right widget from the model class, but in this case, the same model class can be used for all kinds of sequence-to-sequence problems, so we specify it‚Äôs a translation model: ``` trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\") ``` This command returns the URL of the commit it just did, if you want to inspect it: ``` 'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3' ``` At this stage, you can use the inference widget on the Model Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task ‚Äî congratulations! If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using ü§ó Accelerate. ## A custom training loop Let‚Äôs now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in [section 2](/course/chapter7/2) and [Chapter 3](/course/chapter3/4). ### Preparing everything for training You‚Äôve seen all of this a",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 9,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Hub to test your model and share it with your friends. You have successfully fine-tuned a model on a translation task ‚Äî congratulations! If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using ü§ó Accelerate. ## A custom training loop Let‚Äôs now take a look at the full training loop, so you can easily customize the parts you need. It will look a lot like what we did in [section 2](/course/chapter7/2) and [Chapter 3](/course/chapter3/4). ### Preparing everything for training You‚Äôve seen all of this a few times now, so we‚Äôll go through the code quite quickly. First we‚Äôll build the `DataLoader`s from our datasets, after setting the datasets to the `\"torch\"` format so we get PyTorch tensors: ``` from torch.utils.data import DataLoader tokenized_datasets.set_format(\"torch\") train_dataloader = DataLoader( tokenized_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=8, ) eval_dataloader = DataLoader( tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8 ) ``` Next we reinstantiate our model, to make sure we‚Äôre not continuing the fine-tuning from before but starting from the pretrained model again: ``` model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` Then we will need an optimizer: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=2e-5) ``` Once we have all those objects, we can send them to the `accelerator.prepare()` method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn‚Äôt execute any cell that instantiates an `Accelerator`. ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember we should always do this after preparing the dataloader, as that method will change the length of the `DataLoader`. We use a classic linear schedule from the learning rate to 0: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you‚Äôre not logged in already. We‚Äôll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does): ``` from huggingface_hub import Repository, get_full_repo_name model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'sgugger/marian-finetuned-kde4-en-to-fr-accelerate' ``` Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with: ``` output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 10,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does): ``` from huggingface_hub import Repository, get_full_repo_name model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'sgugger/marian-finetuned-kde4-en-to-fr-accelerate' ``` Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with: ``` output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch. ### Training loop We are now ready to write the full training loop. To simplify its evaluation part, we define this `postprocess()` function that takes predictions and labels and converts them to the lists of strings our `metric` object will expect: ``` def postprocess(predictions, labels): predictions = predictions.cpu().numpy() labels = labels.cpu().numpy() decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True) # Replace -100 in the labels as we can't decode them. labels = np.where(labels != -100, labels, tokenizer.pad_token_id) decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Some simple post-processing decoded_preds = [pred.strip() for pred in decoded_preds] decoded_labels = [[label.strip()] for label in decoded_labels] return decoded_preds, decoded_labels ``` The training loop looks a lot like the ones in [section 2](/course/chapter7/2) and [Chapter 3](/course/chapter3), with a few differences in the evaluation part ‚Äî so let‚Äôs focus on that! The first thing to note is that we use the `generate()` method to compute predictions, but this is a method on our base model, not the wrapped model ü§ó Accelerate created in the `prepare()` method. That‚Äôs why we unwrap the model first, then call this method. The second thing is that, like with [token classification](/course/chapter7/2), two processes may have padded the inputs and labels to different shapes, so we use `accelerator.pad_across_processes()` to make the predictions and labels the same shape before calling the `gather()` method. If we don‚Äôt do this, the evaluation will either error out or hang forever. ``` from tqdm.auto import tqdm import torch progress_bar = tqdm(range(num_training_steps)) for epoch in range(num_train_epochs): # Training model.train() for batch in train_dataloader: outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # Evaluation model.eval() for batch in tqdm(eval_dataloader): with torch.no_grad(): generated_tokens = accelerator.unwrap_model(model).generate( batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_length=128, ) labels = batch[\"labels\"] # Necessary to pad predictions and labels for being gathered generated_tokens = accelerator.pad_across_processes( generated_tokens, dim=1, pad_index=tokenizer.pad_token_id ) labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100) predictions_gathered = accelerator.gather(generated_tokens) labels_gathered = accelerator.gather(labels) decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered) metric.add_batch(predictions=decoded_preds, references=decoded_labels) results = metric.compute() print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\") # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` ``` epoch 0, BLEU score: 53.47 epoch 1, BLEU score: 54.24 epoch 2, BLEU score: 54.44 ``` Once this is done, you should have a model that has results pretty similar to the one trained with the `Seq2SeqTrainer`. You can check the one we trained using this code at [huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). And if you want to test out",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 11,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "predictions_gathered = accelerator.gather(generated_tokens) labels_gathered = accelerator.gather(labels) decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered) metric.add_batch(predictions=decoded_preds, references=decoded_labels) results = metric.compute() print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\") # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` ``` epoch 0, BLEU score: 53.47 epoch 1, BLEU score: 54.24 epoch 2, BLEU score: 54.44 ``` Once this is done, you should have a model that has results pretty similar to the one trained with the `Seq2SeqTrainer`. You can check the one we trained using this code at [huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate). And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above! ## Using the fine-tuned model We‚Äôve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a `pipeline`, we just have to specify the proper model identifier: ``` from transformers import pipeline # Replace this with your own checkpoint model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\" translator = pipeline(\"translation\", model=model_checkpoint) translator(\"Default to expanded threads\") ``` ``` [{'translation_text': 'Par d√©faut, d√©velopper les fils de discussion'}] ``` As expected, our pretrained model adapted its knowledge to the corpus we fine-tuned it on, and instead of leaving the English word ‚Äúthreads‚Äù alone, it now translates it to the French official version. It‚Äôs the same for ‚Äúplugin‚Äù: ``` translator( \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\" ) ``` ``` [{'translation_text': \"Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format.\"}] ``` Another great example of domain adaptation! > ‚úèÔ∏èYour turn!What does the model return on the sample with the word ‚Äúemail‚Äù you identified earlier?",
    "metadata": {
      "title": "Translation",
      "url": "https://huggingface.co/learn/llm-course/chapter7/4",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/4",
      "part": 12,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Summarization In this section we‚Äôll take a look at how Transformer models can be used to condense long documents into summaries, a task known as *text summarization*. This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail. Although there already exist various fine-tuned models for summarization on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads), almost all of these are only suitable for English documents. So, to add a twist in this section, we‚Äôll train a bilingual model for English and Spanish. By the end of this section, you‚Äôll have a [model](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) that can summarize customer reviews like the one shown here: As we‚Äôll see, these summaries are concise because they‚Äôre learned from the titles that customers provide in their product reviews. Let‚Äôs start by putting together a suitable bilingual corpus for this task. ## Preparing a multilingual corpus We‚Äôll use the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let‚Äôs download the English and Spanish subsets from the Hugging Face Hub: ``` from datasets import load_dataset spanish_dataset = load_dataset(\"amazon_reviews_multi\", \"es\") english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\") english_dataset ``` ``` DatasetDict({ train: Dataset({ features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'], num_rows: 200000 }) validation: Dataset({ features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'], num_rows: 5000 }) test: Dataset({ features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'], num_rows: 5000 }) }) ``` As you can see, for each language there are 200,000 reviews for the `train` split, and 5,000 reviews for each of the `validation` and `test` splits. The review information we are interested in is contained in the `review_body` and `review_title` columns. Let‚Äôs take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in [Chapter 5](/course/chapter5): ``` def show_samples(dataset, num_samples=3, seed=42): sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples)) for example in sample: print(f\"\\n'>> Title: {example['review_title']}'\") print(f\"'>> Review: {example['review_body']}'\") show_samples(english_dataset) ``` ``` '>> Title: Worked in front position, not rear' '>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.' '>> Title: meh' '>> Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue' '>> Title: Can\\'t beat these for the money' '>> Review: Bought this for handling miscellaneous aircraft parts",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 1,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "for example in sample: print(f\"\\n'>> Title: {example['review_title']}'\") print(f\"'>> Review: {example['review_body']}'\") show_samples(english_dataset) ``` ``` '>> Title: Worked in front position, not rear' '>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.' '>> Title: meh' '>> Review: Does it‚Äôs job and it‚Äôs gorgeous but mine is falling apart, I had to basically put it together again with hot glue' '>> Title: Can\\'t beat these for the money' '>> Review: Bought this for handling miscellaneous aircraft parts and hanger \"stuff\" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.' ``` > ‚úèÔ∏èTry it out!Change the random seed in theDataset.shuffle()command to explore other reviews in the corpus. If you‚Äôre a Spanish speaker, take a look at some of the reviews inspanish_datasetto see if the titles also seem like reasonable summaries. This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the ‚Äúmeh‚Äù title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we‚Äôll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let‚Äôs convert `english_dataset` to a `pandas.DataFrame` and compute the number of reviews per product category: ``` english_dataset.set_format(\"pandas\") english_df = english_dataset[\"train\"][:] # Show counts for top 20 products english_df[\"product_category\"].value_counts()[:20] ``` ``` home 17679 apparel 15951 wireless 15717 other",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 2,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Although the example with the ‚Äúmeh‚Äù title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we‚Äôll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let‚Äôs convert `english_dataset` to a `pandas.DataFrame` and compute the number of reviews per product category: ``` english_dataset.set_format(\"pandas\") english_df = english_dataset[\"train\"][:] # Show counts for top 20 products english_df[\"product_category\"].value_counts()[:20] ``` ``` home 17679 apparel 15951 wireless 15717 other 13418 beauty 12091 drugstore 11730 kitchen 10382 toy 8745 sports 8277 automotive 7506 lawn_and_garden 7327 home_improvement 7136 pet_products 7082 digital_ebook_purchase 6749 pc 6401 electronics 6186 office_product 5521 shoes 5197 grocery 4730 book 3756 Name: product_category, dtype: int64 ``` The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let‚Äôs focus on summarizing book reviews ‚Äî after all, this is what the company was founded on! We can see two product categories that fit the bill (`book` and `digital_ebook_purchase`), so let‚Äôs filter the datasets in both languages for just these products. As we saw in [Chapter 5](/course/chapter5), the `Dataset.filter()` function allows us to slice a dataset very efficiently, so we can define a simple function to do this: ``` def filter_books(example): return ( example[\"product_category\"] == \"book\" or example[\"product_category\"] == \"digital_ebook_purchase\" ) ``` Now when we apply this function to `english_dataset` and `spanish_dataset`, the result will contain just those rows involving the book categories. Before applying the filter, let‚Äôs switch the format of `english_dataset` from `\"pandas\"` back to `\"arrow\"`: ``` english_dataset.reset_format() ``` We can then apply the filter function, and as a sanity check let‚Äôs inspect a sample of reviews to see if they are indeed about books: ``` spanish_books = spanish_dataset.filter(filter_books) english_books = english_dataset.filter(filter_books) show_samples(english_books) ``` ``` '>> Title: I\\'m dissapointed.' '>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.' '>> Title: Good art, good price, poor design' '>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar' '>> Title: Helpful' '>> Review: Nearly all the tips useful and. I consider myself",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 3,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and is kind of a brat. I\\'m dissapointed.' '>> Title: Good art, good price, poor design' '>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar' '>> Title: Helpful' '>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.' ``` Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single `DatasetDict` object. ü§ó Datasets provides a handy `concatenate_datasets()` function that (as the name suggests) will stack two `Dataset` objects on top of each other. So, to create our bilingual dataset, we‚Äôll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn‚Äôt overfit to a single language: ``` from datasets import concatenate_datasets, DatasetDict books_dataset = DatasetDict() for split in english_books.keys(): books_dataset[split] = concatenate_datasets( [english_books[split], spanish_books[split]] ) books_dataset[split] = books_dataset[split].shuffle(seed=42) # Peek at a few examples show_samples(books_dataset) ``` ``` '>> Title: Easy to follow!!!!' '>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.' '>> Title: PARCIALMENTE DA√ëADO' '>> Review: Me lleg√≥ el d√≠a que tocaba, junto a otros libros que ped√≠, pero la caja lleg√≥ en mal estado lo cual da√±√≥ las esquinas de los libros porque ven√≠an sin protecci√≥n (forro).' '>> Title: no lo he podido descargar' '>> Review: igual que el anterior' ``` This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words: ![Word count distributions for the review titles and texts.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg) ![Word count distributions for the review titles and texts.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg) To deal with this, we‚Äôll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we‚Äôre dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 4,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words: ![Word count distributions for the review titles and texts.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg) ![Word count distributions for the review titles and texts.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg) To deal with this, we‚Äôll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we‚Äôre dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty `Dataset.filter()` method as follows: ``` books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2) ``` Now that we‚Äôve prepared our corpus, let‚Äôs take a look at a few possible Transformer models that one might fine-tune on it! ## Models for text summarization If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we‚Äôd like to ‚Äútranslate‚Äù into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in [Chapter 1](/course/chapter1), although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization. Transformer model Description Multilingual? [GPT-2](https://huggingface.co/gpt2-xl) Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending ‚ÄúTL;DR‚Äù at the end of the input text. ‚ùå [PEGASUS](https://huggingface.co/google/pegasus-large) Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks. ‚ùå [T5](https://huggingface.co/t5-base) A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is `summarize: ARTICLE`. ‚ùå [mT5](https://huggingface.co/google/mt5-base) A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages. ‚úÖ [BART](https://huggingface.co/facebook/bart-base) A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2. ‚ùå [mBART-50](https://huggingface.co/facebook/mbart-large-50) A multilingual version of BART, pretrained on 50 languages. ‚úÖ As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a ‚Äúhigh-resource‚Äù language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once! We‚Äôll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 5,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tasks) are monolingual. This is great if your task is in a ‚Äúhigh-resource‚Äù language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once! We‚Äôll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like `summarize:` which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model! ![Different tasks performed by the T5 architecture.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg) ![Different tasks performed by the T5 architecture.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg) mT5 doesn‚Äôt use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we‚Äôve picked a model, let‚Äôs take a look at preparing our data for training. > ‚úèÔ∏èTry it out!Once you‚Äôve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you‚Äôll need to prependsummarize:to the input examples in the preprocessing steps below. ## Preprocessing the data Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We‚Äôll use `mt5-small` as our checkpoint so we can fine-tune the model in a reasonable amount of time: ``` from transformers import AutoTokenizer model_checkpoint = \"google/mt5-small\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` > üí° In the early stages of your NLP projects, a good practice is to train a class of ‚Äúsmall‚Äù models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint! Let‚Äôs test out the mT5 tokenizer on a small example: ``` inputs = tokenizer(\"I loved reading the Hunger Games!\") inputs ``` ``` {'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} ``` Here we can see the familiar `input_ids` and `attention_mask` that we encountered in our first fine-tuning experiments back in [Chapter 3](/course/chapter3). Let‚Äôs decode these input IDs with the tokenizer‚Äôs `convert_ids_to_tokens()` function to see what kind of tokenizer we‚Äôre dealing with: ``` tokenizer.convert_ids_to_tokens(inputs.input_ids) ``` ``` ['‚ñÅI', '‚ñÅ', 'loved', '‚ñÅreading', '‚ñÅthe', '‚ñÅHung', 'er', '‚ñÅGames', '</s>'] ``` The special Unicode character `‚ñÅ` and end-of-sequence token `</s>` indicate that we‚Äôre dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in [Chapter 6](/course/chapter6). Unigram",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 6,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} ``` Here we can see the familiar `input_ids` and `attention_mask` that we encountered in our first fine-tuning experiments back in [Chapter 3](/course/chapter3). Let‚Äôs decode these input IDs with the tokenizer‚Äôs `convert_ids_to_tokens()` function to see what kind of tokenizer we‚Äôre dealing with: ``` tokenizer.convert_ids_to_tokens(inputs.input_ids) ``` ``` ['‚ñÅI', '‚ñÅ', 'loved', '‚ñÅreading', '‚ñÅthe', '‚ñÅHung', 'er', '‚ñÅGames', '</s>'] ``` The special Unicode character `‚ñÅ` and end-of-sequence token `</s>` indicate that we‚Äôre dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in [Chapter 6](/course/chapter6). Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters. To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model‚Äôs maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don‚Äôt pass excessively long inputs to our model. The tokenizers in ü§ó Transformers provide a nifty `text_target` argument that allows you to tokenize the labels in parallel to the inputs. Here is an example of how the inputs and targets are processed for mT5: ``` max_input_length = 512 max_target_length = 30 def preprocess_function(examples): model_inputs = tokenizer( examples[\"review_body\"], max_length=max_input_length, truncation=True, ) labels = tokenizer( examples[\"review_title\"], max_length=max_target_length, truncation=True ) model_inputs[\"labels\"] = labels[\"input_ids\"] return model_inputs ``` Let‚Äôs walk through this code to understand what‚Äôs happening. The first thing we‚Äôve done is define values for `max_input_length` and `max_target_length`, which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we‚Äôve scaled these values accordingly. With `preprocess_function()`, it is then a simple matter to tokenize the whole corpus using the handy `Dataset.map()` function we‚Äôve used extensively throughout this course: ``` tokenized_datasets = books_dataset.map(preprocess_function, batched=True) ``` Now that the corpus has been preprocessed, let‚Äôs take a look at some metrics that are commonly used for summarization. As we‚Äôll see, there is no silver bullet when it comes to measuring the quality of machine-generated text. > üí° You may have noticed that we usedbatched=Truein ourDataset.map()function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in ü§ó Transformers. Where possible, try usingbatched=Trueto get the most out of your preprocessing! ## Metrics for text summarization In comparison to most of the other tasks we‚Äôve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like ‚ÄúI loved reading the Hunger Games‚Äù, there are multiple valid summaries, like ‚ÄúI loved the Hunger Games‚Äù or ‚ÄúHunger Games is a great read‚Äù. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 7,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "of the fast tokenizers in ü§ó Transformers. Where possible, try usingbatched=Trueto get the most out of your preprocessing! ## Metrics for text summarization In comparison to most of the other tasks we‚Äôve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like ‚ÄúI loved reading the Hunger Games‚Äù, there are multiple valid summaries, like ‚ÄúI loved the Hunger Games‚Äù or ‚ÄúHunger Games is a great read‚Äù. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution ‚Äî even humans would fare poorly under such a metric, because we all have our own writing style. For summarization, one of the most commonly used metrics is the [ROUGE score](https://en.wikipedia.org/wiki/ROUGE_(metric)) (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries: ``` generated_summary = \"I absolutely loved reading the Hunger Games\" reference_summary = \"I loved reading the Hunger Games\" ``` One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the *precision* and *recall* scores for the overlap. > üôã Don‚Äôt worry if this is the first time you‚Äôve heard of precision and recall ‚Äî we‚Äôll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out thescikit-learnguides. For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula: Recall=Number of overlapping wordsTotal number of words in reference summary \\mathrm{Recall} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, reference\\, summary}} Recall=TotalnumberofwordsinreferencesummaryNumberofoverlappingwords‚Äã For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been ‚ÄúI really really loved reading the Hunger Games all night‚Äù. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant: Precision=Number of overlapping wordsTotal number of words in generated summary \\mathrm{Precision} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, generated\\, summary}} Precision=TotalnumberofwordsingeneratedsummaryNumberofoverlappingwords‚Äã Applying this to our verbose summary gives a precision of 6/10 = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 8,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant: Precision=Number of overlapping wordsTotal number of words in generated summary \\mathrm{Precision} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, generated\\, summary}} Precision=TotalnumberofwordsingeneratedsummaryNumberofoverlappingwords‚Äã Applying this to our verbose summary gives a precision of 6/10 = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in ü§ó Datasets by first installing the `rouge_score` package: ``` !pip install rouge_score ``` and then loading the ROUGE metric as follows: ``` import evaluate rouge_score = evaluate.load(\"rouge\") ``` Then we can use the `rouge_score.compute()` function to calculate all the metrics at once: ``` scores = rouge_score.compute( predictions=[generated_summary], references=[reference_summary] ) scores ``` ``` {'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)), 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)), 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)), 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))} ``` Whoa, there‚Äôs a lot of information in that output ‚Äî what does it all mean? First, ü§ó Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the `low`, `mid`, and `high` attributes you can see here. Moreover, ü§ó Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The `rouge1` variant is the overlap of unigrams ‚Äî this is just a fancy way of saying the overlap of words and is exactly the metric we‚Äôve discussed above. To verify this, let‚Äôs pull out the `mid` value of our scores: ``` scores[\"rouge1\"].mid ``` ``` Score(precision=0.86, recall=1.0, fmeasure=0.92) ``` Great, the precision and recall numbers match up! Now what about those other ROUGE scores? `rouge2` measures the overlap between bigrams (think the overlap of pairs of words), while `rougeL` and `rougeLsum` measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The ‚Äúsum‚Äù in `rougeLsum` refers to the fact that this metric is computed over a whole summary, while `rougeL` is computed as the average over individual sentences. > ‚úèÔ∏èTry it out!Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for therouge2metric. We‚Äôll use these ROUGE scores to track the performance of our model, but before doing that let‚Äôs do something every good NLP practitioner should do: create a strong, yet simple baseline! ### Creating a strong baseline A common baseline for text summarization is to simply take the first three sentences of",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 9,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "> ‚úèÔ∏èTry it out!Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for therouge2metric. We‚Äôll use these ROUGE scores to track the performance of our model, but before doing that let‚Äôs do something every good NLP practitioner should do: create a strong, yet simple baseline! ### Creating a strong baseline A common baseline for text summarization is to simply take the first three sentences of an article, often called the *lead-3* baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like ‚ÄúU.S.‚Äù or ‚ÄúU.N.‚Äù ‚Äî so instead we‚Äôll use the `nltk` library, which includes a better algorithm to handle these cases. You can install the package using `pip` as follows: ``` !pip install nltk ``` and then download the punctuation rules: ``` import nltk nltk.download(\"punkt\") ``` Next, we import the sentence tokenizer from `nltk` and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let‚Äôs also include this and test it on a training example: ``` from nltk.tokenize import sent_tokenize def three_sentence_summary(text): return \"\\n\".join(sent_tokenize(text)[:3]) print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"])) ``` ``` 'I grew up reading Koontz, and years ago, I stopped,convinced i had \"outgrown\" him.' 'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.' 'She found Strangers.' ``` This seems to work, so let‚Äôs now implement a function that extracts these ‚Äúsummaries‚Äù from a dataset and computes the ROUGE scores for the baseline: ``` def evaluate_baseline(dataset, metric): summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]] return metric.compute(predictions=summaries, references=dataset[\"review_title\"]) ``` We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas: ``` import pandas as pd score = evaluate_baseline(books_dataset[\"validation\"], rouge_score) rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names) rouge_dict ``` ``` {'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96} ``` We can see that the `rouge2` score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let‚Äôs turn our attention toward fine-tuning mT5! ## Fine-tuning mT5 with the Trainer API Fine-tuning a model for summarization is very similar to the other tasks we‚Äôve covered in this chapter. The first thing we need to do is load the pretrained model from the `mt5-small` checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the `AutoModelForSeq2SeqLM` class, which will automatically download and cache the weights: ``` from transformers import AutoModelForSeq2SeqLM model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` > üí° If you‚Äôre wondering why you don‚Äôt see any warnings about fine-tuning the model on a downstream task, that‚Äôs",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 10,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "from, let‚Äôs turn our attention toward fine-tuning mT5! ## Fine-tuning mT5 with the Trainer API Fine-tuning a model for summarization is very similar to the other tasks we‚Äôve covered in this chapter. The first thing we need to do is load the pretrained model from the `mt5-small` checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the `AutoModelForSeq2SeqLM` class, which will automatically download and cache the weights: ``` from transformers import AutoModelForSeq2SeqLM model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` > üí° If you‚Äôre wondering why you don‚Äôt see any warnings about fine-tuning the model on a downstream task, that‚Äôs because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model inChapter 3, where the head of the pretrained model was replaced with a randomly initialized network. The next thing we need to do is log in to the Hugging Face Hub. If you‚Äôre running this code in a notebook, you can do so with the following utility function: ``` from huggingface_hub import notebook_login notebook_login() ``` which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there: ``` huggingface-cli login ``` We‚Äôll need to generate summaries in order to compute ROUGE scores during training. Fortunately, ü§ó Transformers provides dedicated `Seq2SeqTrainingArguments` and `Seq2SeqTrainer` classes that can do this for us automatically! To see how this works, let‚Äôs first define the hyperparameters and other arguments for our experiments: ``` from transformers import Seq2SeqTrainingArguments batch_size = 8 num_train_epochs = 8 # Show the training loss with every epoch logging_steps = len(tokenized_datasets[\"train\"]) // batch_size model_name = model_checkpoint.split(\"/\")[-1] args = Seq2SeqTrainingArguments( output_dir=f\"{model_name}-finetuned-amazon-en-es\", evaluation_strategy=\"epoch\", learning_rate=5.6e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01, save_total_limit=3, num_train_epochs=num_train_epochs, predict_with_generate=True, logging_steps=logging_steps, push_to_hub=True, ) ``` Here, the `predict_with_generate` argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in [Chapter 1](/course/chapter1), the decoder performs inference by predicting tokens one by one, and this is implemented by the model‚Äôs `generate()` method. Setting `predict_with_generate=True` tells the `Seq2SeqTrainer` to use that method for evaluation. We‚Äôve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we‚Äôve set the `save_total_limit` option to only save up to 3 checkpoints during training ‚Äî this is because even the ‚Äúsmall‚Äù version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save. The `push_to_hub=True` argument will allow us to push the model to the Hub after training; you‚Äôll find the repository under your user profile in the location defined by `output_dir`. Note that you can specify the name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the [huggingface-courseorganization](https://huggingface.co/huggingface-course), we added `hub_model_id=\"huggingface-course/mt5-finetuned-amazon-en-es\"` to `Seq2SeqTrainingArguments`. The next thing we need",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 11,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "drive space, and we can save a bit of room by limiting the number of copies we save. The `push_to_hub=True` argument will allow us to push the model to the Hub after training; you‚Äôll find the repository under your user profile in the location defined by `output_dir`. Note that you can specify the name of the repository you want to push to with the `hub_model_id` argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the [huggingface-courseorganization](https://huggingface.co/huggingface-course), we added `hub_model_id=\"huggingface-course/mt5-finetuned-amazon-en-es\"` to `Seq2SeqTrainingArguments`. The next thing we need to do is provide the trainer with a `compute_metrics()` function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling `rouge_score.compute()` on the model‚Äôs predictions, since we need to *decode* the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the `sent_tokenize()` function from `nltk` to separate the summary sentences with newlines: ``` import numpy as np def compute_metrics(eval_pred): predictions, labels = eval_pred # Decode generated summaries into text decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True) # Replace -100 in the labels as we can't decode them labels = np.where(labels != -100, labels, tokenizer.pad_token_id) # Decode reference summaries into text decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # ROUGE expects a newline after each sentence decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds] decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels] # Compute ROUGE scores result = rouge_score.compute( predictions=decoded_preds, references=decoded_labels, use_stemmer=True ) # Extract the median scores result = {key: value.mid.fmeasure * 100 for key, value in result.items()} return {k: round(v, 4) for k, v in result.items()} ``` Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like [causal language modeling](/course/chapter7/6). Luckily, ü§ó Transformers provides a `DataCollatorForSeq2Seq` collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the `tokenizer` and `model`: ``` from transformers import DataCollatorForSeq2Seq data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) ``` Let‚Äôs see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won‚Äôt know how to pad these elements: ``` tokenized_datasets = tokenized_datasets.remove_columns( books_dataset[\"train\"].column_names ) ``` Since the collator expects a list of `dict`s, where each `dict` represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator: ``` features = [tokenized_datasets[\"train\"][i] for i in range(2)] data_collator(features) ```",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 12,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "provide the `tokenizer` and `model`: ``` from transformers import DataCollatorForSeq2Seq data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) ``` Let‚Äôs see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won‚Äôt know how to pad these elements: ``` tokenized_datasets = tokenized_datasets.remove_columns( books_dataset[\"train\"].column_names ) ``` Since the collator expects a list of `dict`s, where each `dict` represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator: ``` features = [tokenized_datasets[\"train\"][i] for i in range(2)] data_collator(features) ``` ``` {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[ 1494, 259, 8622, 390, 259, 262, 2316, 3435, 955, 772, 281, 772, 1617, 263, 305, 14701, 260, 1385, 3031, 259, 24146, 332, 1037, 259, 43906, 305, 336, 260, 1, 0, 0, 0, 0, 0, 0], [ 259, 27531, 13483, 259, 7505, 260, 112240, 15192, 305, 53198, 276, 259, 74060, 263, 260, 459, 25640, 776, 2119, 336, 259, 2220, 259, 18896, 288, 4906, 288, 1037, 3931, 260, 7083, 101476, 1143, 260, 1]]), 'labels': tensor([[ 7483, 259, 2364, 15695, 1, -100], [ 259, 27531, 13483, 259, 7505, 1]]), 'decoder_input_ids': tensor([[ 0, 7483, 259, 2364, 15695, 1], [ 0, 259, 27531, 13483, 259, 7505]])} ``` The main thing to notice here is that the first example is longer than the second one, so the `input_ids` and `attention_mask` of the second example have been padded on the right with a `[PAD]` token (whose ID is `0`). Similarly, we can see that the `labels` have been padded with `-100`s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new `decoder_input_ids` which has shifted the labels to the right by inserting a `[PAD]` token in the first entry. We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments: ``` from transformers import Seq2SeqTrainer trainer = Seq2SeqTrainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation\"], data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics, ) ``` and launch our training run: ``` trainer.train() ``` During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running `Trainer.evaluate()`: ``` trainer.evaluate() ``` ``` {'eval_loss': 3.028524398803711, 'eval_rouge1': 16.9728, 'eval_rouge2': 8.2969, 'eval_rougeL': 16.8366, 'eval_rougeLsum': 16.851, 'eval_gen_len': 10.1597, 'eval_runtime': 6.1054, 'eval_samples_per_second': 38.982, 'eval_steps_per_second': 4.914} ``` From the scores we can see that our model has handily outperformed our lead-3 baseline ‚Äî nice! The final thing to do is push the model weights to the Hub, as follows:",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 13,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics, ) ``` and launch our training run: ``` trainer.train() ``` During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running `Trainer.evaluate()`: ``` trainer.evaluate() ``` ``` {'eval_loss': 3.028524398803711, 'eval_rouge1': 16.9728, 'eval_rouge2': 8.2969, 'eval_rougeL': 16.8366, 'eval_rougeLsum': 16.851, 'eval_gen_len': 10.1597, 'eval_runtime': 6.1054, 'eval_samples_per_second': 38.982, 'eval_steps_per_second': 4.914} ``` From the scores we can see that our model has handily outperformed our lead-3 baseline ‚Äî nice! The final thing to do is push the model weights to the Hub, as follows: ``` trainer.push_to_hub(commit_message=\"Training complete\", tags=\"summarization\") ``` ``` 'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0' ``` This will save the checkpoint and configuration files to `output_dir`, before uploading all the files to the Hub. By specifying the `tags` argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the [ü§ó Hub documentation](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). The output from `trainer.push_to_hub()` is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository! To wrap up this section, let‚Äôs take a look at how we can also fine-tune mT5 using the low-level features provided by ü§ó Accelerate. ## Fine-tuning mT5 with ü§ó Accelerate Fine-tuning our model with ü§ó Accelerate is very similar to the text classification example we encountered in [Chapter 3](/course/chapter3). The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the `Seq2SeqTrainer` took care of the generation for us). Let‚Äôs take a look how we can implement these two requirements within ü§ó Accelerate! ### Preparing everything for training The first thing we need to do is create a `DataLoader` for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to `\"torch\"` in our datasets: ``` tokenized_datasets.set_format(\"torch\") ``` Now that we‚Äôve got datasets consisting of just tensors, the next thing to do is instantiate the `DataCollatorForSeq2Seq` again. For this we need to provide a fresh version of the model, so let‚Äôs load it again from our cache: ``` model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) ``` We can then instantiate the data collator and use this to define our dataloaders: ``` from torch.utils.data import DataLoader batch_size = 8 train_dataloader = DataLoader( tokenized_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=batch_size, ) eval_dataloader = DataLoader( tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size ) ``` The next thing to do is define the optimizer we want to use. As in our other examples, we‚Äôll use `AdamW`, which works well for most problems: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=2e-5) ``` Finally, we feed our model, optimizer, and dataloaders to the `accelerator.prepare()` method: ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` > üö® If you‚Äôre training on a TPU, you‚Äôll need to move all the",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 14,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "8 train_dataloader = DataLoader( tokenized_datasets[\"train\"], shuffle=True, collate_fn=data_collator, batch_size=batch_size, ) eval_dataloader = DataLoader( tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size ) ``` The next thing to do is define the optimizer we want to use. As in our other examples, we‚Äôll use `AdamW`, which works well for most problems: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=2e-5) ``` Finally, we feed our model, optimizer, and dataloaders to the `accelerator.prepare()` method: ``` from accelerate import Accelerator accelerator = Accelerator() model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` > üö® If you‚Äôre training on a TPU, you‚Äôll need to move all the code above into a dedicated training function. SeeChapter 3for more details. Now that we‚Äôve prepared our objects, there are three remaining things to do: - Define the learning rate schedule. - Implement a function to post-process the summaries for evaluation. - Create a repository on the Hub that we can push our model to. For the learning rate schedule, we‚Äôll use the standard linear one from previous sections: ``` from transformers import get_scheduler num_train_epochs = 10 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code: ``` def postprocess_text(preds, labels): preds = [pred.strip() for pred in preds] labels = [label.strip() for label in labels] # ROUGE expects a newline after each sentence preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds] labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels] return preds, labels ``` This should look familiar to you if you recall how we defined the `compute_metrics()` function of the `Seq2SeqTrainer`. Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled ü§ó Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile: ``` from huggingface_hub import get_full_repo_name model_name = \"test-bert-finetuned-squad-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'lewtun/mt5-finetuned-amazon-en-es-accelerate' ``` Now we can use this repository name to clone a local version to our results directory that will store the training artifacts: ``` from huggingface_hub import Repository output_dir = \"results-mt5-finetuned-squad-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` This will allow us to push the artifacts back to the Hub by calling the `repo.push_to_hub()` method during training! Let‚Äôs now wrap up our analysis by writing out the training loop. ### Training loop The training loop for summarization is quite similar to the other ü§ó Accelerate examples that we‚Äôve encountered and is roughly split into four main steps: 1. Train the model by iterating over all the examples in `train_dataloader` for each epoch. 2. Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text. 3. Compute the ROUGE scores using the same",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 15,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "artifacts back to the Hub by calling the `repo.push_to_hub()` method during training! Let‚Äôs now wrap up our analysis by writing out the training loop. ### Training loop The training loop for summarization is quite similar to the other ü§ó Accelerate examples that we‚Äôve encountered and is roughly split into four main steps: 1. Train the model by iterating over all the examples in `train_dataloader` for each epoch. 2. Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text. 3. Compute the ROUGE scores using the same techniques we saw earlier. 4. Save the checkpoints and push everything to the Hub. Here we rely on the nifty `blocking=False` argument of the `Repository` object so that we can push the checkpoints per epoch *asynchronously*. This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model! These steps can be seen in the following block of code: ``` from tqdm.auto import tqdm import torch import numpy as np progress_bar = tqdm(range(num_training_steps)) for epoch in range(num_train_epochs): # Training model.train() for step, batch in enumerate(train_dataloader): outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # Evaluation model.eval() for step, batch in enumerate(eval_dataloader): with torch.no_grad(): generated_tokens = accelerator.unwrap_model(model).generate( batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], ) generated_tokens = accelerator.pad_across_processes( generated_tokens, dim=1, pad_index=tokenizer.pad_token_id ) labels = batch[\"labels\"] # If we did not pad to max length, we need to pad the labels too labels = accelerator.pad_across_processes( batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id ) generated_tokens = accelerator.gather(generated_tokens).cpu().numpy() labels = accelerator.gather(labels).cpu().numpy() # Replace -100 in the labels as we can't decode them labels = np.where(labels != -100, labels, tokenizer.pad_token_id) if isinstance(generated_tokens, tuple): generated_tokens = generated_tokens[0] decoded_preds = tokenizer.batch_decode( generated_tokens, skip_special_tokens=True ) decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) decoded_preds, decoded_labels = postprocess_text( decoded_preds, decoded_labels ) rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels) # Compute metrics result = rouge_score.compute() # Extract the median ROUGE scores result = {key: value.mid.fmeasure * 100 for key, value in result.items()} result = {k: round(v, 4) for k, v in result.items()} print(f\"Epoch {epoch}:\", result) # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` ``` Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005} Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306} Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468} Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518} Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029} Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913} Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701} Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194} Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744} Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509} ``` And that‚Äôs it! Once you run this, you‚Äôll have a model and results that are pretty similar to the ones we obtained with the `Trainer`. ## Using your fine-tuned model Once you‚Äôve pushed the model to the Hub,",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 16,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "11.7518} Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029} Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913} Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701} Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194} Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744} Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509} ``` And that‚Äôs it! Once you run this, you‚Äôll have a model and results that are pretty similar to the ones we obtained with the `Trainer`. ## Using your fine-tuned model Once you‚Äôve pushed the model to the Hub, you can play with it either via the inference widget or with a `pipeline` object, as follows: ``` from transformers import pipeline hub_model_id = \"huggingface-course/mt5-small-finetuned-amazon-en-es\" summarizer = pipeline(\"summarization\", model=hub_model_id) ``` We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let‚Äôs implement a simple function to show the review, title, and generated summary together: ``` def print_summary(idx): review = books_dataset[\"test\"][idx][\"review_body\"] title = books_dataset[\"test\"][idx][\"review_title\"] summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"] print(f\"'>>> Review: {review}'\") print(f\"\\n'>>> Title: {title}'\") print(f\"\\n'>>> Summary: {summary}'\") ``` Let‚Äôs take a look at one of the English examples we get: ``` print_summary(100) ``` ``` '>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn‚Äôt come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It‚Äôs also really expensive for what it is.' '>>> Title: Not impressed at all... buy something else' '>>> Summary: Nothing special at all about this product' ``` This is not too bad! We can see that our model has actually been able to perform *abstractive* summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews: ``` print_summary(0) ``` ``` '>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada' '>>> Title: Buena literatura para adolescentes' '>>> Summary: Muy facil de leer' ``` The summary translates into ‚ÄúVery easy to read‚Äù in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it‚Äôs like to deal with a multilingual corpus! Next, we‚Äôll turn our attention to a slightly more complex task: training a language model from scratch.",
    "metadata": {
      "title": "Summarization",
      "url": "https://huggingface.co/learn/llm-course/chapter7/5",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/5",
      "part": 17,
      "total_parts": 17,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Training a causal language model from scratch Up until now, we‚Äôve mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in [Chapter 1](/course/chapter1), this is commonly referred to as *transfer learning*, and it‚Äôs a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we‚Äôll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub‚Äôs Copilot, powered by OpenAI‚Äôs Codex model, that can generate long sequences of code. This task of text generation is best addressed with auto-regressive or causal language models such as GPT-2. In this section we will build a scaled-down version of a code generation model: we‚Äôll focus on one-line completions instead of full functions or classes, using a subset of Python code. When working with data in Python you are in frequent contact with the Python data science stack, consisting of the `matplotlib`, `seaborn`, `pandas`, and `scikit-learn` libraries. When using those frameworks it‚Äôs common to need to look up specific commands, so it would be nice if we could use a model to complete these calls for us. In [Chapter 6](/course/chapter6) we created an efficient tokenizer to process Python source code, but what we still need is a large-scale dataset to pretrain a model on. Here, we‚Äôll apply our tokenizer to a corpus of Python code derived from GitHub repositories. We will then use the `Trainer` API and ü§ó Accelerate to train the model. Let‚Äôs get to it! This is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it [here](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28). Note that since there is some randomization happening in the text generation, you will probably get a slightly different result. ## Gathering the data Python code is abundantly available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called `codeparrot`, the authors built a dataset that they then shared on the [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot). However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let‚Äôs start by filtering the `codeparrot` dataset",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 1,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "available from code repositories such as GitHub, which we can use to create a dataset by scraping for every Python repository. This was the approach taken in the [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) to pretrain a large GPT-2 model. Using a GitHub dump of about 180 GB containing roughly 20 million Python files called `codeparrot`, the authors built a dataset that they then shared on the [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot). However, training on the full corpus is time- and compute-consuming, and we only need the subset of the dataset concerned with the Python data science stack. So, let‚Äôs start by filtering the `codeparrot` dataset for all files that include any of the libraries in this stack. Because of the dataset‚Äôs size, we want to avoid downloading it; instead, we‚Äôll use the streaming feature to filter it on the fly. To help us filter the code samples using the libraries we mentioned earlier, we‚Äôll use the following function: ``` def any_keyword_in_string(string, keywords): for keyword in keywords: if keyword in string: return True return False ``` Let‚Äôs test it on two examples: ``` filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"] example_1 = \"import numpy as np\" example_2 = \"import pandas as pd\" print( any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters) ) ``` ``` False True ``` We can use this to create a function that will stream the dataset and filter the elements we want: ``` from collections import defaultdict from tqdm import tqdm from datasets import Dataset def filter_streaming_dataset(dataset, filters): filtered_dict = defaultdict(list) total = 0 for sample in tqdm(iter(dataset)): total += 1 if any_keyword_in_string(sample[\"content\"], filters): for k, v in sample.items(): filtered_dict[k].append(v) print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\") return Dataset.from_dict(filtered_dict) ``` Then we can simply apply this function to the streaming dataset: ``` # This cell will take a very long time to execute, so you should skip it and go to # the next one! from datasets import load_dataset split = \"train\" # \"valid\" filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"] data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True) filtered_data = filter_streaming_dataset(data, filters) ``` ``` 3.26% of data after filtering. ``` This leaves us with about 3% of the original dataset, which is still quite sizable ‚Äî the resulting dataset is 6 GB and consists of 600,000 Python scripts! Filtering the full dataset can take 2-3h depending on your machine and bandwidth. If you don‚Äôt want to go through this lengthy process yourself, we provide the filtered dataset on the Hub for you to download: ``` from datasets import load_dataset, DatasetDict ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\") ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\") raw_datasets = DatasetDict( { \"train\": ds_train, # .shuffle().select(range(50000)), \"valid\": ds_valid, # .shuffle().select(range(500)) } ) raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'], num_rows: 606720 }) valid: Dataset({ features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'], num_rows: 3322 }) }) ``` > Pretraining the language model will take a while. We suggest that you first run the training loop on a sample of the data by uncommenting the two partial lines above, and make sure that the training successfully completes and",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 2,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to download: ``` from datasets import load_dataset, DatasetDict ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\") ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\") raw_datasets = DatasetDict( { \"train\": ds_train, # .shuffle().select(range(50000)), \"valid\": ds_valid, # .shuffle().select(range(500)) } ) raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'], num_rows: 606720 }) valid: Dataset({ features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'], num_rows: 3322 }) }) ``` > Pretraining the language model will take a while. We suggest that you first run the training loop on a sample of the data by uncommenting the two partial lines above, and make sure that the training successfully completes and the models are stored. Nothing is more frustrating than a training run failing at the last step because you forgot to create a folder or because there‚Äôs a typo at the end of the training loop! Let‚Äôs look at an example from the dataset. We‚Äôll just show the first 200 characters of each field: ``` for key in raw_datasets[\"train\"][0]: print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\") ``` ``` 'REPO_NAME: kmike/scikit-learn' 'PATH: sklearn/utils/__init__.py' 'COPIES: 3' 'SIZE: 10094' '''CONTENT: \"\"\" The :mod:`sklearn.utils` module includes various utilites. \"\"\" from collections import Sequence import numpy as np from scipy.sparse import issparse import warnings from .murmurhash import murm LICENSE: bsd-3-clause''' ``` We can see that the `content` field contains the code that we want our model to train on. Now that we have a dataset, we need to prepare the texts so they‚Äôre in a format suitable for pretraining. ## Preparing the dataset The first step will be to tokenize the data, so we can use it for training. Since our goal is to mainly autocomplete short function calls, we can keep the context size relatively small. This has the benefit that we can train the model much faster and it requires significantly less memory. If it is important for your application to have more context (for example, if you want the model to write unit tests based on a file with the function definition), make sure you increase that number, but also keep in mind that this comes with a greater GPU memory footprint. For now, let‚Äôs fix the context size at 128 tokens, as opposed to the 1,024 or 2,048 used in GPT-2 or GPT-3, respectively. Most documents contain many more than 128 tokens, so simply truncating the inputs to the maximum length would eliminate a large fraction of our dataset. Instead, we‚Äôll use the `return_overflowing_tokens` option to tokenize the whole input and split it into several chunks, as we did in [Chapter 6](/course/chapter6/4). We‚Äôll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we‚Äôll get rid of these pieces to avoid padding issues; we don‚Äôt really need them as we have plenty of data anyway. ![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg) ![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg) Let‚Äôs see exactly how this works by looking at the first two examples: ``` from transformers import AutoTokenizer context_length = 128",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 3,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tokenize the whole input and split it into several chunks, as we did in [Chapter 6](/course/chapter6/4). We‚Äôll also use the `return_length` option to return the length of each created chunk automatically. Often the last chunk will be smaller than the context size, and we‚Äôll get rid of these pieces to avoid padding issues; we don‚Äôt really need them as we have plenty of data anyway. ![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg) ![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts-dark.svg) Let‚Äôs see exactly how this works by looking at the first two examples: ``` from transformers import AutoTokenizer context_length = 128 tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\") outputs = tokenizer( raw_datasets[\"train\"][:2][\"content\"], truncation=True, max_length=context_length, return_overflowing_tokens=True, return_length=True, ) print(f\"Input IDs length: {len(outputs['input_ids'])}\") print(f\"Input chunk lengths: {(outputs['length'])}\") print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\") ``` ``` Input IDs length: 34 Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41] Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ``` We can see that we get 34 segments in total from those two examples. Looking at the chunk lengths, we can see that the chunks at the ends of both documents have less than 128 tokens (117 and 41, respectively). These represent just a small fraction of the total chunks that we have, so we can safely throw them away. With the `overflow_to_sample_mapping` field, we can also reconstruct which chunks belonged to which input samples. With this operation we‚Äôre using a handy feature of the `Dataset.map()` function in ü§ó Datasets, which is that it does not require one-to-one maps; as we saw in [section 3](/course/chapter7/3), we can create batches with more or fewer elements than the input batch. This is useful when doing operations like data augmentation or data filtering that change the number of elements. In our case, when tokenizing each element into chunks of the specified context size, we create many samples from each document. We just need to make sure to delete the existing columns, since they have a conflicting size. If we wanted to keep them, we could repeat them appropriately and return them within the `Dataset.map()` call: ``` def tokenize(element): outputs = tokenizer( element[\"content\"], truncation=True, max_length=context_length, return_overflowing_tokens=True, return_length=True, ) input_batch = [] for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]): if length == context_length: input_batch.append(input_ids) return {\"input_ids\": input_batch} tokenized_datasets = raw_datasets.map( tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names ) tokenized_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['input_ids'], num_rows: 16702061 }) valid: Dataset({ features: ['input_ids'], num_rows: 93164 }) }) ``` We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAI‚Äôs GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 4,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "tokenizer( element[\"content\"], truncation=True, max_length=context_length, return_overflowing_tokens=True, return_length=True, ) input_batch = [] for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]): if length == context_length: input_batch.append(input_ids) return {\"input_ids\": input_batch} tokenized_datasets = raw_datasets.map( tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names ) tokenized_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['input_ids'], num_rows: 16702061 }) valid: Dataset({ features: ['input_ids'], num_rows: 93164 }) }) ``` We now have 16.7 million examples with 128 tokens each, which corresponds to about 2.1 billion tokens in total. For reference, OpenAI‚Äôs GPT-3 and Codex models are trained on 300 and 100 billion tokens, respectively, where the Codex models are initialized from the GPT-3 checkpoints. Our goal in this section is not to compete with these models, which can generate long, coherent texts, but to create a scaled-down version providing a quick autocomplete function for data scientists. Now that we have the dataset ready, let‚Äôs set up the model! > ‚úèÔ∏èTry it out!Getting rid of all the chunks that are smaller than the context size wasn‚Äôt a big issue here because we‚Äôre using small context windows. As you increase the context size (or if you have a corpus of short documents), the fraction of chunks that are thrown away will also grow. A more efficient way to prepare the data is to join all the tokenized samples in a batch with aneos_token_idtoken in between, and then perform the chunking on the concatenated sequences. As an exercise, modify thetokenize()function to make use of that approach. Note that you‚Äôll want to settruncation=Falseand remove the other arguments from the tokenizer to get the full sequence of token IDs. ## Initializing a new model Our first step is to freshly initialize a GPT-2 model. We‚Äôll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the `bos` and `eos` (beginning and end of sequence) token IDs: ``` from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig config = AutoConfig.from_pretrained( \"gpt2\", vocab_size=len(tokenizer), n_ctx=context_length, bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, ) ``` With that configuration, we can load a new model. Note that this is the first time we don‚Äôt use the `from_pretrained()` function, since we‚Äôre actually initializing a model ourself: ``` model = GPT2LMHeadModel(config) model_size = sum(t.numel() for t in model.parameters()) print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\") ``` ``` GPT-2 size: 124.2M parameters ``` Our model has 124M parameters that we‚Äôll have to tune. Before we can start training, we need to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels ‚Äî in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don‚Äôt need to duplicate the `input_ids`. Note that `DataCollatorForLanguageModeling` supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 5,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "to set up a data collator that will take care of creating the batches. We can use the `DataCollatorForLanguageModeling` collator, which is designed specifically for language modeling (as the name subtly suggests). Besides stacking and padding batches, it also takes care of creating the language model labels ‚Äî in causal language modeling the inputs serve as labels too (just shifted by one element), and this data collator creates them on the fly during training so we don‚Äôt need to duplicate the `input_ids`. Note that `DataCollatorForLanguageModeling` supports both masked language modeling (MLM) and causal language modeling (CLM). By default it prepares data for MLM, but we can switch to CLM by setting the argument `mlm=False`: ``` from transformers import DataCollatorForLanguageModeling tokenizer.pad_token = tokenizer.eos_token data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False) ``` Let‚Äôs have a look at an example: ``` out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)]) for key in out: print(f\"{key} shape: {out[key].shape}\") ``` ``` input_ids shape: torch.Size([5, 128]) attention_mask shape: torch.Size([5, 128]) labels shape: torch.Size([5, 128]) ``` We can see that the examples have been stacked and all the tensors have the same shape. > ‚ö†Ô∏è Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels. Now we have everything in place to actually train our model ‚Äî that wasn‚Äôt so much work after all! Before we start training we should log in to Hugging Face. If you‚Äôre working in a notebook, you can do so with the following utility function: ``` from huggingface_hub import notebook_login notebook_login() ``` This will display a widget where you can enter your Hugging Face login credentials. If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` All that‚Äôs left to do is configure the training arguments and fire up the `Trainer`. We‚Äôll use a cosine learning rate schedule with some warmup and an effective batch size of 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). Gradient accumulation is used when a single batch does not fit into memory, and incrementally builds up the gradient through several forward/backward passes. We‚Äôll see this in action when we create the training loop with ü§ó Accelerate. ``` from transformers import Trainer, TrainingArguments args = TrainingArguments( output_dir=\"codeparrot-ds\", per_device_train_batch_size=32, per_device_eval_batch_size=32, evaluation_strategy=\"steps\", eval_steps=5_000, logging_steps=5_000, gradient_accumulation_steps=8, num_train_epochs=1, weight_decay=0.1, warmup_steps=1_000, lr_scheduler_type=\"cosine\", learning_rate=5e-4, save_steps=5_000, fp16=True, push_to_hub=True, ) trainer = Trainer( model=model, tokenizer=tokenizer, args=args, data_collator=data_collator, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"valid\"], ) ``` Now we can just start the `Trainer` and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read! ``` trainer.train() ``` After training completes, we can push the model and tokenizer to the Hub: ``` trainer.push_to_hub() ``` > ‚úèÔ∏èTry it out!It only took us about 30 lines of code in addition to theTrainingArgumentsto get from raw texts to training GPT-2. Try it out with your own dataset and see if",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 6,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Now we can just start the `Trainer` and wait for training to finish. Depending on whether you run it on the full or a subset of the training set this will take 20 or 2 hours, respectively, so grab a few coffees and a good book to read! ``` trainer.train() ``` After training completes, we can push the model and tokenizer to the Hub: ``` trainer.push_to_hub() ``` > ‚úèÔ∏èTry it out!It only took us about 30 lines of code in addition to theTrainingArgumentsto get from raw texts to training GPT-2. Try it out with your own dataset and see if you can get good results! > üí° If you have access to a machine with multiple GPUs, try to run the code there. TheTrainerautomatically manages multiple machines, and this can speed up training tremendously. ## Code generation with a pipeline Now is the moment of truth: let‚Äôs see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let‚Äôs take a look at how well it works on some prompts. To do that we‚Äôll wrap the model in a text generation `pipeline`, and we‚Äôll put it on the GPU for fast generations if there is one available: ``` import torch from transformers import pipeline device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") pipe = pipeline( \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device ) ``` Let‚Äôs start with the simple task of creating a scatter plot: ``` txt = \"\"\"\\ # create some data x = np.random.randn(100) y = np.random.randn(100) # create scatter plot with x, y \"\"\" print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) ``` ``` # create some data x = np.random.randn(100) y = np.random.randn(100) # create scatter plot with x, y plt.scatter(x, y) # create scatter ``` The result looks correct. Does it also work for a `pandas` operation? Let‚Äôs see if we can create a `DataFrame` from two arrays: ``` txt = \"\"\"\\ # create some data x = np.random.randn(100) y = np.random.randn(100) # create dataframe from x and y \"\"\" print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) ``` ``` # create some data x = np.random.randn(100) y = np.random.randn(100) # create dataframe from x and y df = pd.DataFrame({'x': x, 'y': y}) df.insert(0,'x', x) for ``` Nice, that‚Äôs the correct answer ‚Äî although it then inserts the column `x` again. Since the number of generated tokens is limited, the following `for` loop is cut off. Let‚Äôs see if we can do something a bit more complex and have the model help us use the `groupby` operation: ``` txt = \"\"\"\\ # dataframe with profession, income and name df = pd.DataFrame({'profession': x, 'income':y, 'name': z}) # calculate the mean income per profession \"\"\" print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) ``` ``` # dataframe with profession, income and name df = pd.DataFrame({'profession': x, 'income':y, 'name': z}) # calculate the mean income per profession profession = df.groupby(['profession']).mean() # compute the ``` Not bad; that‚Äôs the right way to do it. Finally, let‚Äôs see if we can also use it for `scikit-learn`",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 7,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "off. Let‚Äôs see if we can do something a bit more complex and have the model help us use the `groupby` operation: ``` txt = \"\"\"\\ # dataframe with profession, income and name df = pd.DataFrame({'profession': x, 'income':y, 'name': z}) # calculate the mean income per profession \"\"\" print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) ``` ``` # dataframe with profession, income and name df = pd.DataFrame({'profession': x, 'income':y, 'name': z}) # calculate the mean income per profession profession = df.groupby(['profession']).mean() # compute the ``` Not bad; that‚Äôs the right way to do it. Finally, let‚Äôs see if we can also use it for `scikit-learn` and set up a Random Forest model: ``` txt = \"\"\" # import random forest regressor from scikit-learn from sklearn.ensemble import RandomForestRegressor # fit random forest model with 300 estimators on X, y: \"\"\" print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"]) ``` ``` # import random forest regressor from scikit-learn from sklearn.ensemble import RandomForestRegressor # fit random forest model with 300 estimators on X, y: rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3) rf.fit(X, y) rf ``` Looking at these few examples, it seems that the model has learned some of the syntax of the Python data science stack (of course, we would need to evaluate it more thoroughly before deploying the model in the real world). Sometimes it requires more customization of the model training to achieve the necessary performance for a given use case, however. For example, what if we would like to dynamically update the batch size or have a conditional training loop that skips bad examples on the fly? One option would be to subclass the `Trainer` and add the necessary changes, but sometimes it‚Äôs simpler to write the training loop from scratch. That‚Äôs where ü§ó Accelerate comes in. ## Training with ü§ó Accelerate We‚Äôve seen how to train a model with the `Trainer`, which can allow for some customization. However, sometimes we want full control over the training loop, or we want to make some exotic changes. In this case ü§ó Accelerate is a great choice, and in this section we‚Äôll go through the steps to use it to train our model. To make things more interesting, we‚Äôll also add a twist to the training loop. Since we are mainly interested in sensible autocompletion for the the data science libraries, it makes sense to give more weight to training samples that make more use of these libraries. We can easily identify these examples through the use of keywords such as `plt`, `pd`, `sk`, `fit`, and `predict`, which are the most frequent import names for `matplotlib.pyplot`, `pandas`, and `sklearn` as well as the fit/predict pattern of the latter. If these are each represented as a single token, we can easily check if they occur in the input sequence. Tokens can have a whitespace prefix, so we‚Äôll also check for those versions in the tokenizer vocabulary. To verify that it works, we‚Äôll add one test token which should be split into multiple tokens: ``` keytoken_ids = [] for keyword in [ \"plt\", \"pd\", \"sk\", \"fit\",",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 8,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "through the use of keywords such as `plt`, `pd`, `sk`, `fit`, and `predict`, which are the most frequent import names for `matplotlib.pyplot`, `pandas`, and `sklearn` as well as the fit/predict pattern of the latter. If these are each represented as a single token, we can easily check if they occur in the input sequence. Tokens can have a whitespace prefix, so we‚Äôll also check for those versions in the tokenizer vocabulary. To verify that it works, we‚Äôll add one test token which should be split into multiple tokens: ``` keytoken_ids = [] for keyword in [ \"plt\", \"pd\", \"sk\", \"fit\", \"predict\", \" plt\", \" pd\", \" sk\", \" fit\", \" predict\", \"testtest\", ]: ids = tokenizer([keyword]).input_ids[0] if len(ids) == 1: keytoken_ids.append(ids[0]) else: print(f\"Keyword has not single token: {keyword}\") ``` ``` 'Keyword has not single token: testtest' ``` Great, that seems to work nicely! We can now write a custom loss function that takes the input sequence, the logits, and the key tokens we just selected as inputs. First we need to align the logits and inputs: the input sequence shifted by one to the right forms the labels, since the next token is the label for the current token. We can achieve this by starting the labels from the second token of the input sequence, since the model does not make a prediction for the first token anyway. Then we cut off the last logit, as we don‚Äôt have a label for the token that follows the full input sequence. With that we can compute the loss per sample and count the occurrences of all keywords in each sample. Finally, we calculate the weighted average over all samples using the occurrences as weights. Since we don‚Äôt want to throw away all the samples that have no keywords, we add 1 to the weights: ``` from torch.nn import CrossEntropyLoss import torch def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0): # Shift so that tokens < n predict n shift_labels = inputs[..., 1:].contiguous() shift_logits = logits[..., :-1, :].contiguous() # Calculate per-token loss loss_fct = CrossEntropyLoss(reduce=False) loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)) # Resize and average loss per sample loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1) # Calculate and scale weighting weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum( axis=[0, 2] ) weights = alpha * (1.0 + weights) # Calculate weighted average weighted_loss = (loss_per_sample * weights).mean() return weighted_loss ``` Before we can start training with this awesome new loss function, we need to prepare a few things: - We need dataloaders to load the data in batches. - We need to set up weight decay parameters. - From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function. Let‚Äôs start with the dataloaders. We only need to set the dataset‚Äôs format to `\"torch\"`, and then we can pass it to a PyTorch `DataLoader` with the appropriate batch size: ``` from torch.utils.data.dataloader import DataLoader tokenized_datasets.set_format(\"torch\") train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True) eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=32) ``` Next, we group the",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 9,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "new loss function, we need to prepare a few things: - We need dataloaders to load the data in batches. - We need to set up weight decay parameters. - From time to time we want to evaluate, so it makes sense to wrap the evaluation code in a function. Let‚Äôs start with the dataloaders. We only need to set the dataset‚Äôs format to `\"torch\"`, and then we can pass it to a PyTorch `DataLoader` with the appropriate batch size: ``` from torch.utils.data.dataloader import DataLoader tokenized_datasets.set_format(\"torch\") train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True) eval_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=32) ``` Next, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; here‚Äôs how we can do this: ``` weight_decay = 0.1 def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]): params_with_wd, params_without_wd = [], [] for n, p in model.named_parameters(): if any(nd in n for nd in no_decay): params_without_wd.append(p) else: params_with_wd.append(p) return [ {\"params\": params_with_wd, \"weight_decay\": weight_decay}, {\"params\": params_without_wd, \"weight_decay\": 0.0}, ] ``` Since we want to evaluate the model regularly on the validation set during training, let‚Äôs write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes: ``` def evaluate(): model.eval() losses = [] for step, batch in enumerate(eval_dataloader): with torch.no_grad(): outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"]) losses.append(accelerator.gather(outputs.loss)) loss = torch.mean(torch.cat(losses)) try: perplexity = torch.exp(loss) except OverflowError: perplexity = float(\"inf\") return loss.item(), perplexity.item() ``` With the `evaluate()` function we can report loss and [perplexity](/course/chapter7/3) at regular intervals. Next, we redefine our model to make sure we train from scratch again: ``` model = GPT2LMHeadModel(config) ``` We can then define our optimizer, using the function from before to split the parameters for weight decay: ``` from torch.optim import AdamW optimizer = AdamW(get_grouped_params(model), lr=5e-4) ``` Now let‚Äôs prepare the model, optimizer, and dataloaders so we can start training: ``` from accelerate import Accelerator accelerator = Accelerator(fp16=True) model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` > üö® If you‚Äôre training on a TPU, you‚Äôll need to move all the code starting at the cell above into a dedicated training function. SeeChapter 3for more details. Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0: ``` from transformers import get_scheduler num_train_epochs = 1 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( name=\"linear\", optimizer=optimizer, num_warmup_steps=1_000, num_training_steps=num_training_steps, ) ``` Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren‚Äôt logged in already. We‚Äôll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 10,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "change its length. We use a classic linear schedule from the learning rate to 0: ``` from transformers import get_scheduler num_train_epochs = 1 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( name=\"linear\", optimizer=optimizer, num_warmup_steps=1_000, num_training_steps=num_training_steps, ) ``` Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren‚Äôt logged in already. We‚Äôll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does): ``` from huggingface_hub import Repository, get_full_repo_name model_name = \"codeparrot-ds-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'sgugger/codeparrot-ds-accelerate' ``` Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with: ``` output_dir = \"codeparrot-ds-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch. Before we train, let‚Äôs run a quick test to see if the evaluation function works properly: ``` evaluate() ``` ``` (10.934126853942871, 56057.14453125) ``` Those are very high values for loss and perplexity, but that‚Äôs not surprising as we haven‚Äôt trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new `evaluate()` function: ``` from tqdm.notebook import tqdm gradient_accumulation_steps = 8 eval_steps = 5_000 model.train() completed_steps = 0 for epoch in range(num_train_epochs): for step, batch in tqdm( enumerate(train_dataloader, start=1), total=num_training_steps ): logits = model(batch[\"input_ids\"]).logits loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids) if step % 100 == 0: accelerator.print( { \"samples\": step * samples_per_step, \"steps\": completed_steps, \"loss/train\": loss.item() * gradient_accumulation_steps, } ) loss = loss / gradient_accumulation_steps accelerator.backward(loss) if step % gradient_accumulation_steps == 0: accelerator.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() lr_scheduler.step() optimizer.zero_grad() completed_steps += 1 if (step % (eval_steps * gradient_accumulation_steps)) == 0: eval_loss, perplexity = evaluate() accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity}) model.train() accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress step {step}\", blocking=False ) ``` And that‚Äôs it ‚Äî you now have your own custom training loop for causal language models such as GPT-2 that you can further customize to your needs. > ‚úèÔ∏èTry it out!Either create your own custom loss function tailored to your use case, or add another custom step into the training loop. > ‚úèÔ∏èTry it out!When",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 11,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "== 0: accelerator.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() lr_scheduler.step() optimizer.zero_grad() completed_steps += 1 if (step % (eval_steps * gradient_accumulation_steps)) == 0: eval_loss, perplexity = evaluate() accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity}) model.train() accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress step {step}\", blocking=False ) ``` And that‚Äôs it ‚Äî you now have your own custom training loop for causal language models such as GPT-2 that you can further customize to your needs. > ‚úèÔ∏èTry it out!Either create your own custom loss function tailored to your use case, or add another custom step into the training loop. > ‚úèÔ∏èTry it out!When running long training experiments it‚Äôs a good idea to log important metrics using tools such as TensorBoard or Weights & Biases. Add proper logging to the training loop so you can always check how the training is going.",
    "metadata": {
      "title": "Training a causal language model from scratch",
      "url": "https://huggingface.co/learn/llm-course/chapter7/6",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/6",
      "part": 12,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Question answering Time to look at question answering! This task comes in many flavors, but the one we‚Äôll focus on in this section is called *extractive* question answering. This involves posing questions about a document and identifying the answers as *spans of text* in the document itself. We will fine-tune a BERT model on the [SQuAD dataset](https://rajpurkar.github.io/SQuAD-explorer/), which consists of questions posed by crowdworkers on a set of Wikipedia articles. This will give us a model able to compute predictions like this one: This is actually showcasing the model that was trained and uploaded to the Hub using the code shown in this section. You can find it and double-check the predictions [here](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F). > üí° Encoder-only models like BERT tend to be great at extracting answers to factoid questions like ‚ÄúWho invented the Transformer architecture?‚Äù but fare poorly when given open-ended questions like ‚ÄúWhy is the sky blue?‚Äù In these more challenging cases, encoder-decoder models like T5 and BART are typically used to synthesize the information in a way that‚Äôs quite similar totext summarization. If you‚Äôre interested in this type ofgenerativequestion answering, we recommend checking out ourdemobased on theELI5 dataset. ## Preparing the data The dataset that is used the most as an academic benchmark for extractive question answering is [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), so that‚Äôs the one we‚Äôll use here. There is also a harder [SQuAD v2](https://huggingface.co/datasets/squad_v2) benchmark, which includes questions that don‚Äôt have an answer. As long as your own dataset contains a column for contexts, a column for questions, and a column for answers, you should be able to adapt the steps below. ### The SQuAD dataset As usual, we can download and cache the dataset in just one step thanks to `load_dataset()`: ``` from datasets import load_dataset raw_datasets = load_dataset(\"squad\") ``` We can then have a look at this object to learn more about the SQuAD dataset: ``` raw_datasets ``` ``` DatasetDict({ train: Dataset({ features: ['id', 'title', 'context', 'question', 'answers'], num_rows: 87599 }) validation: Dataset({ features: ['id', 'title', 'context', 'question', 'answers'], num_rows: 10570 }) }) ``` It looks like we have everything we need with the `context`, `question`, and `answers` fields, so let‚Äôs print those for the first element of our training set: ``` print(\"Context: \", raw_datasets[\"train\"][0][\"context\"]) print(\"Question: \", raw_datasets[\"train\"][0][\"question\"]) print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"]) ``` ``` Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 1,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.' Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?' Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]} ``` The `context` and `question` fields are very straightforward to use. The `answers` field is a bit trickier as it comports a dictionary with two fields that are both lists. This is the format that will be expected by the `squad` metric during evaluation; if you are using your own data, you don‚Äôt necessarily need to worry about putting the answers in the same format. The `text` field is rather obvious, and the `answer_start` field contains the starting character index of each answer in the context. During training, there is only one possible answer. We can double-check this by using the `Dataset.filter()` method: ``` raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1) ``` ``` Dataset({ features: ['id', 'title', 'context', 'question', 'answers'], num_rows: 0 }) ``` For evaluation, however, there are several possible answers for each sample, which may be the same or different: ``` print(raw_datasets[\"validation\"][0][\"answers\"]) print(raw_datasets[\"validation\"][2][\"answers\"]) ``` ``` {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]} {'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]} ``` We won‚Äôt dive into the evaluation script as it will all be wrapped up by a ü§ó Datasets metric for us, but the short version is that some of the questions have several possible answers, and this script will compare a predicted answer to all the acceptable answers and take the best score. If we take a look at the sample at index 2, for instance: ``` print(raw_datasets[\"validation\"][2][\"context\"]) print(raw_datasets[\"validation\"][2][\"question\"]) ``` ``` 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.' 'Where did",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 2,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "(AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24‚Äì10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.' 'Where did Super Bowl 50 take place?' ``` we can see that the answer can indeed be one of the three possibilities we saw before. ### Processing the training data Let‚Äôs start with preprocessing the training data. The hard part will be to generate labels for the question‚Äôs answer, which will be the start and end positions of the tokens corresponding to the answer inside the context. But let‚Äôs not get ahead of ourselves. First, we need to convert the text in the input into IDs the model can make sense of, using a tokenizer: ``` from transformers import AutoTokenizer model_checkpoint = \"bert-base-cased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` As mentioned previously, we‚Äôll be fine-tuning a BERT model, but you can use any other model type as long as it has a fast tokenizer implemented. You can see all the architectures that come with a fast version in [this big table](https://huggingface.co/transformers/#supported-frameworks), and to check that the `tokenizer` object you‚Äôre using is indeed backed by ü§ó Tokenizers you can look at its `is_fast` attribute: ``` tokenizer.is_fast ``` ``` True ``` We can pass to our tokenizer the question and the context together, and it will properly insert the special tokens to form a sentence like this: ``` [CLS] question [SEP] context [SEP] ``` Let‚Äôs double-check: ``` context = raw_datasets[\"train\"][0][\"context\"] question = raw_datasets[\"train\"][0][\"question\"] inputs = tokenizer(question, context) tokenizer.decode(inputs[\"input_ids\"]) ``` ``` '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, ' 'the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin ' 'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms ' 'upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred ' 'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a ' 'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette ' 'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues ' 'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]' ``` The labels will then be the index of the tokens starting and ending the answer, and the model will be tasked to predicted one",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 3,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the Sacred ' 'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a ' 'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette ' 'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues ' 'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]' ``` The labels will then be the index of the tokens starting and ending the answer, and the model will be tasked to predicted one start and end logit per token in the input, with the theoretical labels being as follow: ![One-hot encoded labels for question answering.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg) ![One-hot encoded labels for question answering.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels-dark.svg) In this case the context is not too long, but some of the examples in the dataset have very long contexts that will exceed the maximum length we set (which is 384 in this case). As we saw in [Chapter 6](/course/chapter6/4) when we explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several training features from one sample of our dataset, with a sliding window between them. To see how this works using the current example, we can limit the length to 100 and use a sliding window of 50 tokens. As a reminder, we use: - `max_length` to set the maximum length (here 100) - `truncation=\"only_second\"` to truncate the context (which is in the second position) when the question with its context is too long - `stride` to set the number of overlapping tokens between two successive chunks (here 50) - `return_overflowing_tokens=True` to let the tokenizer know we want the overflowing tokens ``` inputs = tokenizer( question, context, max_length=100, truncation=\"only_second\", stride=50, return_overflowing_tokens=True, ) for ids in inputs[\"input_ids\"]: print(tokenizer.decode(ids)) ``` ``` '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]' '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]' '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 4,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]' '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]' '[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]' ``` As we can see, our example has been in split into four inputs, each of them containing the question and some part of the context. Note that the answer to the question (‚ÄúBernadette Soubirous‚Äù) only appears in the third and last inputs, so by dealing with long contexts in this way we will create some training examples where the answer is not included in the context. For those examples, the labels will be `start_position = end_position = 0` (so we predict the `[CLS]` token). We will also set those labels in the unfortunate case where the answer has been truncated so that we only have the start (or end) of it. For the examples where the answer is fully in the context, the labels will be the index of the token where the answer starts and the index of the token where the answer ends. The dataset provides us with the start character of the answer in the context, and by adding the length of the answer, we can find the end character in the context. To map those to token indices, we will need to use the offset mappings we studied in [Chapter 6](/course/chapter6/4). We can have our tokenizer return these by passing along `return_offsets_mapping=True`: ``` inputs = tokenizer( question, context, max_length=100, truncation=\"only_second\", stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() ``` ``` dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']) ``` As we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, `overflow_to_sample_mapping`. The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 5,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "need to use the offset mappings we studied in [Chapter 6](/course/chapter6/4). We can have our tokenizer return these by passing along `return_offsets_mapping=True`: ``` inputs = tokenizer( question, context, max_length=100, truncation=\"only_second\", stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) inputs.keys() ``` ``` dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']) ``` As we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, `overflow_to_sample_mapping`. The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact that our tokenizer is backed by Rust). Since one sample can give several features, it maps each feature to the example it originated from. Because here we only tokenized one example, we get a list of `0`s: ``` inputs[\"overflow_to_sample_mapping\"] ``` ``` [0, 0, 0, 0] ``` But if we tokenize more examples, this will become more useful: ``` inputs = tokenizer( raw_datasets[\"train\"][2:6][\"question\"], raw_datasets[\"train\"][2:6][\"context\"], max_length=100, truncation=\"only_second\", stride=50, return_overflowing_tokens=True, return_offsets_mapping=True, ) print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\") print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\") ``` ``` 'The 4 examples gave 19 features.' 'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].' ``` As we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features. This information will be useful to map each feature we get to its corresponding label. As mentioned earlier, those labels are: - `(0, 0)` if the answer is not in the corresponding span of the context - `(start_position, end_position)` if the answer is in the corresponding span of the context, with `start_position` being the index of the token (in the input IDs) at the start of the answer and `end_position` being the index of the token (in the input IDs) where the answer ends To determine which of these is the case and, if relevant, the positions of the tokens, we first find the indices that start and end the context in the input IDs. We could use the token type IDs to do this, but since those do not necessarily exist for all models (DistilBERT does not require them, for instance), we‚Äôll instead use the `sequence_ids()` method of the `BatchEncoding` our tokenizer returns. Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is `(0, 0)`). If that‚Äôs not the case, we loop to find the first and last token of the answer: ``` answers = raw_datasets[\"train\"][2:6][\"answers\"] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[\"offset_mapping\"]): sample_idx = inputs[\"overflow_to_sample_mapping\"][i] answer",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 6,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "of the `BatchEncoding` our tokenizer returns. Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is `(0, 0)`). If that‚Äôs not the case, we loop to find the first and last token of the answer: ``` answers = raw_datasets[\"train\"][2:6][\"answers\"] start_positions = [] end_positions = [] for i, offset in enumerate(inputs[\"offset_mapping\"]): sample_idx = inputs[\"overflow_to_sample_mapping\"][i] answer = answers[sample_idx] start_char = answer[\"answer_start\"][0] end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] > start_char or offset[context_end][1] < end_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it's the start and end token positions idx = context_start while idx <= context_end and offset[idx][0] <= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx >= context_start and offset[idx][1] >= end_char: idx -= 1 end_positions.append(idx + 1) start_positions, end_positions ``` ``` ([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0], [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0]) ``` Let‚Äôs take a look at a few results to verify that our approach is correct. For the first feature we find `(83, 85)` as labels, so let‚Äôs compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive): ``` idx = 0 sample_idx = inputs[\"overflow_to_sample_mapping\"][idx] answer = answers[sample_idx][\"text\"][0] start = start_positions[idx] end = end_positions[idx] labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1]) print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\") ``` ``` 'Theoretical answer: the Main Building, labels give: the Main Building' ``` So that‚Äôs a match! Now let‚Äôs check index 4, where we set the labels to `(0, 0)`, which means the answer is not in the context chunk of that feature: ``` idx = 4 sample_idx = inputs[\"overflow_to_sample_mapping\"][idx] answer = answers[sample_idx][\"text\"][0] decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx]) print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\") ``` ``` 'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]' ``` Indeed, we don‚Äôt see the answer inside the context. > ‚úèÔ∏èYour turn!When using the XLNet architecture, padding",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 7,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]' ``` Indeed, we don‚Äôt see the answer inside the context. > ‚úèÔ∏èYour turn!When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and addpadding=True). Be aware that the[CLS]token may not be at the 0 position with padding applied. Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We‚Äôll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here: ``` max_length = 384 stride = 128 def preprocess_training_examples(examples): questions = [q.strip() for q in examples[\"question\"]] inputs = tokenizer( questions, examples[\"context\"], max_length=max_length, truncation=\"only_second\", stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\", ) offset_mapping = inputs.pop(\"offset_mapping\") sample_map = inputs.pop(\"overflow_to_sample_mapping\") answers = examples[\"answers\"] start_positions = [] end_positions = [] for i, offset in enumerate(offset_mapping): sample_idx = sample_map[i] answer = answers[sample_idx] start_char = answer[\"answer_start\"][0] end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) sequence_ids = inputs.sequence_ids(i) # Find the start and end of the context idx = 0 while sequence_ids[idx] != 1: idx += 1 context_start = idx while sequence_ids[idx] == 1: idx += 1 context_end = idx - 1 # If the answer is not fully inside the context, label is (0, 0) if offset[context_start][0] > start_char or offset[context_end][1] < end_char: start_positions.append(0) end_positions.append(0) else: # Otherwise it's the start and end token positions idx = context_start while idx <= context_end and offset[idx][0] <= start_char: idx += 1 start_positions.append(idx - 1) idx = context_end while idx >= context_start and offset[idx][1] >= end_char: idx -= 1 end_positions.append(idx + 1) inputs[\"start_positions\"] = start_positions inputs[\"end_positions\"] = end_positions return inputs ``` Note that we defined two constants to determine the maximum length used as well as the length of the sliding window, and that we added a tiny bit of cleanup before tokenizing: some of the questions in the SQuAD dataset have extra spaces at the beginning and the end that don‚Äôt add anything (and take up space when being tokenized if you use a model like RoBERTa), so we removed those extra spaces. To apply this function to the whole training set, we use the `Dataset.map()` method with the `batched=True` flag. It‚Äôs necessary here as we are changing the length of the dataset (since one example can give several training features): ``` train_dataset = raw_datasets[\"train\"].map( preprocess_training_examples, batched=True,",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 8,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "of the sliding window, and that we added a tiny bit of cleanup before tokenizing: some of the questions in the SQuAD dataset have extra spaces at the beginning and the end that don‚Äôt add anything (and take up space when being tokenized if you use a model like RoBERTa), so we removed those extra spaces. To apply this function to the whole training set, we use the `Dataset.map()` method with the `batched=True` flag. It‚Äôs necessary here as we are changing the length of the dataset (since one example can give several training features): ``` train_dataset = raw_datasets[\"train\"].map( preprocess_training_examples, batched=True, remove_columns=raw_datasets[\"train\"].column_names, ) len(raw_datasets[\"train\"]), len(train_dataset) ``` ``` (87599, 88729) ``` As we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to be used ‚Äî let‚Äôs dig into the preprocessing of the validation set! ### Processing the validation data Preprocessing the validation data will be slightly easier as we don‚Äôt need to generate labels (unless we want to compute a validation loss, but that number won‚Äôt really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we‚Äôll use that ID. The only thing we‚Äôll add here is a tiny bit of cleanup of the offset mappings. They will contain offsets for the question and the context, but once we‚Äôre in the post-processing stage we won‚Äôt have any way to know which part of the input IDs corresponded to the context and which part was the question (the `sequence_ids()` method we used is available for the output of the tokenizer only). So, we‚Äôll set the offsets corresponding to the question to `None`: ``` def preprocess_validation_examples(examples): questions = [q.strip() for q in examples[\"question\"]] inputs = tokenizer( questions, examples[\"context\"], max_length=max_length, truncation=\"only_second\", stride=stride, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\", ) sample_map = inputs.pop(\"overflow_to_sample_mapping\") example_ids = [] for i in range(len(inputs[\"input_ids\"])): sample_idx = sample_map[i] example_ids.append(examples[\"id\"][sample_idx]) sequence_ids = inputs.sequence_ids(i) offset = inputs[\"offset_mapping\"][i] inputs[\"offset_mapping\"][i] = [ o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) ] inputs[\"example_id\"] = example_ids return inputs ``` We can apply this function on the whole validation dataset like before: ``` validation_dataset = raw_datasets[\"validation\"].map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[\"validation\"].column_names, ) len(raw_datasets[\"validation\"]), len(validation_dataset) ``` ``` (10570, 10822) ``` In this case we‚Äôve only added a couple of hundred samples, so it appears the contexts in the validation dataset are a bit shorter. Now that we have preprocessed all the data, we can get to the training. ## Fine-tuning the model with the Trainer API The training code for this example will look a lot like the code in the previous sections ‚Äî the hardest thing will be to write the `compute_metrics()` function. Since we padded all the samples to the maximum length we set, there is no data",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 9,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "batched=True, remove_columns=raw_datasets[\"validation\"].column_names, ) len(raw_datasets[\"validation\"]), len(validation_dataset) ``` ``` (10570, 10822) ``` In this case we‚Äôve only added a couple of hundred samples, so it appears the contexts in the validation dataset are a bit shorter. Now that we have preprocessed all the data, we can get to the training. ## Fine-tuning the model with the Trainer API The training code for this example will look a lot like the code in the previous sections ‚Äî the hardest thing will be to write the `compute_metrics()` function. Since we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The difficult part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the ü§ó Datasets library will do most of the work for us. ### Post-processing The model will output logits for the start and end positions of the answer in the input IDs, as we saw during our exploration of the [question-answeringpipeline](/course/chapter6/3b). The post-processing step will be similar to what we did there, so here‚Äôs a quick reminder of the actions we took: - We masked the start and end logits corresponding to tokens outside of the context. - We then converted the start and end logits into probabilities using a softmax. - We attributed a score to each `(start_token, end_token)` pair by taking the product of the corresponding two probabilities. - We looked for the pair with the maximum score that yielded a valid answer (e.g., a `start_token` lower than `end_token`). Here we will change this process slightly because we don‚Äôt need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also won‚Äôt score all the possible `(start_token, end_token)` pairs, but only the ones corresponding to the highest `n_best` logits (with `n_best=20`). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rulelog‚Å°(ab)=log‚Å°(a)+log‚Å°(b)\\log(ab) = \\log(a) + \\log(b)log(ab)=log(a)+log(b)). To demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant `tokenizer`, we just have to change that object to the tokenizer of the model we want to use temporarily: ``` small_eval_set = raw_datasets[\"validation\"].select(range(100)) trained_checkpoint = \"distilbert-base-cased-distilled-squad\" tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[\"validation\"].column_names, ) ``` Now that the preprocessing is done, we change the tokenizer back to the one we originally picked: ``` tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` We then remove the columns of our `eval_set` that are not expected by the model, build a",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 10,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant `tokenizer`, we just have to change that object to the tokenizer of the model we want to use temporarily: ``` small_eval_set = raw_datasets[\"validation\"].select(range(100)) trained_checkpoint = \"distilbert-base-cased-distilled-squad\" tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint) eval_set = small_eval_set.map( preprocess_validation_examples, batched=True, remove_columns=raw_datasets[\"validation\"].column_names, ) ``` Now that the preprocessing is done, we change the tokenizer back to the one we originally picked: ``` tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) ``` We then remove the columns of our `eval_set` that are not expected by the model, build a batch with all of that small validation set, and pass it through the model. If a GPU is available, we use it to go faster: ``` import torch from transformers import AutoModelForQuestionAnswering eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"]) eval_set_for_model.set_format(\"torch\") device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names} trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to( device ) with torch.no_grad(): outputs = trained_model(**batch) ``` Since the `Trainer` will give us predictions as NumPy arrays, we grab the start and end logits and convert them to that format: ``` start_logits = outputs.start_logits.cpu().numpy() end_logits = outputs.end_logits.cpu().numpy() ``` Now, we need to find the predicted answer for each example in our `small_eval_set`. One example may have been split into several features in `eval_set`, so the first step is to map each example in `small_eval_set` to the corresponding features in `eval_set`: ``` import collections example_to_features = collections.defaultdict(list) for idx, feature in enumerate(eval_set): example_to_features[feature[\"example_id\"]].append(idx) ``` With this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, we‚Äôll look at the logit scores for the `n_best` start logits and end logits, excluding positions that give: - An answer that wouldn‚Äôt be inside the context - An answer with negative length - An answer that is too long (we limit the possibilities at `max_answer_length=30`) Once we have all the scored possible answers for one example, we just pick the one with the best logit score: ``` import numpy as np n_best = 20 max_answer_length = 30 predicted_answers = [] for example in small_eval_set: example_id = example[\"id\"] context = example[\"context\"] answers = [] for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = eval_set[\"offset_mapping\"][feature_index] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either < 0 or > max_answer_length. if ( end_index < start_index or end_index - start_index + 1 > max_answer_length ): continue answers.append( { \"text\": context[offsets[start_index][0] : offsets[end_index][1]], \"logit_score\": start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[\"logit_score\"]) predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}) ``` The final format of the predicted answers is the one that will be expected by the metric we",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 11,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either < 0 or > max_answer_length. if ( end_index < start_index or end_index - start_index + 1 > max_answer_length ): continue answers.append( { \"text\": context[offsets[start_index][0] : offsets[end_index][1]], \"logit_score\": start_logit[start_index] + end_logit[end_index], } ) best_answer = max(answers, key=lambda x: x[\"logit_score\"]) predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}) ``` The final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the ü§ó Evaluate library: ``` import evaluate metric = evaluate.load(\"squad\") ``` This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers): ``` theoretical_answers = [ {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set ] ``` We can now check that we get sensible results by looking at the first element of both lists: ``` print(predicted_answers[0]) print(theoretical_answers[0]) ``` ``` {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'} {'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}} ``` Not too bad! Now let‚Äôs have a look at the score the metric gives us: ``` metric.compute(predictions=predicted_answers, references=theoretical_answers) ``` ``` {'exact_match': 83.0, 'f1': 88.25} ``` Again, that‚Äôs rather good considering that according to [its paper](https://arxiv.org/abs/1910.01108v2) DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset. Now let‚Äôs put everything we just did in a `compute_metrics()` function that we will use in the `Trainer`. Normally, that `compute_metrics()` function only receives a tuple `eval_preds` with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won‚Äôt be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results. The `compute_metrics()` function groups the same steps as before; we just add a small check in case we don‚Äôt come up with any valid answers (in which case we predict an empty string). ``` from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[\"example_id\"]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[\"id\"] context = example[\"context\"] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][\"offset_mapping\"] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 12,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "come up with any valid answers (in which case we predict an empty string). ``` from tqdm.auto import tqdm def compute_metrics(start_logits, end_logits, features, examples): example_to_features = collections.defaultdict(list) for idx, feature in enumerate(features): example_to_features[feature[\"example_id\"]].append(idx) predicted_answers = [] for example in tqdm(examples): example_id = example[\"id\"] context = example[\"context\"] answers = [] # Loop through all features associated with that example for feature_index in example_to_features[example_id]: start_logit = start_logits[feature_index] end_logit = end_logits[feature_index] offsets = features[feature_index][\"offset_mapping\"] start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist() end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() for start_index in start_indexes: for end_index in end_indexes: # Skip answers that are not fully in the context if offsets[start_index] is None or offsets[end_index] is None: continue # Skip answers with a length that is either < 0 or > max_answer_length if ( end_index < start_index or end_index - start_index + 1 > max_answer_length ): continue answer = { \"text\": context[offsets[start_index][0] : offsets[end_index][1]], \"logit_score\": start_logit[start_index] + end_logit[end_index], } answers.append(answer) # Select the answer with the best score if len(answers) > 0: best_answer = max(answers, key=lambda x: x[\"logit_score\"]) predicted_answers.append( {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]} ) else: predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"}) theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples] return metric.compute(predictions=predicted_answers, references=theoretical_answers) ``` We can check it works on our predictions: ``` compute_metrics(start_logits, end_logits, eval_set, small_eval_set) ``` ``` {'exact_match': 83.0, 'f1': 88.25} ``` Looking good! Now let‚Äôs use this to fine-tune our model. ### Fine-tuning the model We are now ready to train our model. Let‚Äôs create it first, using the `AutoModelForQuestionAnswering` class like before: ``` model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) ``` As usual, we get a warning that some weights are not used (the ones from the pretraining head) and some others are initialized randomly (the ones for the question answering head). You should be used to this by now, but that means this model is not ready to be used just yet and needs fine-tuning ‚Äî good thing we‚Äôre about to do that! To be able to push our model to the Hub, we‚Äôll need to log in to Hugging Face. If you‚Äôre running this code in a notebook, you can do so with the following utility function, which displays a widget where you can enter your login credentials: ``` from huggingface_hub import notebook_login notebook_login() ``` If you aren‚Äôt working in a notebook, just type the following line in your terminal: ``` huggingface-cli login ``` Once this is done, we can define our `TrainingArguments`. As we said when we defined our function to compute the metric, we won‚Äôt be able to have a regular evaluation loop because of the signature of the `compute_metrics()` function. We could write our own subclass of `Trainer` to do this (an approach you can find in the [question answering example script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), but that‚Äôs a bit too long for this section. Instead, we will only evaluate the model at the end of training here and show you how to do a regular evaluation in ‚ÄúA custom training loop‚Äù below. This is really where the `Trainer` API shows its",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 13,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "our `TrainingArguments`. As we said when we defined our function to compute the metric, we won‚Äôt be able to have a regular evaluation loop because of the signature of the `compute_metrics()` function. We could write our own subclass of `Trainer` to do this (an approach you can find in the [question answering example script](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)), but that‚Äôs a bit too long for this section. Instead, we will only evaluate the model at the end of training here and show you how to do a regular evaluation in ‚ÄúA custom training loop‚Äù below. This is really where the `Trainer` API shows its limits and the ü§ó Accelerate library shines: customizing the class to a specific use case can be painful, but tweaking a fully exposed training loop is easy. Let‚Äôs take a look at our `TrainingArguments`: ``` from transformers import TrainingArguments args = TrainingArguments( \"bert-finetuned-squad\", evaluation_strategy=\"no\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, fp16=True, push_to_hub=True, ) ``` We‚Äôve seen most of these before: we set some hyperparameters (like the learning rate, the number of epochs we train for, and some weight decay) and indicate that we want to save the model at the end of every epoch, skip evaluation, and upload our results to the Model Hub. We also enable mixed-precision training with `fp16=True`, as it can speed up the training nicely on a recent GPU. By default, the repository used will be in your namespace and named after the output directory you set, so in our case it will be in `\"sgugger/bert-finetuned-squad\"`. We can override this by passing a `hub_model_id`; for instance, to push the model to the `huggingface_course` organization we used `hub_model_id=\"huggingface_course/bert-finetuned-squad\"` (which is the model we linked to at the beginning of this section). > üí° If the output directory you are using exists, it needs to be a local clone of the repository you want to push to (so set a new name if you get an error when defining yourTrainer). Finally, we just pass everything to the `Trainer` class and launch the training: ``` from transformers import Trainer trainer = Trainer( model=model, args=args, train_dataset=train_dataset, eval_dataset=validation_dataset, tokenizer=tokenizer, ) trainer.train() ``` Note that while the training happens, each time the model is saved (here, every epoch) it is uploaded to the Hub in the background. This way, you will be able to to resume your training on another machine if necessary. The whole training takes a while (a little over an hour on a Titan RTX), so you can grab a coffee or reread some of the parts of the course that you‚Äôve found more challenging while it proceeds. Also note that as soon as the first epoch is finished, you will see some weights uploaded to the Hub and you can start playing with your model on its page. Once the training is complete, we can finally evaluate our model (and pray we didn‚Äôt spend all that compute time on nothing). The `predict()` method of the `Trainer` will return a tuple where the first elements will be the predictions of the",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 14,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "on a Titan RTX), so you can grab a coffee or reread some of the parts of the course that you‚Äôve found more challenging while it proceeds. Also note that as soon as the first epoch is finished, you will see some weights uploaded to the Hub and you can start playing with your model on its page. Once the training is complete, we can finally evaluate our model (and pray we didn‚Äôt spend all that compute time on nothing). The `predict()` method of the `Trainer` will return a tuple where the first elements will be the predictions of the model (here a pair with the start and end logits). We send this to our `compute_metrics()` function: ``` predictions, _, _ = trainer.predict(validation_dataset) start_logits, end_logits = predictions compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]) ``` ``` {'exact_match': 81.18259224219489, 'f1': 88.67381321905516} ``` Great! As a comparison, the baseline scores reported in the BERT article for this model are 80.8 and 88.5, so we‚Äôre right where we should be. Finally, we use the `push_to_hub()` method to make sure we upload the latest version of the model: ``` trainer.push_to_hub(commit_message=\"Training complete\") ``` This returns the URL of the commit it just did, if you want to inspect it: ``` 'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68' ``` The `Trainer` also drafts a model card with all the evaluation results and uploads it. At this stage, you can use the inference widget on the Model Hub to test the model and share it with your friends, family, and favorite pets. You have successfully fine-tuned a model on a question answering task ‚Äî congratulations! > ‚úèÔ∏èYour turn!Try another model architecture to see if it performs better on this task! If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using ü§ó Accelerate. ## A custom training loop Let‚Äôs now have a look at the full training loop, so you can easily customize the parts you need. It will look a lot like the training loop in [Chapter 3](/course/chapter3/4), with the exception of the evaluation loop. We will be able to evaluate the model regularly since we‚Äôre not constrained by the `Trainer` class anymore. ### Preparing everything for training First we need to build the `DataLoader`s from our datasets. We set the format of those datasets to `\"torch\"`, and remove the columns in the validation set that are not used by the model. Then, we can use the `default_data_collator` provided by Transformers as a `collate_fn` and shuffle the training set, but not the validation set: ``` from torch.utils.data import DataLoader from transformers import default_data_collator train_dataset.set_format(\"torch\") validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"]) validation_set.set_format(\"torch\") train_dataloader = DataLoader( train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=8, ) eval_dataloader = DataLoader( validation_set, collate_fn=default_data_collator, batch_size=8 ) ``` Next we reinstantiate our model, to make sure we‚Äôre not continuing the fine-tuning from before but starting from the BERT pretrained model again: ``` model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) ``` Then we will need an optimizer. As usual we use the classic `AdamW`, which is like Adam, but",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 15,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "model. Then, we can use the `default_data_collator` provided by Transformers as a `collate_fn` and shuffle the training set, but not the validation set: ``` from torch.utils.data import DataLoader from transformers import default_data_collator train_dataset.set_format(\"torch\") validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"]) validation_set.set_format(\"torch\") train_dataloader = DataLoader( train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=8, ) eval_dataloader = DataLoader( validation_set, collate_fn=default_data_collator, batch_size=8 ) ``` Next we reinstantiate our model, to make sure we‚Äôre not continuing the fine-tuning from before but starting from the BERT pretrained model again: ``` model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint) ``` Then we will need an optimizer. As usual we use the classic `AdamW`, which is like Adam, but with a fix in the way weight decay is applied: ``` from torch.optim import AdamW optimizer = AdamW(model.parameters(), lr=2e-5) ``` Once we have all those objects, we can send them to the `accelerator.prepare()` method. Remember that if you want to train on TPUs in a Colab notebook, you will need to move all of this code into a training function, and that shouldn‚Äôt execute any cell that instantiates an `Accelerator`. We can force mixed-precision training by passing `fp16=True` to the `Accelerator` (or, if you are executing the code as a script, just make sure to fill in the ü§ó Accelerate `config` appropriately). ``` from accelerate import Accelerator accelerator = Accelerator(fp16=True) model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare( model, optimizer, train_dataloader, eval_dataloader ) ``` As you should know from the previous sections, we can only use the `train_dataloader` length to compute the number of training steps after it has gone through the `accelerator.prepare()` method. We use the same linear schedule as in the previous sections: ``` from transformers import get_scheduler num_train_epochs = 3 num_update_steps_per_epoch = len(train_dataloader) num_training_steps = num_train_epochs * num_update_steps_per_epoch lr_scheduler = get_scheduler( \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps, ) ``` To push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you‚Äôre not logged in already. We‚Äôll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does): ``` from huggingface_hub import Repository, get_full_repo_name model_name = \"bert-finetuned-squad-accelerate\" repo_name = get_full_repo_name(model_name) repo_name ``` ``` 'sgugger/bert-finetuned-squad-accelerate' ``` Then we can clone that repository in a local folder. If it already exists, this local folder should be a clone of the repository we are working with: ``` output_dir = \"bert-finetuned-squad-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch. ## Training loop We are now ready to write the full training loop. After defining a progress bar to follow how training goes, the loop has three parts: - The training in itself, which is the classic iteration over the `train_dataloader`, forward pass through the model, then backward pass and optimizer step. - The evaluation, in which we",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 16,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "repository we are working with: ``` output_dir = \"bert-finetuned-squad-accelerate\" repo = Repository(output_dir, clone_from=repo_name) ``` We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch. ## Training loop We are now ready to write the full training loop. After defining a progress bar to follow how training goes, the loop has three parts: - The training in itself, which is the classic iteration over the `train_dataloader`, forward pass through the model, then backward pass and optimizer step. - The evaluation, in which we gather all the values for `start_logits` and `end_logits` before converting them to NumPy arrays. Once the evaluation loop is finished, we concatenate all the results. Note that we need to truncate because the `Accelerator` may have added a few samples at the end to ensure we have the same number of examples in each process. - Saving and uploading, where we first save the model and the tokenizer, then call `repo.push_to_hub()`. As we did before, we use the argument `blocking=False` to tell the ü§ó Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background. Here‚Äôs the complete code for the training loop: ``` from tqdm.auto import tqdm import torch progress_bar = tqdm(range(num_training_steps)) for epoch in range(num_train_epochs): # Training model.train() for step, batch in enumerate(train_dataloader): outputs = model(**batch) loss = outputs.loss accelerator.backward(loss) optimizer.step() lr_scheduler.step() optimizer.zero_grad() progress_bar.update(1) # Evaluation model.eval() start_logits = [] end_logits = [] accelerator.print(\"Evaluation!\") for batch in tqdm(eval_dataloader): with torch.no_grad(): outputs = model(**batch) start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy()) end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy()) start_logits = np.concatenate(start_logits) end_logits = np.concatenate(end_logits) start_logits = start_logits[: len(validation_dataset)] end_logits = end_logits[: len(validation_dataset)] metrics = compute_metrics( start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"] ) print(f\"epoch {epoch}:\", metrics) # Save and upload accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) if accelerator.is_main_process: tokenizer.save_pretrained(output_dir) repo.push_to_hub( commit_message=f\"Training in progress epoch {epoch}\", blocking=False ) ``` In case this is the first time you‚Äôre seeing a model saved with ü§ó Accelerate, let‚Äôs take a moment to inspect the three lines of code that go with it: ``` accelerator.wait_for_everyone() unwrapped_model = accelerator.unwrap_model(model) unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save) ``` The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the `unwrapped_model`, which is the base model we defined. The `accelerator.prepare()` method changes the model to work in distributed training, so it won‚Äôt have the `save_pretrained()` method anymore; the `accelerator.unwrap_model()` method undoes that step. Lastly, we call `save_pretrained()` but tell that method to use `accelerator.save()` instead of `torch.save()`. Once this is done, you should have a model that produces results pretty similar to the one trained with the `Trainer`. You can check the model we trained using this code at [huggingface-course/bert-finetuned-squad-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate). And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above! ## Using the fine-tuned",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 17,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "The `accelerator.prepare()` method changes the model to work in distributed training, so it won‚Äôt have the `save_pretrained()` method anymore; the `accelerator.unwrap_model()` method undoes that step. Lastly, we call `save_pretrained()` but tell that method to use `accelerator.save()` instead of `torch.save()`. Once this is done, you should have a model that produces results pretty similar to the one trained with the `Trainer`. You can check the model we trained using this code at [huggingface-course/bert-finetuned-squad-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate). And if you want to test out any tweaks to the training loop, you can directly implement them by editing the code shown above! ## Using the fine-tuned model We‚Äôve already shown you how you can use the model we fine-tuned on the Model Hub with the inference widget. To use it locally in a `pipeline`, you just have to specify the model identifier: ``` from transformers import pipeline # Replace this with your own checkpoint model_checkpoint = \"huggingface-course/bert-finetuned-squad\" question_answerer = pipeline(\"question-answering\", model=model_checkpoint) context = \"\"\" ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other. \"\"\" question = \"Which deep learning libraries back ü§ó Transformers?\" question_answerer(question=question, context=context) ``` ``` {'score': 0.9979003071784973, 'start': 78, 'end': 105, 'answer': 'Jax, PyTorch and TensorFlow'} ``` Great! Our model is working as well as the default one for this pipeline!",
    "metadata": {
      "title": "Question answering",
      "url": "https://huggingface.co/learn/llm-course/chapter7/7",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/7",
      "part": 18,
      "total_parts": 18,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Mastering LLMs\n\n   \nIf you‚Äôve made it this far in the course, congratulations ‚Äî you now have all the knowledge and tools you need to tackle (almost) any language task with ü§ó Transformers and the Hugging Face ecosystem!\n \n\n## From NLP to LLMs\n\n \nWhile we‚Äôve covered many traditional NLP tasks in this course, the field has been revolutionized by Large Language Models (LLMs). These models have dramatically expanded what‚Äôs possible in language processing:\n \n- They can handle multiple tasks without task-specific fine-tuning\n- They excel at following instructions and adapting to different contexts\n- They can generate coherent, contextually appropriate text for various applications\n- They can perform reasoning and solve complex problems through techniques like chain-of-thought prompting\n \nThe foundational NLP skills you‚Äôve learned are still essential for working with LLMs effectively. Understanding tokenization, model architectures, fine-tuning approaches, and evaluation metrics provides the knowledge needed to leverage LLMs to their full potential.\n \nWe have seen a lot of different data collators, so we made this little video to help you find which one to use for each task:\n  \nAfter completing this lightning tour through the core language tasks, you should:\n \n- Know which architectures (encoder, decoder, or encoder-decoder) are best suited for each task\n- Understand the difference between pretraining and fine-tuning a language model\n- Know how to train Transformer models using either the `Trainer` API and distributed training features of ü§ó Accelerate or TensorFlow and Keras, depending on which track you‚Äôve been following\n- Understand the meaning and limitations of metrics like ROUGE and BLEU for text generation tasks\n- Know how to interact with your fine-tuned models, both on the Hub and using the `pipeline` from ü§ó Transformers\n- Appreciate how LLMs build upon and extend traditional NLP techniques\n \nDespite all this knowledge, there will come a time when you‚Äôll either encounter a difficult bug in your code or have a question about how to solve a particular language processing problem. Fortunately, the Hugging Face community is here to help you! In the final chapter of this part of the course, we‚Äôll explore how you can debug your Transformer models and ask for help effectively.",
    "metadata": {
      "title": "Mastering LLMs",
      "url": "https://huggingface.co/learn/llm-course/chapter7/8",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/8",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # End-of-chapter quiz Let‚Äôs test what you learned in this chapter! ### 1. Which of the following tasks can be framed as a token classification problem? Find the grammatical components in a sentence. Find whether a sentence is grammatically correct or not. Find the persons mentioned in a sentence. Find the chunk of words in a sentence that answers a question. ### 2. What part of the preprocessing for token classification differs from the other preprocessing pipelines? There is no need to do anything; the texts are already tokenized. The texts are given as words, so we only need to apply subword tokenization. We use `-100` to label the special tokens. We need to make sure to truncate or pad the labels to the same size as the inputs, when applying truncation/padding. ### 3. What problem arises when we tokenize the words in a token classification problem and want to label the tokens? The tokenizer adds special tokens and we have no labels for them. Each word can produce several tokens, so we end up with more tokens than we have labels. The added tokens have no labels, so there is no problem. ### 4. What does ‚Äúdomain adaptation‚Äù mean? It's when we run a model on a dataset and get the predictions for each sample in that dataset. It's when we train a model on a dataset. It's when we fine-tune a pretrained model on a new dataset, and it gives predictions that are more adapted to that dataset It's when we add misclassified samples to a dataset to make our model more robust. ### 5. What are the labels in a masked language modeling problem? Some of the tokens in the input sentence are randomly masked and the labels are the original input tokens. Some of the tokens in the input sentence are randomly masked and the labels are the original input tokens, shifted to the left. Some of the tokens in the input sentence are randomly masked, and the label is whether the sentence is positive or negative. Some of the tokens in the two input sentences are randomly masked, and the label is whether the two sentences are similar or not. ### 6. Which of these tasks can be seen as a sequence-to-sequence problem? Writing short reviews of long documents Answering questions about a document Translating a text in Chinese into English Fixing the messages sent by my nephew/friend so they're in proper English ### 7. What is the proper way to preprocess the data for a sequence-to-sequence problem? The inputs and targets have to be sent together to the tokenizer with `inputs=...` and `targets=...`. The inputs and the targets both have to be preprocessed, in two separate calls to the tokenizer. As usual, we just have to tokenize the inputs. The inputs have to be sent to the tokenizer, and the targets too, but under a special context manager. ### 8. Why is there a specific subclass of Trainer for sequence-to-sequence problems? Because sequence-to-sequence",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter7/9",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/9",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "sent by my nephew/friend so they're in proper English ### 7. What is the proper way to preprocess the data for a sequence-to-sequence problem? The inputs and targets have to be sent together to the tokenizer with `inputs=...` and `targets=...`. The inputs and the targets both have to be preprocessed, in two separate calls to the tokenizer. As usual, we just have to tokenize the inputs. The inputs have to be sent to the tokenizer, and the targets too, but under a special context manager. ### 8. Why is there a specific subclass of Trainer for sequence-to-sequence problems? Because sequence-to-sequence problems use a custom loss, to ignore the labels set to `-100` Because sequence-to-sequence problems require a special evaluation loop Because the targets are texts in sequence-to-sequence problems Because we use two models in sequence-to-sequence problems ### 10. When should you pretrain a new model? When there is no pretrained model available for your specific language When you have lots of data available, even if there is a pretrained model that could work on it When you have concerns about the bias of the pretrained model you are using When the pretrained models available are just not good enough ### 11. Why is it easy to pretrain a language model on lots and lots of texts? Because there are plenty of texts available on the internet Because the pretraining objective does not require humans to label the data Because the ü§ó Transformers library only requires a few lines of code to start the training ### 12. What are the main challenges when preprocessing data for a question answering task? You need to tokenize the inputs. You need to deal with very long contexts, which give several training features that may or may not have the answer in them. You need to tokenize the answers to the question as well as the inputs. From the answer span in the text, you have to find the start and end token in the tokenized input. ### 13. How is post-processing usually done in question answering? The model gives you the start and end positions of the answer, and you just have to decode the corresponding span of tokens. The model gives you the start and end positions of the answer for each feature created by one example, and you just have to decode the corresponding span of tokens in the one that has the best score. The model gives you the start and end positions of the answer for each feature created by one example, and you just have to match them to the span in the context for the one that has the best score. The model generates an answer, and you just have to decode it.",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter7/9",
      "course": "llm-course",
      "chapter": "7. Classical NLP tasks",
      "chapter_id": "chapter7/9",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter7/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction\n\n   \nNow that you know how to tackle the most common NLP tasks with ü§ó Transformers, you should be able to get started on your own projects! In this chapter we will explore what to do when you hit a problem. You‚Äôll learn how to successfully debug your code or your training, and how to ask the community for help if you don‚Äôt manage to solve the problem by yourself. And if you think you‚Äôve found a bug in one of the Hugging Face libraries, we‚Äôll show you the best way to report it so that the issue is resolved as quickly as possible.\n \nMore precisely, in this chapter you will learn:\n \n- The first thing to do when you get an error\n- How to ask for help on the [forums](https://discuss.huggingface.co/)\n- How to debug your training pipeline\n- How to write a good issue\n \nNone of this is specifically related to ü§ó Transformers or the Hugging Face ecosystem, of course; the lessons from this chapter are applicable to most open source projects!",
    "metadata": {
      "title": "Introduction",
      "url": "https://huggingface.co/learn/llm-course/chapter8/1",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# What to do when you get an error In this section we‚Äôll look at some common errors that can occur when you‚Äôre trying to generate predictions from your freshly tuned Transformer model. This will prepare you for [section 4](/course/chapter8/section4), where we‚Äôll explore how to debug the training phase itself. We‚Äôve prepared a [template model repository](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) for this section, and if you want to run the code in this chapter you‚Äôll first need to copy the model to your account on the [Hugging Face Hub](https://huggingface.co). To do so, first log in by running either the following in a Jupyter notebook: ``` from huggingface_hub import notebook_login notebook_login() ``` or the following in your favorite terminal: ``` huggingface-cli login ``` This will prompt you to enter your username and password, and will save a token under *~/.cache/huggingface/*. Once you‚Äôve logged in, you can copy the template repository with the following function: ``` from distutils.dir_util import copy_tree from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name def copy_repository_template(): # Clone the repo and extract the local path template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\" commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\" template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash) # Create an empty repo on the Hub model_name = template_repo_id.split(\"/\")[1] create_repo(model_name, exist_ok=True) # Clone the empty repo new_repo_id = get_full_repo_name(model_name) new_repo_dir = model_name repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id) # Copy files copy_tree(template_repo_dir, new_repo_dir) # Push to Hub repo.push_to_hub() ``` Now when you call `copy_repository_template()`, it will create a copy of the template repository under your account. ## Debugging the pipeline from ü§ó Transformers To kick off our journey into the wonderful world of debugging Transformer models, consider the following scenario: you‚Äôre working with a colleague on a question answering project to help the customers of an e-commerce website find answers about consumer products. Your colleague shoots you a message like: > G‚Äôday! I just ran an experiment using the techniques inChapter 7of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is ‚Äúlewtun/distillbert-base-uncased-finetuned-squad-d5716d28‚Äù. Feel free to test it out :) and the first thing you think of is to load the model using the `pipeline` from ü§ó Transformers: ``` from transformers import pipeline model_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\") reader = pipeline(\"question-answering\", model=model_checkpoint) ``` ``` \"\"\" OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that: - 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models' - or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file \"\"\" ``` Oh no, something seems to have gone wrong! If you‚Äôre new to programming, these kind of errors can seem a bit cryptic at first (what even is an `OSError`?!). The error displayed here is just the last part of a much larger error report called a *Python traceback* (aka stack trace). For example, if you‚Äôre running this code on Google Colab, you should see something like the following screenshot: ![A Python traceback.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png) There‚Äôs a lot of information contained in these reports, so let‚Äôs walk through the key parts",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 1,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the correct path to a directory containing a config.json file \"\"\" ``` Oh no, something seems to have gone wrong! If you‚Äôre new to programming, these kind of errors can seem a bit cryptic at first (what even is an `OSError`?!). The error displayed here is just the last part of a much larger error report called a *Python traceback* (aka stack trace). For example, if you‚Äôre running this code on Google Colab, you should see something like the following screenshot: ![A Python traceback.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/traceback.png) There‚Äôs a lot of information contained in these reports, so let‚Äôs walk through the key parts together. The first thing to note is that tracebacks should be read *from bottom to top*. This might sound weird if you‚Äôre used to reading English text from top to bottom, but it reflects the fact that the traceback shows the sequence of function calls that the `pipeline` makes when downloading the model and tokenizer. (Check out [Chapter 2](/course/chapter2) for more details on how the `pipeline` works under the hood.) > üö® See that blue box around ‚Äú6 frames‚Äù in the traceback from Google Colab? That‚Äôs a special feature of Colab, which compresses the traceback into ‚Äúframes.‚Äù If you can‚Äôt seem to find the source of an error, make sure you expand the full traceback by clicking on those two little arrows. This means that the last line of the traceback indicates the last error message and gives the name of the exception that was raised. In this case, the exception type is `OSError`, which indicates a system-related error. If we read the accompanying error message, we can see that there seems to be a problem with the model‚Äôs *config.json* file, and we‚Äôre given two suggestions to fix it: ``` \"\"\" Make sure that: - 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models' - or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file \"\"\" ``` > üí° If you encounter an error message that is difficult to understand, just copy and paste the message into the Google orStack Overflowsearch bar (yes, really!). There‚Äôs a good chance that you‚Äôre not the first person to encounter the error, and this is a good way to find solutions that others in the community have posted. For example, searching forOSError: Can't load config foron Stack Overflow gives severalhitsthat could be used as a starting point for solving the problem. The first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub‚Äôs search bar: ![The wrong model name.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png) Hmm, it indeed looks like our colleague‚Äôs model is not on the Hub‚Ä¶ aha, but there‚Äôs a typo in the name of the model! DistilBERT only has one ‚Äúl‚Äù in its name, so let‚Äôs fix that and look for ‚Äúlewtun/distilbert-base-uncased-finetuned-squad-d5716d28‚Äù instead: ![The right model name.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png) Okay, this got a hit. Now let‚Äôs try to download the model again with the correct model",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 2,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "The first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub‚Äôs search bar: ![The wrong model name.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/wrong-model-id.png) Hmm, it indeed looks like our colleague‚Äôs model is not on the Hub‚Ä¶ aha, but there‚Äôs a typo in the name of the model! DistilBERT only has one ‚Äúl‚Äù in its name, so let‚Äôs fix that and look for ‚Äúlewtun/distilbert-base-uncased-finetuned-squad-d5716d28‚Äù instead: ![The right model name.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/true-model-id.png) Okay, this got a hit. Now let‚Äôs try to download the model again with the correct model ID: ``` model_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\") reader = pipeline(\"question-answering\", model=model_checkpoint) ``` ``` \"\"\" OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that: - 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models' - or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file \"\"\" ``` Argh, foiled again ‚Äî welcome to the daily life of a machine learning engineer! Since we‚Äôve fixed the model ID, the problem must lie in the repository itself. A quick way to access the contents of a repository on the ü§ó Hub is via the `list_repo_files()` function of the `huggingface_hub` library: ``` from huggingface_hub import list_repo_files list_repo_files(repo_id=model_checkpoint) ``` ``` ['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt'] ``` Interesting ‚Äî there doesn‚Äôt seem to be a *config.json* file in the repository! No wonder our `pipeline` couldn‚Äôt load the model; our colleague must have forgotten to push this file to the Hub after they fine-tuned it. In this case, the problem seems pretty straightforward to fix: we could ask them to add the file, or, since we can see from the model ID that the pretrained model used was [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), we can download the config for this model and push it to our repo to see if that resolves the problem. Let‚Äôs try that. Using the techniques we learned in [Chapter 2](/course/chapter2), we can download the model‚Äôs configuration with the `AutoConfig` class: ``` from transformers import AutoConfig pretrained_checkpoint = \"distilbert-base-uncased\" config = AutoConfig.from_pretrained(pretrained_checkpoint) ``` > üö® The approach we‚Äôre taking here is not foolproof, since our colleague may have tweaked the configuration ofdistilbert-base-uncasedbefore fine-tuning the model. In real life, we‚Äôd want to check with them first, but for the purposes of this section we‚Äôll assume they used the default configuration. We can then push this to our model repository with the configuration‚Äôs `push_to_hub()` function: ``` config.push_to_hub(model_checkpoint, commit_message=\"Add config.json\") ``` Now we can test if this worked by loading the model from the latest commit on the `main` branch: ``` reader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\") context = r\"\"\" Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script. ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX frameworks, so you can use your",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 3,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "config.push_to_hub(model_checkpoint, commit_message=\"Add config.json\") ``` Now we can test if this worked by loading the model from the latest commit on the `main` branch: ``` reader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\") context = r\"\"\" Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script. ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX frameworks, so you can use your favourite tools for a wide variety of tasks! \"\"\" question = \"What is extractive question answering?\" reader(question=question, context=context) ``` ``` {'score': 0.38669535517692566, 'start': 34, 'end': 95, 'answer': 'the task of extracting an answer from a text given a question'} ``` Woohoo, it worked! Let‚Äôs recap what you‚Äôve just learned: - The error messages in Python are known as *tracebacks* and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem. - If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs. - If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue. - The `huggingface_hub` // ü§ó Hub? library provides a suite of tools that you can use to interact with and debug repositories on the Hub. Now that you know how to debug a pipeline, let‚Äôs take a look at a trickier example in the forward pass of the model itself. ## Debugging the forward pass of your model Although the `pipeline` is great for most applications where you need to quickly generate predictions, sometimes you‚Äôll need to access the model‚Äôs logits (say, if you have some custom post-processing that you‚Äôd like to apply). To see what can go wrong in this case, let‚Äôs first grab the model and tokenizer from our `pipeline`: ``` tokenizer = reader.tokenizer model = reader.model ``` Next we need a question, so let‚Äôs see if our favorite frameworks are supported: ``` question = \"Which frameworks can I use?\" ``` As we saw in [Chapter 7](/course/chapter7), the usual steps we need to take are tokenizing the inputs, extracting the logits of the start and end tokens, and then decoding the answer span: ``` import torch inputs = tokenizer(question, context, add_special_tokens=True) input_ids = inputs[\"input_ids\"][0] outputs = model(**inputs) answer_start_scores = outputs.start_logits answer_end_scores = outputs.end_logits # Get the most likely beginning of answer with the argmax of the score answer_start = torch.argmax(answer_start_scores) # Get the most likely end of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]) ) print(f\"Question: {question}\") print(f\"Answer: {answer}\") ``` ``` \"\"\" --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module> 1 inputs = tokenizer(question, text, add_special_tokens=True) 2 input_ids",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 4,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the logits of the start and end tokens, and then decoding the answer span: ``` import torch inputs = tokenizer(question, context, add_special_tokens=True) input_ids = inputs[\"input_ids\"][0] outputs = model(**inputs) answer_start_scores = outputs.start_logits answer_end_scores = outputs.end_logits # Get the most likely beginning of answer with the argmax of the score answer_start = torch.argmax(answer_start_scores) # Get the most likely end of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]) ) print(f\"Question: {question}\") print(f\"Answer: {answer}\") ``` ``` \"\"\" --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module> 1 inputs = tokenizer(question, text, add_special_tokens=True) 2 input_ids = inputs[\"input_ids\"] ----> 3 outputs = model(**inputs) 4 answer_start_scores = outputs.start_logits 5 answer_end_scores = outputs.end_logits ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -> 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict) 723 return_dict = return_dict if return_dict is not None else self.config.use_return_dict 724 --> 725 distilbert_output = self.distilbert( 726 input_ids=input_ids, 727 attention_mask=attention_mask, ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs) 1049 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1050 or _global_forward_hooks or _global_forward_pre_hooks): -> 1051 return forward_call(*input, **kwargs) 1052 # Do not call functions when jit is used 1053 full_backward_hooks, non_full_backward_hooks = [], [] ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict) 471 raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\") 472 elif input_ids is not None: --> 473 input_shape = input_ids.size() 474 elif inputs_embeds is not None: 475 input_shape = inputs_embeds.size()[:-1] AttributeError: 'list' object has no attribute 'size' \"\"\" ``` Oh dear, it looks like we have a bug in our code! But we‚Äôre not afraid of a little debugging. You can use the Python debugger in a notebook: or in a terminal: Here, reading the error message tells us that `'list' object has no attribute 'size'`, and we can see a `-->` arrow pointing to the line where the problem was raised in `model(**inputs)`.You can debug this interactively using the Python debugger, but for now we‚Äôll simply print out a slice of `inputs` to see what we have: ``` inputs[\"input_ids\"][:5] ``` ``` [101, 2029, 7705, 2015, 2064] ``` This certainly looks like an ordinary Python `list`, but let‚Äôs double-check the type: ``` type(inputs[\"input_ids\"]) ``` ``` list ``` Yep, that‚Äôs a Python `list` for sure. So what went wrong? Recall from [Chapter 2](/course/chapter2) that the `AutoModelForXxx` classes in ü§ó Transformers operate on *tensors* (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using `Tensor.size()` in, say, PyTorch. Let‚Äôs take another look at the traceback, to see which line triggered the exception: ``` ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict) 471 raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\") 472 elif input_ids is not None: --> 473 input_shape = input_ids.size() 474 elif inputs_embeds",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 5,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "``` Yep, that‚Äôs a Python `list` for sure. So what went wrong? Recall from [Chapter 2](/course/chapter2) that the `AutoModelForXxx` classes in ü§ó Transformers operate on *tensors* (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using `Tensor.size()` in, say, PyTorch. Let‚Äôs take another look at the traceback, to see which line triggered the exception: ``` ~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict) 471 raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\") 472 elif input_ids is not None: --> 473 input_shape = input_ids.size() 474 elif inputs_embeds is not None: 475 input_shape = inputs_embeds.size()[:-1] AttributeError: 'list' object has no attribute 'size' ``` It looks like our code tried to call `input_ids.size()`, but this clearly won‚Äôt work for a Python `list`, which is just a container. How can we solve this problem? Searching for the error message on Stack Overflow gives quite a few relevant [hits](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f). Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below: ![An answer from Stack Overflow.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/stack-overflow.png) The answer recommends that we add `return_tensors='pt'` to the tokenizer, so let‚Äôs see if that works for us: ``` inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\") input_ids = inputs[\"input_ids\"][0] outputs = model(**inputs) answer_start_scores = outputs.start_logits answer_end_scores = outputs.end_logits # Get the most likely beginning of answer with the argmax of the score answer_start = torch.argmax(answer_start_scores) # Get the most likely end of answer with the argmax of the score answer_end = torch.argmax(answer_end_scores) + 1 answer = tokenizer.convert_tokens_to_string( tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]) ) print(f\"Question: {question}\") print(f\"Answer: {answer}\") ``` ``` \"\"\" Question: Which frameworks can I use? Answer: pytorch, tensorflow, and jax \"\"\" ``` Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won‚Äôt always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co/) that can help you out! In the next section, we‚Äôll take a look at how you can craft good forum questions that are likely to get answered.",
    "metadata": {
      "title": "What to do when you get an error",
      "url": "https://huggingface.co/learn/llm-course/chapter8/2",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/2",
      "part": 6,
      "total_parts": 6,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Asking for help on the forums The [Hugging Face forums](https://discuss.huggingface.co) are a great place to get help from the open source team and wider Hugging Face community. Here‚Äôs what the main page looks like on any given day: ![The Hugging Face forums.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forums.png) On the lefthand side you can see all the categories that the various topics are grouped into, while the righthand side shows the most recent topics. A topic is a post that contains a title, category, and description; it‚Äôs quite similar to the GitHub issues format that we saw when creating our own dataset in [Chapter 5](/course/chapter5). As the name suggests, the [Beginners](https://discuss.huggingface.co/c/beginners/5) category is primarily intended for people just starting out with the Hugging Face libraries and ecosystem. Any question on any of the libraries is welcome there, be it to debug some code or to ask for help about how to do something. (That said, if your question concerns one library in particular, you should probably head to the corresponding library category on the forum.) Similarly, the [Intermediate](https://discuss.huggingface.co/c/intermediate/6) and [Research](https://discuss.huggingface.co/c/research/7) categories are for more advanced questions, for example about the libraries or some cool new NLP research that you‚Äôd like to discuss. And naturally, we should also mention the [Course](https://discuss.huggingface.co/c/course/20) category, where you can ask any questions you have that are related to the Hugging Face course! Once you have selected a category, you‚Äôll be ready to write your first topic. You can find some [guidelines](https://discuss.huggingface.co/t/how-to-request-support/3128) in the forum on how to do this, and in this section we‚Äôll take a look at some features that make up a good topic. ## Writing a good forum post As a running example, let‚Äôs suppose that we‚Äôre trying to generate embeddings from Wikipedia articles to create a custom search engine. As usual, we load the tokenizer and model as follows: ``` from transformers import AutoTokenizer, AutoModel model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) model = AutoModel.from_pretrained(model_checkpoint) ``` Now suppose we try to embed a whole section of the [Wikipedia article](https://en.wikipedia.org/wiki/Transformers) on Transformers (the franchise, not the library!): ``` text = \"\"\" Generation One is a retroactive term for the Transformers characters that appeared between 1984 and 1993. The Transformers began with the 1980s Japanese toy lines Micro Change and Diaclone. They presented robots able to transform into everyday vehicles, electronic items or weapons. Hasbro bought the Micro Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall story, and gave the task of creating the characthers to writer Dennis O'Neil. Unhappy with O'Neil's work (although O'Neil created the name \"Optimus Prime\"), Shooter chose Bob Budiansky to create the characters. The Transformers mecha were largely designed by Sh≈çji Kawamori, the creator of the Japanese mecha anime franchise Macross (which was adapted into the Robotech franchise in North America). Kawamori came up with the idea of transforming mechs while working on the Diaclone and Macross franchises in the early 1980s (such as the VF-1 Valkyrie in",
    "metadata": {
      "title": "Asking for help on the forums",
      "url": "https://huggingface.co/learn/llm-course/chapter8/3",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/3",
      "part": 1,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "hired by Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall story, and gave the task of creating the characthers to writer Dennis O'Neil. Unhappy with O'Neil's work (although O'Neil created the name \"Optimus Prime\"), Shooter chose Bob Budiansky to create the characters. The Transformers mecha were largely designed by Sh≈çji Kawamori, the creator of the Japanese mecha anime franchise Macross (which was adapted into the Robotech franchise in North America). Kawamori came up with the idea of transforming mechs while working on the Diaclone and Macross franchises in the early 1980s (such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs later providing the basis for Transformers. The primary concept of Generation One is that the heroic Optimus Prime, the villainous Megatron, and their finest soldiers crash land on pre-historic Earth in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through the Neutral zone as an effect of the war. The Marvel comic was originally part of the main Marvel Universe, with appearances from Spider-Man and Nick Fury, plus some cameos, as well as a visit to the Savage Land. The Transformers TV series began around the same time. Produced by Sunbow Productions and Marvel Productions, later Hasbro Productions, from the start it contradicted Budiansky's backstories. The TV series shows the Autobots looking for new energy sources, and crash landing as the Decepticons attack. Marvel interpreted the Autobots as destroying a rogue asteroid approaching Cybertron. Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a stalemate during his absence, but in the comic book he attempts to take command of the Decepticons. The TV series would also differ wildly from the origins Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire (known as Skyfire on TV), the Constructicons (who combine to form Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on that Prime wields the Creation Matrix, which gives life to machines. In the second season, the two-part episode The Key to Vector Sigma introduced the ancient Vector Sigma computer, which served the same original purpose as the Creation Matrix (giving life to Transformers), and its guardian Alpha Trion. \"\"\" inputs = tokenizer(text, return_tensors=\"pt\") logits = model(**inputs).logits ``` ``` IndexError: index out of range in self ``` Uh-oh, we‚Äôve hit a problem ‚Äî and the error message is far more cryptic than the ones we saw in [section 2](/course/chapter8/section2)! We can‚Äôt make head or tails of the full traceback, so we decide to turn to the Hugging Face forums for help. How might we craft the topic? To get started, we need to click the ‚ÄúNew Topic‚Äù button at the upper-right corner (note that to create a topic, we‚Äôll need to be logged in): ![Creating a new forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forums-new-topic.png) This brings up a writing interface where we can input the title of our topic, select a category, and draft the content: ![The interface for creating a forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic01.png) Since the error seems to be exclusively",
    "metadata": {
      "title": "Asking for help on the forums",
      "url": "https://huggingface.co/learn/llm-course/chapter8/3",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/3",
      "part": 2,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "saw in [section 2](/course/chapter8/section2)! We can‚Äôt make head or tails of the full traceback, so we decide to turn to the Hugging Face forums for help. How might we craft the topic? To get started, we need to click the ‚ÄúNew Topic‚Äù button at the upper-right corner (note that to create a topic, we‚Äôll need to be logged in): ![Creating a new forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forums-new-topic.png) This brings up a writing interface where we can input the title of our topic, select a category, and draft the content: ![The interface for creating a forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic01.png) Since the error seems to be exclusively about ü§ó Transformers, we‚Äôll select this for the category. Our first attempt at explaining the problem might look something like this: ![Drafting the content for a new forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic02.png) Although this topic contains the error message we need help with, there are a few problems with the way it is written: 1. The title is not very descriptive, so anyone browsing the forum won‚Äôt be able to tell what the topic is about without reading the body as well. 2. The body doesn‚Äôt provide enough information about *where* the error is coming from and *how* to reproduce it. 3. The topic tags a few people directly with a somewhat demanding tone. Topics like this one are not likely to get a fast answer (if they get one at all), so let‚Äôs look at how we can improve it. We‚Äôll start with the first issue of picking a good title. ### Choosing a descriptive title If you‚Äôre trying to get help with a bug in your code, a good rule of thumb is to include enough information in the title so that others can quickly determine whether they think they can answer your question or not. In our running example, we know the name of the exception that‚Äôs being raised and have some hints that it‚Äôs triggered in the forward pass of the model, where we call `model(**inputs)`. To communicate this, one possible title could be: > Source of IndexError in the AutoModel forward pass? This title tells the reader *where* you think the bug is coming from, and if they‚Äôve encountered an `IndexError` before, there‚Äôs a good chance they‚Äôll know how to debug it. Of course, the title can be anything you want, and other variations like: > Why does my model produce an IndexError? could also be fine. Now that we‚Äôve got a descriptive title, let‚Äôs take a look at improving the body. ### Formatting your code snippets Reading source code is hard enough in an IDE, but it‚Äôs even harder when the code is copied and pasted as plain text! Fortunately, the Hugging Face forums support the use of Markdown, so you should always enclose your code blocks with three backticks (```) so it‚Äôs more easily readable. Let‚Äôs do this to prettify the error message ‚Äî and while we‚Äôre at it, let‚Äôs make the body a bit more polite than our original version: ![Our revised forum topic, with",
    "metadata": {
      "title": "Asking for help on the forums",
      "url": "https://huggingface.co/learn/llm-course/chapter8/3",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/3",
      "part": 3,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "that we‚Äôve got a descriptive title, let‚Äôs take a look at improving the body. ### Formatting your code snippets Reading source code is hard enough in an IDE, but it‚Äôs even harder when the code is copied and pasted as plain text! Fortunately, the Hugging Face forums support the use of Markdown, so you should always enclose your code blocks with three backticks (```) so it‚Äôs more easily readable. Let‚Äôs do this to prettify the error message ‚Äî and while we‚Äôre at it, let‚Äôs make the body a bit more polite than our original version: ![Our revised forum topic, with proper code formatting.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic03.png) As you can see in the screenshot, enclosing the code blocks in backticks converts the raw text into formatted code, complete with color styling! Also note that single backticks can be used to format inline variables, like we‚Äôve done for `distilbert-base-uncased`. This topic is looking much better, and with a bit of luck we might find someone in the community who can guess what the error is about. However, instead of relying on luck, let‚Äôs make life easier by including the traceback in its full gory detail! ### Including the full traceback Since the last line of the traceback is often enough to debug your own code, it can be tempting to just provide that in your topic to ‚Äúsave space.‚Äù Although well intentioned, this actually makes it *harder* for others to debug the problem since the information that‚Äôs higher up in the traceback can be really useful too. So, a good practice is to copy and paste the *whole* traceback, while making sure that it‚Äôs nicely formatted. Since these tracebacks can get rather long, some people prefer to show them after they‚Äôve explained the source code. Let‚Äôs do this. Now, our forum topic looks like the following: ![Our example forum topic, with the complete traceback.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic04.png) This is much more informative, and a careful reader might be able to point out that the problem seems to be due to passing a long input because of this line in the traceback: > Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). However, we can make things even easier for them by providing the actual code that triggered the error. Let‚Äôs do that now. ### Providing a reproducible example If you‚Äôve ever tried to debug someone else‚Äôs code, you‚Äôve probably first tried to recreate the problem they‚Äôve reported so you can start working your way through the traceback to pinpoint the error. It‚Äôs no different when it comes to getting (or giving) assistance on the forums, so it really helps if you can provide a small example that reproduces the error. Half the time, simply walking through this exercise will help you figure out what‚Äôs going wrong. In any case, the missing piece of our example is to show the *inputs* that we provided to the model. Doing that gives us something like the following completed example: ![The final version",
    "metadata": {
      "title": "Asking for help on the forums",
      "url": "https://huggingface.co/learn/llm-course/chapter8/3",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/3",
      "part": 4,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "first tried to recreate the problem they‚Äôve reported so you can start working your way through the traceback to pinpoint the error. It‚Äôs no different when it comes to getting (or giving) assistance on the forums, so it really helps if you can provide a small example that reproduces the error. Half the time, simply walking through this exercise will help you figure out what‚Äôs going wrong. In any case, the missing piece of our example is to show the *inputs* that we provided to the model. Doing that gives us something like the following completed example: ![The final version of our forum topic.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic05.png) This topic now contains quite a lot of information, and it‚Äôs written in a way that is much more likely to attract the attention of the community and get a helpful answer. With these basic guidelines, you can now create great topics to find the answers to your ü§ó Transformers questions!",
    "metadata": {
      "title": "Asking for help on the forums",
      "url": "https://huggingface.co/learn/llm-course/chapter8/3",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/3",
      "part": 5,
      "total_parts": 5,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "[Pytorch](?fw=pt)[TensorFlow](?fw=tf) # Debugging the training pipeline You‚Äôve written a beautiful script to train or fine-tune a model on a given task, dutifully following the advice from [Chapter 7](/course/chapter7). But when you launch the command `trainer.train()`, something horrible happens: you get an error üò±! Or worse, everything seems to be fine and the training runs without error, but the resulting model is crappy. In this section, we will show you what you can do to debug these kinds of issues. ## Debugging the training pipeline The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric. The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve. To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue): ``` from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) trainer = Trainer( model, args, train_dataset=raw_datasets[\"train\"], eval_dataset=raw_datasets[\"validation_matched\"], compute_metrics=compute_metrics, ) trainer.train() ``` If you try to execute it, you will be met with a rather cryptic error: ``` 'ValueError: You have to specify either input_ids or inputs_embeds' ``` ### Check your data This goes without saying, but if your data is corrupted, the `Trainer` is not going to be able to form batches, let alone train your model. So first things first, you need to have a look at what is inside your training set. To avoid countless hours spent trying to fix something that is not the source of the bug, we recommend you use `trainer.train_dataset` for your checks and nothing else. So let‚Äôs do that here: ``` trainer.train_dataset[0] ``` ``` {'hypothesis': 'Product and geography are what make cream skimming work. ', 'idx': 0, 'label': 1, 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'} ``` Do you notice something wrong? This, in conjunction with the error message about `input_ids` missing, should make you realize those are texts, not numbers the model can make sense of. Here, the",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 1,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your training set. To avoid countless hours spent trying to fix something that is not the source of the bug, we recommend you use `trainer.train_dataset` for your checks and nothing else. So let‚Äôs do that here: ``` trainer.train_dataset[0] ``` ``` {'hypothesis': 'Product and geography are what make cream skimming work. ', 'idx': 0, 'label': 1, 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'} ``` Do you notice something wrong? This, in conjunction with the error message about `input_ids` missing, should make you realize those are texts, not numbers the model can make sense of. Here, the original error is very misleading because the `Trainer` automatically removes the columns that don‚Äôt match the model signature (that is, the arguments expected by the model). That means here, everything apart from the labels was discarded. There was thus no issue with creating batches and then sending them to the model, which in turn complained it didn‚Äôt receive the proper input. Why wasn‚Äôt the data processed? We did use the `Dataset.map()` method on the datasets to apply the tokenizer on each sample. But if you look closely at the code, you will see that we made a mistake when passing the training and evaluation sets to the `Trainer`. Instead of using `tokenized_datasets` here, we used `raw_datasets` ü§¶. So let‚Äôs fix this! ``` from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, ) trainer.train() ``` This new code will now give a different error (progress!): ``` 'ValueError: expected sequence of length 43 at dim 1 (got 37)' ``` Looking at the traceback, we can see the error happens in the data collation step: ``` ~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features) 105 batch[k] = torch.stack([f[k] for f in features]) 106 else: --> 107 batch[k] = torch.tensor([f[k] for f in features]) 108 109 return batch ``` So, we should move to that. Before we do, however, let‚Äôs finish inspecting our data, just to be 100% sure it‚Äôs correct. One thing you should always do when debugging a training session is have a look at the decoded inputs of your model. We can‚Äôt make sense of the numbers that we feed it directly, so we should look at what those numbers represent. In computer vision, for example, that means looking at the decoded pictures of the pixels you pass, in speech it means listening to the decoded audio samples, and for our NLP example here it means using our tokenizer to decode the inputs: ``` tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]) ``` ``` '[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]' ``` So that seems correct. You",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 2,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "inputs of your model. We can‚Äôt make sense of the numbers that we feed it directly, so we should look at what those numbers represent. In computer vision, for example, that means looking at the decoded pictures of the pixels you pass, in speech it means listening to the decoded audio samples, and for our NLP example here it means using our tokenizer to decode the inputs: ``` tokenizer.decode(trainer.train_dataset[0][\"input_ids\"]) ``` ``` '[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]' ``` So that seems correct. You should do this for all the keys in the inputs: ``` trainer.train_dataset[0].keys() ``` ``` dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise']) ``` Note that the keys that don‚Äôt correspond to inputs accepted by the model will be automatically discarded, so here we will only keep `input_ids`, `attention_mask`, and `label` (which will be renamed `labels`). To double-check the model signature, you can print the class of your model, then go check its documentation: ``` type(trainer.model) ``` ``` transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification ``` So in our case, we can check the parameters accepted on [this page](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification). The `Trainer` will also log the columns it‚Äôs discarding. We have checked that the input IDs are correct by decoding them. Next is the `attention_mask`: ``` trainer.train_dataset[0][\"attention_mask\"] ``` ``` [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ``` Since we didn‚Äôt apply padding in our preprocessing, this seems perfectly natural. To be sure there is no issue with that attention mask, let‚Äôs check it is the same length as our input IDs: ``` len(trainer.train_dataset[0][\"attention_mask\"]) == len( trainer.train_dataset[0][\"input_ids\"] ) ``` ``` True ``` That‚Äôs good! Lastly, let‚Äôs check our label: ``` trainer.train_dataset[0][\"label\"] ``` ``` 1 ``` Like the input IDs, this is a number that doesn‚Äôt really make sense on its own. As we saw before, the map between integers and label names is stored inside the `names` attribute of the corresponding *feature* of the dataset: ``` trainer.train_dataset.features[\"label\"].names ``` ``` ['entailment', 'neutral', 'contradiction'] ``` So `1` means `neutral`, which means the two sentences we saw above are not in contradiction, and the first one does not imply the second one. That seems correct! We don‚Äôt have token type IDs here, since DistilBERT does not expect them; if you have some in your model, you should also make sure that they properly match where the first and second sentences are in the input. > ‚úèÔ∏èYour turn!Check that everything seems correct with the second element of the training dataset. We are only doing the check on the training set here, but you should of course double-check the validation and test sets the same way. Now that we know our datasets look good, it‚Äôs time to check the next step of the training pipeline. ### From datasets to dataloaders The next thing that can go wrong in the training pipeline is when the `Trainer`",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 3,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your model, you should also make sure that they properly match where the first and second sentences are in the input. > ‚úèÔ∏èYour turn!Check that everything seems correct with the second element of the training dataset. We are only doing the check on the training set here, but you should of course double-check the validation and test sets the same way. Now that we know our datasets look good, it‚Äôs time to check the next step of the training pipeline. ### From datasets to dataloaders The next thing that can go wrong in the training pipeline is when the `Trainer` tries to form batches from the training or validation set. Once you are sure the `Trainer`‚Äôs datasets are correct, you can try to manually form a batch by executing the following (replace `train` with `eval` for the validation dataloader): ``` for batch in trainer.get_train_dataloader(): break ``` This code creates the training dataloader, then iterates through it, stopping at the first iteration. If the code executes without error, you have the first training batch that you can inspect, and if the code errors out, you know for sure the problem is in the dataloader, as is the case here: ``` ~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features) 105 batch[k] = torch.stack([f[k] for f in features]) 106 else: --> 107 batch[k] = torch.tensor([f[k] for f in features]) 108 109 return batch ValueError: expected sequence of length 45 at dim 1 (got 76) ``` Inspecting the last frame of the traceback should be enough to give you a clue, but let‚Äôs do a bit more digging. Most of the problems during batch creation arise because of the collation of examples into a single batch, so the first thing to check when in doubt is what `collate_fn` your `DataLoader` is using: ``` data_collator = trainer.get_train_dataloader().collate_fn data_collator ``` ``` <function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]> ``` So this is the `default_data_collator`, but that‚Äôs not what we want in this case. We want to pad our examples to the longest sentence in the batch, which is done by the `DataCollatorWithPadding` collator. And this data collator is supposed to be used by default by the `Trainer`, so why is it not used here? The answer is because we did not pass the `tokenizer` to the `Trainer`, so it couldn‚Äôt create the `DataCollatorWithPadding` we want. In practice, you should never hesitate to explicitly pass along the data collator you want to use, to make sure you avoid these kinds of errors. Let‚Äôs adapt our code to do exactly that: ``` from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, data_collator=data_collator, tokenizer=tokenizer, ) trainer.train() ``` The good news?",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 4,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "use, to make sure you avoid these kinds of errors. Let‚Äôs adapt our code to do exactly that: ``` from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, data_collator=data_collator, tokenizer=tokenizer, ) trainer.train() ``` The good news? We don‚Äôt get the same error as before, which is definitely progress. The bad news? We get an infamous CUDA error instead: ``` RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)` ``` This is bad because CUDA errors are extremely hard to debug in general. We will see in a minute how to solve this, but first let‚Äôs finish our analysis of batch creation. If you are sure your data collator is the right one, you should try to apply it on a couple of samples of your dataset: ``` data_collator = trainer.get_train_dataloader().collate_fn batch = data_collator([trainer.train_dataset[i] for i in range(4)]) ``` This code will fail because the `train_dataset` contains string columns, which the `Trainer` usually removes. You can remove them manually, or if you want to replicate exactly what the `Trainer` is doing behind the scenes, you can call the private `Trainer._remove_unused_columns()` method that does that: ``` data_collator = trainer.get_train_dataloader().collate_fn actual_train_set = trainer._remove_unused_columns(trainer.train_dataset) batch = data_collator([actual_train_set[i] for i in range(4)]) ``` You should then be able to manually debug what happens inside the data collator if the error persists. Now that we‚Äôve debugged the batch creation process, it‚Äôs time to pass one through the model! ### Going through the model You should be able to get a batch by executing the following command: ``` for batch in trainer.get_train_dataloader(): break ``` If you‚Äôre running this code in a notebook, you may get a CUDA error that‚Äôs similar to the one we saw earlier, in which case you need to restart your notebook and reexecute the last snippet without the `trainer.train()` line. That‚Äôs the second most annoying thing about CUDA errors: they irremediably break your kernel. The most annoying thing about them is the fact that they are hard to debug. Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don‚Äôt know it instantly. It‚Äôs only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 5,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don‚Äôt know it instantly. It‚Äôs only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass. So how do we debug those errors? The answer is easy: we don‚Äôt. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it. To do this in our case, we just have to put the model back on the CPU and call it on our batch ‚Äî the batch returned by the `DataLoader` has not been moved to the GPU yet: ``` outputs = trainer.model.cpu()(**batch) ``` ``` ~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction) 2386 ) 2387 if dim == 2: -> 2388 ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index) 2389 elif dim == 4: 2390 ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index) IndexError: Target 2 is out of bounds. ``` So, the picture is getting clearer. Instead of having a CUDA error, we now have an `IndexError` in the loss computation (so nothing to do with the backward pass, as we said earlier). More precisely, we can see that it‚Äôs target 2 that creates the error, so this is a very good moment to check the number of labels of our model: ``` trainer.model.config.num_labels ``` ``` 2 ``` With two labels, only 0s and 1s are allowed as targets, but according to the error message we got a 2. Getting a 2 is actually normal: if we remember the label names we extracted earlier, there were three, so we have indices 0, 1, and 2 in our dataset. The problem is that we didn‚Äôt tell that to our model, which should have been created with three labels. So let‚Äôs fix that! ``` from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, data_collator=data_collator, tokenizer=tokenizer, ) ``` We aren‚Äôt including the `trainer.train()` line yet, to take the time to check that everything looks good. If we request a batch and pass it to our model, it now works without error! ``` for batch in trainer.get_train_dataloader(): break outputs = trainer.model.cpu()(**batch) ```",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 6,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "= \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred return metric.compute(predictions=predictions, references=labels) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, data_collator=data_collator, tokenizer=tokenizer, ) ``` We aren‚Äôt including the `trainer.train()` line yet, to take the time to check that everything looks good. If we request a batch and pass it to our model, it now works without error! ``` for batch in trainer.get_train_dataloader(): break outputs = trainer.model.cpu()(**batch) ``` The next step is then to move back to the GPU and check that everything still works: ``` import torch device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") batch = {k: v.to(device) for k, v in batch.items()} outputs = trainer.model.to(device)(**batch) ``` If you still get an error, make sure you restart your notebook and only execute the last version of the script. ### Performing one optimization step Now that we know that we can build batches that actually go through the model, we are ready for the next step of the training pipeline: computing the gradients and performing an optimization step. The first part is just a matter of calling the `backward()` method on the loss: ``` loss = outputs.loss loss.backward() ``` It‚Äôs pretty rare to get an error at this stage, but if you do get one, make sure to go back to the CPU to get a helpful error message. To perform the optimization step, we just need to create the `optimizer` and call its `step()` method: ``` trainer.create_optimizer() trainer.optimizer.step() ``` Again, if you‚Äôre using the default optimizer in the `Trainer`, you shouldn‚Äôt get an error at this stage, but if you have a custom optimizer, there might be some problems to debug here. Don‚Äôt forget to go back to the CPU if you get a weird CUDA error at this stage. Speaking of CUDA errors, earlier we mentioned a special case. Let‚Äôs have a look at that now. ### Dealing with CUDA out-of-memory errors Whenever you get an error message that starts with `RuntimeError: CUDA out of memory`, this indicates that you are out of GPU memory. This is not directly linked to your code, and it can happen with a script that runs perfectly fine. This error means that you tried to put too many things in the internal memory of your GPU, and that resulted in an error. Like with other CUDA errors, you will need to restart your kernel to be in a spot where you can run your training again. To solve this issue, you just need to use less GPU space ‚Äî something that is often easier said than done. First, make sure you don‚Äôt have two models on the GPU at the same time (unless that‚Äôs required for your problem, of course). Then, you should probably reduce your batch size, as it directly affects the sizes of all the intermediate outputs of",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 7,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the internal memory of your GPU, and that resulted in an error. Like with other CUDA errors, you will need to restart your kernel to be in a spot where you can run your training again. To solve this issue, you just need to use less GPU space ‚Äî something that is often easier said than done. First, make sure you don‚Äôt have two models on the GPU at the same time (unless that‚Äôs required for your problem, of course). Then, you should probably reduce your batch size, as it directly affects the sizes of all the intermediate outputs of the model and their gradients. If the problem persists, consider using a smaller version of your model. > In the next part of the course, we‚Äôll look at more advanced techniques that can help you reduce your memory footprint and let you fine-tune the biggest models. ### Evaluating the model Now that we‚Äôve solved all the issues with our code, everything is perfect and the training should run smoothly, right? Not so fast! If you run the `trainer.train()` command, everything will look good at first, but after a while you will get the following: ``` # This will take a long time and error out, so you shouldn't run this cell trainer.train() ``` ``` TypeError: only size-1 arrays can be converted to Python scalars ``` You will realize this error appears during the evaluation phase, so this is the last thing we will need to debug. You can run the evaluation loop of the `Trainer` independently form the training like this: ``` trainer.evaluate() ``` ``` TypeError: only size-1 arrays can be converted to Python scalars ``` > üí° You should always make sure you can runtrainer.evaluate()before launchingtrainer.train(), to avoid wasting lots of compute resources before hitting an error. Before attempting to debug a problem in the evaluation loop, you should first make sure that you‚Äôve had a look at the data, are able to form a batch properly, and can run your model on it. We‚Äôve completed all of those steps, so the following code can be executed without error: ``` for batch in trainer.get_eval_dataloader(): break batch = {k: v.to(device) for k, v in batch.items()} with torch.no_grad(): outputs = trainer.model(**batch) ``` The error comes later, at the end of the evaluation phase, and if we look at the traceback we see this: ``` ~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references) 431 \"\"\" 432 batch = {\"predictions\": predictions, \"references\": references} --> 433 batch = self.info.features.encode_batch(batch) 434 if self.writer is None: 435 self._init_writer() ``` This tells us that the error originates in the `datasets/metric.py` module ‚Äî so this is a problem with our `compute_metrics()` function. It takes a tuple with the logits and the labels as NumPy arrays, so let‚Äôs try to feed it that: ``` predictions = outputs.logits.cpu().numpy() labels = batch[\"labels\"].cpu().numpy() compute_metrics((predictions, labels)) ``` ``` TypeError: only size-1 arrays can be converted to Python scalars ``` We get the same error, so the problem definitely lies with that function. If we look back",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 8,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "432 batch = {\"predictions\": predictions, \"references\": references} --> 433 batch = self.info.features.encode_batch(batch) 434 if self.writer is None: 435 self._init_writer() ``` This tells us that the error originates in the `datasets/metric.py` module ‚Äî so this is a problem with our `compute_metrics()` function. It takes a tuple with the logits and the labels as NumPy arrays, so let‚Äôs try to feed it that: ``` predictions = outputs.logits.cpu().numpy() labels = batch[\"labels\"].cpu().numpy() compute_metrics((predictions, labels)) ``` ``` TypeError: only size-1 arrays can be converted to Python scalars ``` We get the same error, so the problem definitely lies with that function. If we look back at its code, we see it‚Äôs just forwarding the `predictions` and the `labels` to `metric.compute()`. So is there a problem with that method? Not really. Let‚Äôs have a quick look at the shapes: ``` predictions.shape, labels.shape ``` ``` ((8, 3), (8,)) ``` Our predictions are still logits, not the actual predictions, which is why the metric is returning this (somewhat obscure) error. The fix is pretty easy; we just have to add an argmax in the `compute_metrics()` function: ``` import numpy as np def compute_metrics(eval_pred): predictions, labels = eval_pred predictions = np.argmax(predictions, axis=1) return metric.compute(predictions=predictions, references=labels) compute_metrics((predictions, labels)) ``` ``` {'accuracy': 0.625} ``` Now our error is fixed! This was the last one, so our script will now train a model properly. For reference, here is the completely fixed script: ``` import numpy as np from datasets import load_dataset import evaluate from transformers import ( AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer, ) raw_datasets = load_dataset(\"glue\", \"mnli\") model_checkpoint = \"distilbert-base-uncased\" tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) def preprocess_function(examples): return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True) tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3) args = TrainingArguments( f\"distilbert-finetuned-mnli\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, ) metric = evaluate.load(\"glue\", \"mnli\") def compute_metrics(eval_pred): predictions, labels = eval_pred predictions = np.argmax(predictions, axis=1) return metric.compute(predictions=predictions, references=labels) data_collator = DataCollatorWithPadding(tokenizer=tokenizer) trainer = Trainer( model, args, train_dataset=tokenized_datasets[\"train\"], eval_dataset=tokenized_datasets[\"validation_matched\"], compute_metrics=compute_metrics, data_collator=data_collator, tokenizer=tokenizer, ) trainer.train() ``` In this instance, there are no more problems, and our script will fine-tune a model that should give reasonable results. But what can we do when the training proceeds without any error, and the model trained does not perform well at all? That‚Äôs the hardest part of machine learning, and we‚Äôll show you a few techniques that can help. > üí° If you‚Äôre using a manual training loop, the same steps apply to debug your training pipeline, but it‚Äôs easier to separate them. Make sure you have not forgotten themodel.eval()ormodel.train()at the right places, or thezero_grad()at each step, however! ## Debugging silent errors during training What can we do to debug a training that completes without error but doesn‚Äôt get good results? We‚Äôll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer. ### Check your data (again!) Your model will only learn something if it‚Äôs actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 9,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "them. Make sure you have not forgotten themodel.eval()ormodel.train()at the right places, or thezero_grad()at each step, however! ## Debugging silent errors during training What can we do to debug a training that completes without error but doesn‚Äôt get good results? We‚Äôll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer. ### Check your data (again!) Your model will only learn something if it‚Äôs actually possible to learn anything from your data. If there is a bug that corrupts the data or the labels are attributed randomly, it‚Äôs very likely you won‚Äôt get any model training on your dataset. So always start by double-checking your decoded inputs and labels, and ask yourself the following questions: - Is the decoded data understandable? - Do you agree with the labels? - Is there one label that‚Äôs more common than the others? - What should the loss/metric be if the model predicted a random answer/always the same answer? > ‚ö†Ô∏è If you are doing distributed training, print samples of your dataset in each process and triple-check that you get the same thing. One common bug is to have some source of randomness in the data creation that makes each process have a different version of the dataset. After looking at your data, go through a few of the model‚Äôs predictions and decode them too. If the model is always predicting the same thing, it might be because your dataset is biased toward one category (for classification problems); techniques like oversampling rare classes might help. If the loss/metric you get on your initial model is very different from the loss/metric you would expect for random predictions, double-check the way your loss or metric is computed, as there is probably a bug there. If you are using several losses that you add at the end, make sure they are of the same scale. When you are sure your data is perfect, you can see if the model is capable of training on it with one simple test. ### Overfit your model on one batch Overfitting is usually something we try to avoid when training, as it means the model is not learning to recognize the general features we want it to but is instead just memorizing the training samples. However, trying to train your model on one batch over and over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high. Doing this once you have defined your `Trainer` is really easy; just grab a batch of training data, then run a small manual training loop only using that batch for something like 20 steps: ``` for batch in trainer.get_train_dataloader(): break batch = {k: v.to(device) for k, v in batch.items()} trainer.create_optimizer() for _ in range(20): outputs = trainer.model(**batch) loss = outputs.loss",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 10,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "over again is a good test to check if the problem as you framed it can be solved by the model you are attempting to train. It will also help you see if your initial learning rate is too high. Doing this once you have defined your `Trainer` is really easy; just grab a batch of training data, then run a small manual training loop only using that batch for something like 20 steps: ``` for batch in trainer.get_train_dataloader(): break batch = {k: v.to(device) for k, v in batch.items()} trainer.create_optimizer() for _ in range(20): outputs = trainer.model(**batch) loss = outputs.loss loss.backward() trainer.optimizer.step() trainer.optimizer.zero_grad() ``` > üí° If your training data is unbalanced, make sure to build a batch of training data containing all the labels. The resulting model should have close-to-perfect results on the same `batch`. Let‚Äôs compute the metric on the resulting predictions: ``` with torch.no_grad(): outputs = trainer.model(**batch) preds = outputs.logits labels = batch[\"labels\"] compute_metrics((preds.cpu().numpy(), labels.cpu().numpy())) ``` ``` {'accuracy': 1.0} ``` 100% accuracy, now this is a nice example of overfitting (meaning that if you try your model on any other sentence, it will very likely give you a wrong answer)! If you don‚Äôt manage to have your model obtain perfect results like this, it means there is something wrong with the way you framed the problem or your data, so you should fix that. Only when you manage to pass the overfitting test can you be sure that your model can actually learn something. > ‚ö†Ô∏è You will have to recreate your model and yourTrainerafter this test, as the model obtained probably won‚Äôt be able to recover and learn something useful on your full dataset. ### Don‚Äôt tune anything until you have a first baseline Hyperparameter tuning is always emphasized as being the hardest part of machine learning, but it‚Äôs just the last step to help you gain a little bit on the metric. Most of the time, the default hyperparameters of the `Trainer` will work just fine to give you good results, so don‚Äôt launch into a time-consuming and costly hyperparameter search until you have something that beats the baseline you have on your dataset. Once you have a good enough model, you can start tweaking a bit. Don‚Äôt try launching a thousand runs with different hyperparameters, but compare a couple of runs with different values for one hyperparameter to get an idea of which has the greatest impact. If you are tweaking the model itself, keep it simple and don‚Äôt try anything you can‚Äôt reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn‚Äôt had any unintended consequences. ### Ask for help Hopefully you will have found some advice in this section that helped you solve your issue, but if that‚Äôs not the case, remember you can always ask the community on the [forums](https://discuss.huggingface.co/). Here are some additional resources that may prove helpful: - [‚ÄúReproducibility as a vehicle for engineering best practices‚Äù](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) by Joel Grus - [‚ÄúChecklist",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 11,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the greatest impact. If you are tweaking the model itself, keep it simple and don‚Äôt try anything you can‚Äôt reasonably justify. Always make sure you go back to the overfitting test to verify that your change hasn‚Äôt had any unintended consequences. ### Ask for help Hopefully you will have found some advice in this section that helped you solve your issue, but if that‚Äôs not the case, remember you can always ask the community on the [forums](https://discuss.huggingface.co/). Here are some additional resources that may prove helpful: - [‚ÄúReproducibility as a vehicle for engineering best practices‚Äù](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) by Joel Grus - [‚ÄúChecklist for debugging neural networks‚Äù](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) by Cecelia Shao - [‚ÄúHow to unit test machine learning code‚Äù](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) by Chase Roberts - [‚ÄúA Recipe for Training Neural Networks‚Äù](http://karpathy.github.io/2019/04/25/recipe/) by Andrej Karpathy Of course, not every problem you encounter when training neural nets is your own fault! If you encounter something in the ü§ó Transformers or ü§ó Datasets library that does not seem right, you may have encountered a bug. You should definitely tell us all about it, and in the next section we‚Äôll explain exactly how to do that.",
    "metadata": {
      "title": "Debugging the training pipeline",
      "url": "https://huggingface.co/learn/llm-course/chapter8/4",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/4",
      "part": 12,
      "total_parts": 12,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# How to write a good issue When you encounter something that doesn‚Äôt seem right with one of the Hugging Face libraries, you should definitely let us know so we can fix it (the same goes for any open source library, for that matter). If you are not completely certain whether the bug lies in your own code or one of our libraries, the first place to check is the [forums](https://discuss.huggingface.co/). The community will help you figure this out, and the Hugging Face team also closely watches the discussions there. When you are sure you have a bug in your hand, the first step is to build a minimal reproducible example. ## Creating a minimal reproducible example It‚Äôs very important to isolate the piece of code that produces the bug, as no one in the Hugging Face team is a magician (yet), and they can‚Äôt fix what they can‚Äôt see. A minimal reproducible example should, as the name indicates, be reproducible. This means that it should not rely on any external files or data you may have. Try to replace the data you are using with some dummy values that look like your real ones and still produce the same error. > üö® Many issues in the ü§ó Transformers repository are unsolved because the data used to reproduce them is not accessible. Once you have something that is self-contained, you can try to reduce it into even less lines of code, building what we call a *minimal reproducible example*. While this requires a bit more work on your side, you will almost be guaranteed to get help and a fix if you provide a nice, short bug reproducer. If you feel comfortable enough, go inspect the source code where your bug happens. You might find a solution to your problem (in which case you can even suggest a pull request to fix it), but more generally, this can help the maintainers better understand the source when they read your report. ## Filling out the issue template When you file your issue, you will notice there is a template to fill out. We will follow the one for [ü§ó Transformers issues](https://github.com/huggingface/transformers/issues/new/choose) here, but the same kind of information will be required if you report an issue in another repository. Don‚Äôt leave the template blank: taking the time to fill it in will maximize your chances of getting an answer and solving your problem. In general, when filing an issue, always stay courteous. This is an open source project, so you are using free software, and no one has any obligation to help you. You may include what you feel is justified criticism in your issue, but then the maintainers may very well take it badly and not be in a rush help you. Make sure you read the [code of conduct](https://github.com/huggingface/transformers/blob/master/CODE_OF_CONDUCT.md) of the project. ### Including your environment information ü§ó Transformers provides a utility to get all the information we need about your environment. Just type the following in your terminal:",
    "metadata": {
      "title": "How to write a good issue",
      "url": "https://huggingface.co/learn/llm-course/chapter8/5",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/5",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "your problem. In general, when filing an issue, always stay courteous. This is an open source project, so you are using free software, and no one has any obligation to help you. You may include what you feel is justified criticism in your issue, but then the maintainers may very well take it badly and not be in a rush help you. Make sure you read the [code of conduct](https://github.com/huggingface/transformers/blob/master/CODE_OF_CONDUCT.md) of the project. ### Including your environment information ü§ó Transformers provides a utility to get all the information we need about your environment. Just type the following in your terminal: ``` transformers-cli env ``` and you should get something like this: ``` Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points. - `transformers` version: 4.12.0.dev0 - Platform: Linux-5.10.61-1-MANJARO-x86_64-with-arch-Manjaro-Linux - Python version: 3.7.9 - PyTorch version (GPU?): 1.8.1+cu111 (True) - Tensorflow version (GPU?): 2.5.0 (True) - Flax version (CPU?/GPU?/TPU?): 0.3.4 (cpu) - Jax version: 0.2.13 - JaxLib version: 0.1.65 - Using GPU in script?: <fill in> - Using distributed or parallel set-up in script?: <fill in> ``` You can also add a `!` at the beginning of the `transformers-cli env` command to execute it from a notebook cell, and then copy and paste the result at the beginning of your issue. ### Tagging people Tagging people by typing an `@` followed by their GitHub handle will send them a notification so they will see your issue and might reply quicker. Use this with moderation, because the people you tag might not appreciate being notified if it‚Äôs something they have no direct link to. If you have looked at the source files related to your bug, you should tag the last person that made changes at the line you think is responsible for your problem (you can find this information by looking at said line on GitHub, selecting it, then clicking ‚ÄúView git blame‚Äù). Otherwise, the template offers suggestions of people to tag. In general, never tag more than three people! ### Including a reproducible example If you have managed to create a self-contained example that produces the bug, now is the time to include it! Type a line with three backticks followed by `python`, like this: ``` ```python ``` then paste in your minimal reproducible example and type a new line with three backticks. This will ensure your code is properly formatted. If you didn‚Äôt manage to create a reproducible example, explain in clear steps how you got to your issue. Include a link to a Google Colab notebook where you got the error if you can. The more information you share, the better able the maintainers will be to reply to you. In all cases, you should copy and paste the whole error message you are getting. If you‚Äôre working in Colab, remember that some of the frames may be automatically collapsed in the stack trace, so make sure you expand them before copying. Like with the code sample, put that error message between",
    "metadata": {
      "title": "How to write a good issue",
      "url": "https://huggingface.co/learn/llm-course/chapter8/5",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/5",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "manage to create a reproducible example, explain in clear steps how you got to your issue. Include a link to a Google Colab notebook where you got the error if you can. The more information you share, the better able the maintainers will be to reply to you. In all cases, you should copy and paste the whole error message you are getting. If you‚Äôre working in Colab, remember that some of the frames may be automatically collapsed in the stack trace, so make sure you expand them before copying. Like with the code sample, put that error message between two lines with three backticks, so it‚Äôs properly formatted. ### Describing the expected behavior Explain in a few lines what you expected to get, so that the maintainers get a full grasp of the problem. This part is generally pretty obvious, so it should fit in one sentence, but in some cases you may have a lot to say. ## And then what? Once your issue is filed, make sure to quickly check everything looks okay. You can edit the issue if you made a mistake, or even change its title if you realize the problem is different from what you initially thought. There is no point pinging people if you don‚Äôt get an answer. If no one helps you in a few days, it‚Äôs likely that no one could make sense of your problem. Don‚Äôt hesitate to go back to the reproducible example. Can you make it shorter and more to the point? If you don‚Äôt get an answer in a week, you can leave a message gently asking for help, especially if you‚Äôve edited your issue to include more information on the problem.",
    "metadata": {
      "title": "How to write a good issue",
      "url": "https://huggingface.co/learn/llm-course/chapter8/5",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/5",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Part 2 completed!\n\n   \nCongratulations, you‚Äôve made it through the second part of the course! We‚Äôre actively working on the third one, so subscribe to our [newsletter](https://huggingface.curated.co/) to make sure you don‚Äôt miss its release.\n \nYou should now be able to tackle a range of NLP tasks, and fine-tune or pretrain a model on them. Don‚Äôt forget to share your results with the community on the [Model Hub](https://huggingface.co/models).\n \nWe can‚Äôt wait to see what you will build with the knowledge that you‚Äôve gained!",
    "metadata": {
      "title": "Part 2 completed!",
      "url": "https://huggingface.co/learn/llm-course/chapter8/6",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/6",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter quiz\n\n   \nLet‚Äôs test what you learned in this chapter!\n \n\n### 1. In which order should you read a Python traceback?\n\n  From top to bottom  From bottom to top   \n\n### 2. What is a minimal reproducible example?\n\n  A simple implementation of a Transformer architecture from a research article  A compact and self-contained block of code that can be run without any external dependencies on private files or data  A screenshot of the Python traceback  A notebook that contains your whole analysis, including parts unrelated to the error   \n\n### 3. Suppose you try to run the following code, which throws an error:\n\n  \n```\nfrom transformers import GPT3ForSequenceClassification\n\n# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)\n# ---------------------------------------------------------------------------\n# ImportError                               Traceback (most recent call last)\n# /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_30848/333858878.py in <module>\n# ----> 1 from transformers import GPT3ForSequenceClassification\n\n# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)\n```\n \nWhich of the following might be a good choice for the title of a forum topic to ask for help?\n  `ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)`  Problem with `from transformers import GPT3ForSequenceClassification`  Why can't I import `GPT3ForSequenceClassification`?  Is GPT-3 supported in ü§ó Transformers?   \n\n### 4. Suppose you‚Äôve tried to run trainer.train() and are faced with a cryptic error that doesn‚Äôt tell you exactly where the error is coming from. Which of the following is the first place you should look for errors in your training pipeline?\n\n  The optimization step where we compute gradients and perform backpropagation  The evaluation step where we compute metrics  The datasets  The dataloaders   \n\n### 5. What is the best way to debug a CUDA error?\n\n  Post the error message on the forums or GitHub.  Execute the same code on the CPU.  Read the traceback to find out what caused the error.  Reduce the batch size.  Restart the Jupyter kernel.   \n\n### 6. What is the best way to get an issue on GitHub fixed?\n\n  Post a full reproducible example of the bug.  Ask every day for an update.  Inspect the source code around the bug and try to find the reason why it happens. Post the results in the issue.   \n\n### 7. Why is overfitting to one batch usually a good debugging technique?\n\n  It isn't; overfitting is always bad and should be avoided.  It allows us to verify that the model is able to reduce the loss to zero.  It allows us to verify that the tensor shapes of our inputs and labels are correct.   \n\n### 8. Why is it a good idea to include details on your compute environment with transformers-cli env when creating a new issue in the ü§ó Transformers repo?\n\n  It allows the maintainers to understand which version of the library you're using.  It allows the maintainers to know whether you're running code on Windows, macOS, or Linux.  It allows the maintainers to know whether you're running code on a GPU or CPU.",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter8/7",
      "course": "llm-course",
      "chapter": "8. How to ask for help",
      "chapter_id": "chapter8/7",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter8/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Gradio\n\n   \nIn this chapter we will be learning about how to build **interactive demos** for your machine learning models.\n \nWhy build a demo or a GUI for your machine learning model in the first place? Demos allow:\n \n- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers\n- **Researchers** to more easily reproduce machine learning models and behavior\n- **Quality testers** or **end users** to more easily identify and debug failure points of models\n- **Diverse users** to discover algorithmic biases in models\n \nWe‚Äôll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.\n \nHere are some examples of machine learning demos built with Gradio:\n \n- A **sketch recognition** model that takes in a sketch and outputs labels of what it thinks is being drawn:\n  \n- An extractive **question answering** model that takes in a context paragraph and a quest and outputs a response and a probability score (we discussed this kind of model [in Chapter 7](/course/chapter7/7)):\n  \n- A **background removal** model that takes in an image and outputs the image with the background removed:\n  \nThis chapter is broken down into sections which include both *concepts* and *applications*. After you learn the concept in each section, you‚Äôll apply it to build a particular kind of demo, ranging from image classification to speech recognition. By the time you finish this chapter, you‚Äôll be able to build these demos (and many more!) in just a few lines of Python code.\n \n> üëÄ Check outHugging Face Spacesto see many recent examples of machine learning demos built by the machine learning community!",
    "metadata": {
      "title": "Introduction to Gradio",
      "url": "https://huggingface.co/learn/llm-course/chapter9/1",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Building your first demo Let‚Äôs start by installing Gradio! Since it is a Python package, simply run: `$ pip install gradio` You can run Gradio anywhere, be it from your favourite Python IDE, to Jupyter notebooks or even in Google Colab ü§Ø! So install Gradio wherever you run Python! Let‚Äôs get started with a simple ‚ÄúHello World‚Äù example to get familiar with the Gradio syntax: ``` import gradio as gr def greet(name): return \"Hello \" + name demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\") demo.launch() ``` Let‚Äôs walk through the code above: - First, we define a function called `greet()`. In this case, it is a simple function that adds ‚ÄúHello‚Äù before your name, but it can be *any* Python function in general. For example, in machine learning applications, this function would *call a model to make a prediction* on an input and return the output. - Then, we create a Gradio `Interface` with three arguments, `fn`, `inputs`, and `outputs`. These arguments define the prediction function, as well as the *type* of input and output components we would like. In our case, both components are simple text boxes. - We then call the `launch()` method on the `Interface` that we created. If you run this code, the interface below will appear automatically within a Jupyter/Colab notebook, or pop in a browser on **http://localhost:7860** if running from a script. Try using this GUI right now with your own name or some other input! You‚Äôll notice that in this GUI, Gradio automatically inferred the name of the input parameter (`name`) and applied it as a label on top of the textbox. What if you‚Äôd like to change that? Or if you‚Äôd like to customize the textbox in some other way? In that case, you can instantiate a class object representing the input component. Take a look at the example below: ``` import gradio as gr def greet(name): return \"Hello \" + name # We instantiate the Textbox class textbox = gr.Textbox(label=\"Type your name here:\", placeholder=\"John Doe\", lines=2) gr.Interface(fn=greet, inputs=textbox, outputs=\"text\").launch() ``` Here, we‚Äôve created an input textbox with a label, a placeholder, and a set number of lines. You could do the same for the output textbox, but we‚Äôll leave that for now. We‚Äôve seen that with just a few lines of code, Gradio lets you create a simple interface around any function with any kind of inputs or outputs. In this section, we‚Äôve started with a simple textbox, but in the next sections, we‚Äôll cover other kinds of inputs and outputs. Let‚Äôs now take a look at including some NLP in a Gradio application. ## ü§ñ Including model predictions Let‚Äôs now build a simple interface that allows you to demo a **text-generation** model like GPT-2. We‚Äôll load our model using the `pipeline()` function from ü§ó Transformers. If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3#text-generation). First, we define a prediction function that takes in a text prompt and returns the text completion: ``` from transformers",
    "metadata": {
      "title": "Building your first demo",
      "url": "https://huggingface.co/learn/llm-course/chapter9/2",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/2",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "we‚Äôve started with a simple textbox, but in the next sections, we‚Äôll cover other kinds of inputs and outputs. Let‚Äôs now take a look at including some NLP in a Gradio application. ## ü§ñ Including model predictions Let‚Äôs now build a simple interface that allows you to demo a **text-generation** model like GPT-2. We‚Äôll load our model using the `pipeline()` function from ü§ó Transformers. If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3#text-generation). First, we define a prediction function that takes in a text prompt and returns the text completion: ``` from transformers import pipeline model = pipeline(\"text-generation\") def predict(prompt): completion = model(prompt)[0][\"generated_text\"] return completion ``` This function completes prompts that you provide, and you can run it with your own input prompts to see how it works. Here is an example (you might get a different completion): ``` predict(\"My favorite programming language is\") ``` ``` >> My favorite programming language is Haskell. I really enjoyed the Haskell language, but it doesn't have all the features that can be applied to any other language. For example, all it does is compile to a byte array. ``` Now that we have a function for generating predictions, we can create and launch an `Interface` in the same way we did earlier: ``` import gradio as gr gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch() ``` That‚Äôs it! You can now use this interface to generate text using the GPT-2 model as shown below ü§Ø. Keep reading to see how to build other kinds of demos with Gradio!",
    "metadata": {
      "title": "Building your first demo",
      "url": "https://huggingface.co/learn/llm-course/chapter9/2",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/2",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Understanding the Interface class In this section, we will take a closer look at the `Interface` class, and understand the main parameters used to create one. ## How to create an Interface You‚Äôll notice that the `Interface` class has 3 required parameters: `Interface(fn, inputs, outputs, ...)` These parameters are: - `fn`: the prediction function that is wrapped by the Gradio interface. This function can take one or more parameters and return one or more values - `inputs`: the input component type(s). Gradio provides many pre-built components such as`\"image\"` or `\"mic\"`. - `outputs`: the output component type(s). Again, Gradio provides many pre-built components e.g. `\"image\"` or `\"label\"`. For a complete list of components, [see the Gradio docs](https://gradio.app/docs). Each pre-built component can be customized by instantiating the class corresponding to the component. For example, as we saw in the [previous section](/course/chapter9/2), instead of passing in `\"textbox\"` to the `inputs` parameter, you can pass in a `Textbox(lines=7, label=\"Prompt\")` component to create a textbox with 7 lines and a label. Let‚Äôs take a look at another example, this time with an `Audio` component. ## A simple example with audio As mentioned earlier, Gradio provides many different inputs and outputs. So let‚Äôs build an `Interface` that works with audio. In this example, we‚Äôll build an audio-to-audio function that takes an audio file and simply reverses it. We will use for the input the `Audio` component. When using the `Audio` component, you can specify whether you want the `source` of the audio to be a file that the user uploads or a microphone that the user records their voice with. In this case, let‚Äôs set it to a `\"microphone\"`. Just for fun, we‚Äôll add a label to our `Audio` that says ‚ÄúSpeak here‚Ä¶‚Äú. In addition, we‚Äôd like to receive the audio as a numpy array so that we can easily ‚Äúreverse‚Äù it. So we‚Äôll set the `\"type\"` to be `\"numpy\"`, which passes the input data as a tuple of (`sample_rate`, `data`) into our function. We will also use the `Audio` output component which can automatically render a tuple with a sample rate and numpy array of data as a playable audio file. In this case, we do not need to do any customization, so we will use the string shortcut `\"audio\"`. ``` import numpy as np import gradio as gr def reverse_audio(audio): sr, data = audio reversed_audio = (sr, np.flipud(data)) return reversed_audio mic = gr.Audio(source=\"microphone\", type=\"numpy\", label=\"Speak here...\") gr.Interface(reverse_audio, mic, \"audio\").launch() ``` The code above will produce an interface like the one below (if your browser doesn‚Äôt ask you for microphone permissions, [open the demo in a separate tab](https://huggingface.co/spaces/course-demos/audio-reverse).) You should now be able to record your voice and hear yourself speaking in reverse - spooky üëª! ## Handling multiple inputs and outputs Let‚Äôs say we had a more complicated function, with multiple inputs and outputs. In the example below, we have a function that takes a dropdown index, a slider value, and number, and returns an audio sample of a musical tone. Take a",
    "metadata": {
      "title": "Understanding the Interface class",
      "url": "https://huggingface.co/learn/llm-course/chapter9/3",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/3",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "type=\"numpy\", label=\"Speak here...\") gr.Interface(reverse_audio, mic, \"audio\").launch() ``` The code above will produce an interface like the one below (if your browser doesn‚Äôt ask you for microphone permissions, [open the demo in a separate tab](https://huggingface.co/spaces/course-demos/audio-reverse).) You should now be able to record your voice and hear yourself speaking in reverse - spooky üëª! ## Handling multiple inputs and outputs Let‚Äôs say we had a more complicated function, with multiple inputs and outputs. In the example below, we have a function that takes a dropdown index, a slider value, and number, and returns an audio sample of a musical tone. Take a look how we pass a list of input and output components, and see if you can follow along what‚Äôs happening. The key here is that when you pass: - a list of input components, each component corresponds to a parameter in order. - a list of output coponents, each component corresponds to a returned value. The code snippet below shows how three input components line up with the three arguments of the `generate_tone()` function: ``` import numpy as np import gradio as gr notes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"] def generate_tone(note, octave, duration): sr = 48000 a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9) frequency = a4_freq * 2 ** (tones_from_a4 / 12) duration = int(duration) audio = np.linspace(0, duration, duration * sr) audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16) return (sr, audio) gr.Interface( generate_tone, [ gr.Dropdown(notes, type=\"index\"), gr.Slider(minimum=4, maximum=6, step=1), gr.Number(value=1, label=\"Duration in seconds\"), ], \"audio\", ).launch() ``` ### The launch() method So far, we have used the `launch()` method to launch the interface, but we haven‚Äôt really discussed what it does. By default, the `launch()` method will launch the demo in a web server that is running locally. If you are running your code in a Jupyter or Colab notebook, then Gradio will embed the demo GUI in the notebook so you can easily use it. You can customize the behavior of `launch()` through different parameters: - `inline` - whether to display the interface inline on Python notebooks. - `inbrowser` - whether to automatically launch the interface in a new tab on the default browser. - `share` - whether to create a publicly shareable link from your computer for the interface. Kind of like a Google Drive link! We‚Äôll cover the `share` parameter in a lot more detail in the next section! ## ‚úèÔ∏è Let‚Äôs apply it! Let‚Äôs build an interface that allows you to demo a **speech-recognition** model. To make it interesting, we will accept *either* a mic input or an uploaded file. As usual, we‚Äôll load our speech recognition model using the `pipeline()` function from ü§ó Transformers. If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3). Next, we‚Äôll implement a `transcribe_audio()` function that processes the audio and returns the transcription. Finally, we‚Äôll wrap this function in an `Interface` with the `Audio` components",
    "metadata": {
      "title": "Understanding the Interface class",
      "url": "https://huggingface.co/learn/llm-course/chapter9/3",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/3",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "`share` parameter in a lot more detail in the next section! ## ‚úèÔ∏è Let‚Äôs apply it! Let‚Äôs build an interface that allows you to demo a **speech-recognition** model. To make it interesting, we will accept *either* a mic input or an uploaded file. As usual, we‚Äôll load our speech recognition model using the `pipeline()` function from ü§ó Transformers. If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3). Next, we‚Äôll implement a `transcribe_audio()` function that processes the audio and returns the transcription. Finally, we‚Äôll wrap this function in an `Interface` with the `Audio` components for the inputs and just text for the output. Altogether, the code for this application is the following: ``` from transformers import pipeline import gradio as gr model = pipeline(\"automatic-speech-recognition\") def transcribe_audio(audio): transcription = model(audio)[\"text\"] return transcription gr.Interface( fn=transcribe_audio, inputs=gr.Audio(type=\"filepath\"), outputs=\"text\", ).launch() ``` If your browser doesn‚Äôt ask you for microphone permissions, [open the demo in a separate tab](https://huggingface.co/spaces/course-demos/audio-reverse). That‚Äôs it! You can now use this interface to transcribe audio. Notice here that by passing in the `optional` parameter as `True`, we allow the user to either provide a microphone or an audio file (or neither, but that will return an error message). Keep going to see how to share your interface with others!",
    "metadata": {
      "title": "Understanding the Interface class",
      "url": "https://huggingface.co/learn/llm-course/chapter9/3",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/3",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Sharing demos with others Now that you‚Äôve built a demo, you‚Äôll probably want to share it with others. Gradio demos can be shared in two ways: using a **temporary share link** or **permanent hosting on Spaces**. We‚Äôll cover both of these approaches shortly. But before you share your demo, you may want to polish it up üíÖ. ### Polishing your Gradio demo: ![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview.png) ![Overview of a gradio interface](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter9/gradio-demo-overview-dark.png) To add additional content to your demo, the `Interface` class supports some optional parameters: - `title`: you can give a title to your demo, which appears *above* the input and output components. - `description`: you can give a description (in text, Markdown, or HTML) for the interface, which appears above the input and output components and below the title. - `article`: you can also write an expanded article (in text, Markdown, or HTML) explaining the interface. If provided, it appears *below* the input and output components. - `theme`: don‚Äôt like the default colors? Set the theme to use one of `default`, `huggingface`, `grass`, `peach`. You can also add the `dark-` prefix, e.g. `dark-peach` for dark theme (or just `dark` for the default dark theme). - `examples`: to make your demo *way easier to use*, you can provide some example inputs for the function. These appear below the UI components and can be used to populate the interface. These should be provided as a nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. - `live`: if you want to make your demo ‚Äúlive‚Äù, meaning that your model reruns every time the input changes, you can set `live=True`. This makes sense to use with quick models (we‚Äôll see an example at the end of this section) Using the options above, we end up with a more complete interface. Run the code below so you can chat with Rick and Morty: ``` title = \"Ask Rick a Question\" description = \"\"\" The bot was trained to answer questions based on Rick and Morty dialogues. Ask Rick anything! <img src=\"https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/resolve/main/rick.png\" width=200px> \"\"\" article = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick_and_Morty_Bot) that this demo is based off of.\" gr.Interface( fn=predict, inputs=\"textbox\", outputs=\"text\", title=title, description=description, article=article, examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]], ).launch() ``` Using the options above, we end up with a more complete interface. Try the interface below: ### Sharing your demo with temporary links Now that we have a working demo of our machine learning model, let‚Äôs learn how to easily share a link to our interface. Interfaces can be easily shared publicly by setting `share=True` in the `launch()` method: ``` gr.Interface(classify_image, \"image\", \"label\").launch(share=True) ``` This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser for up to 72 hours. Because the processing happens on your device (as long",
    "metadata": {
      "title": "Sharing demos with others",
      "url": "https://huggingface.co/learn/llm-course/chapter9/4",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/4",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "more complete interface. Try the interface below: ### Sharing your demo with temporary links Now that we have a working demo of our machine learning model, let‚Äôs learn how to easily share a link to our interface. Interfaces can be easily shared publicly by setting `share=True` in the `launch()` method: ``` gr.Interface(classify_image, \"image\", \"label\").launch(share=True) ``` This generates a public, shareable link that you can send to anybody! When you send this link, the user on the other side can try out the model in their browser for up to 72 hours. Because the processing happens on your device (as long as your device stays on!), you don‚Äôt have to worry about packaging any dependencies. If you‚Äôre working out of a Google Colab notebook, a share link is always automatically created. It usually looks something like this: **XXXXX.gradio.app**. Although the link is served through a Gradio link, we are only a proxy for your local server, and do not store any data sent through the interfaces. Keep in mind, however, that these links are publicly accessible, meaning that anyone can use your model for prediction! Therefore, make sure not to expose any sensitive information through the functions you write, or allow any critical changes to occur on your device. If you set `share=False` (the default), only a local link is created. ### Hosting your demo on Hugging Face Spaces A share link that you can pass around to collegues is cool, but how can you permanently host your demo and have it exist in its own ‚Äúspace‚Äù on the internet? Hugging Face Spaces provides the infrastructure to permanently host your Gradio model on the internet, **for free**! Spaces allows you to create and push to a (public or private) repo, where your Gradio interface code will exist in an `app.py` file. [Read a step-by-step tutorial](https://huggingface.co/blog/gradio-spaces) to get started, or watch an example video below. ## ‚úèÔ∏è Let‚Äôs apply it! Using what we just learned in the sections so far, let‚Äôs create the sketch recognition demo we saw in [section one of this chapter](/course/chapter9/1). Let‚Äôs add some customization to our interface and set `share=True` to create a public link we can pass around. We can load the labels from [class_names.txt](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/class_names.txt) and load the pre-trained pytorch model from [pytorch_model.bin](https://huggingface.co/spaces/dawood/Sketch-Recognition/blob/main/pytorch_model.bin). Download these files by following the link and clicking download on the top left corner of the file preview. Let‚Äôs take a look at the code below to see how we use these files to load our model and create a `predict()` function: ``` from pathlib import Path import torch import gradio as gr from torch import nn LABELS = Path(\"class_names.txt\").read_text().splitlines() model = nn.Sequential( nn.Conv2d(1, 32, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(1152, 256), nn.ReLU(), nn.Linear(256, len(LABELS)), ) state_dict = torch.load(\"pytorch_model.bin\", map_location=\"cpu\") model.load_state_dict(state_dict, strict=False) model.eval() def predict(im): x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0 with torch.no_grad(): out = model(x) probabilities = torch.nn.functional.softmax(out[0], dim=0) values, indices = torch.topk(probabilities, 5) return {LABELS[i]: v.item() for i, v",
    "metadata": {
      "title": "Sharing demos with others",
      "url": "https://huggingface.co/learn/llm-course/chapter9/4",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/4",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "the code below to see how we use these files to load our model and create a `predict()` function: ``` from pathlib import Path import torch import gradio as gr from torch import nn LABELS = Path(\"class_names.txt\").read_text().splitlines() model = nn.Sequential( nn.Conv2d(1, 32, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(32, 64, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Conv2d(64, 128, 3, padding=\"same\"), nn.ReLU(), nn.MaxPool2d(2), nn.Flatten(), nn.Linear(1152, 256), nn.ReLU(), nn.Linear(256, len(LABELS)), ) state_dict = torch.load(\"pytorch_model.bin\", map_location=\"cpu\") model.load_state_dict(state_dict, strict=False) model.eval() def predict(im): x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0 with torch.no_grad(): out = model(x) probabilities = torch.nn.functional.softmax(out[0], dim=0) values, indices = torch.topk(probabilities, 5) return {LABELS[i]: v.item() for i, v in zip(indices, values)} ``` Now that we have a `predict()` function. The next step is to define and launch our gradio interface: ``` interface = gr.Interface( predict, inputs=\"sketchpad\", outputs=\"label\", theme=\"huggingface\", title=\"Sketch Recognition\", description=\"Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!\", article=\"<p style='text-align: center'>Sketch Recognition | Demo Model</p>\", live=True, ) interface.launch(share=True) ``` Notice the `live=True` parameter in `Interface`, which means that the sketch demo makes a prediction every time someone draws on the sketchpad (no submit button!). Furthermore, we also set the `share=True` argument in the `launch()` method. This will create a public link that you can send to anyone! When you send this link, the user on the other side can try out the sketch recognition model. To reiterate, you could also host the model on Hugging Face Spaces, which is how we are able to embed the demo above. Next up, we‚Äôll cover other ways that Gradio can be used with the Hugging Face ecosystem!",
    "metadata": {
      "title": "Sharing demos with others",
      "url": "https://huggingface.co/learn/llm-course/chapter9/4",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/4",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/4.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Integrations with the Hugging Face Hub\n\n    \nTo make your life even easier, Gradio integrates directly with Hugging Face Hub and Hugging Face Spaces.\nYou can load demos from the Hub and Spaces with only *one line of code*.\n \n\n### Loading models from the Hugging Face Hub\n\n \nTo start with, choose one of the thousands of models Hugging Face offers through the Hub, as described in [Chapter 4](/course/chapter4/2).\n \nUsing the special `Interface.load()` method, you pass `\"model/\"` (or, equivalently, `\"huggingface/\"`)\nfollowed by the model name.\nFor example, here is the code to build a demo for [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B), a large language model, add a couple of example inputs:\n  \n```\nimport gradio as gr\n\ntitle = \"GPT-J-6B\"\ndescription = \"Gradio Demo for GPT-J 6B, a transformer model trained using Ben Wang's Mesh Transformer JAX. 'GPT-J' refers to the class of model, while '6B' represents the number of trainable parameters. To use it, simply add your text, or click one of the examples to load them. Read more at the links below.\"\narticle = \"<p style='text-align: center'><a href='https://github.com/kingoflolz/mesh-transformer-jax' target='_blank'>GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</a></p>\"\n\ngr.Interface.load(\n    \"huggingface/EleutherAI/gpt-j-6B\",\n    inputs=gr.Textbox(lines=5, label=\"Input Text\"),\n    title=title,\n    description=description,\n    article=article,\n).launch()\n```\n \nThe code above will produce the interface below:\n  \nLoading a model in this way uses Hugging Face‚Äôs [Inference API](https://huggingface.co/inference-api),\ninstead of loading the model in memory. This is ideal for huge models like GPT-J or T0pp which\nrequire lots of RAM.\n \n\n### Loading from Hugging Face Spaces\n\n \nTo load any Space from the Hugging Face Hub and recreate it locally, you can pass `spaces/` to the `Interface`, followed by the name of the Space.\n \nRemember the demo from section 1 that removes the background of an image? Let‚Äôs load it from Hugging Face Spaces:\n  \n```\ngr.Interface.load(\"spaces/abidlabs/remove-bg\").launch()\n```\n  \nOne of the cool things about loading demos from the Hub or Spaces is that you customize them\nby overriding any of the\nparameters. Here, we add a title and get it to work with a webcam instead:\n  \n```\ngr.Interface.load(\n    \"spaces/abidlabs/remove-bg\", inputs=\"webcam\", title=\"Remove your webcam background!\"\n).launch()\n```\n  \nNow that we‚Äôve explored a few ways to integrate Gradio with the Hugging Face Hub, let‚Äôs take a look at some advanced features of the `Interface` class. That‚Äôs the topic of the next section!",
    "metadata": {
      "title": "Integrations with the Hugging Face Hub",
      "url": "https://huggingface.co/learn/llm-course/chapter9/5",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/5",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/5.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Advanced Interface features Now that we can build and share a basic interface, let‚Äôs explore some more advanced features such as state, and interpretation. ### Using state to persist data Gradio supports *session state*, where data persists across multiple submits within a page load. Session state is useful for building demos of, for example, chatbots where you want to persist data as the user interacts with the model. Note that session state does not share data between different users of your model. To store data in a session state, you need to do three things: 1. Pass in an *extra parameter* into your function, which represents the state of the interface. 2. At the end of the function, return the updated value of the state as an *extra return value*. 3. Add the ‚Äòstate‚Äô input and ‚Äòstate‚Äô output components when creating your `Interface`. See the chatbot example below: ``` import random import gradio as gr def chat(message, history): history = history or [] if message.startswith(\"How many\"): response = random.randint(1, 10) elif message.startswith(\"How\"): response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"]) elif message.startswith(\"Where\"): response = random.choice([\"Here\", \"There\", \"Somewhere\"]) else: response = \"I don't know\" history.append((message, response)) return history, history iface = gr.Interface( chat, [\"text\", \"state\"], [\"chatbot\", \"state\"], allow_screenshot=False, allow_flagging=\"never\", ) iface.launch() ``` Notice how the state of the output component persists across submits. Note: you can pass in a default value to the state parameter, which is used as the initial value of the state. ### Using interpretation to understand predictions Most machine learning models are black boxes and the internal logic of the function is hidden from the end user. To encourage transparency, we‚Äôve made it very easy to add interpretation to your model by simply setting the interpretation keyword in the Interface class to default. This allows your users to understand what parts of the input are responsible for the output. Take a look at the simple interface below which shows an image classifier that also includes interpretation: ``` import requests import tensorflow as tf import gradio as gr inception_net = tf.keras.applications.MobileNetV2() # load the model # Download human-readable labels for ImageNet. response = requests.get(\"https://git.io/JJkYN\") labels = response.text.split(\"\\n\") def classify_image(inp): inp = inp.reshape((-1, 224, 224, 3)) inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp) prediction = inception_net.predict(inp).flatten() return {labels[i]: float(prediction[i]) for i in range(1000)} image = gr.Image(shape=(224, 224)) label = gr.Label(num_top_classes=3) title = \"Gradio Image Classifiction + Interpretation Example\" gr.Interface( fn=classify_image, inputs=image, outputs=label, interpretation=\"default\", title=title ).launch() ``` Test the interpretation function by submitting an input then clicking Interpret under the output component. Besides the default interpretation method Gradio provides, you can also specify `shap` for the `interpretation` parameter and set the `num_shap` parameter. This uses Shapley-based interpretation, which you can read more about [here](https://christophm.github.io/interpretable-ml-book/shap.html). Lastly, you can also pass in your own interpretation function into the `interpretation` parameter. See an example in Gradio‚Äôs getting started page [here](https://gradio.app/getting_started/). This wraps up our deep dive into the `Interface` class of Gradio. As we‚Äôve seen, this class makes it simple to create machine learning demos in a few",
    "metadata": {
      "title": "Advanced Interface features",
      "url": "https://huggingface.co/learn/llm-course/chapter9/6",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/6",
      "part": 1,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": ").launch() ``` Test the interpretation function by submitting an input then clicking Interpret under the output component. Besides the default interpretation method Gradio provides, you can also specify `shap` for the `interpretation` parameter and set the `num_shap` parameter. This uses Shapley-based interpretation, which you can read more about [here](https://christophm.github.io/interpretable-ml-book/shap.html). Lastly, you can also pass in your own interpretation function into the `interpretation` parameter. See an example in Gradio‚Äôs getting started page [here](https://gradio.app/getting_started/). This wraps up our deep dive into the `Interface` class of Gradio. As we‚Äôve seen, this class makes it simple to create machine learning demos in a few lines of Python code. However, sometimes you‚Äôll want to customise your demo by changing the layout or chaining multiple prediction functions together. Wouldn‚Äôt it be nice if we could somehow split the `Interface` into customizable ‚Äúblocks‚Äù? Fortunately, there is! That‚Äôs the topic of the final section.",
    "metadata": {
      "title": "Advanced Interface features",
      "url": "https://huggingface.co/learn/llm-course/chapter9/6",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/6",
      "part": 2,
      "total_parts": 2,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/6.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Introduction to Gradio Blocks In the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`. Now, what‚Äôs the difference between `Interface` and `Blocks`? - ‚ö° `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs. - üß± `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in ‚Äúbuilding blocks‚Äù). ### Why Blocks üß±? As we saw in the previous sections, the `Interface` class allows you to easily create full-fledged machine learning demos with just a few lines of code. The `Interface` API is extremely easy to use but lacks the flexibility that the `Blocks` API provides. For example, you might want to: - Group together related demos as multiple tabs in one web application - Change the layout of your demo, e.g. to specify where the inputs and outputs are located - Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general - Change a component‚Äôs properties (for example, the choices in a dropdown) or its visibility based on user input We will explore all of these concepts below. ### Creating a simple demo using Blocks After you have installed Gradio, run the code below as a Python script, a Jupyter notebook, or a Colab notebook. ``` import gradio as gr def flip_text(x): return x[::-1] demo = gr.Blocks() with demo: gr.Markdown( \"\"\" # Flip Text! Start typing below to see the output. \"\"\" ) input = gr.Textbox(placeholder=\"Flip this text\") output = gr.Textbox() input.change(fn=flip_text, inputs=input, outputs=output) demo.launch() ``` This simple example above introduces 4 concepts that underlie Blocks: 1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context. > üôãIf you‚Äôre not familiar with thewithstatement in Python, we recommend checking out the excellenttutorialfrom Real Python. Come back here after reading that ü§ó > The order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below) 2. You can define regular Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that ‚Äúflips‚Äù the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model. 3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should",
    "metadata": {
      "title": "Introduction to Gradio Blocks",
      "url": "https://huggingface.co/learn/llm-course/chapter9/7",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/7",
      "part": 1,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Python functions anywhere in your code and run them with user input using `Blocks`. In our example, we have a simple function that ‚Äúflips‚Äù the input text, but you can write any Python function, from a simple calculation to processing the predictions from a machine learning model. 3. You can assign events to any `Blocks` component. This will run your function when the component is clicked, changed, etc. When you assign an event, you pass in three parameters: `fn`: the function that should be called, `inputs`: the (list) of input component(s), and `outputs`: the (list) of output components that should be called. In the example above, we run the `flip_text()` function when the value in the `Textbox` named input `input` changes. The event reads the value in `input`, passes it as the name parameter to `flip_text()`, which then returns a value that gets assigned to our second `Textbox` named `output`. To see a list of events that each component supports, see the Gradio [documentation](https://www.gradio.app/docs/). 4. Blocks automatically figures out whether a component should be interactive (accept user input) or not, based on the event triggers you define. In our example, the first textbox is interactive, since its value is used by the `flip_text()` function. The second textbox is not interactive, since its value is never used as an input. In some cases, you might want to override this, which you can do by passing a boolean to the `interactive` parameter of the component (e.g. `gr.Textbox(placeholder=\"Flip this text\", interactive=True)`). ### Customizing the layout of your demo How can we use `Blocks` to customize the layout of our demo? By default, `Blocks` renders the components that you create vertically in one column. You can change that by creating additional columns `with gradio.Column():` or rows `with gradio.Row():` and creating components within those contexts. Here‚Äôs what you should keep in mind: any components created under a `Column` (this is also the default) will be laid out vertically. Any component created under a `Row` will be laid out horizontally, similar to the [flexbox model in web development](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Flexible_Box_Layout/Basic_Concepts_of_Flexbox). Finally, you can also create tabs for your demo by using the `with gradio.Tabs()` context manager. Within this context, you can create multiple tabs by specifying `with gradio.TabItem(name_of_tab):` children. Any component created inside of a `with gradio.TabItem(name_of_tab):` context appears in that tab. Now let‚Äôs add a `flip_image()` function to our demo and add a new tab that flips images. Below is an example with 2 tabs and also uses a Row: ``` import numpy as np import gradio as gr demo = gr.Blocks() def flip_text(x): return x[::-1] def flip_image(x): return np.fliplr(x) with demo: gr.Markdown(\"Flip text or image files using this demo.\") with gr.Tabs(): with gr.TabItem(\"Flip Text\"): with gr.Row(): text_input = gr.Textbox() text_output = gr.Textbox() text_button = gr.Button(\"Flip\") with gr.TabItem(\"Flip Image\"): with gr.Row(): image_input = gr.Image() image_output = gr.Image() image_button = gr.Button(\"Flip\") text_button.click(flip_text, inputs=text_input, outputs=text_output) image_button.click(flip_image, inputs=image_input, outputs=image_output) demo.launch() ``` You‚Äôll notice that in this example, we‚Äôve also created a `Button` component in each tab, and we‚Äôve assigned a",
    "metadata": {
      "title": "Introduction to Gradio Blocks",
      "url": "https://huggingface.co/learn/llm-course/chapter9/7",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/7",
      "part": 2,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "images. Below is an example with 2 tabs and also uses a Row: ``` import numpy as np import gradio as gr demo = gr.Blocks() def flip_text(x): return x[::-1] def flip_image(x): return np.fliplr(x) with demo: gr.Markdown(\"Flip text or image files using this demo.\") with gr.Tabs(): with gr.TabItem(\"Flip Text\"): with gr.Row(): text_input = gr.Textbox() text_output = gr.Textbox() text_button = gr.Button(\"Flip\") with gr.TabItem(\"Flip Image\"): with gr.Row(): image_input = gr.Image() image_output = gr.Image() image_button = gr.Button(\"Flip\") text_button.click(flip_text, inputs=text_input, outputs=text_output) image_button.click(flip_image, inputs=image_input, outputs=image_output) demo.launch() ``` You‚Äôll notice that in this example, we‚Äôve also created a `Button` component in each tab, and we‚Äôve assigned a click event to each button, which is what actually runs the function. ### Exploring events and state Just as you can control the layout, `Blocks` gives you fine-grained control over what events trigger function calls. Each component and many layouts have specific events that they support. For example, the `Textbox` component has 2 events: `change()` (when the value inside of the textbox changes), and `submit()` (when a user presses the enter key while focused on the textbox). More complex components can have even more events: for example, the `Audio` component also has separate events for when the audio file is played, cleared, paused, etc. See the documentation for the events each component supports. You can attach event trigger to none, one, or more of these events. You create an event trigger by calling the name of the event on the component instance as a function ‚Äî e.g. `textbox.change(...)` or `btn.click(...)`. The function takes in three parameters, as discussed above: - `fn`: the function to run - `inputs`: a (list of) component(s) whose values should supplied as the input parameters to the function. Each component‚Äôs value gets mapped to the corresponding function parameter, in order. This parameter can be None if the function does not take any parameters. - `outputs`: a (list of) component(s) whose values should be updated based on the values returned by the function. Each return value sets the corresponding component‚Äôs value, in order. This parameter can be None if the function does not return anything. You can even make the input and output component be the same component, as we do in this example that uses a GPT model to do text completion: ``` import gradio as gr api = gr.Interface.load(\"huggingface/EleutherAI/gpt-j-6B\") def complete_with_gpt(text): # Use the last 50 characters of the text as context return text[:-50] + api(text[-50:]) with gr.Blocks() as demo: textbox = gr.Textbox(placeholder=\"Type here and press enter...\", lines=4) btn = gr.Button(\"Generate\") btn.click(complete_with_gpt, textbox, textbox) demo.launch() ``` ### Creating multi-step demos In some cases, you might want a *multi-step demo*, in which you reuse the output of one function as the input to the next. This is really easy to do with `Blocks`, as you can use a component for the input of one event trigger but the output of another. Take a look at the text component in the example below, its value is the result of a speech-to-text model, but also gets passed into",
    "metadata": {
      "title": "Introduction to Gradio Blocks",
      "url": "https://huggingface.co/learn/llm-course/chapter9/7",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/7",
      "part": 3,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "api(text[-50:]) with gr.Blocks() as demo: textbox = gr.Textbox(placeholder=\"Type here and press enter...\", lines=4) btn = gr.Button(\"Generate\") btn.click(complete_with_gpt, textbox, textbox) demo.launch() ``` ### Creating multi-step demos In some cases, you might want a *multi-step demo*, in which you reuse the output of one function as the input to the next. This is really easy to do with `Blocks`, as you can use a component for the input of one event trigger but the output of another. Take a look at the text component in the example below, its value is the result of a speech-to-text model, but also gets passed into a sentiment analysis model: ``` from transformers import pipeline import gradio as gr asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\") classifier = pipeline(\"text-classification\") def speech_to_text(speech): text = asr(speech)[\"text\"] return text def text_to_sentiment(text): return classifier(text)[0][\"label\"] demo = gr.Blocks() with demo: audio_file = gr.Audio(type=\"filepath\") text = gr.Textbox() label = gr.Label() b1 = gr.Button(\"Recognize Speech\") b2 = gr.Button(\"Classify Sentiment\") b1.click(speech_to_text, inputs=audio_file, outputs=text) b2.click(text_to_sentiment, inputs=text, outputs=label) demo.launch() ``` ### Updating Component Properties So far, we have seen how to create events to update the value of another component. But what happens if you want to change other properties of a component, like the visibility of a textbox or the choices in a radio button group? You can do this by returning a component class‚Äôs `update()` method instead of a regular return value from your function. This is most easily illustrated with an example: ``` import gradio as gr def change_textbox(choice): if choice == \"short\": return gr.Textbox.update(lines=2, visible=True) elif choice == \"long\": return gr.Textbox.update(lines=8, visible=True) else: return gr.Textbox.update(visible=False) with gr.Blocks() as block: radio = gr.Radio( [\"short\", \"long\", \"none\"], label=\"What kind of essay would you like to write?\" ) text = gr.Textbox(lines=2, interactive=True) radio.change(fn=change_textbox, inputs=radio, outputs=text) block.launch() ``` We just explored all the core concepts of `Blocks`! Just like with `Interfaces`, you can create cool demos that can be shared by using `share=True` in the `launch()` method or deployed on [Hugging Face Spaces](https://huggingface.co/spaces).",
    "metadata": {
      "title": "Introduction to Gradio Blocks",
      "url": "https://huggingface.co/learn/llm-course/chapter9/7",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/7",
      "part": 4,
      "total_parts": 4,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/7.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Gradio, check!\n\n   \nThis wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:\n \n- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities.\n- Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces).\n- How to integrate Gradio demos with models and Spaces on the Hugging Face Hub.\n- Advanced features like storing state in a demo or providing authentication.\n- How to have full control of the data flow and layout of your demo with Gradio Blocks.\n \nIf you‚Äôd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section!\n \n\n## Where to next?\n\n \nIf you want to learn more about Gradio you can\n \n- Take a look at [Demos](https://github.com/gradio-app/gradio/tree/main/demo) in the repo, there are quite a lot of examples there.\n- See the [Guides](https://gradio.app/guides/) page, where you can find guides about cool and advanced features.\n- Check the [Docs](https://gradio.app/docs/) page to learn the details.",
    "metadata": {
      "title": "Gradio, check!",
      "url": "https://huggingface.co/learn/llm-course/chapter9/8",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/8",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/8.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# End-of-chapter quiz\n\n   \nLet‚Äôs test what you learned in this chapter!\n \n\n### 1. What can you use Gradio to do?\n\n  Create a demo for your machine learning model  Share your machine learning model with others  Debug your model  Train your model   \n\n### 2. Gradio ONLY works with PyTorch models\n\n  True  False   \n\n### 3. Where can you launch a Gradio demo from?\n\n  Standard python IDEs  Google Colab notebooks  Jupyter notebooks   \n\n### 4. Gradio is designed primarily for NLP models\n\n  True  False   \n\n### 5. Which of the following features are supported by Gradio?\n\n  Multiple inputs and outputs  State for data persistance  Username and passwords authentication  Automatic analytics for who uses your gradio demo  Loading a model from Hugging Face's model hub or Hugging Face Spaces   \n\n### 6. Which of the following are valid ways of loading a Hugging Face model from Hub or Spaces?\n\n  gr.Interface.load('huggingface/{user}/{model_name}')  gr.Interface.load('model/{user}/{model_name}')  gr.Interface.load('demos/{user}/{model_name}')  gr.Interface.load('spaces/{user}/{model_name}')   \n\n### 7. Select all the steps necessary for adding state to your Gradio interface\n\n  Pass in an extra parameter into your prediction function, which represents the state of the interface.  At the end of the prediction function, return the updated value of the state as an extra return value.  Add the state input and state output components when creating your Interface   \n\n### 8. Which of the following are components included in the Gradio library?\n\n  Textbox.  Graph.  Image.  Audio.   \n\n### 9. What does Gradio Blocks allow you to do?\n\n  Combine multiple demos into one web app  Assign event triggers such as clicked/changed/etc to `Blocks` components  Automatically determine which `Blocks` component should be interactive vs. static  Create multi-step demos; meaning allowing you to reuse the output of one component as the input to the next   \n\n### 10. You can share a public link to a Blocks demo and host a Blocks demo on Hugging Face spaces.\n\n  True  False",
    "metadata": {
      "title": "End-of-chapter quiz",
      "url": "https://huggingface.co/learn/llm-course/chapter9/9",
      "course": "llm-course",
      "chapter": "9. Building and sharing demos",
      "chapter_id": "chapter9/9",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/chapter9/9.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Live sessions and workshops\n\n \nFor the release of parts 1 and 2 of the course, we organized several live coding sessions and workshops. You can find links to the recordings of these sessions and workshops below.\n \n\n## Live coding sessions\n\n \nFor the first session, Sylvain goes through Chapter 1 of the course with you, explaining it step by step:\n  \nIn the second session, it is Lewis‚Äô turn to present Chapter 2:\n  \nBecause Chapter 2 is so cool, Sylvain has also given a walkthrough of it!\n  \nFor Chapter 3, Lewis returns to guide you through the code:\n  \nFinally, Omar concludes the live sessions related to the first part of the course by tackling chapter 4:\n  \n\n## Workshops\n\n \nIn the first workshop, Merve welcomes Lewis to discuss section 7 of chapter 7 about [question answering](https://huggingface.co/course/chapter7/7?fw=pt).\n  \nFor the second workshop, Merve hosts Leandro to talk about chapter 7, section 6 on [training a causal language model from scratch](https://huggingface.co/course/chapter7/6?fw=pt) with an application with [CodeParrot](https://huggingface.co/codeparrot).",
    "metadata": {
      "title": "Live sessions and workshops",
      "url": "https://huggingface.co/learn/llm-course/events/1",
      "course": "llm-course",
      "chapter": "Course Events",
      "chapter_id": "events/1",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/events/1.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Part 2 Release Event For the release of part 2 of the course, we organized a live event with two days of talks before a fine-tuning sprint. If you missed it, you can catch up with the talks which are all listed below! ## Day 1: A high-level view of Transformers and how to train them **Thomas Wolf:** *Transfer Learning and the birth of the Transformers library* ![A visual summary of Thom's talk](https://i.imgur.com/9eq8oUi.png) Thomas Wolf is co-founder and Chief Science Officer of Hugging Face. The tools created by Thomas Wolf and the Hugging Face team are used across more than 5,000 research organisations including Facebook Artificial Intelligence Research, Google Research, DeepMind, Amazon Research, Apple, the Allen Institute for Artificial Intelligence as well as most university departments. Thomas Wolf is the initiator and senior chair of the largest research collaboration that has ever existed in Artificial Intelligence: [‚ÄúBigScience‚Äù](https://bigscience.huggingface.co), as well as a set of widely used [libraries and tools](https://github.com/huggingface/). Thomas Wolf is also a prolific educator, a thought leader in the field of Artificial Intelligence and Natural Language Processing, and a regular invited speaker to conferences all around the world [https://thomwolf.io](https://thomwolf.io). **Jay Alammar:** *A gentle visual intro to Transformers models* ![A visual summary of Jay's talk](https://i.imgur.com/rOZAuE9.png) Through his popular ML blog, Jay has helped millions of researchers and engineers visually understand machine learning tools and concepts from the basic (ending up in NumPy, Pandas docs) to the cutting-edge (Transformers, BERT, GPT-3). **Margaret Mitchell:** *On Values in ML Development* ![A visual summary of Margaret's talk](https://i.imgur.com/NuIsnY3.png) Margaret Mitchell is a researcher working on Ethical AI, currently focused on the ins and outs of ethics-informed AI development in tech. She has published over 50 papers on natural language generation, assistive technology, computer vision, and AI ethics, and holds multiple patents in the areas of conversation generation and sentiment classification. She previously worked at Google AI as a Staff Research Scientist, where she founded and co-led Google‚Äôs Ethical AI group, focused on foundational AI ethics research and operationalizing AI ethics Google-internally. Before joining Google, she was a researcher at Microsoft Research, focused on computer vision-to-language generation; and was a postdoc at Johns Hopkins, focused on Bayesian modeling and information extraction. She holds a PhD in Computer Science from the University of Aberdeen and a Master‚Äôs in computational linguistics from the University of Washington. While earning her degrees, she also worked from 2005-2012 on machine learning, neurological disorders, and assistive technology at Oregon Health and Science University. She has spearheaded a number of workshops and initiatives at the intersections of diversity, inclusion, computer science, and ethics. Her work has received awards from Secretary of Defense Ash Carter and the American Foundation for the Blind, and has been implemented by multiple technology companies. She likes gardening, dogs, and cats. **Matthew Watson and Chen Qian:** *NLP workflows with Keras* ![A visual summary of Matt and Chen's talk](https://i.imgur.com/1vD2az8.png) Matthew Watson is a machine learning engineer on the Keras team, with a focus on high-level modeling APIs. He studied",
    "metadata": {
      "title": "Part 2 Release Event",
      "url": "https://huggingface.co/learn/llm-course/events/2",
      "course": "llm-course",
      "chapter": "Course Events",
      "chapter_id": "events/2",
      "part": 1,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/events/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "learning, neurological disorders, and assistive technology at Oregon Health and Science University. She has spearheaded a number of workshops and initiatives at the intersections of diversity, inclusion, computer science, and ethics. Her work has received awards from Secretary of Defense Ash Carter and the American Foundation for the Blind, and has been implemented by multiple technology companies. She likes gardening, dogs, and cats. **Matthew Watson and Chen Qian:** *NLP workflows with Keras* ![A visual summary of Matt and Chen's talk](https://i.imgur.com/1vD2az8.png) Matthew Watson is a machine learning engineer on the Keras team, with a focus on high-level modeling APIs. He studied Computer Graphics during undergrad and a Masters at Stanford University. An almost English major who turned towards computer science, he is passionate about working across disciplines and making NLP accessible to a wider audience. Chen Qian is a software engineer from Keras team, with a focus on high-level modeling APIs. Chen got a Master degree of Electrical Engineering from Stanford University, and he is especially interested in simplifying code implementations of ML tasks and large-scale ML. **Mark Saroufim:** *How to Train a Model with Pytorch* ![A visual summary of Mark's talk](https://i.imgur.com/TPmlkm8.png) Mark Saroufim is a Partner Engineer at Pytorch working on OSS production tools including TorchServe and Pytorch Enterprise. In his past lives, Mark was an Applied Scientist and Product Manager at Graphcore, [yuri.ai](http://yuri.ai/), Microsoft and NASA‚Äôs JPL. His primary passion is to make programming more fun. **Jakob Uszkoreit:** *It Ain‚Äôt Broke So Don‚Äôt Fix Let‚Äôs Break It* ![A visual summary of Jakob's talk](https://i.imgur.com/5dWQeNB.png) Jakob Uszkoreit is the co-founder of Inceptive. Inceptive designs RNA molecules for vaccines and therapeutics using large-scale deep learning in a tight loop with high throughput experiments with the goal of making RNA-based medicines more accessible, more effective and more broadly applicable. Previously, Jakob worked at Google for more than a decade, leading research and development teams in Google Brain, Research and Search working on deep learning fundamentals, computer vision, language understanding and machine translation. ## Day 2: The tools to use **Lewis Tunstall:** *Simple Training with the ü§ó Transformers Trainer* Lewis is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O‚ÄôReilly book [Natural Language Processing with Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/). You can follow him on Twitter (@_lewtun) for NLP tips and tricks! **Matthew Carrigan:** *New TensorFlow Features for ü§ó Transformers and ü§ó Datasets* Matt is responsible for TensorFlow maintenance at Transformers, and will eventually lead a coup against the incumbent PyTorch faction which will likely be co-ordinated via his Twitter account @carrigmat. **Lysandre Debut:** *The Hugging Face Hub as a means to collaborate on and share Machine Learning projects* ![A visual summary of Lysandre's talk](https://i.imgur.com/TarIPCz.png) Lysandre is a Machine Learning Engineer at Hugging Face where he is involved in many open source projects. His aim is to make Machine Learning accessible to everyone by developing powerful tools with a very simple API. **Lucile Saulnier:** *Get your own tokenizer with",
    "metadata": {
      "title": "Part 2 Release Event",
      "url": "https://huggingface.co/learn/llm-course/events/2",
      "course": "llm-course",
      "chapter": "Course Events",
      "chapter_id": "events/2",
      "part": 2,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/events/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "Transformers and ü§ó Datasets* Matt is responsible for TensorFlow maintenance at Transformers, and will eventually lead a coup against the incumbent PyTorch faction which will likely be co-ordinated via his Twitter account @carrigmat. **Lysandre Debut:** *The Hugging Face Hub as a means to collaborate on and share Machine Learning projects* ![A visual summary of Lysandre's talk](https://i.imgur.com/TarIPCz.png) Lysandre is a Machine Learning Engineer at Hugging Face where he is involved in many open source projects. His aim is to make Machine Learning accessible to everyone by developing powerful tools with a very simple API. **Lucile Saulnier:** *Get your own tokenizer with ü§ó Transformers & ü§ó Tokenizers* Lucile is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience. **Sylvain Gugger:** *Supercharge your PyTorch training loop with ü§ó Accelerate* Sylvain is a Research Engineer at Hugging Face and one of the core maintainers of ü§ó Transformers and the developer behind ü§ó Accelerate. He likes making model training more accessible. **Merve Noyan:** *Showcase your model demos with ü§ó Spaces* Merve is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone. **Abubakar Abid:** *Building Machine Learning Applications Fast* ![A visual summary of Abubakar's talk](https://i.imgur.com/qWIFeiF.png) Abubakar Abid is the CEO of [Gradio](www.gradio.app). He received his Bachelor‚Äôs of Science in Electrical Engineering and Computer Science from MIT in 2015, and his PhD in Applied Machine Learning from Stanford in 2021. In his role as the CEO of Gradio, Abubakar works on making machine learning models easier to demo, debug, and deploy. **Mathieu Desv√©:** *AWS ML Vision: Making Machine Learning Accessible to all Customers* ![A visual summary of Mathieu's talk](https://i.imgur.com/oLdZTKy.png) Technology enthusiast, maker on my free time. I like challenges and solving problem of clients and users, and work with talented people to learn every day. Since 2004, I work in multiple positions switching from frontend, backend, infrastructure, operations and managements. Try to solve commons technical and managerial issues in agile manner. **Philipp Schmid:** *Managed Training with Amazon SageMaker and ü§ó Transformers* Philipp Schmid is a Machine Learning Engineer and Tech Lead at Hugging Face, where he leads the collaboration with the Amazon SageMaker team. He is passionate about democratizing and productionizing cutting-edge NLP models and improving the ease of use for Deep Learning.",
    "metadata": {
      "title": "Part 2 Release Event",
      "url": "https://huggingface.co/learn/llm-course/events/2",
      "course": "llm-course",
      "chapter": "Course Events",
      "chapter_id": "events/2",
      "part": 3,
      "total_parts": 3,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/events/2.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  },
  {
    "text": "# Gradio Blocks Party\n\n \nAlong with the release of the Gradio chapter of the course, Hugging Face hosted a community event on building cool machine learning demos using the new Gradio Blocks feature.\n \nYou can find all the demos that the community created under the [Gradio-Blocks](https://huggingface.co/Gradio-Blocks) organisation on the Hub. Here‚Äôs a few examples from the winners:\n \n**Natural language to SQL**",
    "metadata": {
      "title": "Gradio Blocks Party",
      "url": "https://huggingface.co/learn/llm-course/events/3",
      "course": "llm-course",
      "chapter": "Course Events",
      "chapter_id": "events/3",
      "part": 1,
      "total_parts": 1,
      "source_file": "../raw_data/huggingface.co.learn/llm-course/events/3.txt",
      "description": "–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∫—É—Ä—Å–æ–≤ HuggingFace"
    }
  }
]