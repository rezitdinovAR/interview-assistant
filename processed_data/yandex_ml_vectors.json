[
  {
    "text": "Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных\nОб этой книге\nЭта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных. Своим появлением она обязана двум замечательным курсам. Во-первых, это курс Константина Вячеславовича Воронцова, на котором выросло подавляющее большинство авторов книги, да и вообще ML-специалистов в России. Во-вторых, это курс NLP Course | For You Лены Войта, благодаря которому мы поняли, как должен выглядеть современный учебник, и на который мы будем регулярно ссылаться в частях, связанных с анализом текстов.\nИдея была такая: записать сложившийся в ШАДе курс машинного обучения в виде книги, при этом избежав каких-либо компромиссов: нигде ничего не упрощать чрезмерно, дать необходимую теорию, описать и исторически важные алгоритмы, и применяющиеся сегодня, вместе с теорией рассказывать и практические вопросы о реализации алгоритмов и работе с данными.\nМатематика — это один из языков, на котором написан учебник. Мы будем стараться давать необходимые пояснения, но всё же уверенное владение линейной алгеброй, математическим анализом и теорией вероятностей будет большим плюсом. Знания статистики и методов выпуклой оптимизации не обязательны, хотя сделают чтение комфортнее.\nЧитая книгу, вы, возможно, заметите в ней ошибки, неточности и плохо объяснённые детали. В таком случае, пожалуйста, дайте нам знать об этом, написав (сюда) — так вы поможете и другим читателям.\nИтак, приступим.\nПожалуйста, оцените ясность параграфа\nСообщить об ошибке\nСледующий параграф\n1.2. Первые шаги\nВ этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание.\nА кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого.\nСледующий параграф\n1.3. Машинное обучение\nЧто такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучение",
    "metadata": {
      "title": "Об этой книге",
      "url": "https://education.yandex.ru/handbook/ml/article/about",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.1",
      "part": 1,
      "total_parts": 1,
      "source_file": "1.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание. А кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого. Рабочее окружение для ML-специалиста Грубо говоря, оно делится на две большие категории: Железо и вычислительные ресурсы для обучения моделей; Программы и библиотеки для работы с данными. Начнём со второй категории. Чтобы начать работу, нужно установить Python — именно этот язык программирования доминирует в индустрии, благодаря большому количеству библиотек и фреймворкам, предназначенным именно для машинного обучения — TensorFlow или PyTorch. Фреймворк — слой абстракции над языком программирования, который облегчает разработку. Например — нам нужно сделать отверстие в доске. Эту задачу можно решить разными способами: закрутить и выкрутить шуруп, забить и вытащить гвоздь и так далее. А можно взять дрель и сверло. Дрель в этом примере и есть фреймворк. PyTorch чаще выбирают для академических исследований — он более гибкий и больше подходит для экспериментов. TensorFlow — для продакшен-решений, поскольку он более подходит для масштабирования моделей. Далее нужно установить библиотеки для Python. Продолжая строительную аналогию: библиотека — это насадка для дрели. То есть инструмент для конкретной задачи: можно установить сверло для дерева, для бетона, для металла, а можно коронку или щётку — зависит от задачи. Чаще всего применяют: Scikit-learn — библиотека машинного обучения для классических алгоритмов: классификации, регрессии, ансамблей и других. О них мы подробнее поговорим далее в этом хендбуке. Pandas — библиотека для предварительной обработки данных, и работы с данными вообще. С её помощью можно загрузить датасет, обработать недостающие значения, закодировать категориальные переменные и многое другое. Matplotlib и Seaborn — библиотеки для создания визуализаций и графиков в Python. После этого — выбрать IDE, то есть текстовый редактор для кода: Visual Studio Code, Jupyter, Sublime, PyCharm, и так далее. Теоретически, всё это можно установить на домашний компьютер или ноутбук — именно так и делали ещё 15-20 лет назад. Но вам не хватит вычислительных ресурсов для обучения моделей, в первую очередь — объёма памяти GPU (видеокарты). Даже для файнтюнинга небольших языковых моделей, таких как BERT, необходим графический процессор с минимум 16 Гб видеопамяти. Мало кто может позволить себе дома оборудование для обучения более сложных моделей. Сейчас исследователи и студенты чаще берут вычислительные мощности в аренду. Тут есть два способа: арендовать устройство «в облаке» (эта модель называется IaaS), воспользоваться специальной платформой для ML (эта модель называется SaaS). IaaS-сервис, грубо говоря, — очень мощный удалённый компьютер. Это значит, что прежде чем решать задачу на такой машине, её всё равно необходимо настроить: развернуть IDE, установить Python, фреймворки и библиотеки и многое другое. Это не всегда удобно: иногда хочется, чтобы всё работало «из коробки». «Из коробки», как вы могли догадаться, работают SaaS-сервисы: они предоставляют полностью настроенные среды, готовые к немедленному использованию в решении задач. Эти платформы обычно включают в себя: IDE или другие среды программирования, часто представленные в формате ноутбуков. Заранее настроенные рабочие окружения, оптимизированные для конкретной системы. Возможности для загрузки и хранения данных и файлов. Интеграцию с известными сервисами, такими как GitHub. О них мы и поговорим",
    "metadata": {
      "title": "Первые шаги",
      "url": "https://education.yandex.ru/handbook/ml/article/pervie-shagi",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.2",
      "part": 1,
      "total_parts": 3,
      "source_file": "1.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Это значит, что прежде чем решать задачу на такой машине, её всё равно необходимо настроить: развернуть IDE, установить Python, фреймворки и библиотеки и многое другое. Это не всегда удобно: иногда хочется, чтобы всё работало «из коробки». «Из коробки», как вы могли догадаться, работают SaaS-сервисы: они предоставляют полностью настроенные среды, готовые к немедленному использованию в решении задач. Эти платформы обычно включают в себя: IDE или другие среды программирования, часто представленные в формате ноутбуков. Заранее настроенные рабочие окружения, оптимизированные для конкретной системы. Возможности для загрузки и хранения данных и файлов. Интеграцию с известными сервисами, такими как GitHub. О них мы и поговорим далее. Но если вам ближе путь самурая — то вот несколько IaaS провайдеров. По ссылкам можно узнать, как развернуть окружение для ML в IaaS-сервисе. SaaS-платформы Как мы уже выяснили, главное преимущество SaaS – это простота входа: вы получаете доступ к необходимым ресурсам без забот о их настройке и оптимизации, что позволяет быстро приступить к работе над ML-задачами. К популярным SaaS-платформам относят: Google Colab Kaggle Notebooks AWS SageMaker Azure ML Studio Yandex DataSphere Ниже мы собрали в таблицу их возможности, плюсы и минусы. яндекс иллюстрации Далее мы расскажем, как решать ML-задачи на примере Yandex DataSphere. Но если вас заинтересовали другие платформы, то в конце параграфа будет список ссылок на руководства по работе с ними. Получение доступа и настройка DataSphere Прежде чем мы начнём настройку — несколько важных моментов. DataSphere — это платный сервис, но вы можете начать работу бесплатно, с помощью тестового гранта. Также у сервиса есть специальные гранты для учебных программ. Чтобы воспользоваться грантом, нужно попросить своего преподавателя заполнить форму, — это откроет доступ к сервису для всех студентов группы. Отлично, теперь можем приступить к настройке. Для этого: Перейдите на сайт DataSphere Нажмите большую синюю кнопку и авторизуйтесь в Яндекс ID Создайте сообщество и нажмите «Привязать платежный аккаунт» на появившемся красном дисклеймере. В созданном сообществе вы сможете взаимодействовать со всеми важными сущностями в DataSphere. Теперь можно создать проект на вкладке «Проект». 3.webp В созданном проекте вы можете запустить JupyterLab: 2.webp После незначительного ожидания откроется выбор среды исполнения. Там вы можете выбрать любой из примеров ноутбуков с различными снипеттами кода под разные задачи. Создадим новый пустой ноутбук, нажав “DataSphere Kernel”. 1 (1).webp Теперь, в появившемся новом ноутбуке, если мы запустим любой код в одной из ячеек, вам будет предложено выбрать конфигурацию виртуального рабочего места (более подробно о доступных конфигурациях можно почитать тут). После выделения ресурсов, которое тоже займет небольшое количество времени, все последующие выполнения ячеек будут происходить без выбора конфигурации. Теперь, когда у нас всё готово — DataSphere настроена, ресурсы выделены, можем выполнить тестовую лабораторную работу! Лабораторная работа В ней мы будем обучать генеративную трансформерную модель с помощью библиотеки transformers. Сама работа находится в DataSphere — переходите по ссылке, чтобы ознакомиться с заданием. А как закончите — возвращайтесь, чтобы завершить урок. Вот и всё! Если вы читаете эти строки, и у вас всё получилось — вы большой молодец. Если не получилось — ничего страшного, с первого раза мало у кого всё получается. Советуем вступить в сообщество хендбука и попросить помощи или",
    "metadata": {
      "title": "Первые шаги",
      "url": "https://education.yandex.ru/handbook/ml/article/pervie-shagi",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.2",
      "part": 2,
      "total_parts": 3,
      "source_file": "1.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "все последующие выполнения ячеек будут происходить без выбора конфигурации. Теперь, когда у нас всё готово — DataSphere настроена, ресурсы выделены, можем выполнить тестовую лабораторную работу! Лабораторная работа В ней мы будем обучать генеративную трансформерную модель с помощью библиотеки transformers. Сама работа находится в DataSphere — переходите по ссылке, чтобы ознакомиться с заданием. А как закончите — возвращайтесь, чтобы завершить урок. Вот и всё! Если вы читаете эти строки, и у вас всё получилось — вы большой молодец. Если не получилось — ничего страшного, с первого раза мало у кого всё получается. Советуем вступить в сообщество хендбука и попросить помощи или совета. Полезные ссылки Руководство по работе с Google Colab. Гайд для новичков по Kaggle Notebooks. Руководство для AWS SageMaker. Документация по настройке Azure ML Studio Статья про то, как используется DataSphere в образовании Как DataSphere помогает изучать снежных барсов Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 1.1. Об этой книге Эта книга написана коллективом добрых людей, состоящим из преподавателей и выпускников Школы анализа данных Следующий параграф 1.3. Машинное обучение Что такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучение Может ли машинное обучение спасать барсов? Да, если создать нейросеть для обработки данных с фотоловушек, которая поможет учёным обработать тысячи фотографий со 170 фотоловушек из Сайлюгемского заповедника.",
    "metadata": {
      "title": "Первые шаги",
      "url": "https://education.yandex.ru/handbook/ml/article/pervie-shagi",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.2",
      "part": 3,
      "total_parts": 3,
      "source_file": "1.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Что такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучение Машинное обучение — это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту. Когда Алан Тьюринг работал над первыми (компьютерами), он пытался расшифровать сообщения немецких военных, закодированные машиной Энигма. Поиск расшифровки требовал перебора массы вариантов. Люди с этой задачей справлялись плохо, зато машина могла решить её сравнительно быстро. Очевидно, далеко не для каждой задачи, с которой люди справляются с трудом, можно написать программу для эффективного поиска решения. Более того, есть целый класс задач (так называемые NP-трудные задачи), которые нельзя решить за разумное время. Можно даже явно доказать, что никакой компьютер здесь чуда тоже не совершит. Самое интересное это то, что бывают и задачи, которые для людей особенного труда не составляют, но которые почему-то крайне трудно запрограммировать, например: перевести текст с одного языка на другой; диагностировать болезнь по симптомам; сравнить, какой из двух документов в интернете лучше подходит под данный поисковый запрос; сказать, что изображено на картинке; оценить, по какой цене удастся продать квартиру. У всех этих задач есть много общего. Во-первых, их решение можно записать как функцию, которая отображает объекты или примеры (samples) в предсказания (targets). Например, больных надо отобразить в диагнозы, а документы в оценку релевантности. Во-вторых, вряд ли у этих задач есть единственно верное, идеальное решение. Даже профессиональные переводчики могут по-разному перевести один и тот же текст, и оба перевода будут верными. Так что лучшее в этих задачах — враг хорошего. В конце концов, и доктора иногда делают ошибки в диагнозах, и вы не всегда можете сказать, что же именно изображено на картинке. В-третьих, у нас есть много примеров правильных ответов (скажем, переводов предложения на другой язык или подписей к заданной картинке), а примеры неправильных ответов (если они нужны), как правило, не составляет труда сконструировать. Мы назовём функцию, отображающую объекты в предсказания, — моделью, а имеющийся у нас набор примеров — обучающей выборкой или датасетом. Обучающая выборка состоит из: объектов (к примеру, скачанные из интернета картинки, истории больных, активность пользователей сервиса и так далее); и ответов (подписи к картинкам, диагнозы, информация об уходе пользователей с сервиса), которые мы также будем иногда называть таргетами. Постановка задачи Описанные выше задачи являются примерами задач обучения с учителем (supervised learning), так как правильные ответы для каждого объекта обучающей выборки заранее известны. Задачи обучения с учителем делятся на следующие виды в зависимости от того, каким может быть множество Y Y всех возможных ответов (таргетов): Y = R Y=R или Y = R M Y=R M — регрессия. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление — это несколько вещественных чисел, которые формируют вектор нашего предсказания). Y = 0 , 1 Y=0,1 — бинарная классификация. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернёт ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определённое заболевание у пациента, есть ли на картинке банан. Y=1,…,K — многоклассовая (multiclass) классификация. Например, определение предметной области для научной статьи (математика, биология,",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 1,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Y = R M Y=R M — регрессия. Примерами задач регрессии является предсказание продолжительности поездки на каршеринге, спрос на конкретный товар в конкретный день или погода в вашем городе на завтра (температура, влажность и давление — это несколько вещественных чисел, которые формируют вектор нашего предсказания). Y = 0 , 1 Y=0,1 — бинарная классификация. Например, мы можем предсказывать, кликнет ли пользователь по рекламному объявлению, вернёт ли клиент кредит в установленный срок, сдаст ли студент сессию, случится ли определённое заболевание у пациента, есть ли на картинке банан. Y=1,…,K — многоклассовая (multiclass) классификация. Например, определение предметной области для научной статьи (математика, биология, психология и т. д.). Y=0,1 K — многоклассовая классификация с пересекающимися классами (multilabel classification). Например, задача автоматического проставления тегов для ресторанов (логично, что ресторан может одновременно иметь несколько тегов). Y Y — конечное упорядоченное множество — ранжирование. Основным примером является задача ранжирования поисковой выдачи, где для любого запроса нужно отсортировать все возможные документы по релевантности этому запросу; при этом оценка релевантности имеет смысл только в контексте сравнения двух документов между собой, её абсолютное значение информации не несёт. Ответ может быть и более сложным. Так, в задаче сегментации изображения требуется для каждого пикселя предсказать, к какому объекту или типу объектов он относится, а в задаче машинного перевода мы должны сгенерировать предложение (или целый текст), являющееся переводом исходного. Интерес представляют и задачи порождения новых объектов, то есть генерации правдоподобных объектов, из ничего или на основе уже существующих. С помощью такой модели также можно научиться увеличивать разрешение изображения и применять любимые всеми маски в Snapchat или Instagram. Есть и относительно небольшой класс задач, относящихся к обучению без учителя (unsupervised learning), — это задачи, для которых нам известны только данные, а ответы неизвестны или вообще не существуют. Более того, часто поиск \"правильных\" ответов не является самоцелью. Классическим примером обучения без учителя является кластеризация — задача разделения объектов на группы, обладающие некоторыми неизвестными нам, но, как мы в глубине души надеемся, интерпретируемыми свойствами. Примером может служить кластеризация документов из электронной библиотеки по темам или кластеризация новостей с целью выделения крупных сюжетов. Бывают и другие виды (и даже парадигмы) машинного обучения, так что если вы встретите задачу, которую никак не получается отнести к одному из перечисленных выше типов, не расстраивайтесь и знайте, что где-то дальше в учебнике вас ждёт рассказ про такие задачи. Вопрос на подумать. Определите тип следующих задач. По возможности попробуйте отнести их к более узким видам задач. Предсказание курса евро к доллару на следующий день. Стилизация текста. Например, перевод на бюрократический язык: «Пиппина и Мерри похитили!» ↦ ↦ «Граждане Тук, Перегрин Паладинович, 2990 года рождения, и Брендибак, Мериадок Сарадокович, 2982 года рождения, были похищены неустановленными лицами». Детектирование котиков на изображении. Обучение робокота запрыгивать на стол из произвольной позы. Поиск наборов товаров, которые посетители супермаркета часто покупают вместе. Вопрос на подумать. Ранжирование — это задача с таргетом из конечного упорядоченного множества (1,…,K). Казалось бы, её запросто можно было бы рассматривать как задачу классификации на K K классов или задачу регрессии. В чём же проблема? Почему так не делают? Критерии качества По обучающей выборке мы",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 2,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "следующий день. Стилизация текста. Например, перевод на бюрократический язык: «Пиппина и Мерри похитили!» ↦ ↦ «Граждане Тук, Перегрин Паладинович, 2990 года рождения, и Брендибак, Мериадок Сарадокович, 2982 года рождения, были похищены неустановленными лицами». Детектирование котиков на изображении. Обучение робокота запрыгивать на стол из произвольной позы. Поиск наборов товаров, которые посетители супермаркета часто покупают вместе. Вопрос на подумать. Ранжирование — это задача с таргетом из конечного упорядоченного множества (1,…,K). Казалось бы, её запросто можно было бы рассматривать как задачу классификации на K K классов или задачу регрессии. В чём же проблема? Почему так не делают? Критерии качества По обучающей выборке мы хотим построить модель, предсказания которой достаточно хороши. Что вообще значит «достаточно хороши»? Не понимая, чего мы хотим добиться, мы не предложим хорошего решения, поэтому нужно внимательно отнестись к выбору метрик качества. Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику организатор выбирает за вас, и она, как правило, непосредственным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее. Например, мы хотим: решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин, чтобы минимизировать количество товара, который не будет выкуплен, и минимизировать вероятность того, что покупатель к концу дня не найдёт желаемый продукт на полке; увеличить счастье пользователей от работы с нашим сервисом, чтобы пользователи стали лояльнее, а сервис мог получать стабильный прогнозируемый доход; решить, нужно ли направить пациента на дополнительное медицинское обследование. В каждом конкретном случае может возникать целая иерархия метрик. Самый верхний уровень – это бизнес-метрики, например, будущий доход сервиса. Их трудно измерить в моменте, они сложным образом зависят от совокупности всех наших усилий, не только связанных с машинным обучением. Онлайн (online) метрики – это характеристики работающей системы, с помощью которых мы надеемся оценить, что будет с бизнес-метриками. Например, это может быть: – Медианная длина сессии в онлайн-игре. Можно предположить, что пользователь, который долго сидит в игре – это довольный пользователь. – Среднее количество бананов на полках во всех магазинах торговой сети в конце дня. Не всегда плоды наших трудов оцениваются числами. Многое может зависеть от субъективного восприятия людей, и для того, чтобы оценить их реакцию до выпуска в продакшен, применяется оценка специально нанятыми людьми – асессорами. Например, так можно оценивать, получилось ли у нас улучшить качество машинного перевода или релевантность выдачи в поисковой системе. Офлайн (offline) метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным. В задачах, в которых нужно предсказывать какой-то конкретный таргет, офлайн метрики обычно оценивают отклонение предсказаний модели от истинных значений таргета. Например, это может быть точность предсказания, то есть число верно угаданных значений, или среднеквадратичное отклонение. Асессорскую оценку тоже можно считать офлайн-метрикой В этой книге речь в основном пойдёт об офлайновых метриках и о функциях потерь. И прежде, чем вы начнёте знакомиться с методами решения задач обучения с учителем, полезно посмотреть, какими бывают метрики качества. Вот несколько примеров: для задачи постановки диагноза хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля больных, которым удалось поставить правильный диагноз (а вы поняли разницу?); для задачи предсказания цены квартиры метрикой качества может быть",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 3,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "обычно оценивают отклонение предсказаний модели от истинных значений таргета. Например, это может быть точность предсказания, то есть число верно угаданных значений, или среднеквадратичное отклонение. Асессорскую оценку тоже можно считать офлайн-метрикой В этой книге речь в основном пойдёт об офлайновых метриках и о функциях потерь. И прежде, чем вы начнёте знакомиться с методами решения задач обучения с учителем, полезно посмотреть, какими бывают метрики качества. Вот несколько примеров: для задачи постановки диагноза хорошими метриками могут быть, например, доля правильно поставленных диагнозов или доля больных, которым удалось поставить правильный диагноз (а вы поняли разницу?); для задачи предсказания цены квартиры метрикой качества может быть доля квартир, для которых разница между предсказанным и истинным значением цены не превысила какого-то порога, или средний модуль разницы между предсказанным и истинным значением; для задачи ранжирования поисковых документов по запросу — доля пар документов, которые мы упорядочили неправильно. Цель обычно в том, чтобы найти модель, для которой значение метрики будет оптимальным. Вопрос на подумать. Важно помнить, что разные нужды заказчика могут диктовать самые разные метрики. Вернёмся к задаче постановки диагноза пациентам больницы. Какие метрики вы предложили бы использовать в каждом из следующих случаев: обычный год в обычном терапевтическом отделении обычной больницы; определение очень неприятной болезни, которая жутким клеймом падёт на каждого, кому поставили такой диагноз; определение опасной и очень заразной болезни. Вопрос на подумать. Рассмотрим задачу детектирования людей на изображении. Чаще всего под детектированием понимают указание местоположения человека на картинке. Например, модель пытается выделить прямоугольник, в котором, по её мнению, есть человеческая фигура. Подумайте, какие метрики можно было бы использовать в различных ситуациях для измерения качества решения этой задачи. Не забудьте, что метрики — это способ численно измерить то, насколько модель помогает нам в жизни, так что важно думать о том, зачем нам вообще детектировать людей. Критерии качества не всегда сводятся к метрикам. Бизнес или общество могут накладывать и другие требования, например: Модель может выдавать предсказания в режиме реального времени. Заметим, что это требование не только к модели, но и к её реализации, а также к тому железу или к тем серверам, на которых она работает. Модель достаточно компактна, чтобы помещаться на мобильном телефоне или другом устройстве. Можно объяснить, на основании чего модель сделала то или иное предсказание для конкретного объекта. Это может быть важным в случае, если модель решает что-то важное в жизни человека, например, дадут ли кредит или будет ли согласовано дорогостоящее лечение. Такое требование является частным случаем более общего понятия интерпретируемости модели. Предсказания модели не дискриминируют какую-либо категорию пользователей. Например, если двум людям с одинаковой и достаточно длинной историей просмотров онлайн-кинотеатр рекомендует разные фильмы только из-за того, что у них разный пол, то это не здорово. Данные Машинное обучение начинается с данных. Важно, чтобы их было достаточно много и чтобы они были достаточно качественными. Некоторые проекты приходится откладывать на неопределённый срок из-за того, что просто невозможно собрать данные. Чем сложнее задача, тем больше данных нужно, чтобы её решить. Например, существенные успехи в задачах распознавания изображений были достигнуты лишь с появлением очень больших датасетов (и, стоит добавить, вычислительных мощностей). Вычислительные ресурсы продолжают совершенствовать, но во многих ситуациях",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 4,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "какую-либо категорию пользователей. Например, если двум людям с одинаковой и достаточно длинной историей просмотров онлайн-кинотеатр рекомендует разные фильмы только из-за того, что у них разный пол, то это не здорово. Данные Машинное обучение начинается с данных. Важно, чтобы их было достаточно много и чтобы они были достаточно качественными. Некоторые проекты приходится откладывать на неопределённый срок из-за того, что просто невозможно собрать данные. Чем сложнее задача, тем больше данных нужно, чтобы её решить. Например, существенные успехи в задачах распознавания изображений были достигнуты лишь с появлением очень больших датасетов (и, стоит добавить, вычислительных мощностей). Вычислительные ресурсы продолжают совершенствовать, но во многих ситуациях размеченных данных (то есть объектов, которым кто-то сопоставил ответ) было бы по-прежнему слишком мало: например, для решения задачи аннотирования изображений (image captioning) потребовалось бы огромное количество пар (изображение, описание). В некоторых случаях можно воспользоваться открытыми датасетами. Сейчас их доступно довольно много и некоторые весьма велики, но чаще всего они создаются для довольно простых задач, например, для задачи классификации изображений. Иногда датасет можно купить. Но для каких-то задач вы нигде не найдёте данных. Скажем, авторам неизвестно больших и качественных корпусов телефонных разговоров с расшифровками – в том числе и по причинам конфиденциальности таких данных. Бороться с проблемой нехватки данных можно двумя способами. Первый – использование краудсорсинга, то есть привлечение людей, готовых разметить много данных. Во многих ситуациях (например, когда речь заходит об оценке поисковой выдачи) без дополнительной разметки никак не обойтись. Мы расскажем про краудсорсинг подробнее в соответствующем параграфе. Некоторые проекты, в первую очередь научные и социальные, используют также citizen science – разметку данных волонтёрами без какого-либо вознаграждения, просто за чувство причастности к доброму делу исследования животных Африки или формы галактик. Второй же способ состоит в использовании неразмеченных данных. К примеру, в задаче аннотирования изображений у нас есть огромное количество никак не связанных друг с другом изображений и текстов. Однако, мы можем использовать их для того, чтобы помочь компьютеру понять, какие слова в принципе могут стоять рядом в предложении. Подходы, связанные с использованием неразмеченных данных для решения задач обучения с учителем, объединяются термином self-supervised learning и очень активно используются сейчас. Важной составляющей является обучение представлений (representation learning) — задача построения компактных векторов небольшой размерности из сложных по структуре данных, например, изображений, звука, текстов, графов, так, чтобы близкие по структуре или семантике данные получали метрически близкие представления. Делать это можно разными способами — например, используя фрагменты моделей, обученных для решения какой-либо другой задачи, или строя модель, предсказывающую скрытую часть объекта по оставшейся его части — например, пропущенное слово в предложении. Этому будет посвящен отдельный параграф нашего учебника. Но кроме количества данных важно ещё и то, насколько они хороши и удобны для анализа. Давайте разберёмся, что это значит и какие с этим бывают проблемы. Для работы с объектом модель должна опираться на какие-то его свойства, например, доход человека, цвет левого верхнего пикселя на изображении или частоту встречаемости слова «интеграл» в тексте. Эти свойства обычно называются признаками, а совокупность свойств, которые мы выделили у объекта – его признаковым описанием. Вот несколько простых и распространённых разновидностей признаков: Численные – например, рост или доход. Иногда отдельно",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 5,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "его части — например, пропущенное слово в предложении. Этому будет посвящен отдельный параграф нашего учебника. Но кроме количества данных важно ещё и то, насколько они хороши и удобны для анализа. Давайте разберёмся, что это значит и какие с этим бывают проблемы. Для работы с объектом модель должна опираться на какие-то его свойства, например, доход человека, цвет левого верхнего пикселя на изображении или частоту встречаемости слова «интеграл» в тексте. Эти свойства обычно называются признаками, а совокупность свойств, которые мы выделили у объекта – его признаковым описанием. Вот несколько простых и распространённых разновидностей признаков: Численные – например, рост или доход. Иногда отдельно выделяют вещественные и целочисленные признаки. Категориальные признаки принимают значения из некоторого дискретного множества. Например, профессия человека или день недели. Бинарные признаки принимают два значения: 0 0 и 1 1 или «да» и «нет». С ними можно работать и как с численными, и как с категориальными. Среди категориальных признаков иногда выделяют ординальные. Они принимают значения из некоторого упорядоченного дискретного множества. Например, класс опасности химического вещества (бывает от 1-го до 4-го) или год обучения для студента являются ординальными. Приходится иметь дело и с более сложно устроенными признаками. Например, описание ресторана может содержать тексты отзывов или фотографии, а профиль человека в социальной сети – список его друзей. Для многих однородных типов данных, таких как изображения, видео, тексты, звук, графы, разработано большое количество методов извлечения признаков – сейчас в первую очередь нейросетевых. О них вы сможете прочитать в разделах про нейросетевые архитектуры для соответствующих типов данных. Если же попадаются какие-то более сложно устроенные данные, могут потребоваться дополнительные усилия для извлечения из них признаков – этот процесс называют feature engineering. Удобно бывает записать данные в виде таблицы, строки которой соответствуют объектам, а столбцы – признакам. Например: Объекты Возраст Оценка по ML Наташа 21 отл Вася N/A удовл Игорь 47 хор Данные, представленные в таком виде, называются табличными. Табличные данные – один из самых удобных для анализа форматов. Свои успешные пайплайны работы есть также для уже упомянутых текстов, звука, изображений, видео, графов. Лучше всего, если все признаки являются численными. Тогда с таблицей можно работать как с объектом линейной алгебры – матрицей объекты-признаки. Создание информативного признакового описания очень важно для дальнейшего анализа. Но нужно также следить за качеством полученных данных. Вам могут встретиться, например, следующие проблемы: Пропуски (пропущенные значения). Так, в примере табличных данных выше нам неизвестен возраст Васи. Объекты или признаки, в которых есть пропуски, можно удалять из выборки, но если пропусков довольно много, мы можем потерять таким образом слишком много информации. Кроме того, наличие пропуска само по себе может нести информацию: скажем, это может говорить о систематической проблеме в сборе данных для того или иного сегмента выборки. Некоторые модели, например, решающие деревья, обладают собственными средствами для работы с пропусками, другие же – например, линейные модели или нейросети – требуют, чтобы пропуски были вычищены или заменены на что-то. Выбросы, то есть объекты, которые резко отличаются от большинства остальных. Например, в датасете с информацией о клиентах банка 140-летний человек, очевидно, будет весьма нетипичным. Выбросы могут возникать из-за ошибок при сборе данных или представлять собой реально существующие аномалии.",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 6,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "потерять таким образом слишком много информации. Кроме того, наличие пропуска само по себе может нести информацию: скажем, это может говорить о систематической проблеме в сборе данных для того или иного сегмента выборки. Некоторые модели, например, решающие деревья, обладают собственными средствами для работы с пропусками, другие же – например, линейные модели или нейросети – требуют, чтобы пропуски были вычищены или заменены на что-то. Выбросы, то есть объекты, которые резко отличаются от большинства остальных. Например, в датасете с информацией о клиентах банка 140-летний человек, очевидно, будет весьма нетипичным. Выбросы могут возникать из-за ошибок при сборе данных или представлять собой реально существующие аномалии. Обычно выбросы лучше удалять, но в некоторых случаях выбросами могут быть важные объекты (например, очень богатые клиенты банка), и тогда их, возможно, стоит отлавливать и обрабатывать отдельно. Ошибки разметки. Если, например, вы собираете данные с помощью разметчиков-людей, то вы должны быть готовы к тому, что часть таргетов будет отмечена неправильно. Даже если не думать о том, что не все из разметчиков совершенно честные и старательные, задача может оказаться для них сложной. Data drift. С течением времени данные могут меняться. Например, может измениться схема сбора данных, и они начнут приходить в формате, который вообще не обрабатывается моделью. Или же может поменяться распределение данных: скажем, если вы делали образовательный сервис для студентов, а к вам стали приходить и более зрелые люди. Data drift – это суровая реальность для любой системы, которая решает не сиюминутную задачу, поэтому нужно уметь мониторить распределение данных и, если нужно, обновлять модель. Встречаются и другие проблемы. Нередко существенную часть данных приходится выкидывать, потому что в процессе сбора что-нибудь сломалось или потому, что полгода назад в сервисе изменили систему логирования и более старые данные невозможно склеить с более новыми. Модель и алгоритм обучения Модель — это некоторый способ описания мира. Например, «Земля плоская» — это модель, и не такая плохая, как вам может показаться. Ей активно пользуются, когда всё происходит в масштабах одного города и кривизной поверхности можно пренебрегать. С другой стороны, если мы попробуем рассчитать кратчайший путь из Парижа в Лос-Анджелес, модель плоской Земли выдаст неадекватный ответ, она войдёт в противоречие с имеющимися данными, и её придётся заменить на «Земля круглая», «Земля имеет форму эллипсоида» и так далее — в той мере, в которой нам важна точность и в какой нам это позволяет (не)совершенство измерительной техники. Так, модель «Земля — это похожая на геоид с шершавостями на месте горных хребтов» очень точная и замечательная, но, возможно, будет избыточно сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений. В первых параграфах мы будем рассматривать в основном предсказательные модели, то есть модели вида y=f(x), которые пытаются уловить зависимость между признаковым описанием x x объекта и таргетом y y. Но порой мы будем иметь дело и с моделями данных: например, «такой-то признак имеет нормальное распределение». Чаще всего предсказательные модели мы будем брать из некоторого параметрического семейства y=f w (x), где w w — параметры, которые мы будем подбирать по данным. Для примера давайте возьмём задачу предсказания цены квартиры. В качестве класса моделей выберем константные функции f(x)=c (то",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 7,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сложной для большинства практических задач и при этом слишком тяжёлой в плане вычислений. В первых параграфах мы будем рассматривать в основном предсказательные модели, то есть модели вида y=f(x), которые пытаются уловить зависимость между признаковым описанием x x объекта и таргетом y y. Но порой мы будем иметь дело и с моделями данных: например, «такой-то признак имеет нормальное распределение». Чаще всего предсказательные модели мы будем брать из некоторого параметрического семейства y=f w (x), где w w — параметры, которые мы будем подбирать по данным. Для примера давайте возьмём задачу предсказания цены квартиры. В качестве класса моделей выберем константные функции f(x)=c (то есть будем для всех квартир предсказывать одно и то же значение цены). Поскольку значение не зависит от x x, нам не очень важно, в каком виде получено признаковое описание: это может быть набор совершенно любых сведений о квартире. Не забудем зафиксировать метрику качества — среднее абсолютное отклонение (mean absolute error, она же MAE). min ⁡ f , MAE(f,X,y)=L(f,X,y)= N 1 i=1 ∑ N ∣f(x i )−y i ∣→ f min , где f f — это модель (та самая, f(x)=c), X=(x 1 ,…,x N ) — обучающие примеры (данные о квартирах, которые мы смогли достать), y=(y 1 ,…,y N ) — правильные ответы (то есть цены на известные нам квартиры). Чтобы найти минимум MAE, возьмём производную от выражения min ⁡ f , N 1 i=1 ∑ N ∣c−y i ∣→ f min , и приравняем её к нулю: L(f,X,y)= N 1 i=1 ∑ N sign(c−y i )=0 #{i∣y i <c}−#{i∣y i >c}=0 Нам подходят точки c c, для которых число y i y i , строго меньших c c, равно числу y i y i , строго больших c c. Таким образом, нам подходит медиана набора ,…,y f(x)=median(y). Вопрос на подумать. Давайте теперь в задаче предсказания цены квартиры рассмотрим метрику среднеквадратичное отклонение (MSE): MSE(f,X,y)= N 1 i=1 ∑ N (f(x i )−y i ) 2 Каким будет оптимальное значение параметра c c для константной модели f(x)=c? Прекрасно, значит, в классе константных функций мы можем найти оптимальную модель. Может быть, это можно сделать и в каком-нибудь более интересном классе? Этому вопросу и будет посвящена большая часть нашей книги. Классический курс ML состоит из описания классов моделей и способов работы с ними. Несмотря на то что для решения большинства практических задач на сегодня достаточно знать только два типа моделей — градиентный бустинг на решающих деревьях и нейросетевые модели — мы постараемся рассказать и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов — уже в роли исследователя, а не просто инженера. Кроме выбора модели важен также выбор алгоритма обучения. Алгоритм обучения — это процедура, которая превращает обучающую выборку в обученную модель. Скажем, в примере выше для константной модели мы в качестве алгоритма обучения использовали поиск нуля градиента. Как мы увидим дальше, градиентные методы используются для обучения многих моделей, и это очень богатый класс методов оптимизации, из которого порой не",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 8,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и про другие, чтобы развить у вас глубинное понимание предмета и дать возможность не только использовать лучшие сложившиеся практики, но и, при вашем желании, участвовать в проработке новых идей и поиске новых методов — уже в роли исследователя, а не просто инженера. Кроме выбора модели важен также выбор алгоритма обучения. Алгоритм обучения — это процедура, которая превращает обучающую выборку в обученную модель. Скажем, в примере выше для константной модели мы в качестве алгоритма обучения использовали поиск нуля градиента. Как мы увидим дальше, градиентные методы используются для обучения многих моделей, и это очень богатый класс методов оптимизации, из которого порой не так просто выбрать лучший. В качестве примера рассмотрим задачу бинарной классификации точек на плоскости, для которой выберем линейную модель: linear2 Метрикой будет accuracy, то есть доля верных предсказаний. Теперь нам нужно по обучающей выборке подобрать оптимальную разделяющую прямую y=w 1 x+w 0 . Числа являются настраиваемыми (обучаемыми) параметрами модели, именно их будет по выборке восстанавливать алгоритм обучения. Но есть проблема: метрика accuracy не дифференцируема. Поэтому мы должны подобрать другую дифференцируемую функцию L(X,y,w), минимизация которой будет более или менее соответствовать оптимизации вероятности. Такая функция называется функцией потерь, лоссом (от слова loss) или лосс-функцией. О том, как могут выглядеть лосс-функции для бинарной линейной классификации, вы можете почитать в параграфе про линейные модели. В качестве алгоритма обучения мы можем взять теперь градиентный спуск: t+1 =w t −α∇ w L, где α α — шаг оптимизации — коэффициент, влияющий на скорость и устойчивость алгоритма. Отметим, что разный выбор коэффициента α α, вообще говоря, даёт разные алгоритмы обучения, которые могут приводить к разным результатам: если α α слишком мал, то спуск может не дойти до оптимума, а если слишком велик, то алгоритм будет «скакать» вокруг оптимума и никогда туда не попадёт. Мы видим, что важен не только выбор модели, но и выбор алгоритма обучения. Число α α является гиперпараметром алгоритма, то есть задаётся до начала обучения — но его тоже можно подбирать по данным. Более подробно о подборе гиперпараметров вы можете узнать в соответствующем параграфе. Выбор модели, переобучение Может показаться, что мы вас обманули, когда пугали сложностями: очевидно, что для любой задачи машинного обучения можно построить идеальную модель, надо всего лишь запомнить всю обучающую выборку с ответами. Такая модель может достичь идеального качества по любой метрике, но радости от неё довольно мало, ведь мы хотим, чтобы она выявила какие-то закономерности в данных и помогла нам с ответами там, где мы их не знаем. Важно понимать, какая у построенной модели обобщающая способность, то есть насколько она способна выучить общие закономерности, присущие не только обучающей выборке, и давать адекватные предсказания на новых данных. Для того чтобы предохранить себя от конфуза, поступают обычно так: делят выборку с данными на две части: обучающую выборку и тестовую выборку (train и test). Обучающую выборку используют для собственно обучения модели, а метрики считают на тестовой. Такой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла генерализация (generalization), то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 9,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "какая у построенной модели обобщающая способность, то есть насколько она способна выучить общие закономерности, присущие не только обучающей выборке, и давать адекватные предсказания на новых данных. Для того чтобы предохранить себя от конфуза, поступают обычно так: делят выборку с данными на две части: обучающую выборку и тестовую выборку (train и test). Обучающую выборку используют для собственно обучения модели, а метрики считают на тестовой. Такой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла генерализация (generalization), то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать полезные предсказания для объектов, которых не видели. Например, рассмотрим три модели регрессионной зависимости, построенные на одном и том же синтетическом датасете с одним-единственным признаком. Жёлтым нарисованы точки обучающей выборки. Здесь мы представим, что есть «истинная» закономерность (пунктир), которая искажена шумом (погрешности измерения, влияние других факторов и т.д.). three Левая, линейная модель недостаточно хороша: она сделала, что могла, но плохо приближает зависимость, особенно при маленьких и при больших x x. Правая «запомнила» всю обучающую выборку (и в самом деле, чтобы вычислить значение этой функции, нам надо знать координаты всех исходных точек) вместо того, чтобы моделировать исходную зависимость. Наконец, центральная, хоть и не проходит через точки обучающей выборки, довольно неплохо моделирует истинную зависимость. Алгоритм, избыточно подстроившийся под данные, называют переобученным. С увеличением сложности модели ошибка на обучающей выборке падает. Во многих задачах очень сложная модель будет работать примерно так же, как модель, «просто запомнившая всю обучающую выборку», но с генерализацией всё будет плохо: ведь выученные закономерности будут слишком специфическими, подогнанными под то, что происходит на обучающей выборке. Мы видим это на трёх графиках сверху: линейная функция очень проста, но и закономерность приближает лишь очень грубо; на правом же графике мы видим довольно хитрую функцию, которая точно подобрана под значения из обучающей выборки, но явно слишком эксцентрична, чтобы соответствовать какой-то природной зависимости. Оптимальная же генерализация достигается на модели не слишком сложной и не слишком простой. В качестве иллюстрации для того же самого датасета рассмотрим модели вида y = многочлен степени D y = многочлен степени D Ясно, что с ростом D D сложность модели растёт, и она достигает всё лучшего качества на обучающей выборке. А что, если у нас есть ещё тестовая выборка? Каким будет качество на ней? Вот так могут выглядеть графики среднеквадратичного отклонения (MSE) для обучающей и тестовой выборок: train Мы видим здесь типичную для классических моделей картину: MSE на обучающей выборке падает (может быть, даже до нуля), а на тестовой сперва падает, а затем начинает снова расти. Замечание. Для моделей глубинного обучения всё немного интереснее: в некоторых ситуациях есть грань, за которой метрика на тестовой выборке снова начинает падать. Но об этом в своё время. Пока давайте запомним, что слишком сложная модель — это вредно, а переобучение — боль. Точный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной —",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 10,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "выборке падает (может быть, даже до нуля), а на тестовой сперва падает, а затем начинает снова расти. Замечание. Для моделей глубинного обучения всё немного интереснее: в некоторых ситуациях есть грань, за которой метрика на тестовой выборке снова начинает падать. Но об этом в своё время. Пока давайте запомним, что слишком сложная модель — это вредно, а переобучение — боль. Точный способ выбрать алгоритм оптимальной сложности по данной задаче нам пока неизвестен, хотя какую-то теоретическую базу имеющимся философским наблюдениям мы дадим в главе про теорию обучения; при этом есть хорошо продуманная методология сравнения разных моделей и выбора среди них оптимальной — об этом мы обязательно расскажем вам в следующих главах. А пока дадим самый простой и неизменно ценный совет: не забывайте считать метрики на тестовой выборке и никогда не смешивайте её с обучающей! Вопрос на подумать. Обсуждая переобучение, мы упоминали про сложность модели, но не сказали, что это такое. Как бы вы её определили? Как описать / сравнить сложность моделей для двух приведённых ниже задач? Почему, кстати, мы решили, что средняя модель ОК, а правая переобученная? regression classification После обучения В момент, когда подобраны все обучаемые параметры и гиперпараметры модели, работа специалиста по машинному обучению не заканчивается. Во-первых, модель чаще всего создают для того, чтобы она работала в некотором продакшене. И чтобы она там оказалась, нужно эффективно её закодить, научить работать параллельно и подружить с используемыми вами фреймворками. Процесс выкатки в продакшен называется словом деплой или деплоймент (от deploy). После деплоя можно посчитать онлайн-метрики. Также имеет смысл провести АБ-тестирование, то есть сравнение с предыдущей версией модели на случайно выбранных подмножествах пользователей или сессий. Более подробно об АБ-тестировании вы сможете почитать в соответствующем параграфе. Если новая модель работает не очень здорово, должна быть возможность откатиться к старой. После деплоймента модели важно продолжать дообучать или переобучать её при поступлении новых данных, а также мониторить качество. Мы уже обсуждали data drift, но бывает также и concept drift — изменение зависимости между признаками и таргетом. Например, если вы делаете музыкальные рекомендации, вам нужно будет учитывать и появление новых треков, и изменение вкусов аудитории. О мониторинге качества моделей мы подробнее расскажем в соответствующем параграфе. Теперь предлагаем вам потренировать изученный материал на практике. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике! Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Выполните задачи урока 0 / 11 выполнено Сообщить об ошибке Предыдущий параграф 1.2. Первые шаги В этой главе мы поговорим о рабочем окружении ML-специалиста — какие сервисы и библиотеки в него входят, как его развернуть, на что обратить внимание. А кроме того, в качестве быстрой практики обучим собственную модель генерировать ответы в стиле Льва Толстого. Следующий параграф 2.1. Линейные модели Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая классификация",
    "metadata": {
      "title": "Машинное обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/mashinnoye-obucheniye",
      "course": "ml",
      "chapter": "1. Введение",
      "chapter_id": "1.3",
      "part": 11,
      "total_parts": 11,
      "source_file": "1.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Методы кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризации Задача кластеризации В задаче классификации мы имели дело с восстановлением отображения из множества объектов в конечный набор меток классов. При этом классы были зафиксированы заранее, то есть мы с самого начала примерно понимали, какого рода объекты должны относиться к каждому из них, и мы располагали обучающей выборкой с примерами объектов и классов, к которым они относятся. В задаче кластеризации мы тоже разбиваем объекты на конечное множество классов, но у нас нет ни обучающей выборки, ни понимания, какой будет природа этих классов. То, что модель кластеризации какие-то объекты сочла «похожими», отнеся к одному классу, будет новой информацией, «открытием», сделанным этой моделью. Обучающей выборки у нас также не будет: ведь мы не знаем заранее, что за классы получатся (а иногда и сколько их будет). Таким образом, кластеризация — это задача обучения без учителя. Из-за общего сходства постановок задач в литературе кластеризацию иногда называют unsupervised classification. Методы кластеризации часто применяют, когда фактически нужно решить задачу классификации, но обучающую выборку собрать затруднительно (дорого или долго). При этом валидационную выборку для оценки результатов кластеризации собрать значительно проще, так как для неё требуется меньше примеров. При этом стоит помнить, что точность работы supervised-методов значительно выше. Поэтому, если обучающую выборку всё-таки можно собрать, лучше решать задачу классификации, чем задачу кластеризации. Примеры задач кластеризации Хороший пример применения методов кластеризации — анализ геоданных. В мобильных приложениях, собирающих геоданные пользователей, часто требуется понять, где именно пользователь находился. GPS-координаты известны с некоторой погрешностью, пользователь тоже обычно двигается, поэтому вместо точного положения часто приходится иметь дело с роем точек. Положение усугубляется, когда мы пытаемся анализировать поведение сразу тысяч людей в какой-то локации — например, определить, в каких точках люди чаще всего садятся в такси у аэропорта. Может показаться, что достаточно посмотреть на данные — и мы увидим в точности нужные нам кластеры. Изображение ниже показывает, как может выглядеть ситуация всего для нескольких пользователей: согласно данным GPS, такси подбирают пассажиров и внутри здания аэропорта, и на взлётной полосе, и там, где это происходит на самом деле: 19 Подобная задача решалась в Яндекс.Такси при разработке пикап-пойнтов (наиболее удобных точек вызова такси, подсвечиваемых в приложении). Координаты точек заказа кластеризовались таким образом, чтобы кластер соответствовал какому-то одному, удобному для пользователя месту, и центры кластеров использовались как кандидаты в пикап-пойнты. Те кандидаты, которые удовлетворяли простым фильтрам (например, не попадали в здание или в воду), использовались в приложении. При этом не обходилось и без вручную проставленных пикап-пойнтов: например, такое решение использовалось в окрестностях аэропортов. Другой пример кластеризации геоданных, который всегда рядом с нами, — это интерфейсы для просмотра фотографий в вашем смартфоне. Почти наверняка вы можете просмотреть их в привязке к местам, где они были сделаны, и по мере масштабирования карты вы будете видеть разное количество кластеров фотографий. Кстати, если говорить об интерфейсах, то есть и другой интересный пример: если нужно подстроить цветовую схему вашего интерфейса под выбираемое пользователем изображение (например, фоновую картинку), достаточно кластеризовать цвета из пользовательского изображения, используя RGB-представление (или любое другое) как признаки цвета, и воспользоваться для оформления цветами, соответствующими центрам кластеров. Простейшие методы кластеризации с",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 1,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "решение использовалось в окрестностях аэропортов. Другой пример кластеризации геоданных, который всегда рядом с нами, — это интерфейсы для просмотра фотографий в вашем смартфоне. Почти наверняка вы можете просмотреть их в привязке к местам, где они были сделаны, и по мере масштабирования карты вы будете видеть разное количество кластеров фотографий. Кстати, если говорить об интерфейсах, то есть и другой интересный пример: если нужно подстроить цветовую схему вашего интерфейса под выбираемое пользователем изображение (например, фоновую картинку), достаточно кластеризовать цвета из пользовательского изображения, используя RGB-представление (или любое другое) как признаки цвета, и воспользоваться для оформления цветами, соответствующими центрам кластеров. Простейшие методы кластеризации с помощью графов Можно приводить примеры не только про геоаналитику, однако тема геоданных поможет нам придумать пару наиболее простых и наглядных методов кластеризации. Представим, что перед нами рой геокоординат и нам нужно предложить по этим данным пикап-пойнты для такси. Разберём пару очевидных методов. Выделение компонент связности Логично попробовать объединить точки, которые находятся друг от друга на расстоянии двух-трёх метров, а потом просто выбрать наиболее популярные места. Для этого давайте построим на известных нам точках граф: точки, расстояние между которыми в пределах трёх метров, мы соединим рёбрами. Выделим в этом графе компоненты связности, они и будут нашими кластерами. У этого способа есть пара очевидных недостатков. Во-первых, может найтись сколько угодно длинная цепочка точек, в которой соседние отстоят друг от друга на пару метров, — и вся она попадёт в одну компоненту связности. В итоге наша отсечка по трём метрам имеет очень опосредованное отношение к диаметру кластеров, а сами кластеры будут получаться значительно больше, чем нам хотелось бы. Во-вторых (и с первой проблемой это тоже связано), непонятно, как мы выбираем максимальное расстояние, при котором соединяем точки ребром. В данной задаче ещё можно предъявить хоть какую-то логику, а вот если бы мы кластеризовали не геометки, а что-то многомерное, например электронные письма по их тематике, придумать отсечку было бы уже сложнее. Если наша цель — не только решить практическую задачу, но и придумать достаточно общий метод кластеризации, понятно, что нам хочется понимать, как подбирать параметры этого метода (в данном случае условие добавления рёбер в граф). Эти соображения могут привести нас к другому решению. Минимальное остовное дерево Вместо того чтобы проводить рёбра в графе, давайте их удалять. Построим минимальное остовное дерево, считая расстояния между точками весами рёбер. Тогда, удалив N N рёбер с наибольшим весом, мы получим N + 1 N+1 компоненту связности, которые, как и в прошлом подходе, будем считать кластерами. Различие в том, что теперь нам нужно задавать не расстояние, при котором проводится ребро, а количество кластеров. С одной стороны, если мы решаем задачу расчёта пикап-пойнтов в какой-то конкретной локации (аэропорт, торговый центр, жилой дом), нам может быть понятно, сколько примерно пикап-пойнтов мы хотим получить. С другой стороны, даже без локального рассмотрения можно просто сделать достаточно много кластеров, чтобы было из чего выбирать, но при этом достаточно мало, чтобы в каждый кластер попадало репрезентативное количество точек. Аналогичная логика будет справедлива и во многих других задачах кластеризации: количество кластеров — достаточно общий и достаточно хорошо интерпретируемый параметр, чтобы настраивать его вручную, поэтому во многих методах",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 2,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "теперь нам нужно задавать не расстояние, при котором проводится ребро, а количество кластеров. С одной стороны, если мы решаем задачу расчёта пикап-пойнтов в какой-то конкретной локации (аэропорт, торговый центр, жилой дом), нам может быть понятно, сколько примерно пикап-пойнтов мы хотим получить. С другой стороны, даже без локального рассмотрения можно просто сделать достаточно много кластеров, чтобы было из чего выбирать, но при этом достаточно мало, чтобы в каждый кластер попадало репрезентативное количество точек. Аналогичная логика будет справедлива и во многих других задачах кластеризации: количество кластеров — достаточно общий и достаточно хорошо интерпретируемый параметр, чтобы настраивать его вручную, поэтому во многих методах кластеризации количество кластеров выступает как гиперпараметр. Далее будем рассматривать некоторую обобщённую задачу кластеризации без привязки к нашему примеру с анализом геоданных. Мы приведём три наиболее популярных метода кластеризации — k-средних, иерархическую кластеризацию и DBSCAN, а затем рассмотрим вопросы оценки качества кластеризации. Метод K средних Пожалуй, один из наиболее популярных методов кластеризации — это метод K-средних (K-means). Основная идея метода — итеративное повторение двух шагов: распределение объектов выборки по кластерам; пересчёт центров кластеров. В начале работы алгоритма выбираются K K случайных центров в пространстве признаков. Каждый объект выборки относят к тому кластеру, к центру которого объект оказался ближе. Далее центры кластеров пересчитывают как среднее арифметическое векторов признаков всех вошедших в этот кластер объектов (то есть центр масс кластера). Как только мы обновили центры кластеров, объекты заново перераспределяются по ним, а затем можно снова уточнить положение центров. Процесс продолжается до тех пор, пока центры кластеров не перестанут меняться. kmeans Выбор начального приближения Первый вопрос при выборе начального положения центров — как, выбирая центры из некоторого случайного распределения, не попасть в область пространства признаков, где нет точек выборки. Базовое решение — просто выбрать в качестве центров какие-то из объектов выборки. Вторая потенциальная проблема — кучное размещение центров. В этом случае их начальное положение с большой вероятностью окажется далёким от итогового положения центров кластеров. Например, для таких изначальных положений центров 19 мы получим неправильную кластеризацию. 19 Чтобы бороться с этим явлением, выгодно брать максимально удаленные друг от друга центры. На практике работает следующая эвристика: первый центр выбираем случайно из равномерного распределения на точках выборки; каждый следующий центр выбираем из случайного распределения на объектах выборки, в котором вероятность выбрать объект пропорциональна квадрату расстояния от него до ближайшего к нему центра кластера. Модификация K-means, использующая эту эвристику для выбора начальных приближений, называется K-means++. Выбор метрик Так как работа метода K-средних состоит из последовательного повторения до сходимости двух шагов, обоснованность применения различных метрик (расстояний между точками, а не метрик качества 😃 или функций близости связана с тем, «ломают» они какой-либо из этих шагов или нет. Первый шаг с отнесением объектов к ближайшим центрам не зависит от вида метрики. Второй шаг предполагает пересчёт центров как среднего арифметического входящих в кластер точек, и вот здесь будет подвох: к оптимальности выбора центров в среднем арифметическом приводит именно евклидова метрика (подробнее в разделе «Что оптимизирует K-means»). Однако на практике никто не мешает использовать метод и без должного обоснования, поэтому можно экспериментировать с любыми расстояниями, с той лишь оговоркой, что не будет",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 3,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "двух шагов, обоснованность применения различных метрик (расстояний между точками, а не метрик качества 😃 или функций близости связана с тем, «ломают» они какой-либо из этих шагов или нет. Первый шаг с отнесением объектов к ближайшим центрам не зависит от вида метрики. Второй шаг предполагает пересчёт центров как среднего арифметического входящих в кластер точек, и вот здесь будет подвох: к оптимальности выбора центров в среднем арифметическом приводит именно евклидова метрика (подробнее в разделе «Что оптимизирует K-means»). Однако на практике никто не мешает использовать метод и без должного обоснования, поэтому можно экспериментировать с любыми расстояниями, с той лишь оговоркой, что не будет никаких теоретических гарантий, что метод сработает. Наиболее распространённая альтернатива евклидовой метрике — это косинусная мера близости векторов (она особенно популярна в задачах анализа текстов): CosineSimilarity(μ ⋅∥x При её использовании стоит не забывать, что косинусная мера — это функция близости, а не расстояние, так что чем больше её значения, тем ближе друг к другу векторы. Mini-batch K-means Несложно заметить, что, если считать K K и размерность пространства признаков константами, оба шага алгоритма работают за O ( n ) O(n), где n — количество объектов обучающей выборки. Отсюда возникает идея ускорения работы алгоритма. В mini-batch K-means мы не считаем шаги сразу на всей выборке, а на каждой итерации выбираем случайную подвыборку (мини-батч) и работаем на ней. В случае когда исходная выборка очень велика, переход к пакетной обработке не приводит к большой потере качества, зато значительно ускоряет работу алгоритма. Понижение размерности С другой стороны, вычисление расстояний и средних делается за O ( d ) O(d), где d d — размерность пространства признаков, так что другая идея ускорения K-means — это предварительно понизить размерность пространства признаков (с помощью PCA или эмбеддингов). Особенно удачно эта идея работает в задачах кластеризации текстов, когда K-means применяют на эмбеддингах слов: получается выиграть не только в скорости работы, но и в интерпретируемости результатов кластеризации. Кстати, сам алгоритм кластеризации тоже можно использовать как метод понижения размерности. Если вы решаете задачу обучения с учителем и пространство признаков очень разнообразно (то есть обучающая выборка не даёт вам достаточно статистики при столь большом числе признаков), можно выполнить кластеризацию объектов выборки на 500 или 1000 кластеров и оперировать попаданием объектов в какой-то кластер как признаком. Такой подход называется квантизацией пространства признаков (feature space quantization) и часто помогает на практике, когда нужно огрубить признаки, добавить им интерпретируемости или же, наоборот, обезличить. Хрестоматийный пример такого использования кластеризации — метод bag of visual words, расширяющий bag of words из анализа текстов на работу с изображениями. Идея метода в том, чтобы строить признаковое описание изображений на основе входящих в него фрагментов: так, изображения с лицами будут содержать фрагменты с носом, глазами, ртом, а изображения с машинами — колёса, зеркала, двери. Но проблема здесь в том, что нарезать такие фрагменты из обучающей выборки и искать точные совпадения в новых примерах изображений, которые нужно классифицировать, — безнадёжная затея. В жизни фрагменты изображений не повторяются в других изображениях с попиксельной точностью. Решение этой проблемы оказалось возможным при помощи алгоритмов кластеризации (исторически использовался именно K-means): фрагменты изображений из обучающей выборки кластеризовали на",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 4,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "words из анализа текстов на работу с изображениями. Идея метода в том, чтобы строить признаковое описание изображений на основе входящих в него фрагментов: так, изображения с лицами будут содержать фрагменты с носом, глазами, ртом, а изображения с машинами — колёса, зеркала, двери. Но проблема здесь в том, что нарезать такие фрагменты из обучающей выборки и искать точные совпадения в новых примерах изображений, которые нужно классифицировать, — безнадёжная затея. В жизни фрагменты изображений не повторяются в других изображениях с попиксельной точностью. Решение этой проблемы оказалось возможным при помощи алгоритмов кластеризации (исторически использовался именно K-means): фрагменты изображений из обучающей выборки кластеризовали на 100–1000 кластеров («визуальных слов»), а проходясь по новым изображениям, также нарезали их на фрагменты и относили к одному из этих кластеров. В итоге как новые изображения, так и изображения из обучающей выборки можно было описать количеством вхождений в них фрагментов из различных кластеров («визуальных слов»), так же как в анализе текстов описывают текст количеством вхождений в него слов из словаря. В таком признаковом пространстве уже можно было успешно обучать модели машинного обучения. Сейчас выделение «визуальных слов» в задаче классификации изображений происходит автоматически: с одной стороны, задачи компьютерного зрения теперь решаются нейросетями, но с другой стороны — если мы визуализируем отдельные слои этих нейросетей, станет понятно, что их логика работы во многом похожа на описанную выше. При этом идея квантизации признаков не утратила своей актуальности. Вот лишь несколько современных примеров её применения: Если вам необходимо дать возможность заказчику (например, внешней компании) анализировать используемые вами признаки — отсутствие провалов в данных и какие-то другие общие показатели, но нельзя отдавать признаки как есть (например, из-за законодательства, регулирующего передачу пользовательских данных), возможное решение — это агрегировать признаки по кластерам. Та же цель может быть отчасти достигнута, если перейти к самим кластерам как к признакам, чтобы скрыть исходные признаки. Переход к кластерам может быть сделан не с целью что-то скрыть, а наоборот, с целью повысить интерпретируемость: исходные сырые данные часто не вполне понятны бизнесу, но позволяют построить маркетинговые сегменты по различным коммерческим интересам пользователей, из-за чего становится удобно показывать принадлежность к этим сегментам как исходные признаки, не вдаваясь в детали о том, на каких данных эти сегменты построены. Для ускорения поиска похожих объектов в пространстве признаков вы также можете проводить поиск внутри того же кластера и соседних кластеров, так что за счёт «огрублённого» вида признаков какие-то процессы можно ещё и ускорить. Что оптимизирует K-means Проговорим на интуитивном уровне, какую оптимизационную задачу решает K-means. Оба шага алгоритма работают на уменьшение среднего квадрата евклидова расстояния от объектов до центров их кластеров: k=1 ∑ K i=1 I[a(x i )=k] На шаге отнесения объектов к одному из кластеров мы выбираем кластер с ближайшим центроидом, то есть минимизируем каждое слагаемое в Φ 0 Φ 0 : все потенциально большие слагаемые мы зануляем, а оставляем ненулевыми только наименьшие из возможных (при условии фиксирования центров кластеров). На шаге пересчёта центров кластеров мы выбираем центр таким образом, чтобы при фиксированном наборе объектов, относящихся к кластеру, для всех k k минимизировать выражение, стоящее под суммой по i=1 I[a(x i )=k] Здесь уже становится принципиально,",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 5,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "алгоритма работают на уменьшение среднего квадрата евклидова расстояния от объектов до центров их кластеров: k=1 ∑ K i=1 I[a(x i )=k] На шаге отнесения объектов к одному из кластеров мы выбираем кластер с ближайшим центроидом, то есть минимизируем каждое слагаемое в Φ 0 Φ 0 : все потенциально большие слагаемые мы зануляем, а оставляем ненулевыми только наименьшие из возможных (при условии фиксирования центров кластеров). На шаге пересчёта центров кластеров мы выбираем центр таким образом, чтобы при фиксированном наборе объектов, относящихся к кластеру, для всех k k минимизировать выражение, стоящее под суммой по i=1 I[a(x i )=k] Здесь уже становится принципиально, что мы определяем квадрат расстояния как квадрат разности векторов, так как именно отсюда при дифференцировании по μ k μ k и записи необходимого условия экстремума получается, что центры кластеров нужно пересчитывать как средние арифметические x i x i , принадлежащих кластеру. Этих соображений, конечно, недостаточно, чтобы утверждать, что мы найдём минимум Φ 0 Φ 0 . Более того, гарантии того, что мы найдём глобальный минимум, вообще говоря, нет. Однако, потратив чуть больше усилий, можно доказать, что процесс сойдётся в один из локальных минимумов. Также можно справедливо заметить, что, так как любой центр кластера — это среднее арифметическое входящих в кластер объектов x i x i , на выборке фиксированного размера есть лишь конечное множество потенциальных центров кластеров. Если предположить, что в ходе работы K-means не зацикливается, отсюда следует, что рано или поздно центры кластеров не изменятся на следующем шаге и алгоритм сойдётся. При этом фактическая сходимость, конечно же, происходит задолго до полного перебора всех возможных центров кластеров. Иерархическая агломеративная кластеризация Другой классический метод кластеризации — это иерархическая кластеризация. Иногда дополнительно уточняют: иерархическая агломеративная кластеризация. Название указывает сразу на два обстоятельства. Во-первых, есть деление алгоритмов кластеризации на агломеративные (agglomerative) и дивизивные, или дивизионные (divisive). Агломеративные алгоритмы начинают с небольших кластеров (обычно с кластеров, состоящих из одного объекта) и постепенно объединяют их в кластеры побольше. Дивизивные начинают с больших кластеров (обычно – с одного единственного кластера) и постепенно делят на кластеры поменьше. 19 Во-вторых, кластеризация бывает, по аналогии с оргструктурой в организациях, плоской (когда все кластеры равноправны и находятся на одном уровне кластеризации) и иерархической (когда кластеры бывают вложены друг в друга и образуют древовидную структуру). В случае иерархической агломеративной кластеризации мы действительно будем начинать с кластеров из одного объекта, постепенно объединяя их, а уже последовательность этих объединений даст структуру вложенности кластеров. Даже если в итоге мы будем использовать кластеры с одного уровня, не углубляясь ни в какую вложенность, кластеризация всё равно называется иерархической, так как иерархия естественным образом возникает в процессе работы алгоритма. Сам алгоритм выглядит предельно просто: Создаём столько кластеров, сколько у нас объектов в выборке, каждый объект — в своём отдельном кластере. Повторяем итеративно слияние двух ближайших кластеров, пока не выполнится критерий останова. Расстояния в иерархической кластеризации Как измерить расстояние между кластерами из одного объекта? Нужно просто взять расстояние между этими объектами. Остаётся вопрос, как обобщить расстояние между объектами до расстояния между кластерами (если в них более одного объекта). Традиционные решения — брать среднее расстояние между объектами кластеров, минимальное",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 6,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "уровня, не углубляясь ни в какую вложенность, кластеризация всё равно называется иерархической, так как иерархия естественным образом возникает в процессе работы алгоритма. Сам алгоритм выглядит предельно просто: Создаём столько кластеров, сколько у нас объектов в выборке, каждый объект — в своём отдельном кластере. Повторяем итеративно слияние двух ближайших кластеров, пока не выполнится критерий останова. Расстояния в иерархической кластеризации Как измерить расстояние между кластерами из одного объекта? Нужно просто взять расстояние между этими объектами. Остаётся вопрос, как обобщить расстояние между объектами до расстояния между кластерами (если в них более одного объекта). Традиционные решения — брать среднее расстояние между объектами кластеров, минимальное расстояние или максимальное. Если обозначить кластеры U U и V V, расстояние между ними в этом случае можем вычислять по одной из формул: avg (U,V)= ∣U∣⋅∣V∣ 1 u∈U ∑ v∈V ∑ ρ(u,v) min min (U,V)= (u,v)∈U×V min ρ(u,v) max max (U,V)= (u,v)∈U×V max ρ(u,v) Используемая формула расстояния между кластерами — один из гиперпараметров алгоритма. Кроме приведённых стандартных вариантов бывают и более экзотичные, например расстояние Уорда (Ward distance). В наиболее общем виде способы задания расстояния между кластерами даются формулой Ланса — Уильямса (Lance — Williams; более подробно вы можете почитать в этой статье). Сами же расстояния между объектами можно задавать любой метрикой, как евклидовой, так и манхэттенским расстоянием или, например, косинусной мерой (с той лишь поправкой, что это мера близости, а не расстояние). Условия окончания работы алгоритма (критерии останова) В качестве условия для завершения работы алгоритма можем выбрать либо получение нужного количества кластеров (количество кластеров может быть гиперпараметром алгоритма), либо выполнение эвристик на основе расстояния между объединяемыми кластерами (например, если расстояние сливаемых кластеров значительно выросло по сравнению с прошлой итерацией). На практике же обычно кластеризацию проводят вплоть до одного кластера, включающего в себя всю выборку, а затем анализируют получившуюся иерархическую структуру с помощью дендрограммы. Дендрограмма По мере объединения кластеров, каждой итерации алгоритма соответствует пара объединяемых на этой итерации кластеров, а также расстояние между кластерами в момент слияния. Расстояния с ростом итерации будут только увеличиваться, поэтому возникает возможность построить следующую схему, называемую дендрограммой: 19 Здесь по горизонтали внизу отмечены объекты кластеризуемой выборки, под горизонтальной осью подписаны номера объектов, а их расположение вдоль оси продиктовано только эстетическими соображениями: нам удобно строить дендрограмму так, чтобы никакие дуги в ней не пересекались. По вертикали отложены расстояния между кластерами в момент слияния. Когда происходит объединение кластеров, состоящих из нескольких объектов, соответствующая этой итерации алгоритма дуга идёт не до конкретных объектов выборки, а до дуги другого кластера. Таким образом мы получаем наглядную визуализацию древовидной структуры процесса кластеризации. В частности, на дендрограмме мы можем визуально заметить, в какой момент происходит скачок расстояний между кластерами, и попытаться определить «естественное» количество кластеров в нашей задаче. На практике же это соображение, как правило, остаётся лишь красивой теорией, так как любую кластеризацию можно делать в разной степени «мелкой» и «естественного» количества кластеров в практических задачах часто не существует. В случае же если данные были получены таким образом, что в них действительно есть какое-то естественное количество кластеров, иерархическая кластеризация обычно справляется с определением числа кластеров по дендрограмме заметно хуже, чем DBSCAN. Именно алгоритму",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 7,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Таким образом мы получаем наглядную визуализацию древовидной структуры процесса кластеризации. В частности, на дендрограмме мы можем визуально заметить, в какой момент происходит скачок расстояний между кластерами, и попытаться определить «естественное» количество кластеров в нашей задаче. На практике же это соображение, как правило, остаётся лишь красивой теорией, так как любую кластеризацию можно делать в разной степени «мелкой» и «естественного» количества кластеров в практических задачах часто не существует. В случае же если данные были получены таким образом, что в них действительно есть какое-то естественное количество кластеров, иерархическая кластеризация обычно справляется с определением числа кластеров по дендрограмме заметно хуже, чем DBSCAN. Именно алгоритму DBSCAN мы и посвятим следующий раздел. DBSCAN Алгоритм DBSCAN (Density-based spatial clustering of applications with noise) развивает идею кластеризации с помощью выделения связных компонент. Прежде чем перейти к построению графа, введём понятие плотности объектов выборки в пространстве признаков. Плотность в DBSCAN определяется в окрестности каждого объекта выборки x i x i как количество других точек выборки в шаре B(ε,x i ). Кроме радиуса ε ε окрестности в качестве гиперпараметра алгоритма задается порог N 0 N 0 по количеству точек в окрестности. Далее все объекты выборки делятся на три типа: внутренние / основные точки (core points), граничные (border points) и шумовые точки (noise points). К основным относятся точки, в окрестности которых больше N 0 N 0 объектов выборки. К граничным — точки, в окрестности которых есть основные, но общее количество точек в окрестности меньше N 0 N 0 . Шумовыми называют точки, в окрестности которых нет основных точек и в целом содержится менее N 0 N 0 объектов выборки. Алгоритм кластеризации выглядит следующим образом: Шумовые точки убираются из рассмотрения и не приписываются ни к какому кластеру. Основные точки, у которых есть общая окрестность, соединяются ребром. В полученном графе выделяются компоненты связности. Каждая граничная точка относится к тому кластеру, в который попала ближайшая к ней основная точка. Удобство DBSCAN заключается в том, что он сам определяет количество кластеров (по модулю задания других гиперпараметров — ε ε и N 0 N 0 ), а также в том, что метод успешно справляется даже с достаточно сложными формами кластеров. Кластеры могут иметь вид протяжённых лент или быть вложенными друг в друга как концентрические гиперсферы. На изображении ниже показан пример выделения кластеров достаточно сложной формы с помощью DBSCAN: 19 DBSCAN — один из самых сильных алгоритмов кластеризации, но работает он, как правило, заметно дольше, чем mini-batch K-means, к тому же весьма чувствителен к размерности пространства признаков, поэтому используется на практике DBSCAN только тогда, когда успевает отрабатывать за приемлемое время. Какой метод кластеризации выбирать? Если сравнивать частоту использования K-means, иерархической кластеризации и DBSCAN, то на первом месте, бесспорно, будет K-means, а второе место будут делить иерархический подход и DBSCAN. Иерархическая кластеризация — более известный и простой в понимании метод, чем DBSCAN, но довольно редко отрабатывающий качественно. Частая проблема иерархической кластеризации — раннее образование одного гигантского кластера и ряда очень небольших, что приводит к сильной несбалансированности количества объектов в итоговых кластерах. В то же время DBSCAN — менее широко известный подход, но, когда его можно применить, качество, как",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 8,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "поэтому используется на практике DBSCAN только тогда, когда успевает отрабатывать за приемлемое время. Какой метод кластеризации выбирать? Если сравнивать частоту использования K-means, иерархической кластеризации и DBSCAN, то на первом месте, бесспорно, будет K-means, а второе место будут делить иерархический подход и DBSCAN. Иерархическая кластеризация — более известный и простой в понимании метод, чем DBSCAN, но довольно редко отрабатывающий качественно. Частая проблема иерархической кластеризации — раннее образование одного гигантского кластера и ряда очень небольших, что приводит к сильной несбалансированности количества объектов в итоговых кластерах. В то же время DBSCAN — менее широко известный подход, но, когда его можно применить, качество, как правило, получается выше, чем в K-means или иерархической кластеризации. Оценка качества кластеризации Далее приведём список основных метрик качества кластеризации и обсудим некоторые особенности их применения. Среднее внутрикластерное расстояние Смысл среднего внутрикластерного расстояния максимально соответствует названию: i=1 ∑ n j=i ∑ n I[a(x i )=a(x j )] i=1 ∑ n j=i ∑ n ρ(x i ,x j )I[a(x i )=a(x j )] Сумма расстояний между точками из одного и того же кластера делится на количество пар точек, принадлежащих к одному кластеру. В приведённой выше формуле пары вида ) включены в рассмотрение, чтобы избежать неопределённости 0 0 0 0 в случае, когда в каждом кластере ровно по одному объекту. Однако иногда записывают суммы по i < j i<j, просто доопределяя F 0 F 0 в описанном случае нулём. Решая задачу кластеризации, мы хотим по возможности получать как можно более кучные кластеры, то есть минимизировать F 0 F 0 . В случае если у кластеров есть центры μ k μ k , часто рассматривается аналогичная метрика — средний квадрат внутрикластерного расстояния: k=1 ∑ K i=1 ∑ n ρ(μ k ,x i ) 2 I[a(x i )=k] Среднее межкластерное расстояние Аналогично среднему внутрикластерному расстоянию вводится среднее межкластерное: i=1 ∑ n j=i ∑ n I[a(x i )  =a(x j )] i=1 ∑ n j=i ∑ n ρ(x i ,x j )I[a(x i )  =a(x j )] Среднее межкластерное расстояние, напротив, нужно максимизировать, то есть имеет смысл выделять в разные кластеры наиболее удалённые друг от друга объекты. Гомогенность Для измерения следующих метрик (гомогенности, полноты и V-меры) нам уже потребуется разметка выборки. Будем обозначать кластеры, к которым наш алгоритм кластеризации относит каждый объект, буквами k∈{1,...,K}, а классы, к которым объекты отнесены разметкой, — буквами с∈{1,...,С}. Разумный вопрос при наличии разметки — зачем нам решать задачу кластеризации, если с разметкой можно поставить задачу как задачу классификации. Это и правда хороший вопрос в том случае, если размеченных данных достаточно много для обучения классификатора. На практике же часто встречаются ситуации, когда данных достаточно для оценки качества кластеризации, но всё ещё не хватает для использования методов обучения с учителем. Пусть n n — общее количество объектов в выборке, n k n k — количество объектов в кластере номер — количество объектов в классе номер с с, а n c k n ck — количество объектов из класса c c в кластере k k. Рассмотрим следующие величины: log ⁡ m c n H class =− c=1 ∑ C n m",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 9,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Это и правда хороший вопрос в том случае, если размеченных данных достаточно много для обучения классификатора. На практике же часто встречаются ситуации, когда данных достаточно для оценки качества кластеризации, но всё ещё не хватает для использования методов обучения с учителем. Пусть n n — общее количество объектов в выборке, n k n k — количество объектов в кластере номер — количество объектов в классе номер с с, а n c k n ck — количество объектов из класса c c в кластере k k. Рассмотрим следующие величины: log ⁡ m c n H class =− c=1 ∑ C n m c log log ⁡ n k n H clust =− k=1 ∑ K n n k log log class∣clust =− c=1 ∑ C k=1 ∑ K n n ck log n k n ck Несложно заметить, что эти величины соответствуют формуле энтропии и условной энтропии для мультиномиальных распределений соответственно. Гомогенность кластеризации определяется следующим выражением: Homogeneity=1− H class H class∣clust Отношение class H class∣clust показывает, во сколько раз энтропия изменяется за счёт того, что мы считаем известной принадлежность объектов к выделенным нашим алгоритмом кластерам. Худший случай — когда отношение оказывается равным единице (энтропия не изменилась, условная энтропия совпала с обычной), лучший — когда каждый кластер содержит элементы только одного класса и номер кластера, таким образом, точно определяет номер класса (в этом случае h = 1 h=1). Тривиальный способ максимизировать гомогенность кластеризации — выделить каждый объект выборки в отдельный кластер. Полнота Полнота задаётся аналогично гомогенности, с той лишь разницей, что вводится величина clust∣class , симметричная class∣clust Completeness=1− H clust H clust∣class Полнота равна единице, когда все объекты класса всегда оказываются в одном кластере. Тривиальный способ максимизировать полноту кластеризации — объединить всю выборку в один кластер. V-мера Гомогенность и полнота кластеризации – это в некотором смысле аналоги точности и полноты классификации. Аналог F-меры для задачи кластеризации тоже есть, он называется V-мерой и связан с гомогенностью и полнотой той же формулой, что и F-мера с точностью и полнотой: β⋅Homogeneity+Completeness (1+β)⋅Homogeneity⋅Completeness В частности, V 1 V 1 по аналогии с F 1 F 1 -мерой в классификации (не путать со средним межкластерным расстоянием, которое мы тоже обозначали F 1 F 1 выше) будет просто средним гармоническим гомогенности и полноты: Homogeneity+Completeness 2⋅Homogeneity⋅Completeness V-мера комбинирует в себе гомогенность и полноту таким образом, чтобы максимизация итоговой метрики не приводила к тривиальным решениям. Коэффициент силуэта Ещё одна метрика кластеризации, на этот раз уже не требующая разметки, это коэффициент силуэта (silhouette coefficient). Изначально коэффициент определяется для каждого объекта выборки, а метрика для результатов кластеризации всей выборки вводится как средний коэффициент силуэта для всех объектов выборки. Чтобы ввести коэффициент силуэта S ( x i ) S(x i ), нам потребуются две вспомогательные величины. Первая, A ( x i ) A(x i ), — это среднее расстояние между x i x i и объектами того же кластера. Вторая, B ( x i ) B(x i ), — это среднее расстояние между x i x i и объектами следующего ближайшего кластера. Коэффициент силуэта вводится следующим образом: max S(x i )= max(B(x i ),A(x",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 10,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(silhouette coefficient). Изначально коэффициент определяется для каждого объекта выборки, а метрика для результатов кластеризации всей выборки вводится как средний коэффициент силуэта для всех объектов выборки. Чтобы ввести коэффициент силуэта S ( x i ) S(x i ), нам потребуются две вспомогательные величины. Первая, A ( x i ) A(x i ), — это среднее расстояние между x i x i и объектами того же кластера. Вторая, B ( x i ) B(x i ), — это среднее расстояние между x i x i и объектами следующего ближайшего кластера. Коэффициент силуэта вводится следующим образом: max S(x i )= max(B(x i ),A(x i )) B(x i )−A(x i ) В идеальном случае объекты «родного» кластера x i x i должны быть ближе к x i x i , чем объекты соседнего кластера, то есть A(x i )<B(x i ). Однако это неравенство выполняется далеко не всегда. Если «родной» кластер x i x i , например, имеет форму очень протяжённой ленты или просто большой размер, а недалеко от x i x i есть кластер поменьше, может оказаться, что A(x i )>B(x i ). Таким образом, если мы посмотрим на разность B(x i )−A(x i ), она может оказаться как положительной, так и отрицательной, но в идеальном сценарии всё же следует ожидать положительное значение. Сам же коэффициент S ( x i ) S(x i ) принимает значения от –1 до +1 и максимизируется, когда кластеры кучные и хорошо отделены друг от друга. Коэффициент силуэта особенно полезен (по сравнению с другими приведёнными метриками) тем, что одновременно и не требует разметки, и позволяет подбирать количество кластеров. См. подробнее в примере из документации scikit-learn. Различия и выбор метрик качества кластеризации Подводя итог в теме метрик качества в задаче кластеризации, отметим, что есть несколько разных сценариев использования этих метрик. Если вы уже определились с количеством кластеров, можно оптимизировать среднее внутрикластерное или среднее межкластерное расстояние. Если у вас ещё и есть разметка — гомогенность и полноту. V-мера за счёт сочетания гомогенности и полноты в целом позволяет выполнять и подбор количества кластеров. Однако разметка, с одной стороны, есть далеко не всегда, а с другой стороны, в задаче кластеризации часто очень субъективна. Сложность кластеризации в том, что на одной и той же выборке нас вполне могут устроить сразу несколько различных вариантов кластеризации, то есть задача поставлена некорректно и имеет более одного решения. Формализовать, какие решения нас устроят, на практике довольно сложно, поэтому сама по себе задача кластеризации решается не слишком хорошо. Если разметки нет и число кластеров не фиксировано, лучшая метрика на практике — коэффициент силуэта. Исключение — ситуация, когда результат кластеризации используется далее для решения некоторой задачи обучения с учителем (как было в примере классификации изображений с помощью visual bag of words). В этом случае можно абстрагироваться от качества кластеризации и выбирать такой алгоритм кластеризации и такие его гиперпараметры, которые позволят лучше всего решить итоговую задачу. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 9.4. Хорошие свойства рекомендательных систем Следующий параграф 10.2. Временные ряды",
    "metadata": {
      "title": "Кластеризация",
      "url": "https://education.yandex.ru/handbook/ml/article/klasterizaciya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.1",
      "part": 11,
      "total_parts": 11,
      "source_file": "10.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Временной ряд — значения меняющихся во времени признаков, полученные в некоторые моменты времени. ts Задача прогнозирования Пусть ,t∈N) – временной ряд, для которого известны значения ,…,y T . Требуется построить прогноз – функцию f f, такую что величина T+h =f(y 1 ,…,y T ,h) как можно лучше приближает значение y T + h y T+h , где h∈{1,…,H} – количество шагов, на которое нужно построить прогноз, а величина H H – горизонт прогнозирования, то есть максимальное количество шагов для построения прогноза. Иными словами, прогноз значения ряда в момент времени T + h T+h строится на основе известных значений ряда до момента времени T T. Кроме этого имеет смысл строить предсказательный интервал, то есть интервал T+h ,u T+h ), т.ч. P(d T+h ⩽y T+h ⩽u T+h )⩾α. Например, пусть y t y t – значение какого-то признака в момент времени t t, и у нас есть значения ряда за месяц, то есть ,…,y 30 . Пусть также требуется предсказать значения ряда на неделю вперед. Тогда прогноз на первые сутки вперед будет равен =f(y 1 ,…,y 30 ,1), а прогноз на пятые сутки =f(y 1 ,…,y 30 ,5). Спустя некоторое время прогноз можно перестроить. Например, пусть прогноз перестраивается один раз в трое суток. Тогда оценку значения y 35 y 35 мы уточним как =f(y 1 ,…,y 33 ,2). При этом может оказаться, что функция f f умеет принимать на вход лишь фиксированное количество предыдущих значений ряда. Например, если она умеем строить прогноз по последним 30 значениям ряда, то запись будет иметь вид =f(y 4 ,…,y 33 ,2). Иногда для уточнения того, в какой момент построен прогноз значения y 35 y 35 , указывают момент времени построения прогноза. Например, запись 35∣30 означает, что прогноз на 35-й день построен в день 30, а 35∣33 – в день 33. Если признаков несколько, не обязательно прогнозировать каждый признак. Часто выделяется один или несколько целевых признаков, а остальные признаки являются вспомогательными и могут улучшить прогноз. Практические примеры: Прогноз погоды на 10 дней вперед. Прогноз осадков на 2 часа вперед. Примеры временных рядов Ежемесячные продажи антидиабетических лекарств в Австралии с июля 1991 по июнь 2008. На этом графике мы можем видеть возрастающий тренд, возможно, даже нелинейный, и кроме этого есть сезонность (периодичность) значений по годам. ex Максимальный спрос на электричество в штате Виктория (Австралия) за 30-минутные интервалы с 10 января 2000 в течении 115 дней. Здесь мы можем наблюдать сразу две сезонности – суточную и недельную. Первая сезонность вызвана тем, что люди обычно больше потребляют электричество днем, чем ночью. Недельная сезонность вызвана более высоким потреблением электричества по будням. Если бы мы посмотрели данные за несколько лет, то увидели бы еще одну сезонность – годовую, например, вызванную тем, что в теплое время года работает больше кондиционеров. ex Почему бы нам не построить прогноз простыми методами, например, линейной регрессией по времени? Общий тренд так можно уловить. Но в остатках (то есть в разности между истинными значениями и прогнозом) этой модели будет достаточно много информации, которую хотелось бы как-то учесть. ex Можно также добавить квадрат значения времени. Тогда можно уловить квадратичный",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 1,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вызвана тем, что люди обычно больше потребляют электричество днем, чем ночью. Недельная сезонность вызвана более высоким потреблением электричества по будням. Если бы мы посмотрели данные за несколько лет, то увидели бы еще одну сезонность – годовую, например, вызванную тем, что в теплое время года работает больше кондиционеров. ex Почему бы нам не построить прогноз простыми методами, например, линейной регрессией по времени? Общий тренд так можно уловить. Но в остатках (то есть в разности между истинными значениями и прогнозом) этой модели будет достаточно много информации, которую хотелось бы как-то учесть. ex Можно также добавить квадрат значения времени. Тогда можно уловить квадратичный тренд, но не более. ex Прогнозирование с помощью сведения к задаче регрессии Давайте для начала поймем, что мы вообще хотим сделать. Посмотрим на этот график, на котором показаны продажи одного из товаров в магазине за разные года. ts Мы знаем значения ряда (зеленые) до момента времени t t, в данном случае за 4 года с 2013 по 2016 включительно. Предположим также, что в данный момент мы отмечаем Новый год 2017. В этот момент мы хотели бы предсказать (синее) будущие значения ряда (оранжевое) за весь 2017 год на основе четырехлетней истории продаж. Основная идея – подадим известные (зеленые) значения ряда в какую-то регрессионную функцию, получив тем самым предсказания. При этом можем брать не все известные значения ряда, а только p p последних значений. Иначе говоря, модель имеет вид =f(y t−1 ,…,y t−p ), где f f – произвольная функция. Ее можно построить некоторым известным методом машинного обучения, например, линейной регрессией, решающими деревьями, бустингами, нейронными сетями (как сверточными, так и рекуррентными). Разберёмся, какие признаки мы подадим на вход регрессии. Признаки Общий принцип На практике при генерации идей о том, какие признаки можно создавать для построения модели, рекомендуется строить следующий график. На нем нужно отметить момент времени t t и мысленно поставить себя в этот момент времени. Затем нужно подумать, какие данные нам при этом доступны. В модель можно брать любые признаки, которые доступны к моменту времени t t. Если все данные поступают сразу, то можно брать все признаки, которые зависят только от значений до момента времени t t. В реальности часть данных может поступать с задержкой. Например, если данные загружаются в базу данных раз в сутки в полночь, то в полдень нам не доступны данные за последние 12 часов. 1 Также нужно помнить о том, на сколько времени вперед нужно сделать прогноз. Например, пусть у нас задача состоит в том, чтобы построить прогнозы продаж в магазине с целью планирования новых поставок. После того, как на основе прогноза мы примем решение о составе товаров в новой поставке, необходимо сначала собрать данные товары на складе, потом отправить машину в магазин, и затем еще разложить товар на полки в магазине. На эту процедуру может уходить от нескольких часов до нескольких дней. Тем самым еще до момента начала формирования новой поставки модель прогнозирования продаж должна построить прогноз спроса на товар к тому моменту, когда его выложат на полки. Даты Посмотрим на то, какие признаки можно извлечь Пусть дана какая-то дата: 13.04.2021 09:00. Отсюда можно получить следующие",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 2,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "прогнозы продаж в магазине с целью планирования новых поставок. После того, как на основе прогноза мы примем решение о составе товаров в новой поставке, необходимо сначала собрать данные товары на складе, потом отправить машину в магазин, и затем еще разложить товар на полки в магазине. На эту процедуру может уходить от нескольких часов до нескольких дней. Тем самым еще до момента начала формирования новой поставки модель прогнозирования продаж должна построить прогноз спроса на товар к тому моменту, когда его выложат на полки. Даты Посмотрим на то, какие признаки можно извлечь Пусть дана какая-то дата: 13.04.2021 09:00. Отсюда можно получить следующие признаки: день недели: [2]; месяц: [4]; год: [2021]; сезон: [весна]; праздник: [0]; выходной: [0]; час: [9]. Предыдущие значения ряда. Например, если мы хотим построить признаки в момент времени t t для прогнозирования y t y t , то можно рассмотреть в качестве признаков p p предыдущих значений ряда t−1 ,…,y t−p . Время Таргет Признаки t−1 ,…,y t−p t − 1 t−1 y t − 1 y t−1 t−2 ,…,y t−p−1 t − 2 t−2 y t − 2 y t−2 t−3 ,…,y t−p−2 Для реализации таких признаков можно выполнить сдвиги вперед временного ряда на 1,2,…,p шагов. Например, в таблице для прогнозирования значений ряда мы рассматриваем два предыдущих значений ряда, выполняя тем самым два сдвига вперед. Таким образом, для прогнозирования значения 5 января, которое равно 235, мы берем признаки 230 и 215, которые являются значением ряда за 4 и 3 января соответственно. 1 Скользящее окно Не всегда имеет смысл в качестве признаков в чистом виде брать все предыдущие значения ряда. Например, если данные приходят раз в секунду, то для того чтобы учесть изменения ряда за последний час, пришлось бы создавать 3600 признаков. Вместо этого по предыдущим значениям ряда t−1 ,…,y t−p можно посчитать: среднее; взвешенное среднее; экспоненциальное сглаживание; медиана; минимум/максимум; стандартное отклонение; любую другую статистику. Подобное скользящее окно можно рассматривать и по другим временным факторам, которые мы не прогнозируем. Примеры: Средняя температура на прошлой неделе для предсказания температуры на завтра. Средняя влажность на прошлой неделе для предсказания температуры на завтра. Если в задаче данные хорошие и удаётся использовать более-менее стандартные признаки, то можно воспользоваться готовыми инструментами. Если данные не очень приятные, стоит подумать над тем, какие признаки использовать и как реализовать их получение. Сезонность Если во временном ряду наблюдается сезонность, то стоит использовать сезонные признаки, например, следующие. Значение переменной сутки/неделю/месяц/год назад. Такие факторы также можно усреднять. Сезонность, полученная методами декомпозирования ряда (об этом расскажем ниже). Примеры: Значение температуры год назад. Среднее значение температуры 23 ноября за 5 последних лет. Среднее значение температуры за 5 последних лет на неделе, в которую входит 23 ноября. Счётчики Идея состоит в том, чтобы группировать данные не только по временным факторам, но и по любым категориальным. Например, пусть сегодня нет ветра. Тогда в качестве признака можно рассмотреть среднюю температуру в безветренные дни по историческим данным. Можно также использовать сразу несколько факторов. Например, мы строим прогноз в апреле. Тогда можно рассматривать среднюю температуру в безветренные дни в апреле по историческим данным. Резюме Подведем итог о том,",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 3,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ряда (об этом расскажем ниже). Примеры: Значение температуры год назад. Среднее значение температуры 23 ноября за 5 последних лет. Среднее значение температуры за 5 последних лет на неделе, в которую входит 23 ноября. Счётчики Идея состоит в том, чтобы группировать данные не только по временным факторам, но и по любым категориальным. Например, пусть сегодня нет ветра. Тогда в качестве признака можно рассмотреть среднюю температуру в безветренные дни по историческим данным. Можно также использовать сразу несколько факторов. Например, мы строим прогноз в апреле. Тогда можно рассматривать среднюю температуру в безветренные дни в апреле по историческим данным. Резюме Подведем итог о том, какие признаки можем использовать для построения нашей модели. Используются только данные из прошлого, никакие данные из будущего нельзя использовать при прогнозировании. Нужно также учитывать возможные задержки в поступлении данных. Большое количество признаков может привести к вычислительным затратам. Можно генерировать и другие признаки с учетом знаний о предметной области. Построение прогноза Мы определились с тем, какие брать признаки. Теперь разберемся с тем, как прогнозировать. Пусть требуется построить прогноз на H H шагов вперед. Выделяют три основных способа построить прогнозы: Рекурсивная стратегия; Прямая стратегия; Гибридная стратегия. Рекурсивная стратегия Для каждого момента времени ⩽t⩽T создается объект обучающей выборки: Признаковое описание – история ряда до момента времени t − 1 t−1. Целевая метка – значение y t y t . По этим данным мы обучаем какую-либо регрессионную модель строить прогнозы на один шаг вперед. При построении прогноза на несколько шагов вперед мы сначала построим прогноз на один шаг. Затем – на второй шаг, используя полученный прогноз на первый шаг в качестве признаков, и далее аналогично. Иначе говоря, если для прогнозирования y t y t признаковое описание имеет вид t−p ,…,y t−1 ), то для построения прогноза y t + 1 y t+1 рассматривается признаковое описание t−p+1 ,…,y t−1 , y t ). На картинке считаем, что M-2, M-1 и M это названия признаков у построенной модели. recursive Прямая стратегия В прямой стратегии предполагается, что построением каждого прогноза в рамках горизонта прогнозирования должна заниматься своя модель. Тем самым создается H H моделей прогнозирования для каждого момента времени ⩽t⩽t 0 +H−1. Признаковое описание – история ряда до момента времени −1, причем признаки одни и те же для каждой модели. Целевая метка – значение y t y t . direct Гибридная стратегия Гибридная стратегия объединяет в себе преимущества рекурсивной и прямой стратегий. Как и в прямой стратегии, создается H H моделей прогнозирования, но при этом каждая следующая модель использует прогнозы предыдущей подобно тому, как это делает рекурсивная стратегия. Итак, мы должны построить модель для прогноза на 1 шаг вперед; модель для прогноза на 2 шага вперед, используя прогноз уже обученной модели на 1 шаг вперед в качестве признака; модель для прогноза на 3 шага вперед, используя прогноз уже обученных моделей на 1 и 2 шага вперед в качестве признаков; и так далее обучается H H моделей. Признаковое описание: история ряда до момента времени −1; предсказание предыдущих моделей для −1,…,t−1. На картинке показаны признаковые описания для моделей в такой стратегии. hybrid Можно задаться вопросом: что лучше брать при",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 4,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "модель использует прогнозы предыдущей подобно тому, как это делает рекурсивная стратегия. Итак, мы должны построить модель для прогноза на 1 шаг вперед; модель для прогноза на 2 шага вперед, используя прогноз уже обученной модели на 1 шаг вперед в качестве признака; модель для прогноза на 3 шага вперед, используя прогноз уже обученных моделей на 1 и 2 шага вперед в качестве признаков; и так далее обучается H H моделей. Признаковое описание: история ряда до момента времени −1; предсказание предыдущих моделей для −1,…,t−1. На картинке показаны признаковые описания для моделей в такой стратегии. hybrid Можно задаться вопросом: что лучше брать при обучении моделей для прогноза на несколько шагов вперёд – истинные значения или же предсказания предыдущих моделей. Если брать истинные, то мы можем точнее построить модели прогнозирования, но, с другой стороны, на этапе применения вы будете использовать прогнозы, а они могут иметь другое распределение, чем истинные данные, в частности, могут иметь смещение и большую дисперсию. В таком случае мы получим плохие следующие прогнозы. Модели для нескольких временных рядов В реальности очень часто нужно прогнозировать сразу огромное количество временных рядов. Примеры: Предсказание температуры для различных регионов/городов. Предсказания уровня продаж для различных типов товаров (молоко/яблоки/мясо). Проблема: модель на каждый временной ряд – слишком много ресурсов и не масштабируемо; мало моделей – плохие предсказания для каждого ряда по отдельности. Идея: создавать модели не для каждого временного ряда, а для группы временных рядов. Иначе говоря, разделить объекты на категории, и для каждой категории создавать отдельную модель. cluster Оценка качества моделей Для оценка качества моделей прогнозирования временного ряда в основном используются метрики качества регрессии. Средняя квадратичная ошибка MSE= T−R+1 1 t=R Средняя абсолютная ошибка MAE= T−R+1 1 t=R Эти метрики показывают, например, на сколько рублей или на сколько единиц товара мы ошибемся. Также могут использоваться: Средняя абсолютная ошибка в процентах M A P E = 100 MAPE= T−R+1 100 t=R Взвешенная средняя ошибка в процентах. W A P E = 100 WAPE=100⋅ ∑ t=R T ∣y t ∣ ∑ t=R Эти метрики достаточно популярны из-за того, что позволяют оценить качество в относительных величинах без зависимости от шкалы измерений. Кросс-валидация для временных рядов. Стандартные схемы кросс-валидации нельзя применять для временных рядов потому что значения во временных рядах нельзя перемешивать. Существует два способа построить кросс-валидацию на временных рядах. Схема 1. Обучаемся на первых t t значениях временного ряда ...y t , прогнозируем следующие Δ t Δt значений ряда t+1 ... y t+Δt . Обучаемся на ...y t+Δt , прогнозируем t+Δt+1 ... y t+2Δt . ... Обучаемся на ...y t+(k−1)Δt , прогнозируем t+(k−1)Δt+1 ... y t+kΔt . На каждой итерации считаем ошибки и усредняем. Group Схема 2. Обучаемся на первых t t значениях временного ряда ...y t , прогнозируем следующие Δ t Δt значений ряда t+1 ... y t+Δt . Обучаемся на 1+Δt ...y t+Δt , прогнозируем t+Δt+1 ... y t+2Δt . ... Обучаемся на 1+(k−1)Δt ...y t+(k−1)Δt ,~прогнозируем~ t+(k−1)Δt+1 ... y t+kΔt . Считаем ошибки и усредняем. cross Эти две схемы отличаются только размером обучающего множества. В первом случае он постоянно растет, во втором – не меняется, а",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 5,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "прогнозируем t+Δt+1 ... y t+2Δt . ... Обучаемся на ...y t+(k−1)Δt , прогнозируем t+(k−1)Δt+1 ... y t+kΔt . На каждой итерации считаем ошибки и усредняем. Group Схема 2. Обучаемся на первых t t значениях временного ряда ...y t , прогнозируем следующие Δ t Δt значений ряда t+1 ... y t+Δt . Обучаемся на 1+Δt ...y t+Δt , прогнозируем t+Δt+1 ... y t+2Δt . ... Обучаемся на 1+(k−1)Δt ...y t+(k−1)Δt ,~прогнозируем~ t+(k−1)Δt+1 ... y t+kΔt . Считаем ошибки и усредняем. cross Эти две схемы отличаются только размером обучающего множества. В первом случае он постоянно растет, во втором – не меняется, а само обучающее множество при этом сдвигается. Ту или иную схему на практике стоит использовать в зависимости от того, какая решается задача. Например, если данных достаточно много и предполагается онлайн работа модели с периодическим дообучением, то обычно при каждом дообучении размер обучающего множества фиксируют. В таком случае имеет смысл воспользоваться второй схемой, чтобы оценить качество модели, обученной именно на таким количестве данных. Если же данных немного, то для обучения желательно использовать все доступные данные. В таком случае имеет смысл использовать первую схему. Обратите внимание, что во всех случаях размер тестового интервала времени фиксирован. Это необходимое условие, потому как распределение значений метрики на разных размерах данных может отличаться. Вспомните, например, про зависимость дисперсии выборочного от размера выборки. Резюме: стандартные модели ML для временных рядов Преимущества Свободно используют дополнительную информацию – экзогенные факторы или признаки. Много рядов – много моделей. Нейронная сеть может иметь несколько выходов, и это позволяет прогнозировать сразу несколько рядов одной моделью. Пример: прогнозирование продаж различных товаров. Недостатки Предсказательные интервалы напрямую не строятся. Иногда работают хуже стандартных моделей. Обработка признаков может быть труднее, чем в других моделях, которые мы рассмотрим далее. Интерпретация моделей может вызывать трудности у заказчика. Декомпозиция временных рядов Декомпозиция – процедура разложения временного ряда на три временных компоненты: Тренд T t T t – плавное долгосрочное изменение временного ряда. Сезонность S t S t – циклические изменения временного ряда с постоянным периодом сезонности s s. Ошибка R t R t – непрогнозируемая случайная компонента ряда. Можно рассматривать аддитивную декомпозицию, в которой ряд представляется в виде , а также мультипликативную в виде . Нетрудно понять, что для построения мультипликативного разложения достаточно выполнить аддитивную декомпозицию для ряда log ⁡ y t logy t . Декомпозиция на основе скользящего среднего Пусть s s – известный заранее период сезонности. Компоненты разложения вычисляются последовательно по следующим правилам. Тренд Вычисляется с помощью скользящего окна длины i=t−s/2 ∑ t+s/2 y i . Сезонность Усреднение значений по сезону после удаления тренда. Вычитаем тренд :=y t −T t ; Формируются s s подгрупп: ={y i ,y i+s ,…,y i+ks }; i i-е значение сезонности вычисляется усреднением по i i-й группе Ошибка (t mod s) . ex STL-декомпозиция Название метода расшифровывается как Seasonal-Trend decomposition using LOESS. Является более продвинутой моделю для декомпозиции временного ряда. Напоминание: LOESS – взвешенная линейная регрессия, где вес объекта пропорционален расстоянию от него до точек обучающей выборки. Принцип работы: Инициализация тренда нулем: =0; В цикле по k k до сходимости: Вычитаем из ряда",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 6,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "окна длины i=t−s/2 ∑ t+s/2 y i . Сезонность Усреднение значений по сезону после удаления тренда. Вычитаем тренд :=y t −T t ; Формируются s s подгрупп: ={y i ,y i+s ,…,y i+ks }; i i-е значение сезонности вычисляется усреднением по i i-й группе Ошибка (t mod s) . ex STL-декомпозиция Название метода расшифровывается как Seasonal-Trend decomposition using LOESS. Является более продвинутой моделю для декомпозиции временного ряда. Напоминание: LOESS – взвешенная линейная регрессия, где вес объекта пропорционален расстоянию от него до точек обучающей выборки. Принцип работы: Инициализация тренда нулем: =0; В цикле по k k до сходимости: Вычитаем из ряда текущее значение тренда :=y t −T t k . Формируются s s подгрупп: ={y i ,y i+s ,…,y i+ks }. С помощью LOESS в каждой группе в каждый момент времени предсказывается сезонность S t S t . Вычитаем из ряда полученную сезонность :=y t −S t . С помощью LOESS предсказывается новое значение тренда k+1 . Замечание. Пропущен шаг работы с выбросами. stl Преимущества STL-декомпозиции: Больше настраиваемых параметров, позволяющих подогнать модель под любые данные. Сезонная компонента может изменяться с течением времени, и это изменение контролируется пользователем. Модель может быть устойчива к выбросам. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 10.1. Кластеризация Методы кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризации Следующий параграф 10.3. Аналитика временных рядов",
    "metadata": {
      "title": "Временные ряды",
      "url": "https://education.yandex.ru/handbook/ml/article/vremennye-ryady",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.2",
      "part": 7,
      "total_parts": 7,
      "source_file": "10.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Временной ряд – зависимые между собой наблюдения. Например, температура воздуха сегодня достаточно сильно зависит от вчерашнего показателя температуры. Эту зависимость хотелось бы описать численно. Для этого часто используют разные виды коэффициентов корреляции, например, корреляции Пирсона, Спирмена и Кендалла. Каждый из этих коэффициентов корреляции вычисляется по двум выборкам, корреляцию между которыми требуется посчитать. В данном случае мы имеем один временной ряд, и наша задача – оценить корреляцию между разными наблюдениями ряда, считая, что она не меняется со временем. В качестве оценки корреляции значений t+τ для любых t t рассмотрим коэффициент корреляции Пирсона ряда с самим собой со сдвигом на τ τ. Тем самым мы получим численную оценку степени влияния значения y t y t на значение y t + τ y t+τ corr (y t ,y t+τ )= t=1 t=1 ∑ T−τ (y t − y )(y t+τ − y ) , где τ τ – лаг автокорреляции, а среднее вычисляется по всему ряду t=1 ∑ T y t . Например, если мы хотим оценить степень влияния сегодняшней температуры на завтрашнюю, то посчитаем коэффициент корреляции исходного ряда и им же, сдвинутым на 1 день. Замечание. Формула содержит некоторые упрощения при оценке ковариации и дисперсий. Свойства коэффициента корреляции: ∣⩽1; =0 – отсутствие автокорреляции, при этом значения могут быть зависимыми (см. подробнее про разницу между независимостью и некоррелированностью); >0 – положительная корреляция, то есть чем больше было значение вчера, тем оно будет больше сегодня; <0 – отрицательная корреляция, то есть чем больше было значение вчера, тем оно будет меньше сегодня; ∣=1 означает строгую линейную зависимость. Посмотреть простые примеры и потренировать свою интуицию вы можете в игре Guess The Correlation. Пусть мы посчитали значение автокорреляции. А как понять, значение 0.1 – это много или мало? На этот вопрос может ответить статистический критерий Льюнга-Бокса (Ljung–Box), который проверяет значимость отклонения r τ r τ от нуля. Основное правило, которое нужно здесь понять – если значение p-value критерия не превосходит 0.05 0.05 (или другого заранее фиксированного порога значимости), то автокорреляция с лагом τ τ значима. Это число вычисляется с помощью стандартных статистических пакетов (например, statsmodels в Питоне). Рассмотрим временной ряд дорожно-транспортных происшествий за 14 лет с дискретностью 1 месяц, то есть с 12 измерениями в год. На графике мы видим явную сезонность. На нижнем графике изображена коррелограмма – график, визуализирующий автокорреляционную функцию. Точками на графике показаны значения автокорреляционной функции. Ее значение в нуле всегда равно 1, так как это автокорреляция ряда с собой же. Также мы видим, что значение r 12 r 12 является локальным максимумом, что означает высокую положительную корреляцию значения ряда за текущий месяц с аналогичным значением год назад. Иными словами, ряд со сдвигом на год ведёт себя «похожим образом», и это подкрепляет наше наблюдение про наличие годичной сезонности. Наоборот, значение r 6 r 6 минимально в своей окрестности, что означает высокую отрицательную корреляцию значения ряда со значением полгода назад. Закрашенная область визуализирует границу незначимой автокорреляции, то есть тех значений автокорреляции, для которых не выявлена статистически значимое отличие от 0 (иначе говоря, доверительный интервал пересекает 0). Так мы видим, что последняя значимая сезонная автокорреляция это",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 1,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "что значение r 12 r 12 является локальным максимумом, что означает высокую положительную корреляцию значения ряда за текущий месяц с аналогичным значением год назад. Иными словами, ряд со сдвигом на год ведёт себя «похожим образом», и это подкрепляет наше наблюдение про наличие годичной сезонности. Наоборот, значение r 6 r 6 минимально в своей окрестности, что означает высокую отрицательную корреляцию значения ряда со значением полгода назад. Закрашенная область визуализирует границу незначимой автокорреляции, то есть тех значений автокорреляции, для которых не выявлена статистически значимое отличие от 0 (иначе говоря, доверительный интервал пересекает 0). Так мы видим, что последняя значимая сезонная автокорреляция это – это r 24 r 24 . Кроме нее также значима сезонная корреляция r 12 r 12 и корреляция за половину сезона r 6 r 6 . Из несезонных автокорреляций значимы оказались . Отсюда можно сделать вывод, что для построения прогноза значения y t y t имеет смысл рассматривать признаки t−1 ,y t−2 ,y t−6 ,y t−12 ,y t−24 . ex ex Рассмотрим временной ряд потребления электричества в Австралии с дискретностью 30 минут. По графику мы можем заметить две разных сезонности – суточную и недельную. Кроме того, при наличии большего количества данных мы смогли бы увидеть еще и годовую сезонность. На данном графике видно повышенное потребление электричества в январе-феврале, когда в Австралии жарко и работает много кондиционеров. По графику автокорреляций мы видим, что наибольшую корреляцию имеют соседние измерения, а также значения сутки назад, двое суток назад, и т.д. Наоборот, значения 12, 36, ... часов назад имеют отрицательную корреляцию. ex ex Стационарные временные ряды Временной ряд y t y t называется стационарным в узком смысле, если для любых ,...,t n ,τ вектор ,...,y t n +τ ) совпадает по распределению с ,...,y t n ), то есть при сдвиге всех моментов времени на одно и тоже число совместное распределение значений временного ряда в эти моменты времени не поменяется. в широком смысле, если <+∞ для любого не зависит от t t, то есть в среднем значение временного ряда постоянно. cov(y t+τ ,y s+τ )=cov(y t ,y s ) для любых t , s , τ t,s,τ, то есть значение автокорреляции зависит только от длины отрезка времени между двумя значениями. для гауссовских распределений, то есть для случая, когда все векторы вида ,...,y t n ) имеют нормальное распределение, определения эквивалентны. Это следует из того, что распределение гауссовского случайного вектора полностью определяется математическим ожиданием и ковариациями. Пример. Рассмотрим временной ряд y t = ξ 1 cos ⁡ t + ξ 2 sin cost+ξ 2 sint, где и независимы и одинаково распределены, причем P(ξ 1 =1)=P(ξ 1 =−1)=1/2. Можем заметить, что =0 и cos ⁡ t cos sin ⁡ t sin cos cov(y t ,y s )=costcoss Dξ 1 +sintsins Dξ 2 =cos(t−s). Тем самым имеем стационарность в широком смысле. Но при t = 0 t=0 получаем ∈{−1,1}, а при t = π / 4 t=π/4 получаем π/4 ∈{− 2 ,0, 2 }. Мы получили разные распределения, поэтому нет стационарности в узком смысле. Некоторые примеры нестационарных временных рядов: Случайное блуждание – пример: t−1 +ε t",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 2,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "1 cos ⁡ t + ξ 2 sin cost+ξ 2 sint, где и независимы и одинаково распределены, причем P(ξ 1 =1)=P(ξ 1 =−1)=1/2. Можем заметить, что =0 и cos ⁡ t cos sin ⁡ t sin cos cov(y t ,y s )=costcoss Dξ 1 +sintsins Dξ 2 =cos(t−s). Тем самым имеем стационарность в широком смысле. Но при t = 0 t=0 получаем ∈{−1,1}, а при t = π / 4 t=π/4 получаем π/4 ∈{− 2 ,0, 2 }. Мы получили разные распределения, поэтому нет стационарности в узком смысле. Некоторые примеры нестационарных временных рядов: Случайное блуждание – пример: t−1 +ε t , где ε t ε t – белый шум, то есть независимые одинаково распределенные случайные величины. Математическое ожидание постоянно, но ряд не является стационарным, поскольку D y t Dy t бесконечно растет. Временной ряд с трендом – пример: =α+βt+ε t , где ε t ε t – белый шум. Ряд не стационарен, так как E y t Ey t меняется с течением времени. Временной ряд с сезонностью – пример: y t = sin =sint+ε t , где ε t ε t – белый шум. Не стационарен, так как при при −1, при t=−π/2+2πk; 1, при t=t=π/2+2πk. Стационарный ряд визуально не имеет предсказуемых закономерностей. Если посмотреть на график такого ряда издалека, то он будет горизонтален. Ряд можно проверить строго на станционарность с помощью различных статистических критериев. Наиболее популярны следующие критерии: критерий KPSS (Kwiatkowski–Phillips–Schmidt–Shin): если p-value ⩽ 0.05 ⩽0.05, то отвергаем стационарность; критерий Дики-Фуллера: если p-value ⩽ 0.05 ⩽0.05, то отвергаем НЕ НЕстационарность. Рассмотрим несколько примеров временных рядов: ряды а, c, d, e, f, i не стационарны, поскольку они имеют тренд; ряд b скорее всего стационарный, имеется выброс; ряды d, g, h, i не стационарны, потому что имеют сезонность. stationary Когда вы определяете, чем вызвано изменение данных – трендом или шумом – стоит учитывать природу данных. Например, колебания значений ряда f теоретически можно было бы объяснить шумом в данных, но по временной оси мы видим, что данные представлены за 15 лет, соответственно, понимаем данные колебания как изменения тренда. Аналогично, для ряда d мы говорим о наличии меняющегося тренда помимо годовой сезонности. Приведение к стационарным: стабилизация дисперсии Зачем? Данные методы рекомендуется использовать, если задача требует некоторой аналитики временного ряда. Если же требуется только построить точечный прогноз на будущее без построения предсказательных интервалов, то стабилизация дисперсии не является необходимой процедурой. Если же нас интересует предсказательный интервал, то многие методы лучше обрабатывают именно стационарные ряды, поэтому имеет смысл стабилизировать дисперсию. Преобразования: Класс преобразований Бокса-Кокса с параметром lny t , (y t λ −1)/λ, λ=0 λ  =0 . Если есть предположения о зависимости t, то можно рассмотреть ряд После построения прогноза для преобразованного ряда нужно сделать обратное преобразование. Приведение к стационарным: тренд и сезонность Преобразования: Дифференцирование ряда, то есть переход к ряду ,t∈{2,...,T}), где t−1 . Данное преобразование используется для снятие тренда. Сезонное дифференцирование ряда, то есть переход к ряду ,t∈{s+1,...,T}), где t−s , s s – длина сезона. Преобразования можно применять несколько раз. Обычно сначала применяют сезонное дифференцирование. Посмотрим на пример. В критерии KPSS",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 3,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "поэтому имеет смысл стабилизировать дисперсию. Преобразования: Класс преобразований Бокса-Кокса с параметром lny t , (y t λ −1)/λ, λ=0 λ  =0 . Если есть предположения о зависимости t, то можно рассмотреть ряд После построения прогноза для преобразованного ряда нужно сделать обратное преобразование. Приведение к стационарным: тренд и сезонность Преобразования: Дифференцирование ряда, то есть переход к ряду ,t∈{2,...,T}), где t−1 . Данное преобразование используется для снятие тренда. Сезонное дифференцирование ряда, то есть переход к ряду ,t∈{s+1,...,T}), где t−s , s s – длина сезона. Преобразования можно применять несколько раз. Обычно сначала применяют сезонное дифференцирование. Посмотрим на пример. В критерии KPSS для исходного ряда 0.01 pvalue<0.01, то есть ряд можно считать нестационарным. После логарифмирования ряда 0.01 pvalue<0.01, а после ещё и сезонного дифференцированная 0.1 pvalue>0.1, тем самым полученный ряд мы уже не можем отличить от стационарного. ex Модели вида экспоненциального сглаживания Простое экспоненциальное сглаживание Не редко временной ряд выглядит довольно шумным, что может достаточно плохо сказаться на работе других моделей и подходов к анализу этого временного ряда. В таком случае можно попытаться сгладить значения ряда. Далее мы рассмотрим несколько моделей сглаживания ряда, в том числе при наличии тренда и сезонности ряда. Помимо сглаживания истории ряда, с помощью данных методов можно также осуществлять простое прогнозирование ряда. Пусть имеется временной ряд y t y t . В результате экспоненциального сглаживания получается новый временной ряд y ^ y по правилу t+1∣t =αy t +(1−α) y t∣t−1 , где t+h∣t – прогноз значения y t + h y t+h в момент времени t t, а α α – параметр сглаживания. Смысл преобразования следующий – сглаженное значение в момент времени t + 1 t+1 есть взвешенная комбинация предыдущего значения ряда y t y t и предыдущего сглаженного значения ряда t∣t−1 . Свойства: при α ≈ 1 α≈1 больший вес последнему значению ряда, поэтому получается слабое сглаживание T+1∣T ≈y T ; при α ≈ 0 α≈0 больший вес отдается предыдущему сглаженному значению, и получается сильное сглаживание, что в пределе вырождается в среднее T+1∣T ≈ y ; Оптимальное значение α ∗ α ∗ можно подобрать либо по графику, либо оптимизируя min ⁡ α . t=t (α)−y t ) 2 → α min . Существуют следующие эмпирические правила: если 0.3 ) α ∗ ∈(0,0.3) то ряд стационарен, можно применять экспоненциальное сглаживание без риска большой потери информации; если α ∗ ∈ ( 0.3 , 1 ) α ∗ ∈(0.3,1) то ряд нестационарен, применение экспоненциального сглаживания может привести к потере информации или смещению. Примеры: на каждом из графиков изображен исходный ряд (синий) и сглаженный ряд (оранжевый) для разных значений параметра сглаживания. Если имеется тренд или сезонность, то при большом сглаживании полученный ряд начинает «запаздывать» за исходным рядом. es001 es01 Откуда взялась эта формула экспоненциального сглаживания? Покажем, что сглаженное значение соответствует прогнозу величины x x в момент времени T + 1 T+1, подбираемому по правилу min ⁡ x . t=0 ∑ T β T−t (y t −x) 2 → x min . Иначе говоря, для прогнозирования мы берем взвешенный MSE с экспоненциально убывающими по времени весами. Приравняем производную к нулю: t=0 ∑",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 4,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "или смещению. Примеры: на каждом из графиков изображен исходный ряд (синий) и сглаженный ряд (оранжевый) для разных значений параметра сглаживания. Если имеется тренд или сезонность, то при большом сглаживании полученный ряд начинает «запаздывать» за исходным рядом. es001 es01 Откуда взялась эта формула экспоненциального сглаживания? Покажем, что сглаженное значение соответствует прогнозу величины x x в момент времени T + 1 T+1, подбираемому по правилу min ⁡ x . t=0 ∑ T β T−t (y t −x) 2 → x min . Иначе говоря, для прогнозирования мы берем взвешенный MSE с экспоненциально убывающими по времени весами. Приравняем производную к нулю: t=0 ∑ T β T−t (y t −x)=0 Отсюда выразим x x и воспользуемся разложением функции 1 1 − x 1−x 1 в ряд Тейлора t=0 ∑ T β t t=0 ∑ T β T−t y t ≈ 1/(1−β) t=0 ∑ T β T−t y t =(1−β) t=0 ∑ T β T−t =(1−β)y T +(1−β)β t=0 ∑ T−1 β T−1−t y t =(1−β)y T +β y T∣T−1 Тем самым мы получили модель экспоненциального сглаживания для β = 1 − α β=1−α. Модель Хольта Аддитивный линейный тренд: Прогноз на d d шагов вперед выражается с помощью линейной функции от числа шагов, где коэффициенты меняются по формулам, аналогичным экспоненциальному сглаживанию t+d∣t =a t +b t ⋅d, =αy t +(1−α)(a t−1 +b t−1 =β(a t −a t−1 )+(1−β)b t−1 Модель для мультипликативного линейного тренда выглядит аналогично t+d∣t =αy t +(1−α)(a t−1 b t−1 t−1 a t +(1−β)b t−1 Модель Хольта: пояснение формул Поясним на примере аддитивного тренда, почему формулы для получаются именно такими. Заметим, что для d = 0 d=0 и d = 1 d=1 и момента времени t + 1 t+1 получаем t+1∣t+1 =a t+1 t+1∣t =a t +b t . Хотелось бы, чтобы эти прогнозы примерно совпадали, то есть чтобы имело место t+1 −a t ≈b t . Рассмотрим ряд разностей t+1 −a t и задачу константного прогноза для него (то есть t∣t−1 =b) методом простого экспоненциального сглаживания min ⁡ b . i=0 ∑ t (1−β) t−i (Δy i −b) 2 → b min . Ее решение мы уже получили ранее: =βΔy t−1 +(1−β)b t−1 =β(a t −a t−1 )+(1−β)b t−1 . Получилась формулу для b t b t в модели аддитивного тренда. Далее, мы хотим, чтобы имело место t+1 ≈b t +a t . Рассматривая для y t y t экспоненциальное сглаживание, в котором в качестве предыдущего значения сглаженного ряда берется , а в качестве нового – a t + 1 a t+1 , получаем =αy t +(1−α)(a t−1 +b t−1 ). Модель Хольта-Уинтерса Аддитивная сезонность с трендом: mod m ) , y t+d∣t =a t +db t +s t−m+(d mod m) =α(y t −s t−m )+(1−α)(a t−1 +b t−1 =β(a t −a t−1 )+(1−β)b t−1 =γ(y t −a t )+(1−γ)s t−m где m m – длина сезона Мультипликативная сезонность Без тренда mod m ) , y t+d∣t =a t ⋅s t−m+(d mod m) =α(y t /s t−m )+(1−α)a t−1 =γ(y t /a t )+(1−γ)s t−m С линейным трендом mod m )",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 5,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "а в качестве нового – a t + 1 a t+1 , получаем =αy t +(1−α)(a t−1 +b t−1 ). Модель Хольта-Уинтерса Аддитивная сезонность с трендом: mod m ) , y t+d∣t =a t +db t +s t−m+(d mod m) =α(y t −s t−m )+(1−α)(a t−1 +b t−1 =β(a t −a t−1 )+(1−β)b t−1 =γ(y t −a t )+(1−γ)s t−m где m m – длина сезона Мультипликативная сезонность Без тренда mod m ) , y t+d∣t =a t ⋅s t−m+(d mod m) =α(y t /s t−m )+(1−α)a t−1 =γ(y t /a t )+(1−γ)s t−m С линейным трендом mod m ) , y t+d∣t =(a t +db t )s t−m+(d mod m) t−m y t +(1−α)(a t−1 +b t−1 =β(a t −a t−1 )+(1−β)b t−1 +(1−γ)s t−m Разные модели с трендом и сезонностью trends Адаптивное сглаживание В примерах выше мы видели, что при изменении локального тренда ряда экспоненциальное сглаживание запаздывает за значениями ряда при использовании сильного сглаживания. Если же использовать слабое сглаживание, то существенного запаздывания не происходит, но ряд остается шумным. Если для ряда предполагаются значительные структурные изменения, можно использовать модель адаптивного экспоненциального сглаживания, в которой параметр сглаживания может меняться для разных отрезков временного ряда. Пусть y ^ t y t – прогноз значения y t y t в момент времени t − 1 t−1 обычным экспоненциальным сглаживанием, а – ошибка прогноза, сделанного на шаге t − 1 t−1. Определим следующие значения Среднее значение ошибки: +(1−γ)E t−1 . Средний разброс ошибки: =γ∣ ε t ∣+(1−γ)A t−1 – статистика, которая сигнализирует, насколько адекватно модель работает в момент времени ≈±1 – модель систематически ошибается в одну сторону. ≈0 – модель работает адекватно. Обычно берут значения γ ∈ ( 0.05 , 0.1 ) γ∈(0.05,0.1). Чтобы экспоненциальное сглаживание быстро приспосабливалось к резким структурным изменениям берут α t = min =min(∣K t ∣,1). Однако, у данного подхода есть и недостатки, например, плохо реагирует на одиночные выбросы; требует подбора γ γ. adaptive Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 10.2. Временные ряды Следующий параграф 10.4. Модели вида ARIMA",
    "metadata": {
      "title": "Аналитика временных рядов",
      "url": "https://education.yandex.ru/handbook/ml/article/analitika-vremennyh-ryadov",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.3",
      "part": 6,
      "total_parts": 6,
      "source_file": "10.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Прежде чем перейти к рассмотрению модели ARIMA, познакомимся сначала с двумя другими моделями: скользящего среднего и моделью авторегрессии. Модель скользящего среднего MA( q q) Модель скользящего среднего порядка q q или просто MA( q q) предполагает следующую зависимость данных: = μ+ε t +θ 1 ε t−1 +...+θ q ε t−q , где y t y t — стационарный ряд со средним μ μ, а ε t ε t — гауссовский белый шум, то есть ∼N(0,σ 2 ) и независимы. По сути наш ряд y t y t выражается через сумму некоторого фиксированного среднего μ μ, значения белого шума в текущий момент времени ε t ε t и не более q q предыдущих значений белого шума, домноженных на некоторые коэффициенты, которые являются параметрами модели. Рассмотрим некоторые свойства модели MA( q q). Как уже было упомянуто выше, ряд y t y t будет являтьcя стационарным со средним μ μ. Найдем также D y t Dy t . Воспользовавшись свойством независимости для ε t ε t , можем заключить, что =(1+θ 1 2 +⋯+θ q 2 )σ 2 . Посчитаем автоковариационную функцию для ряда y t y t , то есть найдем значение cov(y t ,y t+τ ). Легко понять, что если τ > q τ>q, то cov(y t ,y t+τ ) = 0, т.к. ε t ε t независимы. Если же τ ≤ q τ≤q, то тогда cov(y t ,y t+τ )=(θ τ +θ 1 θ τ+1 +⋯+θ q−τ θ q )σ 2 . Записав более компактно, можем получить: cov(y t ,y t+τ )={ σ 2 ∑ j=0 q−τ θ j θ τ+j , 0 τ≤q; τ>q; где =1. Из посчитанных значений для дисперсии и ковариационной функции, можете попробовать получить выражение и для автокорреляционной функции. Ее особенностью будет как раз равенство нулю на лаге, превосходящим q q. Посмотрим на визуализацию: ma2 Модель авторегрессии AR( p p) Модель авторегрессии для временного ряда можно записать следующим образом: = α+φ 1 y t−1 +...+φ p y t−p +ε t , где y t y t — стационарный ряд, а ε t ε t — гауссовский белый шум, то есть ∼N(0,σ 2 ) и независимы. Отметим, что, вообще говоря, для стационарности нужны некоторые условия на коэффициенты ,...,φ p . По сути наш ряд y t y t выражается через сумму некоторого фиксированного числа α α, значения белого шума в текущий момент времени ε t ε t и не более p p предыдущих значений этого же ряда, домноженных на некоторые коэффициенты, которые являются параметрами модели. Другими словами, модель AR( p p) — это модель линейной регрессией линейной регрессией для которой Таргет: y t y t — значение ряда в момент времени t t Признаки: t−1 ,...,y t−p — значения ряда в предыдущие моменты времени Введем L L — оператор сдвига, обладающий следующими свойствами: применение L L к ряду дает предыдущее значение этого же ряда: t−1 . применение L L к белому шуму дает предыдущее значение шума: t−1 . применение L L к константе — это константа: L c = c . Lc=c. Оператор L L иногда называют",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 1,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "домноженных на некоторые коэффициенты, которые являются параметрами модели. Другими словами, модель AR( p p) — это модель линейной регрессией линейной регрессией для которой Таргет: y t y t — значение ряда в момент времени t t Признаки: t−1 ,...,y t−p — значения ряда в предыдущие моменты времени Введем L L — оператор сдвига, обладающий следующими свойствами: применение L L к ряду дает предыдущее значение этого же ряда: t−1 . применение L L к белому шуму дает предыдущее значение шума: t−1 . применение L L к константе — это константа: L c = c . Lc=c. Оператор L L иногда называют также лаговым оператором. Можно рассматривать функции от оператора сдвига, например, кратное применение оператора =L(Ly t )=L(y t−1 )=y t−2 или t+1 . Для записей некоторых моделей временных рядов будет удобно использовать лаговый многочлен: φ(L)= i=1 Обратным к оператору φ ( L ) φ(L) называют оператор (L) такой, что: φ(L)φ −1 (L)y t =φ −1 (L)φ(L)y t =y t Так, например, для ∣ φ ∣ < 1 ∣φ∣<1 можно заключить, что: 1−φL 1 =(1−φL) −1 = i=1 Рассмотрим модель AR( p p): = α+φ 1 y t−1 +...+φ p y t−p +ε t С помощью оператора сдвига ее можно представить в следующем виде: a(L)y t = α+ε t , где a(z)=1−φ 1 z−...−φ p z p — характеристический полином. Сформулируем пару важных утверждений: Любой стационарный (в широком смысле) процесс представим в виде M A ( ∞ ) MA(∞), то есть в виде модели скользящего среднего с неограниченным количеством слагаемых (конечное или бесконечное число). Этот результат так же известен как теорема Волда о декомпозиции временного ряда. Модель A R ( p ) AR(p) задает стационарный временной ряд ⟺ ⟺ все комплексные корни a(z)=0 лежат вне единичного круга. Приведем пояснение второго утверждения. В самом деле, пусть ,...,z p — все его комплексные корни (их ровно p p с учетом кратности), тогда справедливо представление: a(z)=(z−z 1 )...(z−z p )=z 1 ...z p (1− z 1 z )...(1− z p z ) Тогда при представлении временного ряда в виде a(L) α+ε t и дальнейшего его разложения на простые дроби возникнут слагаемые вида Если при этом z j z j лежит внутри единичного круга или на его границе, то соответствующий ряд будет расходящимся. На самом деле, случай =1 мы в дальнейшем учтем. В качестве примера рассмотрим подробнее модель A R ( 1 ) AR(1). Зависимость имеет вид =α+φy t−1 +ε t , где ∼N(0,σ 2 ). Для данного ряда можно выписать следующие свойства: Уравнение 1−φz=0, имеет корень λ = 1 / φ λ=1/φ. Тем самым, A R ( 1 ) AR(1) стационарен ∣φ∣<1. Кроме того, чем меньше φ φ, тем предыдущее значение ряда вносит меньший вклад в текущее значение. Если ряд стационарен, то: 1−φ 1−φ cov(y t ,y t−h )=φ h ⋅ 1−φ 2 σ 2 . Разберем первое равенство, остальные получаются аналогично. Возьмем математическое ожидание в уравнении ряда =α+φEy t−1 +Eε t Поскольку ряд стационарен, то его математическое ожидание не меняется во времени, а для белого шума математическое ожидание равно нулю. Тем самым мы получаем",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 2,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Для данного ряда можно выписать следующие свойства: Уравнение 1−φz=0, имеет корень λ = 1 / φ λ=1/φ. Тем самым, A R ( 1 ) AR(1) стационарен ∣φ∣<1. Кроме того, чем меньше φ φ, тем предыдущее значение ряда вносит меньший вклад в текущее значение. Если ряд стационарен, то: 1−φ 1−φ cov(y t ,y t−h )=φ h ⋅ 1−φ 2 σ 2 . Разберем первое равенство, остальные получаются аналогично. Возьмем математическое ожидание в уравнении ряда =α+φEy t−1 +Eε t Поскольку ряд стационарен, то его математическое ожидание не меняется во времени, а для белого шума математическое ожидание равно нулю. Тем самым мы получаем уравнение на m = E y t m=Ey t , откуда следует доказываемая формула. Таким образом, в зависимости от значения φ φ мы можем получить следующие результаты: Если ∣ φ ∣ < 1 ∣φ∣<1, то =μ+ j=0 ∑ ∞ φ j ε t−j — представление ряда в виде MA( ∞ ∞). Если ∣ φ ∣ = 1 ∣φ∣=1, то A R ( 1 ) AR(1) — это случайное блуждание. Если ∣ φ ∣ > 1 ∣φ∣>1, то A R ( 1 ) AR(1) — экспоненциально растущий процесс. Посмотрим на визуализацию. ar1 В первом случае мы имеем модель y t = − 0.5 =−0.5y t−1 +ε t , отрицательный коэффициент является следствием больших колебаний ряда. Во втором случае модель y t = 0.9 =0.9y t−1 +ε t , большой положительный коэффициент делает ряд менее шумным. В третьем случае показано несколько рядов вида случайного блуждания t−1 +ε t , что соответствует случаю φ = 1 φ=1. В четвертом случае показан экспоненциальный процесс y t = 1.1 =1.1y t−1 +ε t , на графике шум уже не заметен из-за масштаба. На немного вернемся к модели MA( q q). Чуть выше мы выяснили, что при некоторых условиях на коэффиценты φ φ временной ряд модели AR( p p) будет стационарным, а значит имеет представление в виде MA( ∞ ∞). На самом деле, модель скользящего среднего порядка q q тоже можно представить с помощью оператора L L следующим образом: = μ+ε t +θ 1 ε t−1 +...+θ q ε t−q = μ+b(L)ε t где b(z)=1+θ 1 z+...+θ q z q — характеристический многочлен. Для простоты изложения пусть μ = 0 μ=0. Важным при такой записи оказывается понятие обратимости, то есть представления в виде (L)y t , которое означает, что ряд можно представить в виде бесконечной авторегрессионной модели. Здесь, как и в рассуждениях выше, можно заключить, что временной ряд y t y t обратим, если все комплексные корни b(z)=0 лежат вне единичного круга. Модель ARMA( p , q p,q) Модель ARMA( p , q p,q) по сути является суммой моделей A R ( p ) AR(p) и M A ( q ) MA(q), иначе говоря, модель есть сумма нескольких предыдущих значений ряда и нескольких предыдущих значений белого шума с некоторым коэффициентами. = α+φ 1 y t−1 +...+φ p y t−p +ε t +θ 1 ε t−1 +...+θ q ε t−q Эквивалентную запись ряда в терминах оператора сдвига можно получить, рассмотрев два многочлена a(L)y t =α+b(L)ε t или",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 3,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "временной ряд y t y t обратим, если все комплексные корни b(z)=0 лежат вне единичного круга. Модель ARMA( p , q p,q) Модель ARMA( p , q p,q) по сути является суммой моделей A R ( p ) AR(p) и M A ( q ) MA(q), иначе говоря, модель есть сумма нескольких предыдущих значений ряда и нескольких предыдущих значений белого шума с некоторым коэффициентами. = α+φ 1 y t−1 +...+φ p y t−p +ε t +θ 1 ε t−1 +...+θ q ε t−q Эквивалентную запись ряда в терминах оператора сдвига можно получить, рассмотрев два многочлена a(L)y t =α+b(L)ε t или =μ+ a(L) b(L) ε t , где a(z)=1−φ 1 z−...−φ p z p , и b(z)=1+θ 1 z+...+θ q z q . Заметим, что во втором представлении константа α α заменена на μ = E y t μ=Ey t . На самом деле, стационарность такого ряда будет определяться только его AR( p p) компонентой, то есть значениями коэффициентов φ φ, так ряд в модели MA( q q) всегда является стационарным. Модель ARIMA( p , d , q p,d,q) Модель ARIMA( p , d , q p,d,q) — это расширение моделей типа ARMA на нестационарные временные ряды, которые однако могут стать стационарным после применениея процедуры дифференцирования ряда. Модель ARIMA( p , d , q p,d,q) для ряда y t y t определяется как модель ARMA( p , q p,q) для ряда разностей порядка d d ряда y t y t . Разность порядка 1: t−1 =(1−L)y t . Разность порядка 2: (1−L) 2 y t =(1−L)(y t −y t−1 )=(y t −y t−1 )−(y t−1 −y t−2 )=y t −2y t−1 +y t−2 . Получаем формулу модели ARIMA: a(L)(1−L) d y t =α+b(L)ε t или (1−L) d y t =μ+ a(L) b(L) ε t . То есть многочлен (z)=a(z)(1−z) d имеет d d единичных корней. Тем самым такая модель позволяет учесть нестационарности, в частности, тренд. В качестве примера рассмотрим процесс случайного блуждания: t−1 +ε t , где ε t ε t — белый шум. Как уже упомяналось ранее, такой ряд не является стационарным. Однако, если мы применим операцию дифференцирования, то можем перейти к новому, уже стационарном ряду t−1 , который можно записать в виде: Частичная автокорреляция Для модели скользящего среднего порядка q q мы выяснили, что значения автокорреляционной функции для такого ряда оказывается равной нулю после лага q q. Эта особенность позволяет использовать автокорреляционную функцию для определения порядка модели скользящего среднего. Возникает разумный вопрос, как оценить порядок p p для модели AR( p p)? Здесь оказывается полезным понятие частичной (частной) автокорреляционной функции. Частичная автокорреляция (PACF) — корреляция ряда с собой после снятия линеной зависимости от промежуточных значений ряда. Иначе говоря, мы хотим как-то учесть опосредованного влияние промежуточных значений ряда и оценить непосредственное влияние y t − τ y t−τ на y t y t . Чуть более формально частичную автокорреляцию можно записать следующим образом: corr(y t+1 ,y t ), corr(y t+τ −y t+τ h−1 ,y t −y t τ−1 ), τ=1; τ⩾2, где τ−1 — линейная регрессия на t−1 ,y t−2",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 4,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Возникает разумный вопрос, как оценить порядок p p для модели AR( p p)? Здесь оказывается полезным понятие частичной (частной) автокорреляционной функции. Частичная автокорреляция (PACF) — корреляция ряда с собой после снятия линеной зависимости от промежуточных значений ряда. Иначе говоря, мы хотим как-то учесть опосредованного влияние промежуточных значений ряда и оценить непосредственное влияние y t − τ y t−τ на y t y t . Чуть более формально частичную автокорреляцию можно записать следующим образом: corr(y t+1 ,y t ), corr(y t+τ −y t+τ h−1 ,y t −y t τ−1 ), τ=1; τ⩾2, где τ−1 — линейная регрессия на t−1 ,y t−2 ,...,y t−(τ−1) τ−1 =φ 1 y t−1 +φ 2 y t−2 +...+φ τ−1 y t−(τ−1) t+τ τ−1 =φ 1 y t+τ−1 +φ 2 y t+τ−2 +...+φ τ−1 y t+1 Пример для τ = 2 τ=2: =corr(y t+2 −φ 1 y t+1 ,y t −φ 1 y t−1 ) где φ 1 φ 1 — МНК-оценка в модели =φy t−1 . Можно показать, что значение частиной автокорреляции для модели авторегресии AR( p p) будет ненулевой для лагов τ ≤ p τ≤p и равняться нулю для лагов τ > p τ>p. Имеет место быть полная аналогия с автокорреляционной функцией и моделью MA( q q). Таким образом, исследование поведения автокорреляционной и частичной автокорреляционной функции может быть использовано для определения порядка q q модели скользящего среднего и порядка p p модели авторегрессии соответсвтенно. Оценка коэффициентов в ARIMA Пусть гиперпараметры p , d , q p,d,q фиксированы.&tab;В предположении, что ε t ε t — гауссовский белый шум, в нашей модели мы можем выписать функцию правдоподобия (θ,φ,α)=p θ,φ,α (y 1 ,...,y T ), где θ,φ,α (a 1 ,...,a T ) — соместная плотность. Из-за того, что ε t ε t имеют нормальное распределение, она будет иметь разумный вид. Соответственно, в качестве оценок параметров берется оценка максимального правдоподобия. Для поиска начальных приближение для параметров p p и q q воспользуемся автокорреляционной и частичной автокорреляционной функцией. Начальное приближение p p: последний значимый пик у PACF. Начальное приближение q q: последний значимый пик у ACF. Далее обычно используется поиск по сетке вокруг подобранных значений, минимизируя информационный критерий: AIC=−2ℓ ∗ +2(p+q+1) — критерий Акаике; AIC c =−2ℓ ∗ + T−p−q−2 2(p+q+1)(p+q+2) — критерий Акаике (короткие ряды); log BIC=−2ℓ ∗ +(logT−2)(p+q+1) — Байесовский информационный критерий или критерий Шварца, где =lnL ) — логарифм функции правдоподобия, T T — длина временного ряда. Приведем некоторый план при применению модели ARIMA для прогнозирования временных рядов. Анализ выбросов: замена нерелевантых выбросов на NA или усреднение по соседним элементам. Стабилизация дисперсии (преобразования). Дифференцирование, если ряд не стационарен. Выбор пилотных p p и q q по PACF и ACF. Вокруг этих параметров подбираем оптим. модель по A I C AIC/ A I C c AIC c . Пошаговое построение прогноза: — для t ⩽ T t⩽T: — для t > T t>T: ⟹0; — для t > T t>T: Построение предсказательного интервала: — если остатки модели нормальны и гомоскедастичны (дисперсия постоянна), то строится теоретический предсказательный интервал (h)= σ 2 (1+ i=1 ∑ h−1 ψ i 2 )",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 5,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "рядов. Анализ выбросов: замена нерелевантых выбросов на NA или усреднение по соседним элементам. Стабилизация дисперсии (преобразования). Дифференцирование, если ряд не стационарен. Выбор пилотных p p и q q по PACF и ACF. Вокруг этих параметров подбираем оптим. модель по A I C AIC/ A I C c AIC c . Пошаговое построение прогноза: — для t ⩽ T t⩽T: — для t > T t>T: ⟹0; — для t > T t>T: Построение предсказательного интервала: — если остатки модели нормальны и гомоскедастичны (дисперсия постоянна), то строится теоретический предсказательный интервал (h)= σ 2 (1+ i=1 ∑ h−1 ψ i 2 ) где h h — горизонт прогнозирования, σ ^ 2 σ 2 — оценка на дисперсию шума — коэф. для ряда при его представлении в виде бесконечного процесса скользящего среднего. И σ ^ 2 σ 2 , и ψ ^ i ψ i могут быть выражены через оценки на параметры φ φ и θ θ. — иначе интервалы строятся с помощью бутстрепа. Модели SARIMA и ARIMAX Рассмотрим некоторые расширение модели ARIMA. Обобщение модели ARIMA на ряды с наличием сезонной составляющей назвается SARIMA. Пусть s s — известная сезонность ряда. Добавим в модель ARIMA( p , d , q p,d,q) компоненты, отвечающие за значения в предыдущие сезоны. Тогда модель SARIMA (p,d,q)×(P,D,Q) s может быть записана следующим образом: (1−L) d (1−L s ) D y t =μ+ a(L)A(L s ) b(L)B(L s ) ε t , где a(z)=1−φ 1 z−...−φ b(z)=1+θ 1 z+...+θ A(z)=1−φ 1 s z−⋯−φ B(z)=1+θ 1 s z+⋯+θ Q s z Q . Параметр сезонного дифференцирования D D, а также параметры P , Q P,Q подбираются из тех же соображений, что и для p , d , q p,d,q, но только с поправкой, что делается это с учетом сезонности s s. ARIMAX — обобщение модели ARIMA, которая учитывает некоторые экзогенные факторы. Пусть — ряд регрессоров, известный до начала прогноза. Простой вариант: (1−L) d y t =μ+ i=1 ∑ n a(L) a(L) b(L) ε t Общий случай: (1−L) d y t =μ+ i=1 ∑ n v i (L) u i (L) x t i + a(L) b(L) ε t Пример: x t = I { в момент времени t праздник } x t =I{в момент времени t праздник} Вышеуказанные модели можно объединить и получить SARIMAX (p,d,q) times(P,D,Q) (1−L) d (1−L s ) D y t =μ+ i=1 ∑ n v i (L) u i (L) x t i + a(L)A(L s ) b(L)B(L s ) ε t Проведем аналогию с линейной регрессией. Это линейная по признакам модель, в которой Отклик: y t y t — значение ряда в моменты времени t t Признаки: t−1 ,...,y t−p — значения ряда в предыдущие моменты времени Значение ряда за предыдущие сезоны Значения признаков в предыдущие моменты времени Значения признаков в предыдущие сезоны Ошибка: сумма шума за предыдущие моменты времени и предыдущие сезоны. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 10.3. Аналитика временных рядов Следующий параграф 10.5. Задача ранжирования",
    "metadata": {
      "title": "Модели вида ARIMA",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-vida-arima",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.4",
      "part": 6,
      "total_parts": 6,
      "source_file": "10.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Человек, который пользуется интернетом, часто начинает решение своих задач с поиска. Поисковая система по запросу помогает найти самую полезную для пользователя информацию, будь то поиск видео или научных статей. Для решения такой задачи необходимо отсортировать по полезности имеющуюся в базе информацию и выдать самую необходимую. Обычно такую сортировку называют ранжированием, полезность — релевантностью, а соответствующую задачу — задачей ранжирования. Опишем задачу формально и введём обозначения. D − коллекция документов (или других объектов) D − коллекция документов (или других объектов) Q − множество запросов Q − множество запросов набор документов, потенциально релевантных запросу ∀q∈Q D q ∈D − набор документов, потенциально релевантных запросу Задача: Отсортировать документы внутри D q D q по убыванию релевантности запросу. Уже по формулировке задачи видно, что построение решения разбивается на несколько стадий. Сначала нужно сформировать набор кандидатов D q D q , а уже потом строить финальную сортировку. Подробнее об этом в разделе про методы ранжирования. Примеры WEB-поиск Классический пример задачи ранжирования — поиск информации в интернете. Пользователь задаёт запрос, и под этот запрос формируется выдача, в которой сайты (документы) расположены по убыванию полезности. В наших обозначениях: D − база веб-страниц, проиндексированных поисковой системой D − база веб-страниц, проиндексированных поисковой системой Q − множество запросов пользователей Q − множество запросов пользователей Чтобы решать задачу ранжирования с помощью машинного обучения, необходимо иметь датасет с оценками асессоров. Обычно по парам (запрос, документ) собирают данные о том, насколько документ релевантен запросу. Такие оценки называются метками релевантности. Поиск синонимов Задача подбора синонимов по слову тоже может восприниматься как задача ранжирования, поскольку похожесть слов друг на друга — неоднозначное свойство, и некоторые пары слов больше похожи, чем другие. Поэтому можно сортировать слова по похожести на слово из запроса. Если V V — доступный словарь, то: D = V − список всех слов D=V − список всех слов Q = V − cлова, для которых можно искать синонимы Q=V − cлова, для которых можно искать синонимы Для построения моделей можно использовать датасет пар синонимичных слов. Обучив модель классификации, обычно мы получаем предсказатор вероятности положительного класса. Отсортировав слова по предсказанной вероятности синонимичности, мы решим задачу ранжирования. Рекомендательная система Рекомендательные системы встроены во многие онлайн сервисы. Если вы заходите на сервис с фильмами, система порекомендует фильмы для вас, если в социальную сеть — посты или новые видео, которые вас заинтересуют. Это тоже отбор самых полезных объектов, но «запрос» в данном случае — это сам пользователь. D = I − список айтемов, доступных на сайте (объявления, фильмы) D=I − список айтемов, доступных на сайте (объявления, фильмы) Q = U − пользователь вместе с его историей Q=U − пользователь вместе с его историей Для решения задачи построения рекомендаций используются свои методы, они рассмотрены в отдельной главе. Метрики качества ранжирования Предположим, мы решили задачу ранжирования. Обычно это делается обучением некоторой функции от запроса и документа (q,d) ( θ θ — это параметры модели). Если такая функция готова, то выдача по запросу q ∈ Q q∈Q получается сортировкой множества D q D q по убыванию (q,d). Здесь и далее, не нарушая общности, будем считать, что мы решаем задачу",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 1,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(объявления, фильмы) D=I − список айтемов, доступных на сайте (объявления, фильмы) Q = U − пользователь вместе с его историей Q=U − пользователь вместе с его историей Для решения задачи построения рекомендаций используются свои методы, они рассмотрены в отдельной главе. Метрики качества ранжирования Предположим, мы решили задачу ранжирования. Обычно это делается обучением некоторой функции от запроса и документа (q,d) ( θ θ — это параметры модели). Если такая функция готова, то выдача по запросу q ∈ Q q∈Q получается сортировкой множества D q D q по убыванию (q,d). Здесь и далее, не нарушая общности, будем считать, что мы решаем задачу WEB-поиска. Выберем множество тестовых запросов Q t Q t , на которых оценим качество нашего решения. Для формул метрик качества введём следующие обозначения: первые K элементов выдачи по запросу q T K (q) − первые K элементов выдачи по запросу q по порядку документ в выдаче d q (k) − k по порядку документ в выдаче Соответственно, (q)={d q (1) ,…,d q (k) } Бинарная релевантность Величина, которая обозначает, насколько документ подходит запросу, называется релевантностью. Способов измерить эту величину много. Обычно решение о том, релевантен документ запросу или нет, принимают асессоры — специальные люди, которые размечают данные для обучения ML-моделей. Собранные оценки называются метками релевантности, будем обозначать метку для запроса q q и документа d d за y(q,d). Для начала введём метрики для случая бинарной релевантности, когда y(q,d)∈{0,1}. Precision / Recall Если у релевантности есть всего 2 класса, то можно вспомнить метрики классификации и обобщить их для задачи ранжирования. Будем считать, что ранжирующая модель считает релевантными те документы, которые попали в первые K K элементов выдачи, то есть (q). Тогда можно вычислить метрики precision и recall в зависимости от число релевантных в топе Precision@K= K число релевантных в топе = K d∈T K (q) ∑ I{y(q,d)=1} число релевантных в топе всего релевантных min Recall@K= всего релевантных число релевантных в топе = min(K, d∈D ∑ I{y(q,d)=1}) d∈T K (q) ∑ I{y(q,d)=1} В случае с recall приходится брать минимум в знаменателе, поскольку в такой постановке модель не может выявить больше, чем K K релевантных документов. Mean Average Precision Заметим, что метрики precision и recall хоть и показывают качество нашей ранжирующей системы, но совсем не смотрят на порядок элементов в (q). Чтобы его учесть, посмотрим на Precision по тем позициям, где стоят релевантные документы, и усредним их. Такая величина называется Average Precision. AP(q)= K 1 k=1 ∑ K Precision@k⋅y(q,d q (k) ) Теперь, чтобы получить труднопереводимую на русский язык метрику Mean Average Precision, нужно усреднить значения AP по всем запросам из набора. MAP(q)= ∣Q t ∣ 1 q∈Q t ∑ AP(q) Mean Reciprocal Rank Название этой метрики переводится как средний обратный ранг. Ранжирование работает тем лучше, чем ближе к началу выдачи релевантный для пользователя документ. Для каждого запроса найдём позицию первого релевантного документа, возьмём обратное от этого числа и усредним по всем запросам. min MRR= ∣Q∣ 1 q∈Q ∑ min(i ∣ y(q,d q (i) )=1) −1 Обозначим релевантность k k-го документа в выдаче как =y(q,d q (k) ). Тогда формулу можно переписать в",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 2,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "чтобы получить труднопереводимую на русский язык метрику Mean Average Precision, нужно усреднить значения AP по всем запросам из набора. MAP(q)= ∣Q t ∣ 1 q∈Q t ∑ AP(q) Mean Reciprocal Rank Название этой метрики переводится как средний обратный ранг. Ранжирование работает тем лучше, чем ближе к началу выдачи релевантный для пользователя документ. Для каждого запроса найдём позицию первого релевантного документа, возьмём обратное от этого числа и усредним по всем запросам. min MRR= ∣Q∣ 1 q∈Q ∑ min(i ∣ y(q,d q (i) )=1) −1 Обозначим релевантность k k-го документа в выдаче как =y(q,d q (k) ). Тогда формулу можно переписать в следующем виде: MRR= ∣Q∣ 1 q∈Q ∑ i ∑ k=1 ∏ i−1 (1−R k )⋅R i ⋅ i 1 Произведение в формуле будет равно 1 1 только в случае, когда i i документ релевантный, а все до него нет. В остальных случаях произведение равно 0 0. Вещественная релевантность Рассмотрим теперь метрики для случая, когда релевантность может принимать вещественные значения. Expected Reciprocal Rank Пусть теперь =P(q,d q (i) ) — вероятность того, что документ (i) релевантен. Например, если y i y i — метка релевантности, а Y m a x Y max — максимальное её значение, то можно определить max 2 y i . Будем считать, что пользователь листает выдачу документ за документом. В каждом он находит информацию, которая ему нужна, с вероятностью R i R i . Если информацию он нашел, то он заканчивает поиск. В такой модели хорошая система позволит пользователю найти информацию как можно быстрее. Но в отличие от бинарной релевантности позиция, где пользователь закончил поиск, теперь случайная величина, как и обратная позиция. Поэтому будем считать матожидание этой величины. пользователь дошёл до док-та и остановился ) ⋅ 1 i ERR(q)= i ∑ P(пользователь дошёл до док-та и остановился)⋅ i 1 Если пользователь остановился на позиции i i, это значит, что на предыдущих документах задачу он не решил, а остановился именно на i i-ом. В модели эти события предполагаем независимыми, поэтому вероятности можно перемножить. ERR(q)= i ∑ k=1 ∏ i−1 (1−R k )⋅R i ⋅ i 1 Чтобы получить финальную метрику, усредняем эту величину по всем запросам. ERR= ∣Q∣ 1 q∈Q ∑ i ∑ k=1 ∏ i−1 (1−R k )⋅R i ⋅ i 1 Заметим, что формула совпала с формулой для метрики MRR для бинарной релевантности. pFound Рассмотрим ещё одну метрику, которая основывается на модели поведения пользователя. Метрика pFound была придумана в Яндексе и некоторое время была основной для ранжирования. Пусть релевантность задаётся одним из классов Not Rel , Rel- , Rel+ , Useful , Vital } y(q,d)∈Y={Not Rel,Rel-,Rel+,Useful,Vital} По историческим данным считаются соответствующие вероятности найти нужное в документе в зависимости от класса 0.07 , 0.14 , 0.41 , 0.61 } P q (d)={0,0.07,0.14,0.41,0.61} Отличием от предыдущей модели является введённая константа 0.15 P break =0.15 — вероятность бросить искать информацию и листать выдачу. Посчитаем вероятность P i P i того, что пользователь дошёл до позиции i i. Для этого он должен дойти до документа на позиции i − 1 i−1, не устать искать, и при этом не найти нужного",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 3,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ранжирования. Пусть релевантность задаётся одним из классов Not Rel , Rel- , Rel+ , Useful , Vital } y(q,d)∈Y={Not Rel,Rel-,Rel+,Useful,Vital} По историческим данным считаются соответствующие вероятности найти нужное в документе в зависимости от класса 0.07 , 0.14 , 0.41 , 0.61 } P q (d)={0,0.07,0.14,0.41,0.61} Отличием от предыдущей модели является введённая константа 0.15 P break =0.15 — вероятность бросить искать информацию и листать выдачу. Посчитаем вероятность P i P i того, что пользователь дошёл до позиции i i. Для этого он должен дойти до документа на позиции i − 1 i−1, не устать искать, и при этом не найти нужного в предыдущем документе. Опять же перемножаем вероятности. =1; P i =P i−1 ⋅(1−P break )⋅(1−P q (d q (i−1) )) Теперь pFound — это вероятность найти нужное в выдаче pFound(q)= i=1 (i) ) Чтобы теперь посчитать финальную метрику, усредняем pFound по всем запросам в тестовом множестве Q t Q t . nDCG Введём метрику DCG (Discounted Cumulative Gain). Будем считать, что релевантный документ в топе приносит некоторую пользу (gain) в зависимости от своей релевантности. При этом до низкого документа в выдаче могут и не долистать, поэтому он приносит меньше пользы, то есть она уменьшается. Будем дисконтировать пользу в зависимости от позиции (discount). DCG n (q)= i=1 (i) )⋅D(i) Здесь (d) — функция пользы, а D D — функция дисконтирования от позиции. Для этих функций возможны разные вариации, рассмотрим классический и упрощённый. Классический вариант: log (d)=2 y(q,d)−1 ; D(i)= log 2 (i+1) 1 Упрощённый вариант: (d)=y(q,d); D(i)= i+1 1 Иногда при реализации поисковой системы может быть понимание, как падает внимание пользователя с ростом позиции в зависимости от типа запроса. В этом случае функция дисконтирования может стать запросозависимой. Однако низкое значение метрики D C G DCG не всегда означает, что ранжирование отработало плохо. Могло быть так, что по запросу просто нет релевантных документов, или же их очень мало. Чтобы избавиться от этой проблемы, значение D C G DCG нормируют на эту метрику при идеальном ранжировании, когда документы отсортированы по истинным значениям релевантности. max nDCG(q)= maxDCG(q) DCG(q) Как и всегда, для получения метрики по набору запросов, считают среднее значение n D C G . nDCG. Дополнительные метрики Другие сигналы и экосистема Описанные выше метрики были введены для агрегации релевантности документов в топе выдачи. Но те же рассуждения и формулы могут быть применены для других сигналов, сопоставляющих запрос и документ. Вы можете придумать любую инструкцию для асессоров и собрать разметку под вашу задачу. Одним из полезных сигналов может быть свежесть документа, которую можно понять и по времени создания документа. Обычно в противовес релевантности смотрят на кликабельность элементов выдачи. Поисковым системам интересно получить больше кликов пользователей, тем не менее могут встречаться «кликбейтные» документы, которые побуждают кликать, но не решают на самом деле задачу пользователя. Кликабельность можно замерять как DCG предсказатора вероятности клика. Также важно следить за чистотой выдачи. Нужно не допускать в топ мошеннические документы, шокирующие документы и документы 18+ в поиске для детей. В качестве метрики можно замерять DCG или MAP «плохих» документов в топе. Разнообразие и метрики рекомендаций Бывает полезно следить за тем,",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 4,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "из полезных сигналов может быть свежесть документа, которую можно понять и по времени создания документа. Обычно в противовес релевантности смотрят на кликабельность элементов выдачи. Поисковым системам интересно получить больше кликов пользователей, тем не менее могут встречаться «кликбейтные» документы, которые побуждают кликать, но не решают на самом деле задачу пользователя. Кликабельность можно замерять как DCG предсказатора вероятности клика. Также важно следить за чистотой выдачи. Нужно не допускать в топ мошеннические документы, шокирующие документы и документы 18+ в поиске для детей. В качестве метрики можно замерять DCG или MAP «плохих» документов в топе. Разнообразие и метрики рекомендаций Бывает полезно следить за тем, как документы в топе соотносятся друг с другом. В частности, нужно не допускать, чтобы все документы выдачи были с одного хоста и чтобы в них не было написано одно и то же. Для этого используются алгоритмы группировки и дедупликации. Также, если рассматривать поиск как решение задачи рекомендации документов, можно измерять метрики новизны и serendipity, про которые подробнее рассказано в главе о рекомендательных системах. Методы обучения ранжированию Рассмотрим некоторую модель (q,d), по предсказаниям которой мы будем сортировать документы d d по запросу q q (здесь θ θ — это параметры модели). Мы хотим обучить модель так, чтобы у неё было оптимальное значение одной из метрик ранжирования, например, NDCG. Заметим, что если совсем немного поменять θ θ, то предсказания модели (q,d) изменятся тоже несильно. Но небольшие изменения в предсказаниях могут не привести к изменению порядка документов. Тогда не изменится и метрика NDCG. Получается, что NDCG как функция от параметров θ θ является кусочно постоянной, поэтому нельзя оптимизировать её напрямую. Наша дальнейшая задача — представить методы, позволяющие получить модель, оптимальную по NDCG или другой аналогичной метрике ранжирования. Методы обучения ранжированию обычно делят на 3 типа: Поточечный (pointwise) подход В этом подходе у нас известны некоторые оценки релевантности каждой пары запрос-документ, и модель учится предсказывать эти оценки. Взаимоотношения между разными документами внутри D q D q не рассматриваются. Попарный (pairwise) подход Здесь во время обучения используют тройки (q,d 1 ,d 2 ), где — документы из D q D q , причём d 1 d 1 релевантнее d 2 d 2 по запросу q q. При этом модель всё равно может давать предсказания релевантности по паре ( q , d ) (q,d). Списочный (listwise) подход В данном подходе для обучения используются перестановки документов из D q D q . Например, асессорские оценки дают наилучшую известную сортировку. Для её получения нужно сначала показать на выдаче докумены с самой высокой оценкой, затем со следующей по порядку и т.д. Будем рассматривать каждый из подходов по очереди. Поточечный подход Сведение к регрессии и классификации Рассмотрим простейшую постановку задачи, в которой у нас есть набор запросов Q Q, для каждого запроса q ∈ Q q∈Q имеется набор документов D q D q , который необходимо отсортировать, а в качестве обучающей выборки известны асессорские оценки релевантности для некоторых пар запрос-документ ( q , d ) (q,d). Будем обозначать множество возможных оценок Y Y, конкретную оценку — y(q,d). Пусть Y = R Y=R — множество действительных чисел. Тогда мы можем обучить",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 5,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "докумены с самой высокой оценкой, затем со следующей по порядку и т.д. Будем рассматривать каждый из подходов по очереди. Поточечный подход Сведение к регрессии и классификации Рассмотрим простейшую постановку задачи, в которой у нас есть набор запросов Q Q, для каждого запроса q ∈ Q q∈Q имеется набор документов D q D q , который необходимо отсортировать, а в качестве обучающей выборки известны асессорские оценки релевантности для некоторых пар запрос-документ ( q , d ) (q,d). Будем обозначать множество возможных оценок Y Y, конкретную оценку — y(q,d). Пусть Y = R Y=R — множество действительных чисел. Тогда мы можем обучить любую модель регрессии на признаках пар ( q , d ) (q,d) для предсказания оценок асессоров. Это может быть и линейная модель, и градиентный бустинг, и нейронная сеть. Обучать модель можно, например, оптимизируя MSE: min ⁡ q∈Q ∑ d∈D q ∑ ( y (q,d)−y(q,d)) 2 →min Аналогично можно сводить задачу к классификации, если метки релевантности y y бинарны или категориальны. Например, при шкале Y={0,1,2,3,4,5}. Оптимизировать в данном случае можно кросс-энтропийную функцию потерь. PRank В случае классификации мы учим модель разделять классы, но никаким образом не задаём, что на метках имеется порядок. Мы не даём алгоритму никакой информации о том, что метка 4 находится между метками 5 и 3 и наоборот. Чтобы побороть эту проблему, была придумана модификация линейной модели, которая получила название PRank и была описана в статье Pranking with ranking. Если предположить, что метки релевантности — это целые числа от 0 0 до K K, то есть Y={0,1,2,…,K}, то можно ввести пороги для значений ранжирующей функции, которые разделяют классы друг от друга. В качестве ранжирущей функции возьмём обычную линейную, то есть ⟨ θ , x ⟩ ⟨θ,x⟩, где x x — вектор признаков. Обозначим через ,…,b K−1 границы классов. Они будут изменяться в процессе обучения. Также фиктивно введём =∞. Чтобы предсказать класс, будем искать первую границу, которая больше вычисленной линейной функции: min (q,d)=min{r : x ⊤ θ−b r <0} Коротко опишем процесс обучения. Представим, что мы получили очередной объект и вычислили линейную функцию ⟨ θ , x ⟩ ⟨θ,x⟩. Отметим на оси её значение и границы классов: Artboard Если предсказан ранг 1, а правильный класс для объекта 4, то необходимо, во-первых, обновить вектор θ θ, а во-вторых, сдвинуть границы других классов в сторону получившегося предсказания. Artboard После осуществления сдвигов, предсказание на точке из обучающей выборки становится ближе к правильному. Artboard Мы не будем приводить конкретные формулы для обновления обучаемых параметров; вы можете посмотреть их в статье. Попарный подход Начнём рассмотрение попарного подхода. Чтобы его применить, нужен датасет, состоящий из троек (q,d 1 ,d 2 ), где , причём известно, что по запросу q q документ d 1 d 1 релевантнее, чем d 2 d 2 . Будем обозначать такое соотношение Замечание. Такие данные можно собирать с использованием асессорской Side-by-Side разметки, в которой асессоры отмечают, какой из двух предложенных документов релевантнее по заданному запросу. Также можно собирать данные с помощью пользовательских логов. Например, если пользователь пролистал первые 3 документа по запросу q q, а решил свою задачу только на 4-ом,",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 6,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "параметров; вы можете посмотреть их в статье. Попарный подход Начнём рассмотрение попарного подхода. Чтобы его применить, нужен датасет, состоящий из троек (q,d 1 ,d 2 ), где , причём известно, что по запросу q q документ d 1 d 1 релевантнее, чем d 2 d 2 . Будем обозначать такое соотношение Замечание. Такие данные можно собирать с использованием асессорской Side-by-Side разметки, в которой асессоры отмечают, какой из двух предложенных документов релевантнее по заданному запросу. Также можно собирать данные с помощью пользовательских логов. Например, если пользователь пролистал первые 3 документа по запросу q q, а решил свою задачу только на 4-ом, можно считать, что первые 3 документа были хуже. Пользовательских логов достаточно много, с их помощью можно обучать разные модели для ранжирования на основе кликов. Но надо помнить, что совсем не всегда те документы, на которые первоначально хочется кликнуть, релевантны и содержат необходимую информацию. Чтобы учесть это в модели, можно рассматривать только клики, после которых пользователь надолго остался на странице. Дополнительно можно смешивать модели на кликах с моделями на оценках асессоров, что поможет избежать проблемы смещения обучающей выборки в текущее ранжирование. Поясним, из-за чего может возникнуть эта проблема. Логи строятся по результатам взаимодействия пользователя с нашей ранжирующей моделью, и может оказаться так, что обучающая выборка будет состоять только из документов, уже попавших в топ выдачи. Классификатор на тройках В качестве самого простого решения снова рассмотрим сведение задачи ранжирования к уже известным задачам машинного обучения. А именно, будем решать задачу классификации троек (q,d 1 ,d 2 ). В качестве целевой переменной запишем 0 0, если лучше документ d 1 d 1 , и 1 1, если лучше d 2 d 2 . Собрав признаки для каждой тройки, можем обучить любой классификатор. Для этого можем взять и нейронную сеть, и линейную модель, и методы на основе деревьев. Чтобы отсортировать документы внутри D q D q по запросу q q, можем воспользоваться стандартным алгоритмом сортировки с компаратором, задаваемым предсказаниями нашего классификатора. Проблема в том, что компаратор должен быть как минимум транзитивным. Гарантировать такое свойство для большинства моделей машинного обучения мы не можем. RankingSVM Но если ввести модель специальным образом, можно гарантировать транзитивность предсказаний. Снова воспользуемся линейной моделью. Задача. Найти вектор весов θ θ, для которого ∀q∈Q ∀d i ,d j : d i ≺d j было бы выполнено ,θ⟩<⟨x j ,θ⟩. Здесь – векторы признаков для пар (q,d i ) и (q,d j ) соответственно. Так как требуется выполнение скалярных произведений вида ,θ⟩, где — вектор поэлементной разницы признаков, можно обучить линейную модель на парах, где признак пары – это как раз вектор Если в качестве модели взять SVM, получится классический метод обучения ранжированию, который называется RankingSVM. Чтобы сравнить 2 документа по запросу, надо сравнить ,θ⟩ с нулём. Получается, что для двух одинаковых документов это выражение всегда 0 0. Кроме того, выполнена требуемая от компаратора транзитивность. RankNet Пусть имеется обучающая выборка, в которой известны метки релевантности для пар запрос-документ. – признаковые описания – асессорские оценки – запросы . X Y Q ={x i } – признаковые описания (q i ,d i ), ={y i",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 7,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— вектор поэлементной разницы признаков, можно обучить линейную модель на парах, где признак пары – это как раз вектор Если в качестве модели взять SVM, получится классический метод обучения ранжированию, который называется RankingSVM. Чтобы сравнить 2 документа по запросу, надо сравнить ,θ⟩ с нулём. Получается, что для двух одинаковых документов это выражение всегда 0 0. Кроме того, выполнена требуемая от компаратора транзитивность. RankNet Пусть имеется обучающая выборка, в которой известны метки релевантности для пар запрос-документ. – признаковые описания – асессорские оценки – запросы . X Y Q ={x i } – признаковые описания (q i ,d i ), ={y i } – асессорские оценки, ={q i } – запросы. Введём модель для ранжирующей функции (x), дифференцируемую по θ θ. Например, это может быть линейная модель или нейронная сеть. Также возможно применение композиций деревьев. Ниже мы будем описывать оптимизационную процедуру для моделей, которые обучаются с помощью градиентного спуска; для деревьев потребуются другие методы. Будем рассматривать события , то есть события вида «документ d i d i релевантнее d j d j по запросу =q». Из разметки асессоров мы можем задать вероятности таких событий. если если если 1,если y i >y j 0,если y i <y j 2 1 ,если y i =y j Если асессорских оценок на каждый документ несколько, можем по-разному агрегировать эти оценки в Q i j Q ij . Если же доступна попарная разметка (какой из документов релевантнее запросу q q), то вероятностью можно назвать долю оценок, в которых d i d i признан лучше, чем d j d j . Далее введём оценку вероятности этого события, порождённую моделью. 1+e −σ(s i −s j ) 1 , s ), s Для пары ) рассмотрим случайную величину =I[x i ≻x j ] Согласно асессорам, ∼Bern(Q ij ). Согласно модели, ∼Bern(P ij ). Задача обучения в том, чтобы уравнять эти два распределения между собой для всех пар документов. Поэтому в качестве функции потерь будем использовать KL-дивергенцию. Введём для каждой пары лосс log log =KL(Bern(Q ij )∣∣Bern(P ij ))=Q ij log P ij Q ij +(1−Q ij )log 1−P ij 1−Q log log =H(Q ij )−Q ij logP ij −(1−Q ij )log(1−P ij ) Тут H(Q ij ) – энтропия, не зависящая от P i j P ij , а значит и от параметров модели θ θ. Для определённых выше Q i j Q ij выполнены следующие свойства: C i j = log если log если log ⁡ 2 , если если =log(1+e −σ(s i −s j ) ),если y i >y j =log(1+e −σ(s j −s i ) ),если y i <y j =log2,если y i =y j и s i  =s j =0,если y i =y j и s i =s j Видно, что для двух документов с разными истинными метками релевантности такой метод обучения штрафует модель за одинаковые предсказания. Это свойство очень полезно, поскольку в случае одинаковых предсказаний непонятно, в каком порядке располагать документы. Полная функция потерь выглядит следующим образом: min ⁡ θ Q(θ)= q∈Q ∑ i,j:q min Минимизировать её можно при помощи градиентного спуска или различных",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 8,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "если =log(1+e −σ(s i −s j ) ),если y i >y j =log(1+e −σ(s j −s i ) ),если y i <y j =log2,если y i =y j и s i  =s j =0,если y i =y j и s i =s j Видно, что для двух документов с разными истинными метками релевантности такой метод обучения штрафует модель за одинаковые предсказания. Это свойство очень полезно, поскольку в случае одинаковых предсказаний непонятно, в каком порядке располагать документы. Полная функция потерь выглядит следующим образом: min ⁡ θ Q(θ)= q∈Q ∑ i,j:q min Минимизировать её можно при помощи градиентного спуска или различных его модификаций. t+1 =θ k t −η i,j:q i,j:q Вычислим производные функции потерь по s i s i . Пусть . Тогда: 1+e −σ(s i −s j ) −σe −σ(s i −s j ) = 1+e σ(s 1+e −σ(s i −s j ) σe −σ(s i −s j ) = 1+e σ(s то есть Подставим полученное выражение в производную функции потерь по весам. обозначаем обозначаем λ ij 1+e σ(s Аналогично получаем выражения для остальных случаев соотношения между Определим теперь j: x j: x Тогда получаем Итерацию градиентного спуска теперь можно записать в более простом виде: t+1 Таким образом, мы можем делать SGD не по парам документов, а по отдельным документам. Это увеличивает скорость сходимости. Получается, что λ i λ i зависит от номера документа и от попарных разностей скоров модели на документах , при этом не зависит от производных самих s s по параметру θ θ. Введённые λ i λ i можно представить в виде стрелок, которые прикреплены к каждому документу в поисковой выдаче. Направление стрелки означает, куда мы хотим перенести документ, чтобы выросла нужная метрика, а длина – насколько сильно. lambdas LambdaRank Задача этого метода в том, чтобы соединить RankNet и наше желание напрямую оптимизировать введённые ранее кусочно постоянные метрики качества, например, NDCG. Обозначим через Z(q, y )=Z(q,s) значение этой метрики для запроса q q при ранжировании функцией y ^ y . Попытаемся придумать гладкую попарную функцию потерь i,jq где оптимизация которой была бы эквивалентна оптимизации Z Z. Заметим, впрочем, что для обучения сама Q ‾ Q нам не нужна, а нужны только производные, которые, как и в случае RankNet, можно записать в виде В данном случае мы хотели бы задать специальным образом: так, чтобы сдвиг в направлении антиградиента вёл к уменьшению метрики Z Z. Ясно, что не любое выражение для λ i λ i может задавать градиент. Чтобы проверить существование функции потерь C ‾ C , применим следующий частный случай леммы Пуанкаре: Лемма. Пусть ,…,θ n ),f 2 (θ 1 ,…,θ n ),…,f n (θ 1 ,…,θ n ) – функции, такие что ∀i,j Тогда существует функция F F, такая что Значит, нужно ввести лямбды таким образом, чтобы совпадали смешанные производные. Определим y i j y ij – функцию релевантности, в которой поменяли местами если если иначе y(x j ),если x=x i y(x i ),если x=x j y(x) иначе Обозначим также через =Z(q, y ij )−Z(q, y ) приращение метрики при перестановке местами . В методе LambdaRank λ",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 9,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "проверить существование функции потерь C ‾ C , применим следующий частный случай леммы Пуанкаре: Лемма. Пусть ,…,θ n ),f 2 (θ 1 ,…,θ n ),…,f n (θ 1 ,…,θ n ) – функции, такие что ∀i,j Тогда существует функция F F, такая что Значит, нужно ввести лямбды таким образом, чтобы совпадали смешанные производные. Определим y i j y ij – функцию релевантности, в которой поменяли местами если если иначе y(x j ),если x=x i y(x i ),если x=x j y(x) иначе Обозначим также через =Z(q, y ij )−Z(q, y ) приращение метрики при перестановке местами . В методе LambdaRank λ i j λ ij определяется следующим образом: 1+e σ(s i −s j ) −σ ∣ΔZ ij ∣, где σ σ – некоторая константа. Множитель ∣ΔZ ij ∣ кусочно постоянен, так что не повлияет на градиент. Вопрос на подумать. Проверьте, что для указанных λ i j λ ij действительно выполнено условие леммы Пуанкаре. Авторы метода проверяли его для оптимизации NDCG. Они пытались случайными сдвигами параметра улучшить NDCG после оптимизации через LambdaRank. Доля успешных сдвигов оказалось очень мала, так что экспериментально подтверждается успешная оптимизация этим методом недифференцируемых метрик ранжирования. LambdaMART Этот метод является конкретной реализацией подхода LambdaRank. В нём для предсказания (x) строится модель градиентного бустинга на решающих деревьях. Каждое дерево обучается на градиент функции потерь предыдущей итерации алгоритма (и градиент мы как раз умеем считать, хотя саму функцию потерь – нет). Структура дерева определяется жадными по MSE разделениями. При этом размер шага (коэффициент, с которым берётся значение в следующем дереве) задаётся не один для всей модели, а подбирается во всех листах каждого дерева при помощи метода Ньютона. Более подробно о последних трёх методах можно почитать в оригинальной статье от Microsoft: From RankNet to LambdaRank to LambdaMART. Списочный подход SoftRank Вспомним, что основной проблемой, из-за которой невозможно оптимизировать NDCG и похожие метрики напрямую, является их кусочная линейность. Идея метода SoftRank — сгладить метрику, чтобы при небольших изменениях параметров модели она тоже изменялась. Для этого оценку релевантности документа будем рассматривать не как константу, а как случайную величину. Сглаживание метрики рассмотрим на примере. Пусть имеются документы для запроса q q, а ранжирующая модель дала им соответственно оценки релевантности . Тогда, если это случайные величины, то они константны и их распределение вырождено. В связи с этим порядок документов на выдаче тоже определён однозначно, и первое место занимает документ с наибольшей оценкой. Artboard Чтобы поменять метрику, будем считать, что оценка релевантности документа d i d i по нашей модели имеет распределение N(s i ,σ 2 ), где σ 2 σ 2 — гиперпараметр метода. Чтобы отсортировать документы, будем генерировать число из этого распределения и ранжировать по нему. Тогда может случиться так, что документ с самым большим s i s i окажется на последнем месте. Но с наибольшей вероятностью он всё равно будет первым. И распределение позиций документов на выдаче будет выглядеть примерно так: Artboard Теперь, чтобы получить метрику, осталось только в формуле NDCG заменить дискаунт на его математическое ожидание. soft = G max 1 j=1 ∑ N g(d j )ED(j) Можно заметить, что теперь небольшие сдвиги",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 10,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "по нашей модели имеет распределение N(s i ,σ 2 ), где σ 2 σ 2 — гиперпараметр метода. Чтобы отсортировать документы, будем генерировать число из этого распределения и ранжировать по нему. Тогда может случиться так, что документ с самым большим s i s i окажется на последнем месте. Но с наибольшей вероятностью он всё равно будет первым. И распределение позиций документов на выдаче будет выглядеть примерно так: Artboard Теперь, чтобы получить метрику, осталось только в формуле NDCG заменить дискаунт на его математическое ожидание. soft = G max 1 j=1 ∑ N g(d j )ED(j) Можно заметить, что теперь небольшие сдвиги параметров будут менять распределение позиций документов, а значит и soft будет изменяться. Поэтому функция становится дифференцируемой, и её можно использовать как функцию потерь. При этом для того, чтобы вычислить распределение рангов, необходимо использовать отсортированный список документов. А значит, хоть напрямую оптимизируемый функционал и не зависит от перестановок, этот метод можно отнести к списочному подходу. Подробнее об этом методе можно прочитать в статье. ListNet Следующие 2 метода, которые мы рассмотрим, работают с перестановками документов для одного запроса. Если заданы оценки релевантности y(q,d i=1,…,N для запроса q q, то они формируют распределение на перестановках. Пусть дана перестановка π=(q 1 ,…,q N ). Определим её вероятность следующим образом: P(π∣y)= j=1 ∏ N k=j ∑ N y(q,d k ) y(q,d j ) Такая вероятностная модель называется моделью Люcа-Плакетта. Для примера рассмотрим 3 документа и оценки y y. Тогда вероятность перестановки A B C ABC запишется так: на первом месте ) ⋅ P(ABC∣y)=P(A на первом месте)⋅ P ( B на втором месте ∣ A на первом месте ) ⋅ P(B на втором месте ∣ A на первом месте)⋅ P ( C на третьем месте ∣ A на первом месте и B на втором месте ) = P(C на третьем месте ∣A на первом месте и B на втором месте)= y(A)+y(B)+y(C) y(A) ⋅ y(B)+y(C) y(B) ⋅ y(C) y(C) Аналогично методу RankNet, мы имеем распределение, которое можно сформировать из оценок асессоров y y и из оценок модели y ^ y . А значит можно в качестве функции потерь использовать KL-дивергенцию между этими распределениями. min ⁡ θ , KL(P(π∣y)∣∣P(π∣ y θ ))→ θ min , где θ θ — параметр модели. Однако различных перестановок получается N ! N!, а значит, даже чтобы просто вычислить дивергенцию Кульбака-Лейблера, потребуется немало времени, не говоря уже про оптимизацию этой функции потерь. Поэтому вместо того, чтобы рассматривать вероятность полной перестановки, смотрят на распределение индекса первого документа на выдаче. (j)= d∈D q ∑ y (q,d) y (q,d (j)= d∈D q ∑ y(q,d) y(q,d j ) Оптимизируют KL-дивергенцию между этими распределениями: min ⁡ θ KL(P y ∣∣P y )→ θ min ListMLE В этом методе рассматривают вероятность одной «правильной» перестановки. В данном случае под правильной перестановкой π q π q понимается та, которая получается, если упорядочить документы из D q D q по убыванию асессорских оценок y y. Логично, что хорошая модель должна давать большую вероятность такой перестановке, поэтому именно она и максимизируется. max ⁡ θ P(π max Всё сводится к поиску оценки максимального правдоподобия,",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 11,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "индекса первого документа на выдаче. (j)= d∈D q ∑ y (q,d) y (q,d (j)= d∈D q ∑ y(q,d) y(q,d j ) Оптимизируют KL-дивергенцию между этими распределениями: min ⁡ θ KL(P y ∣∣P y )→ θ min ListMLE В этом методе рассматривают вероятность одной «правильной» перестановки. В данном случае под правильной перестановкой π q π q понимается та, которая получается, если упорядочить документы из D q D q по убыванию асессорских оценок y y. Логично, что хорошая модель должна давать большую вероятность такой перестановке, поэтому именно она и максимизируется. max ⁡ θ P(π max Всё сводится к поиску оценки максимального правдоподобия, поэтому метод и называется ListMLE. Практические советы Популярные признаки Выше показано, как можно построить модель, если уже известны признаки для пар запрос-документ ( q , d ) (q,d), а также, как можно собрать целевые переменные для обучения. Но какие стоит взять признаки, чтобы получить хорошую модель? Признаки для моделей ранжирования можно разделить на 3 типа: запросные, документные и запросно-документные. Первые зависят только от запросы, вторые только от документа, а третьи от всей пары, то есть для их вычисления необходимо сопоставить запрос и документ. Конечно, наибольший интерес представляют запросно-документные факторы. Но и другие группы факторов могут быть полезны. Запросными являются, например, следующие факторы: Количество слов в запросе Язык запроса Страна, из которой задали запрос Значение классификаторов P ( запрос про машинное обучение ) P(запрос про машинное обучение) P ( запрос пиратский ) P(запрос пиратский) Многие модели могут подстроиться под эти факторы и отдельно обучиться под различные значения запросных признаков. Например, так модель может по-разному реагировать на запросы из разных стран. TF-IDF Чтобы сопоставить запрос и документ, можно использовать TF-IDF слов запроса. Например, можно просуммировать его по всем словам из запроса и получить фактор для ранжирования. Подробно о том, как считать TF-IDF, можно прочитать в главе про NLP. DSSM и другие нейросетевые факторы Запросно-документные факторы можно получать, «соединяя» векторные представления запроса и документа. Классическим способом такого соединения является простой подсчёт косинуса угла между векторами. Если обучить модель, которая для пар, где документ релевантен запросу, выдаёт вектора, похожие на сонаправленные, то скалярное произведение становится оценкой релевантности. Если оно близко к единице, векторы сонаправлены, а значит документ подходит запросу. При этом обычно нормируют векторы на выходе, чтобы косинус и скалярное произведение были одним и тем же. В качестве модели эмбеддинга можно использовать полносвязную нейронную сеть. На вход такой сети можно подать BagOfWords вектор или же вектор TfIdf. Эти векторы большой размерности нейросеть преобразует в меньшие. Обычно размер выходного слоя выбирают, балансируя между качеством и ресурсами, необходимыми на расчёт и хранение выхода сети. Этот подход был назван Deep Structured Semantic Models и описан в статье от Microsoft. Идею можно применять в целом в любой задаче ранжирования для своих типов «документов». Посмотреть, как она была воплощена в Web поиске для поиска по смыслу, можно в блоге Яндекса. Если вкратце, у модели следующие особенности: На входе у модели не все тексты, а только заголовок документа и запрос; Для уменьшения размера входа текст разбит на буквенные триграммы, вектор на входе - это Bag Of Trigrams; Архитектура обработки запроса",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 12,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "меньшие. Обычно размер выходного слоя выбирают, балансируя между качеством и ресурсами, необходимыми на расчёт и хранение выхода сети. Этот подход был назван Deep Structured Semantic Models и описан в статье от Microsoft. Идею можно применять в целом в любой задаче ранжирования для своих типов «документов». Посмотреть, как она была воплощена в Web поиске для поиска по смыслу, можно в блоге Яндекса. Если вкратце, у модели следующие особенности: На входе у модели не все тексты, а только заголовок документа и запрос; Для уменьшения размера входа текст разбит на буквенные триграммы, вектор на входе - это Bag Of Trigrams; Архитектура обработки запроса и документа разная; Особый способ генерации негативных примеров. Схематически архитектура модели показана на рисунке ниже: Artboard Конечно, чтобы заставить простую архитектуру давать хорошее качество, нужно экспериментировать с методами сбора данных и улучшения качества, которые описаны в других главах учебника. Однако при наличии соответствующих мощностей можно улучшать качество, изменяя архитектуру обработки текста. В частности, модели на основе трансформеров (например, BERT) улучшают качество. Это же касается и косинуса, то есть соединительной части. Вполне можно вместо него использовать полносвязную сеть или даже трансформерную архитектуру. Трансформеры, которые по-отдельности обрабатывают сущности запроса и документа, в Яндексе названы split-моделями и более подробно описаны в том же блоге на Хабре. Метафичи В предыдущем пункте мы уже ввели фактор для модели, который сам является значением модели. Мы улучшили качество с помощью стекинга DSSM и итоговой модели. Аналогичным образом можно использовать предсказания моделей, обученных на разные таргеты и разными способами: их можно добавить к другим фичам для итоговой модели. К сожалению, если факторов-моделей (метафичей) много, такая модель не будет удовлетворять требованиям по времени работы. В этом случае можно прибегнуть к дистилляции знаний большой модели в более компактную. Фичи, зависящие от времени Известно, что модель машинного обучения работает хорошо (а точнее, ожидаемо) только в случае, когда при её применении распределения данных и факторов похожи на те, которые использовались при обучении. Но в продакшн-системах постоянное выполнение этого свойства невозможно обеспечить. Поэтому главный совет: настраивайте мониторинги качества ваших ML-систем для того, чтобы не пропускать моменты поломки. Качество может снизиться как из-за изменений в логике других сервисов, на которые вы полагаетесь при вычислении факторов, так и из-за появления новых трендов. Например, это происходит при появлении новых тем, о которых раньше не было запросов. Показательный случай — пандемия вируса COVID-19, который стал резко появляться среди запросов пользователей. Но бывают факторы, которые зависят от времени сами по себе. Так, для ранжирования новых документов может быть полезно знать возраст документа. Со временем распределение этого фактора сдвигается вправо, поскольку многие старые документы не удаляются из базы. Получается, фактор старого документа меняется, а релевантность нет. Появление фичей вне ожидаемых значений может привести к непредсказуемому поведению модели. Так что их нужно применять с осторожностью, лучше в тех моделях, которые можно быстро обучить и обновить в продакшне. Ещё лучшим решением будет преобразовать или отнормировать фичи таким образом, чтобы их распределение не менялось так кардинально. Наш фактор с возрастом документа можно преобразовать в индикатор того, что возраст документа меньше одного дня. Тогда только один раз за историю документа этот фактор будет",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 13,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "полезно знать возраст документа. Со временем распределение этого фактора сдвигается вправо, поскольку многие старые документы не удаляются из базы. Получается, фактор старого документа меняется, а релевантность нет. Появление фичей вне ожидаемых значений может привести к непредсказуемому поведению модели. Так что их нужно применять с осторожностью, лучше в тех моделях, которые можно быстро обучить и обновить в продакшне. Ещё лучшим решением будет преобразовать или отнормировать фичи таким образом, чтобы их распределение не менялось так кардинально. Наш фактор с возрастом документа можно преобразовать в индикатор того, что возраст документа меньше одного дня. Тогда только один раз за историю документа этот фактор будет изменён, при этом распределение этого фактора в документах изо дня в день будет похожим. Многостадийное ранжирование Представьте, что вы смогли обучить сложную модель, применив все описанные выше подходы: добавили метафичи, которые сами по себе являются формулами, обучили DSSM, BERT, или даже более тяжёлую нейросеть. Наконец, вам пришёл запрос от пользователя, и вы намерены отсортировать все документы в базе по оценке релевантности, которую даёт ваша модель. Если вы ранжируете 1000, 10000 или даже 100000 документов, это ещё может получиться, и пользователь дождётся ответа. Но что делать, если в вашей базе миллионы, а то и миллиарды документов? Конечно же вам на помощь могут прийти распределённые системы, и разбив документы по разным инстансам сервиса, который рассчитывает прогноз, вы ускорите получение ответа. Но даже с учётом распределённых вычислений быстро вычислить BERT миллиард раз будет либо очень дорого, либо очень долго. Поэтому применяется подход многостадийного ранжирования. Тяжёлые модели применяются не сразу. Сначала можно ограничиться применением самых простых оценок релевантности. Как пример, можно просто взять TF-IDF или BM25. Простой моделью отсекаются самые нерелевантные документы, а прошедшие дальше уже сортируются с помощью более ресурсоёмкой и продвинутой модели. В зависимости от количества документов в базе вы можете соединить столько уровней, сколько нужно, для получения приемлемого времени ответа. Конечно, количество параметров такой системы возрастает в несколько раз, но это делает возможным быстрое взаимодействие с пользователем. Готовые решения Если вы разрабатываете продукт, для которого требуется поиск по текстовым документам, для начала вы можете воспользоваться готовыми решениями. Одним из самых популярных сервисов для поиска является Sphinx. Этот сервис позволяет индексировать текстовые документы и сохранять их в базу данных (как SQL, так и NoSQL). Через специальный SQL-подобный язык запросов он позволяет получать списки релевантных документов и сопутствующие им данные. Таким образом можно доставать только документы, подходящие по заданным фильтрам, отсортированные по релевантности. Это может быть полезно, например, для реализации поиска по интернет-магазину. Более того, получив некоторый топ выдачи, вы можете переранжировать его, сразу используя сложную модель или учитывая другие потребности ваших пользователей. Другие альтернативы для текстового поиска можно посмотреть в статье. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 10.4. Модели вида ARIMA Следующий параграф 11.1. Обучение с подкреплением",
    "metadata": {
      "title": "Задача ранжирования",
      "url": "https://education.yandex.ru/handbook/ml/article/zadacha-ranzhirovaniya",
      "course": "ml",
      "chapter": "10. Практические главы",
      "chapter_id": "10.5",
      "part": 14,
      "total_parts": 14,
      "source_file": "10.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "До сих пор опыт, благодаря которому было возможно обучение в наших алгоритмах, был задан в виде обучающей выборки. Насколько такая модель обучения соотносится с тем, как учится, например, человек? Чтобы научиться кататься на велосипеде, печь тортики или играть в теннис, нам не нужны огромные датасеты с примерами того, что нужно делать в каждый момент; вместо этого мы способны обучаться методом проб и ошибок (trial and error), предпринимая попытки решить задачу, взаимодействуя с окружающим миром, и как-то улучшая своё поведение на основе полученного в ходе этого взаимодействия опыта. В обучении с подкреплением (reinforcement learning, RL) мы хотим построить алгоритм, моделирующий обучение методом проб и ошибок. Вместо получения обучающей выборки на вход такой алгоритм будет взаимодействовать с некоторой средой (environment), окружающим миром, а в роли «разметки» будет выступать награда (reward) — скалярная величина, которая выдаётся после каждого шага взаимодействия со средой и показывает, насколько хорошо алгоритм справляется с поставленной ему задачей. Например, если вы печёте тортики, то за каждый испечённый тортик вы получаете +1, а если вы пытаетесь кататься на велосипеде, то за каждое падение с велосипеда вам прилетает -1. Награда не подсказывает, как именно нужно решать задачу и что вообще нужно делать; Награда может быть отложенной во времени (вы нашли в пустыне сокровища, но чтобы получить заслуженные тортики, вам ещё понадобится куча времени, чтобы выбраться из пустыни; а награда приходит только за тортики) или сильно разреженной (большую часть времени давать агенту +0). Всё это сильно отличает задачу от обучения с учителем; Награда предоставляет какой-то «сигнал» для обучения (хорошо/плохо), которого нет, например, в обучении без учителя. источник картинки — курс UC Berkeley AI Постановка задачи Теперь попробуем формализовать всю эту концепцию и познакомиться с местной терминологией. Задача обучения с подкреплением задаётся Марковским Процессом Принятия Решений (Markov Decision Process или сокращённо MDP) это четвёрка (S,A,P,r), где: S S — пространство состояний (state space), множество состояний, в которых в каждый момент времени может находиться среда. A A — пространство действий (action space), множество вариантов, из которых нужно производить выбор на каждом шаге своего взаимодействия со средой. P P — функция переходов (transition function), которая задаёт изменение среды после того, как в состоянии s ∈ S s∈S было выбрано действие a ∈ A a∈A. В общем случае функция переходов может быть стохастична, и тогда такая функция переходов моделируется распределением p(s ′ ∣s,a): с какой вероятностью в какое состояние перейдёт среда после выбора действия a a в состоянии r:S×A→R — функция награды (reward function), выдающая скалярную величину за выбор действия a a в состоянии s s. Это наш «обучающий сигнал». Традиционно субъект, взаимодействующий со средой и влияющий на неё, называется в обучении с подкреплением агентом (agent). Агент руководствуется некоторым правилом, возможно, тоже стохастичным, как выбирать действия в зависимости от текущего состояния среды, которое называется стратегией (policy; термин часто транслитерируют и говорят политика) и моделируется распределением π(a∣s). Стратегия и будет нашим объектом поиска, поэтому, как и в классическом машинном обучении, мы ищем какую-то функцию. Взаимодействие со средой агента со стратегией π(a∣s) моделируется так. Изначально среда находится в некотором состоянии s 0 s 0 . Агент сэмплирует действие",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 1,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "величину за выбор действия a a в состоянии s s. Это наш «обучающий сигнал». Традиционно субъект, взаимодействующий со средой и влияющий на неё, называется в обучении с подкреплением агентом (agent). Агент руководствуется некоторым правилом, возможно, тоже стохастичным, как выбирать действия в зависимости от текущего состояния среды, которое называется стратегией (policy; термин часто транслитерируют и говорят политика) и моделируется распределением π(a∣s). Стратегия и будет нашим объектом поиска, поэтому, как и в классическом машинном обучении, мы ищем какую-то функцию. Взаимодействие со средой агента со стратегией π(a∣s) моделируется так. Изначально среда находится в некотором состоянии s 0 s 0 . Агент сэмплирует действие из своей стратегии ∼π(a 0 ∣s 0 ). Среда отвечает на это, сэмплируя своё следующее состояние ∼p(s 1 ∣s 0 ,a 0 ) из функции переходов, а также выдаёт агенту награду в размере r(s 0 ,a 0 ). Процесс повторяется: агент снова сэмплирует a 1 a 1 , а среда отвечает генерацией s 2 s 2 и скалярной наградой r(s 1 ,a 1 ). Так продолжается до бесконечности или пока среда не перейдёт в терминальное состояние, после попадания в которое взаимодействие прерывается, и сбор агентом награды заканчивается. Если в среде есть терминальные состояния, одна итерация взаимодействия от начального состояния до попадания в терминальное состояние называется эпизодом (episode). Цепочка генерируемых в ходе взаимодействия случайных величин ,… называется траекторией (trajectory). Примечание: функция награды тоже может быть стохастичной, и тогда награды за шаг тоже будут случайными величинами и частью траекторий, но без ограничения общности мы будем рассматривать детерминированные функции награды. MDP Итак, фактически среда для нас — это управляемая марковская цепь: на каждом шаге мы выбором a a определяем то распределение, из которого будет генерироваться следующее состояние. Мы предполагаем, во-первых, марковское свойство: что переход в следующее состояние определяется лишь текущим состоянием и не зависит от всей предыдущей истории: p(s t+1 ∣s t ,a t ,s t−1 ,a t−1 ,…,s 0 ,a 0 )=p(s t+1 ∣s t ,a t ) Во-вторых, мы предполагаем стационарность: функция переходов p(s ′ ∣s,a) не зависит от времени, от того, сколько шагов прошло с начала взаимодействия. Это довольно реалистичные предположения: законы мира не изменяются со временем (стационарность), а состояние — описывает мир целиком (марковость). В этой модели взаимодействия есть только одно нереалистичное допущение: полная наблюдаемость (full observability), которая гласит, что агент в своей стратегии π(a∣s) наблюдает всё состояние s s полностью и может выбирать действия, зная об окружающем мире абсолютно всё; в реальности нам же доступны лишь какие-то частичные наблюдения состояния. Такая более реалистичная ситуация моделируется в частично наблюдаемых MDP (Partially observable MDP, PoMDP), но мы далее ограничимся полностью наблюдаемыми средами. Итак, мы научились на математическом языке моделировать среду, агента и их взаимодействие между собой. Осталось понять, чего же мы хотим. Во время взаимодействия на каждом шаге агенту приходит награда =r(s t ,a t ), однако, состояния и действия в рамках такой постановки — случайные величины. Один и тот же агент может в силу стохастики как внутренней (в силу случайности выбора действий в его стратегии), так и внешней (в силу стохастики в функции переходов) набирать очень разную суммарную награду",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 2,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "наблюдения состояния. Такая более реалистичная ситуация моделируется в частично наблюдаемых MDP (Partially observable MDP, PoMDP), но мы далее ограничимся полностью наблюдаемыми средами. Итак, мы научились на математическом языке моделировать среду, агента и их взаимодействие между собой. Осталось понять, чего же мы хотим. Во время взаимодействия на каждом шаге агенту приходит награда =r(s t ,a t ), однако, состояния и действия в рамках такой постановки — случайные величины. Один и тот же агент может в силу стохастики как внутренней (в силу случайности выбора действий в его стратегии), так и внешней (в силу стохастики в функции переходов) набирать очень разную суммарную награду t≥0 r t в зависимости от везения. Мы скажем, что хотим научиться выбирать действия так, чтобы собирать в среднем как можно больше награды. Что значит в среднем, в среднем по чему? По всей стохастике, которая заложена в нашем процессе взаимодействия со средой. Каждая стратегия π π задаёт распределение в пространстве траекторий — с какой вероятностью нам может встретится траектория T=(s ,…): p(T∣π)=p(s ,⋯∣π)= t≥0 ∏ p(s t+1 ∣s t ,a t )π(a t ∣s t ) Вот по такому распределению мы и хотим взять среднее получаемой агентом награды. Записывают это обычно как-нибудь так: max ⁡ π E T∼π t≥0 ∑ r t → π max Здесь мат.ожидание по траекториям — это бесконечная цепочка вложенных мат.ожиданий: T∼π (⋅)=E a 0 ∼π(a ∼p(s ∼π(a 1 ∣s 1 ) …(⋅) Вот такую конструкцию мы и хотим оптимизировать выбором стратегии π π. На практике, однако, вносят ещё одну маленькую корректировку. В средах, где взаимодействие может продолжаться бесконечно долго, агент может научиться набирать бесконечную награду, с чем могут быть связаны разные парадоксы (например, получать +1 на каждом втором шаге становится также хорошо, как получать +1 на каждом сотом шаге). Поэтому вводят дисконтирование (discounting) награды, которое гласит: тортик сейчас лучше, чем тот же самый тортик завтра. Награду, которую мы получим в будущем, агент будет дисконтировать на некоторое число γ γ, меньшее единицы. Тогда наш функционал примет такой вид: max ⁡ π E T∼π t≥0 max источник картинки — курс UC Berkeley AI Заметим, что обучение с подкреплением - это в первую очередь задача оптимизации, оптимизации функционалов определённого вида. Если в классическом машинном обучении подбор функции потерь можно считать элементом инженерной части решения, то здесь функция награды задана нам готовая и определяет тот функционал, который мы хотим оптимизировать. Окей, и как такое решать? Выглядит сложновато, но у человечества есть уже довольно много наработок, как подойти к этой на вид очень общей задаче, причём с основной идеей вы скорее всего уже сталкивались. Называется она динамическим программированием. Дело в том, что мы оптимизируем не абы какой функционал, а среднюю дисконтированную кумулятивную награду. Чтобы придумать более эффективное решение, чем какой-нибудь подход, не использующий этот факт (например, эволюционные алгоритмы), нам нужно воспользоваться структурой поставленной задачи. Эта структура задана в формализме MDP и определении процесса взаимодействия агента со средой. Интуитивно она выражается так: вот мы сидим в некотором состоянии s s и хотим выбрать действие a a как можно оптимальнее. Мы знаем, что после выбора этого действия мы получим награду за этот",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 3,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на вид очень общей задаче, причём с основной идеей вы скорее всего уже сталкивались. Называется она динамическим программированием. Дело в том, что мы оптимизируем не абы какой функционал, а среднюю дисконтированную кумулятивную награду. Чтобы придумать более эффективное решение, чем какой-нибудь подход, не использующий этот факт (например, эволюционные алгоритмы), нам нужно воспользоваться структурой поставленной задачи. Эта структура задана в формализме MDP и определении процесса взаимодействия агента со средой. Интуитивно она выражается так: вот мы сидим в некотором состоянии s s и хотим выбрать действие a a как можно оптимальнее. Мы знаем, что после выбора этого действия мы получим награду за этот шаг r=r(s,a), среда перекинет нас в состояние s ′ s ′ и, внимание, дальше нас ждёт подзадача эквивалентной структуры: в точности та же задача выбора оптимального действия, только в другом состоянии. Действительно: когда мы будем принимать решение на следующем шаге, на прошлое мы повлиять уже не способны; стационарность означает, что законы, по которым ведёт себя среда, не поменялись, а марковость говорит, что история не влияет на дальнейший процесс нашего взаимодействия. Это наводит на мысль, что задача максимизации награды из текущего состояния тесно связана с задачей максимизации награды из следующего состояния s ′ s ′ , каким бы оно ни было. Чтобы сформулировать это на языке математики, вводятся «дополнительные переменные», вспомогательные величины, называемые оценочными функциями. Познакомимся с одной такой оценочной функцией - оптимальной Q-функцией, которую будем обозначать (s,a). Скажем, что (s,a) - это то, сколько максимально награды можно (в среднем) набрать после выбора действия a a из состояния s s. Итак: max (s,a)= π max E T∼π∣s 0 =s,a 0 =a t≥0 ∑ γ t r t Запись T∼π∣s 0 =s,a 0 =a здесь означает, что мы садимся в состояние =s; выбираем действие =a, а затем продолжаем взаимодействие со средой при помощи стратегии π π, порождая таким образом траекторию T T. По определению, чтобы посчитать (s,a), нужно перебрать все стратегии, посмотреть, сколько каждая из них набирает награды после выбора a a из состояния s s, и взять наилучшую стратегию. Поэтому эта оценочная функция называется оптимальной: она предполагает, что в будущем после выбора действия a a из состояния s s агент будет вести себя оптимально. Определение неконструктивное, конечно, поскольку в реальности мы так сделать не можем, зато обладает интересным свойством. Если мы каким-то чудом узнали (s,a), то мы знаем оптимальную стратегию. Действительно: представьте, что вы находитесь в состоянии s s, вам нужно сделать выбор из трёх действий, и вы знаете значения (s,a). Вы знаете, что если выберете первое действие a = 0 a=0, то в будущем сможете набрать не более чем, допустим, (s,a=0)=+3 награды. При этом вы знаете, что существует какая-то стратегия π π, на которой достигается максимум в определении оптимальной Q-функции, то есть которая действительно позволяет набрать эти +3. Вы знаете, что если выберете второе действие, то в будущем сможете набрать, допустим, (s,a=1)=+10, а для третьего действия (s,a=2)=−1. Вопрос: так как нужно действовать? Интуиция подсказывает, что надо просто выбирать действие a = 1 a=1, что позволит набрать +10, ведь по определению больше набрать никак не получится. Значит, выбор в этом состоянии",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 4,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Вы знаете, что если выберете первое действие a = 0 a=0, то в будущем сможете набрать не более чем, допустим, (s,a=0)=+3 награды. При этом вы знаете, что существует какая-то стратегия π π, на которой достигается максимум в определении оптимальной Q-функции, то есть которая действительно позволяет набрать эти +3. Вы знаете, что если выберете второе действие, то в будущем сможете набрать, допустим, (s,a=1)=+10, а для третьего действия (s,a=2)=−1. Вопрос: так как нужно действовать? Интуиция подсказывает, что надо просто выбирать действие a = 1 a=1, что позволит набрать +10, ведь по определению больше набрать никак не получится. Значит, выбор в этом состоянии действия a = 1 a=1 оптимален. Эта интуиция нас не обманывает, и принцип такого выбора называется принципом оптимальности Беллмана. Выбор того действия, на котором достигается максимум по действиям Q-функции, называется жадным (greedy) по отношению к ней. Таким образом, принцип оптимальности Беллмана гласит: жадный выбор по отношению к оптимальной Q-функции оптимален: argmax (s)=argmax a Q ∗ (s,a) Примечание: если Q-функция достигает максимума на нескольких действиях, то можно выбирать любое из них. Заметим, что эта оптимальная стратегия детерминирована. Этот интересный факт означает, что нам, в общем-то, необязательно искать стохастичную стратегию. Наше рассуждение пока даже показывает, что мы можем просто пытаться найти (s,a), а дальше выводить из неё оптимальную стратегию, выбирая действие жадно. Но как искать (s,a)? Тут на сцене и появляется наше наблюдение про структуру задачи. Оказывается, (s,a) выражается через саму себя. Действительно: рассмотрим некоторую пару состояние-действие s , a s,a. С одной стороны, по определению, мы в будущем сможем при условии оптимального поведения получить (s,a) награды. С другой стороны, после того, как мы выберем действие a a в состоянии s s, мы получим награду за один шаг r(s,a), вся дальнейшая награда будет дисконтирована на γ γ, среда ответит нам сэмплированием ∼p(s ′ ∣s,a) (на результат этого сэмплирования мы уже никак повлиять не можем и по этой стохастике нашу будущую награду надо будет усреднять), а затем в состоянии s ′ s ′ мы, в предположении оптимальности поведения, выберем то действие a ′ a ′ , на котором достигается максимум ). Другими словами, в дальнейшем после попадания в s ′ s ′ мы сможем получить max max ) награды. А значит, верно следующее рекурсивное соотношение, называемое уравнением оптимальности Беллмана для Q-функции: max (s,a)=r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max Мы получили систему уравнений, связывающую значения (s,a) с самой собой. Это нелинейная система уравнений, но оказывается, что она в некотором смысле «хорошая». У неё единственное решение - и, значит, решение этого уравнения можно считать эквивалентным определением (s,a), - и его можно искать методом простой итерации. Метод простой итерации решения систем уравнений позволяет улучшать своё текущее приближение x x решения некоторого уравнения вида x=f(x) его подстановкой в правую часть. То есть: инициализируем произвольную функцию (s,a):S×A→R, которая будет приближать (s,a), затем итеративно будем подставлять её в правую часть уравнений оптимальности Беллмана и полученным значением обновлять наше приближение: max k+1 ∗ (s,a)←r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max Такая процедура в пределе приведёт нас к истинной (s,a), а значит и оптимальной стратегии. Кстати,",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 5,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "У неё единственное решение - и, значит, решение этого уравнения можно считать эквивалентным определением (s,a), - и его можно искать методом простой итерации. Метод простой итерации решения систем уравнений позволяет улучшать своё текущее приближение x x решения некоторого уравнения вида x=f(x) его подстановкой в правую часть. То есть: инициализируем произвольную функцию (s,a):S×A→R, которая будет приближать (s,a), затем итеративно будем подставлять её в правую часть уравнений оптимальности Беллмана и полученным значением обновлять наше приближение: max k+1 ∗ (s,a)←r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max Такая процедура в пределе приведёт нас к истинной (s,a), а значит и оптимальной стратегии. Кстати, когда вы в прошлом встречались с динамическим программированием, вы скорее всего неявно использовали именно эту идею, разве что часто в задачах для решения уравнений оптимальности Беллмана можно просто последовательно исключать неизвестные переменные; но метод простой итерации даёт более общую схему, применимую всегда. А сейчас для нас принципиально следующее: если у нас есть какое-то приближение Q ∗ Q ∗ , то вычисление правой части уравнения оптимальности Беллмана позволит получить приближение лучше. А где же метод проб и ошибок? Решать методом простой итерации уравнения оптимальности Беллмана и таким образом получать (s,a) в реальности можно только при двух очень существенных ограничивающих условиях. Нужно, чтобы, во-первых, мы могли хранить как-то текущее приближение (s,a) в памяти. Это возможно только если пространства состояний и действий конечные и не очень большие, то есть, например, в вашем MDP всего 10 состояний и 5 действий, тогда (s,a) — это табличка 10x5. Но что, если вы хотите научиться играть в видеоигру, и состояние — это входное изображение? Тогда множество картинок, которые вам может показать видеоигра, сохранить в памяти уже не получится. Ну, допустим пока, что число состояний и число действий не очень большое, и мы всё-таки можем хранить таблицу в памяти, а позже мы снимем это ограничение, моделируя (s,a) при помощи нейросети. Во-вторых, нам необходимо уметь считать выражение, стоящее справа в уравнение оптимальности Беллмана: max r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max Мало того, что в сложных средах взять мат.ожидание по функции переходов ∼p(s ′ ∣s,a) в реальности мы не сможем, так ещё и обычно мы эту функцию переходов на самом деле не знаем. Представьте, что вы катаетесь на велосипеде: можете ли вы по текущему состоянию окружающего мира, например, положению всех атомов во вселенной, рассказать, с какими вероятностями в каком состоянии мир окажется в следующий момент времени? Это соображение также подсказывает, что было бы здорово, если б мы смогли решать задачу, избегая даже попыток выучить эту сложную функцию переходов. Что нам доступно? Мы можем взять какую-нибудь стратегию π π (важный момент: мы должны сами выбрать какую) и повзаимодействовать ею со средой. «Попробовать решить задачу». Мы можем сгенерировать при помощи π π целую траекторию или даже сделать всего один шаг в среде. Таким образом мы соберём данные: допустим, мы были в состоянии s s и сделали выбор действия a a, тогда мы узнаем, какую награду r=r(s,a) мы получаем за такой шаг и, самое главное, в какое состояние s ′ s ′ нас перевела среда. Полученный s ′ s ′",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 6,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "смогли решать задачу, избегая даже попыток выучить эту сложную функцию переходов. Что нам доступно? Мы можем взять какую-нибудь стратегию π π (важный момент: мы должны сами выбрать какую) и повзаимодействовать ею со средой. «Попробовать решить задачу». Мы можем сгенерировать при помощи π π целую траекторию или даже сделать всего один шаг в среде. Таким образом мы соберём данные: допустим, мы были в состоянии s s и сделали выбор действия a a, тогда мы узнаем, какую награду r=r(s,a) мы получаем за такой шаг и, самое главное, в какое состояние s ′ s ′ нас перевела среда. Полученный s ′ s ′ — сэмпл из функции переходов ∼p(s ′ ∣s,a). Собранная так информация — четвёрка (s,a,r,s ′ ) — называется переходом (transition), и может быть как-то использована для оптимизации нашей стратегии. Можем ли мы, используя лишь переходы (s,a,r,s ′ ), то есть имея на руках лишь сэмплы ∼p(s ′ ∣s,a), как-то пользоваться схемой динамического программирования? Что, если мы будем заменять значение (s,a) не на max r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max которое мы не можем посчитать, а на его Монте Карло оценку: max r(s,a)+γ a ′ max где s ′ s ′ — сэмпл из функции переходов из собранного нами опыта? В среднем-то такая замена верная. Такая Монте-Карло оценка правой части для заданного переходика (s,a,r,s ′ ) называется Беллмановским таргетом, то есть «целевой переменной». Почему такое название — мы увидим чуть позже. Чтобы понять, как нам нужно действовать, рассмотрим какую-нибудь типичную ситуацию. Допустим, после выполнения действия a a из некоторого состояния s s среда награждает нас r(s,a)=0 и перекидывает нас с равными вероятностями то в состояние s ′ s ′ , для которого max max )=+1, то в состояние s ′ s ′ , для которого max max )=−1. Метод простой итерации говорит, что на очередной итерации нужно заменить (s,a) на 0.5 0.5 0.5γ⋅(+1)+0.5γ⋅(−1)=0, но в реальности мы встретимся лишь с одним исходом, и таргет — Монте-Карло оценка правой части уравнения оптимальности Беллмана — будет с вероятностью 0.5 равен + γ +γ, а с вероятностью 0.5 равен − γ −γ. Ясно, что нельзя просто взять и жёстко заменять наше текущее приближение (s,a) на посчитанный Беллмановский таргет по некоторому одному переходу, поскольку нам могло повезти (мы увидели + γ +γ) или не повезти (мы увидели − γ −γ). Давайте вместо этого поступать также, как учат среднее по выборке: не сдвигать «жёстко» наше текущее приближение в значение очередного сэмпла, а смешивать текущее приближение с очередным сэмплом. То есть: берём переходик (s,a,r,s ′ ), и не заменяем (s,a) на стохастичную оценку правой части уравнения оптимальности Беллмана, а только сдвигаемся в его сторону: max k+1 ∗ (s,a)←(1−α)Q k ∗ (s,a)+α(r+γ a ′ max Таким образом, мы проводим экспоненциальное сглаживание старого приближения (s,a) и новой оценки правой части уравнения оптимальности Беллмана со свежим сэмплом s ′ s ′ . Выбор α α здесь определяет, насколько сильно мы обращаем внимание на последние сэмплы, и имеет тот же физический смысл, что и learning rate. В среднем по стохастике (а стохастика в этой формуле обновления заложена в",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 7,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "текущее приближение с очередным сэмплом. То есть: берём переходик (s,a,r,s ′ ), и не заменяем (s,a) на стохастичную оценку правой части уравнения оптимальности Беллмана, а только сдвигаемся в его сторону: max k+1 ∗ (s,a)←(1−α)Q k ∗ (s,a)+α(r+γ a ′ max Таким образом, мы проводим экспоненциальное сглаживание старого приближения (s,a) и новой оценки правой части уравнения оптимальности Беллмана со свежим сэмплом s ′ s ′ . Выбор α α здесь определяет, насколько сильно мы обращаем внимание на последние сэмплы, и имеет тот же физический смысл, что и learning rate. В среднем по стохастике (а стохастика в этой формуле обновления заложена в случайном s ′ s ′ ) мы будем сдвигаться в сторону max r(s,a)+γE s ′ ∼p(s ′ ∣s,a) a ′ max и значит применять этакий «зашумлённый» метод простой итерации. Итак, возникает следующая идея. Будем как-то взаимодействовать со средой и собирать переходики (s,a,r,s ′ ). Для каждого перехода будем обновлять одну ячейку нашей Q-таблицы размера число состояний на число действий по вышеуказанной формуле. Таким образом мы получим как бы «зашумлённый» метод простой итерации, где мы на каждом шаге обновляем только одну ячейку таблицы, и не заменяем жёстко значение на правую часть уравнений оптимальности, а лишь сдвигаемся по некоторому в среднем верному стохастичному направлению. Очень похоже на стохастическую оптимизацию вроде стохастического градиентного спуска, и поэтому гарантии сходимости выглядят схожим образом. Оказывается, такой алгоритм сходится к истинной (s,a), если для любой пары s , a s,a мы в ходе всего процесса проводим бесконечное количество обновлений, а learning rate (гиперпараметр α α) в них ведёт себя как learning rate из условий сходимости стохастического градиентного спуска: =+∞, i ∑ α i 2 <+∞ Этот алгоритм, к которому мы уже практически пришли, называется Q-learning, «обучение оптимальной Q-функции». Нам, однако, осталось ответить на один вопрос: так как же нужно собирать данные, чтобы удовлетворить требованиям для сходимости? Как взаимодействовать со средой так, чтобы мы каждую ячейку s , a s,a не прекращали обновлять? Дилемма Exploration-exploitation Мы уже встречали дилемму exploration-exploitation (букв. «исследования-использования») в параграфе про тюнинг гиперпараметров. Задача многоруких бандитов, которая там встретилась, на самом деле является частным случаем задачи обучения с подкреплением, в котором после первого выбора действия эпизод гарантированно завершается, и этот частный случай задачи часто используется для изучения этой дилеммы. Рассмотрим эту дилемму в нашем контексте. Допустим, на очередном шаге алгоритма у нас есть некоторое приближение (s,a)≈Q ∗ (s,a). Приближение это, конечно, неточное, поскольку алгоритм, если и сходится к истинной оптимальной Q-функции, то на бесконечности. Как нужно взаимодействовать со средой? Если вы хотите набрать максимальную награду, наверное, стоит воспользоваться нашей теорией и заниматься exploitation-ом, выбирая действие жадно: π ( s ) = argmax π(s)=argmax a Q k (s,a) Увы, такой выбор не факт что совпадёт с истинной оптимальной стратегией, а главное, он детерминирован. Это значит, что при взаимодействии этой стратегией со средой, многие пары s , a s,a никогда не будут встречаться просто потому, что мы никогда не выбираем действие a a в состоянии s s. А тогда мы, получается, рискуем больше никогда не обновить ячейку (s,a) для таких пар! Такие ситуации запросто могут привести",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 8,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как нужно взаимодействовать со средой? Если вы хотите набрать максимальную награду, наверное, стоит воспользоваться нашей теорией и заниматься exploitation-ом, выбирая действие жадно: π ( s ) = argmax π(s)=argmax a Q k (s,a) Увы, такой выбор не факт что совпадёт с истинной оптимальной стратегией, а главное, он детерминирован. Это значит, что при взаимодействии этой стратегией со средой, многие пары s , a s,a никогда не будут встречаться просто потому, что мы никогда не выбираем действие a a в состоянии s s. А тогда мы, получается, рискуем больше никогда не обновить ячейку (s,a) для таких пар! Такие ситуации запросто могут привести к застреванию алгоритма. Мы хотели научиться кататься на велосипеде и получали +0.1 за каждый пройденный метр и -5 за каждое попадание в дерево. После первых проб и ошибок мы обнаружили, что катание на велосипеде приносит нам -5, поскольку мы очень скоро врезаемся в деревья и обновляли нашу аппроксимацию Q-функции сэмплами с негативной наградой; зато если мы не будем даже забираться на велосипед и просто займёмся ничего не деланьем, то мы сможем избежать деревьев и будем получать 0. Просто из-за того, что в нашей стратегии взаимодействия со средой никогда не встречались те s , a s,a, которые приводят к положительной награде, и жадная стратегия по отношению к нашей текущей аппроксимации Q-функции никогда не выбирает их. Поэтому нам нужно экспериментировать и пробовать новые варианты. Режим exploration-а предполагает, что мы взаимодействуем со средой при помощи какой-нибудь стохастичной стратегии ∀s,a:π(a∣s)>0. Например, такой стратегией является случайная стратегия, выбирающая рандомные действия. Как ни странно, сбор опыта при помощи случайной стратегии позволяет побывать с ненулевой вероятностью во всех областях пространства состояний, и теоретически даже наш алгоритм обучения Q-функции будет сходится. Означает ли это, что exploration-а хватит, и на exploitation можно забить? В реальности мы понимаем, что добраться до самых интересных областей пространства состояний, где функция награда самая большая, не так-то просто, и случайная стратегия хоть и будет это делать с ненулевой вероятностью, но вероятность эта будет экспоненциально маленькая. А для сходимости нам нужно обновить ячейки (s,a) для этих интересных состояний бесконечно много раз, то есть нам придётся дожидаться необычайно редкого везения далеко не один раз. Куда разумнее использовать уже имеющиеся знания и при помощи жадной стратегии, которая уже что-то умеет, идти к этим интересным состояниям. Поэтому для решения дилеммы exploration-exploitation обычно берут нашу текущую жадную стратегию и что-нибудь с ней делают такое, чтобы она стала чуть-чуть случайной. Например, с вероятностью ε > 0 ε>0 выбирают случайное действие, а с вероятностью 1 − ε 1−ε — жадное. Тогда мы чаще всё-таки и знаниями пользуемся, и любое действие с ненулевой вероятностью выбираем; такая стратегия называется ε ε-жадной, и она является самым простым способом как-то порешать эту дилемму. Давайте закрепим, что у нас получилось, в виде табличного алгоритма обучения с подкреплением под названием Q-learning: Проинициализировать (s,a) произвольным образом. Пронаблюдать s 0 s 0 из среды. Для k=0,1,2,…: с вероятностью ε ε выбрать действие a k a k случайно, иначе жадно: a k = argmax ​=argmax ​,a k ​) отправить действие a k a k в среду, получить награду за шаг",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 9,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "а с вероятностью 1 − ε 1−ε — жадное. Тогда мы чаще всё-таки и знаниями пользуемся, и любое действие с ненулевой вероятностью выбираем; такая стратегия называется ε ε-жадной, и она является самым простым способом как-то порешать эту дилемму. Давайте закрепим, что у нас получилось, в виде табличного алгоритма обучения с подкреплением под названием Q-learning: Проинициализировать (s,a) произвольным образом. Пронаблюдать s 0 s 0 из среды. Для k=0,1,2,…: с вероятностью ε ε выбрать действие a k a k случайно, иначе жадно: a k = argmax ​=argmax ​,a k ​) отправить действие a k a k в среду, получить награду за шаг r k r k и следующее состояние s k + 1 s k+1 . обновить одну ячейку таблицы: max )←(1−α)Q ∗ (s k ,a k )+α(r k +γ a ′ max Q ∗ (s k+1 ,a ′ )) Добавим нейросеток Наконец, чтобы перейти к алгоритмам, способным на обучение в сложных MDP со сложным пространством состояний, нужно объединять классическую теорию обучения с подкреплением с парадигмами глубокого обучения. Допустим, мы не можем позволить себе хранить (s,a) как таблицу в памяти, например, если мы играем в видеоигру и на вход нам подаются какие-нибудь изображения. Тогда мы можем обрабатывать любые имеющиеся у агента входные сигналы при помощи нейросетки (s,a,θ). Для тех же видеоигр мы легко обработаем изображение экрана небольшой свёрточной сеточкой и выдадим для каждого возможного действия a a вещественный скаляр (s,a,θ). Допустим также, что пространство действий всё ещё конечное и маленькое, чтобы мы могли для такой модели строить жадную стратегию, выбирать argmax argmax a Q ∗ (s,a,θ). Но как обучать такую нейросетку? Давайте ещё раз посмотрим на формулу обновления в Q-learning для одного переходика (s,a,r,s max max k+1 ∗ (s,a)←(1−α)Q k ∗ (s,a)+α(r+γ a ′ max ))= =Q k ∗ (s,a)+α(r+γ a ′ max )−Q k ∗ (s,a)) Теория Q-learning-а подсказывала, что у процесса такого обучения Q-функции много общего с обычным стохастическим градиентным спуском. В таком виде формула подсказывает, что, видимо, r + γ max r+γ a ′ max )−Q k ∗ (s,a) — это стохастическая оценка какого-то градиента. Этот градиент сравнивает Беллмановский таргет r + γ max r+γ a ′ max с нашим текущим приближением (s,a) и чуть-чуть корректирует это значение, сдвигая в сторону таргета. Попробуем «заменить» в этой формуле Q-функцию с табличного представления на нейросетку. Рассмотрим такую задачу регрессии. Чтобы построить один прецедент для обучающей выборки, возьмём один имеющийся у нас переходик (s,a,r,s ′ ). Входом будет пара s , a s,a. Целевой переменной, таргетом, будет Беллмановский таргет y = r + γ max y=r+γ a ′ max ,θ); его зависимость от параметров θ θ нашей нейронки мы далее будем игнорировать и будем «притворяться», что это и есть наш ground truth. Именно поэтому Монте-Карло оценка правой части уравнения оптимальности Беллмана и называют таргетом. Но важно помнить, что эта целевая переменная на самом деле «зашумлена»: в формуле используется взятый из перехода s ′ s ′ , который есть лишь сэмпл из функции переходов. На самом же деле мы хотели бы выучить среднее значение такой целевой переменной, и поэтому в качестве функции потерь",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 10,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "s,a. Целевой переменной, таргетом, будет Беллмановский таргет y = r + γ max y=r+γ a ′ max ,θ); его зависимость от параметров θ θ нашей нейронки мы далее будем игнорировать и будем «притворяться», что это и есть наш ground truth. Именно поэтому Монте-Карло оценка правой части уравнения оптимальности Беллмана и называют таргетом. Но важно помнить, что эта целевая переменная на самом деле «зашумлена»: в формуле используется взятый из перехода s ′ s ′ , который есть лишь сэмпл из функции переходов. На самом же деле мы хотели бы выучить среднее значение такой целевой переменной, и поэтому в качестве функции потерь мы возьмём MSE. Как будет выглядеть шаг стохастического градиентного спуска для решения этой задачи регрессии (для простоты — для одного прецедента)? max k+1 ← = = θ k −α∇ θ (y−Q ∗ (s,a,θ)) 2 = θ k +2α(y−Q ∗ (s,a,θ))∇ θ Q ∗ (s,a,θ)= θ k +2α(r+γ a ′ max ,θ)−Q ∗ (s,a,θ))∇ θ Q ∗ (s,a,θ) Это практически в точности повторяет формулу Q-learning, которая гласит, что если таргет r + γ max r+γmax ,θ) больше (s,a,θ), то нужно подстроить веса нашей модели так, чтобы (s,a,θ) стало чуть побольше, и наоборот. В среднем при такой оптимизации мы будем двигаться в сторону max ∼p(s ′ ∣s,a) y=E s ′ ∼p(s ′ ∣s,a) [r+γ a ′ max ,θ)] — в сторону правой части уравнения оптимальности Беллмана, то есть моделировать метод простой итерации для решения системы нелинейных уравнений. Единственное отличие такой задачи регрессии от тех, с которыми сталкивается традиционное глубокое обучение — то, что целевая переменная зависит от нашей же собственной модели. Раньше целевые переменные были напрямую источником обучающего сигнала. Теперь же, когда мы хотим выучить будущую награду при условии оптимального поведения, мы не знаем этого истинного значения или даже её стохастичных оценок. Поэтому мы применяем идею бутстрапирования (bootstrapping): берём награду за следующий шаг, и нечестно приближаем всю остальную награду нашей же текущей аппроксимацией max max ,θ). Да, за этим кроется идея метода простой итерации, но важно понимать, что такая целевая переменная лишь указывает направление для обучения, но не является истинным приближением будущих наград или даже их несмещённой оценкой. Поэтому говорят, что в этой задаче регрессии очень смещённые (biased) целевые переменные. На практике из-за этого возникает беда. Наша задача регрессии в таком виде меняется после каждого же шага. Если вдруг после очередного шага оптимизации и обновления весов нейросети наша модель начала выдавать какие-то немного неадекватные значения, они рискуют попасть в целевую переменную на следующем шаге, мы сделаем шаг обучения под неадекватные целевые переменные, модель станет ещё хуже, и так далее, начнётся цепная реакция. Алгоритмы, в которых целевая переменная вот так напрямую зависит от текущей же модели, из-за этого страшно нестабильны. Для стабилизации применяется трюк, называемый таргет-сетью (target network). Давайте сделаем так, чтобы у нас задача регрессии менялась не после каждого обновления весов нейросетки, а хотя бы раз, скажем, в 1000 шагов оптимизации. Для этого заведём полную копию нашей нейросети («таргет-сеть»), веса которой будем обозначать θ − θ − . Каждые 1000 шагов будем копировать веса из нашей модели в таргет-сеть ←θ, больше никак",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 11,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "следующем шаге, мы сделаем шаг обучения под неадекватные целевые переменные, модель станет ещё хуже, и так далее, начнётся цепная реакция. Алгоритмы, в которых целевая переменная вот так напрямую зависит от текущей же модели, из-за этого страшно нестабильны. Для стабилизации применяется трюк, называемый таргет-сетью (target network). Давайте сделаем так, чтобы у нас задача регрессии менялась не после каждого обновления весов нейросетки, а хотя бы раз, скажем, в 1000 шагов оптимизации. Для этого заведём полную копию нашей нейросети («таргет-сеть»), веса которой будем обозначать θ − θ − . Каждые 1000 шагов будем копировать веса из нашей модели в таргет-сеть ←θ, больше никак менять θ − θ − не будем. Когда мы захотим для очередного перехода (s,a,r,s ′ ) построить таргет, мы воспользуемся не нашей свежей моделью, а таргет-сетью: y = r + γ max y=r+γ a ′ max Тогда правило, по которому строится целевая переменная, будет меняться раз в 1000 шагов, и мы 1000 шагов будем решать одну и ту же задачу регрессии. Такой процесс будет намного стабильнее. Experience Replay Чтобы окончательно собрать алгоритм Deep Q-learning (обычно называемый DQN, Deep Q-network), нам понадобится сделать последний шаг, связанный опять со сбором данных. Коли мы хотим обучать нейросетку, нам нужно для каждого обновления весов откуда-то взять целый мини-батч данных, то есть батч переходов (s,a,r,s ′ ), чтобы по нему усреднить оценку градиента. Однако, если мы возьмём среду, сделаем в ней N N шагов, то встреченные нами N N переходов будут очень похожи друг на друга: они все придут из одной и той же области пространства состояний. Обучение нейросетки на скоррелированных данных — плохая идея, поскольку такая модель быстро забудет, что она учила на прошлых итерациях. Бороться с этой проблемой можно двумя способами. Первый способ, доступный всегда, когда среда задана при помощи виртуального симулятора — запуск параллельных агентов. Запускается параллельно N N процессов взаимодействия агента со средой, и для того, чтобы собрать очередной мини-батч переходов для обучения, во всех экземплярах проводится по одному шагу взаимодействия, собирается по одному переходику. Такой мини-батч уже будет разнообразным. Более интересный второй способ. Давайте после очередного шага взаимодействия со средой мы не будем тут же использовать переход (s,a,r,s ′ ) для обновления модели, а запомним этот переход и положим его себе в коллекцию. Память со всеми встретившимися в ходе проб и ошибок переходами (s,a,r,s ′ ) называется реплей буфером (replay buffer или experience replay). Теперь для того, чтобы обновить веса нашей сети, мы возьмём и случайно засэмплируем из равномерного распределения желаемое количество переходов из всей истории. Однако, использование реплей буфера возможно далеко не во всех алгоритмах обучения с подкреплением. Дело в том, что некоторые алгоритмы обучения с подкреплением требуют, чтобы данные для очередного шага обновления весов были сгенерированы именно текущей, самой свежей версией стратегии. Такие алгоритмы относят к классу on-policy: они могут улучшать стратегию только по данным из неё же самой («on policy»). Примером on-policy алгоритмов выступают, например, эволюционные алгоритмы. Как они устроены: например, можно завести популяцию стратегий, поиграть каждой со средой, отобрать лучшие и как-то породить новую популяцию (подробнее про одну из самых успешных схем в рамках такой идеи",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 12,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "желаемое количество переходов из всей истории. Однако, использование реплей буфера возможно далеко не во всех алгоритмах обучения с подкреплением. Дело в том, что некоторые алгоритмы обучения с подкреплением требуют, чтобы данные для очередного шага обновления весов были сгенерированы именно текущей, самой свежей версией стратегии. Такие алгоритмы относят к классу on-policy: они могут улучшать стратегию только по данным из неё же самой («on policy»). Примером on-policy алгоритмов выступают, например, эволюционные алгоритмы. Как они устроены: например, можно завести популяцию стратегий, поиграть каждой со средой, отобрать лучшие и как-то породить новую популяцию (подробнее про одну из самых успешных схем в рамках такой идеи можно посмотреть здесь). Как бы ни была устроена эта схема, эволюционный алгоритм никак не может использовать данные из, например, старых, плохих стратегий, которые вели себя, скажем, не сильно лучше случайной стратегии. Поэтому неизбежно в эволюционном подходе нужно свежую популяцию отправлять в среду и собирать новые данные перед каждым следующим шагом. И вот важный момент: Deep Q-learning, как и обычный Q-learning, относится к off-policy алгоритмам обучения с подкреплением. Совершенно неважно, какая стратегия, умная или не очень, старая или новая, породила переход (s,a,r,s ′ ), нам всё равно нужно решать уравнение оптимальности Беллмана в том числе и для этой пары s , a s,a и нам достаточно при построении таргета лишь чтобы s ′ s ′ был сэмплом из функции переходов (а она-то как раз одна вне зависимости от того, какая стратегия взаимодействует в среде). Поэтому обновлять модель (s,a) мы можем по совершенно произвольному опыту, и, значит, мы в том числе можем использовать experience replay. источник картинки — курс UC Berkeley AI В любом случае, даже в сложных средах, при взаимодействии со средой мы всё равно должны как-то разрешить дилемму exploration-exploitation, и пользоваться, например, ε ε-жадной стратегией исследования. Итак, алгоритм DQN выглядит так: Проинициализировать нейросеть (s,a,θ). Проинициализировать таргет-сеть, положив =θ. Пронаблюдать s 0 s 0 из среды. Для k=0,1,2,…: с вероятностью ε ε выбрать действие a k a k случайно, иначе жадно: a k = argmax =argmax ,θ) отправить действие a k a k в среду, получить награду за шаг r k r k и следующее состояние s k + 1 s k+1 . добавить переход k+1 ) в реплей буфер. если в реплей буфере скопилось достаточное число переходиков, провести шаг обучения. Для этого сэмплируем мини-батч переходиков (s,a,r,s ′ ) из буфера. для каждого переходика считаем целевую переменную: y = r + γ max y=r+γ a ′ max сделать шаг градиентного спуска для обновления θ θ, минимизируя ∑(y−Q ∗ (s,a,θ)) 2 если k k делится на 1000, обновить таргет-сеть: ←θ. Алгоритм DQN не требует никаких handcrafted признаков или специфических настроек под заданную игру. Один и тот же алгоритм, с одними и теми же гиперпараметрами, можно запустить на любой из 57 игр древней консоли Atari (пример игры в Breakout) и получить какую-то стратегию. Для сравнения алгоритмов RL между собой результаты обычно усредняют по всем 57 играм Atari. Недавно алгоритм под названием Agent57, объединяющий довольно много модификаций и улучшений DQN и развивающий эту идею, смог победить человека сразу во всех этих 57 играх. А",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 13,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "для обновления θ θ, минимизируя ∑(y−Q ∗ (s,a,θ)) 2 если k k делится на 1000, обновить таргет-сеть: ←θ. Алгоритм DQN не требует никаких handcrafted признаков или специфических настроек под заданную игру. Один и тот же алгоритм, с одними и теми же гиперпараметрами, можно запустить на любой из 57 игр древней консоли Atari (пример игры в Breakout) и получить какую-то стратегию. Для сравнения алгоритмов RL между собой результаты обычно усредняют по всем 57 играм Atari. Недавно алгоритм под названием Agent57, объединяющий довольно много модификаций и улучшений DQN и развивающий эту идею, смог победить человека сразу во всех этих 57 играх. А если пространство действий непрерывно? Всюду в DQN мы предполагали, что пространство действий дискретно и маленькое, чтобы мы могли считать жадную стратегию π ( s ) = argmax π(s)=argmax a Q ∗ (s,a,θ) и считать максимум в формуле целевой переменной max max a Q ∗ (s,a,θ). Если пространство действий непрерывно, и на каждом шаге от агента ожидается выбор нескольких вещественных чисел, то как это делать непонятно. Такая ситуация повсюду возникает в робототехнике. Там каждое сочленение робота можно, например, поворачивать вправо / влево, и такие действия проще описывать набором чисел в диапазоне [-1, 1], где -1 — крайне левое положение, +1 — крайне правое, и доступны любые промежуточные варианты. При этом дискретизация действий не вариант из-за экспоненциального взрыва числа вариантов и потери семантики действий. Нам, в общем-то, нужно в DQN только одну проблему решить: как-то научиться аргмаксимум по действиям брать. А давайте, коли мы не знаем argmax argmax a Q ∗ (s,a), приблизим его другой нейросеткой. А то есть, заведём вторую нейросеть π(s,ϕ) с параметрами ϕ ϕ, и будем учить её так, чтобы argmax π(s,ϕ)≈argmax a Q ∗ (s,a,θ). Как это сделать? Ну, будем на каждой итерации алгоритма брать батч состояний s s из нашего реплей буфера и будем учить π(s,ϕ) выдавать такие действия, на которых наша Q-функция выдаёт большие скалярные значения: max (s,π(s,ϕ),θ)→ ϕ max Причём, поскольку действия непрерывные, всё слева дифференцируемо и мы можем напрямую применять самый обычный backpropagation! DDPG Теперь когда на руках есть приближение argmax π(s,ϕ)≈argmax a Q ∗ (s,a,θ), можно просто использовать его всюду, где нам нужны аргмаксимумы и максимумы от нашей Q-функции. Мы получили Actor-Critic схему: у нас есть актёр, π(s,ϕ) — детерминированная стратегия, и критик (s,a), который оценивает выбор действий актёром и предоставляет градиент для его улучшения. Актёр учится выбирать действия, которые больше всего нравятся критику, а критик учится регрессией с целевой переменной y = r + γ max y=r+γ a ′ max )≈r+γQ ∗ (s ′ ,π(s ′ ,ϕ),θ − ) Эта прикольная рабочая эвристика позволяет придумать off-policy алгоритмы для непрерывных пространств действий; к такому подходу относятся такие алгоритмы, как DDPG, TD3 и SAC. Policy Gradient алгоритмы В рассмотренных алгоритмах есть несколько приниципиальных ограничений, которые вытекают непосредственно из самой идеи подхода. Мы учимся с таргетов, заглядывающих всего на один шаг вперёд, использующих только s ′ s ′ ; это чревато проблемой накапливающейся ошибки, поскольку если между выполнением действия и получением награды +1 проходит 100 шагов, нам нужно на сто шагов «распространять» полученный сигнал. Мы",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 14,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "r + γ max y=r+γ a ′ max )≈r+γQ ∗ (s ′ ,π(s ′ ,ϕ),θ − ) Эта прикольная рабочая эвристика позволяет придумать off-policy алгоритмы для непрерывных пространств действий; к такому подходу относятся такие алгоритмы, как DDPG, TD3 и SAC. Policy Gradient алгоритмы В рассмотренных алгоритмах есть несколько приниципиальных ограничений, которые вытекают непосредственно из самой идеи подхода. Мы учимся с таргетов, заглядывающих всего на один шаг вперёд, использующих только s ′ s ′ ; это чревато проблемой накапливающейся ошибки, поскольку если между выполнением действия и получением награды +1 проходит 100 шагов, нам нужно на сто шагов «распространять» полученный сигнал. Мы должны учить (s,a) вместо того, чтобы как-то напрямую («end-to-end») запомнить, какие действия в каких состояниях хорошие. Наконец, наша стратегия всегда детерминирована, когда для взаимодействия со средой во время сбора данных, например, нам позарез нужна была стохастичная, чтобы гарантированно обновлять Q-функцию для всех пар s , a s,a, и эту проблему пришлось закрывать костылями. Есть второй подход model-free алгоритмов RL, называемый Policy Gradient, который позволяет избежать вышеперечисленных недостатков за счёт on-policy режима работы. Идея выглядит так: давайте будем искать стратегию в классе стохастичных стратегий, то есть заведём нейросеть, моделирующую (a∣s) напрямую. Тогда наш функционал, который мы оптимизируем, max ⁡ θ , J(θ)=E T∼π θ t≥0 max , дифференцируем по параметрам θ θ, и градиент равен: log J(θ)=E T∼π θ t≥0 ∑ ∇ θ logπ где R t R t - reward-to-go с шага t t, то есть награда, собранная в сыгранном эпизоде после шага Эта формула говорит нам, что градиент нашего функционала — это тоже мат.ожидание по траекториям. А значит, мы можем попробовать посчитать какую-то оценку этого градиента, заменив мат.ожидание на Монте Карло оценку, и просто начать оптимизировать наш функционал самым обычным стохастическим градиентным спуском! А то есть: берём нашу стратегию π θ π θ с текущими значениями параметров θ θ, играем эпизод (или несколько) в среде, то есть сэмплируем T ∼ π θ T∼π θ , и затем делаем шаг градиентного подъёма: log θ←θ+α t≥0 ∑ ∇ θ logπ Почему эта идея приводит к on-policy подходу? Для каждого шага градиентного шага нам обязательно нужно взять T ∼ π θ T∼π θ с самыми свежими, с текущими весами θ θ, и никакая другая траектория, порождённая какой-то другой стратегией, нам не подойдёт. Поэтому для каждой итерации алгоритма нам придётся заново играть очередной эпизод со средой. Это sample-inefficient: неэффективно по числу сэмплов, мы собираем слишком много данных и очень неэффективно с ними работаем. Policy Gradient алгоритмы пытаются по-разному бороться с этой неэффективностью, опять же обращаясь к теории оценочных функций и бутстрапированным оценкам, позволяющим предсказывать будущие награды, не доигрывая эпизоды целиком до конца. Большинство этих алгоритмов остаются в on-policy режиме и применимы в любых пространствах действий. К этим алгоритмам относятся такие алгоритмы, как Advantage Actor-Critic (A2C), Trust-Region Policy Optimization (TRPO) и Proximal Policy Optimization (PPO). Что там ещё? Мы до сих пор разбирали model-free алгоритмы RL, которые обходились без знаний о p(s ′ ∣s,a) и никак не пытались приближать это распределение. Однако, в каких-нибудь пятнашках функция переходов нам известна: мы знаем, в какое состояние",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 15,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Policy Gradient алгоритмы пытаются по-разному бороться с этой неэффективностью, опять же обращаясь к теории оценочных функций и бутстрапированным оценкам, позволяющим предсказывать будущие награды, не доигрывая эпизоды целиком до конца. Большинство этих алгоритмов остаются в on-policy режиме и применимы в любых пространствах действий. К этим алгоритмам относятся такие алгоритмы, как Advantage Actor-Critic (A2C), Trust-Region Policy Optimization (TRPO) и Proximal Policy Optimization (PPO). Что там ещё? Мы до сих пор разбирали model-free алгоритмы RL, которые обходились без знаний о p(s ′ ∣s,a) и никак не пытались приближать это распределение. Однако, в каких-нибудь пятнашках функция переходов нам известна: мы знаем, в какое состояние перейдёт среда, если мы выберем некоторое действие в таком-то состоянии. Понятно, что эту информацию было бы здорово как-то использовать. Существует обширный класс model-based, который либо предполагает, что функция переходов дана, либо мы учим её приближение, используя s,a,s ′ из нашего опыта в качестве обучающей выборки. Алгоритм AlphaZero на основе этого подхода превзошёл человека в игру Го, которая считалась куда более сложной игрой, чем шахматы; причём этот алгоритм возможно запустить обучаться на любой игре: как на Го, так и на шахматах или сёги. источник картинки — курс UC Berkeley AI Обучение с подкреплением стремится построить алгоритмы, способные обучаться решать любую задачу, представленную в формализме MDP. Как и обычные методы оптимизации, их можно использовать в виде чёрной коробочки из готовых библиотек, например, OpenAI Stable Baselines. Внутри таких коробочек будет, однако, довольно много гиперпараметров, которые пока не совсем понятно как настраивать под ту или иную практическую задачу. И хотя успехи Deep RL демонстрируют, что эти алгоритмы способны обучаться невероятно сложным задачам вроде победы над людьми в Dota 2 и в StarCraft II, они требуют для этого колоссального количества ресурсов. Поиск более эффективных процедур — открытая задача в Deep RL. В ШАДе есть курс Practical RL, на котором вы погрузитесь глубже в мир глубокого обучения с подкреплением, разберётесь в более продвинутых алгоритмах и попробуете пообучать нейронки решать разные задачки в разных средах. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 10.5. Задача ранжирования Следующий параграф 11.2. Краудсорсинг",
    "metadata": {
      "title": "Обучение с подкреплением",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.1",
      "part": 16,
      "total_parts": 16,
      "source_file": "11.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Вступление Для обучения и проверки качества ML-модели необходимы данные, размеченные человеком. Студенты обычно получают эти данные уже в готовом виде, но в работе над реальными продуктами задачи по сбору и разметке приходится решать самостоятельно, учитывая специфику конкретного продукта. Готовые наборы зачастую однообразны, а иногда и вовсе мешают достичь требуемых результатов: так, модели компьютерного зрения для беспилотного транспорта необходимо обучать на данных, собранных в той же среде, где используется модель. Кроме того, высокие темпы развития нейросетевых технологий провоцируют все большую необходимость в крупных объемах данных: чем лучше текущее качество модели, тем больше новых данных требуется, чтобы поднять это качество на новый уровень. Как следствие, сбор и разметка данных становится неотъемлемой частью почти любого ML-производства, а качество и количество этих данных напрямую влияет на качество конечного продукта. Краудсорсинг зарекомендовал себя, как один из эффективных способов сбора и разметки данных в больших масштабах. Его используют в разработке новых технологий, чтобы создавать обучающие датасеты для ML-моделей беспилотных автомобилей, голосовых помощников, чат-ботов, поисковых систем и других разработок. Качественные данные удается собрать благодаря краудсорсинговым платформам: они помогают снизить количество ошибок с помощью специальных настроек, которые можно найти на платформе, а также дают доступ к огромному количеству исполнителей, способных в любое время присоединиться к работе. Также секрет успеха кроется в той последовательности действий, которые нужно соблюдать, создавая проект на карудсорсинговой платформе. Участникам краудсорсинговых платформ под силу выполнить не все задания, а только простые: сложные задания нужно разбивать на несколько небольших. Качество выполняемой ими работы нужно проверять с помощью доступных на платформе инструментов контроля качества. Полученные результаты в некоторых случаях нужно правильно обрабатывать (здесь полезно разобраться в способах агрегации данных). Чтобы исполнители получили оплату только за правильно выполненные задания, нужно сформулировать подходящую модель ценообразования и т. д. Нюансов в работе с краудсорсингом достаточно много, поэтому мы подготовили этот параграф в учебник. Стоит отметить, что в ней мы уделим внимание нетехническим аспектам краудсорсинга для ML. Такой уклон связан с тем, что использование краудсорсинга в качестве инструмента работы с данными требует не только знания технических и математических методов (они пригодятся в финальной части, когда полученные данные необходимо будет обработать), но и умения правильно организовать процесс сбора данных, понимания самого феномена краудсорсинга, который сегодня используется в разных сферах для решения разных задач. По этой причине структура этого параграфа будет выглядеть следующим образом: В первой части мы сделаем общий обзор краудсорсинга в ML и объясним, для каких задач он применим. Во второй части мы остановимся на основных этапах запуска краудсорсингового проекта: от деления проекта на небольшие задачи до обработки полученных от исполнителей данных. Кроме того, в этом параграфе мы разберем примеры некоторых ML-задач, которые встречаются в проектах в сфере AI и ML. Это сбор данных для поисковых сетей, разметка изображений для беспилотных автомобилей и сбор аудиозаписей для голосовых помощников. Надеемся, что вам будет интересно погружаться в мир краудсорсинга для ML. Будем рады, если мы поможем вам разложить все по полочкам, чтобы вы смогли дальше наращивать свои знания и изучать отдельные темы более глубоко. Что такое краудсорсинг в ML? Существует довольно много определений краудсорсинга, а также близких к нему по значению терминов (например,",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 1,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "деления проекта на небольшие задачи до обработки полученных от исполнителей данных. Кроме того, в этом параграфе мы разберем примеры некоторых ML-задач, которые встречаются в проектах в сфере AI и ML. Это сбор данных для поисковых сетей, разметка изображений для беспилотных автомобилей и сбор аудиозаписей для голосовых помощников. Надеемся, что вам будет интересно погружаться в мир краудсорсинга для ML. Будем рады, если мы поможем вам разложить все по полочкам, чтобы вы смогли дальше наращивать свои знания и изучать отдельные темы более глубоко. Что такое краудсорсинг в ML? Существует довольно много определений краудсорсинга, а также близких к нему по значению терминов (например, «человеческие вычисления», «мудрость толпы» и «коллективный разум»). Это связано с тем, что этот метод используется в разных сферах и применяется для решения разного рода задач, в том числе поиска креативных идей, создания контента, сбора денежных средств. Например, автор термина «краудсорсинг», Джефф Хоу, в 2006 году предложил следующее определение этого метода: Краудсорсинг (от англ. crowd — «толпа», source — «использование ресурсов») — это процесс, в котором компания переносит определенные функции, ранее возлагавшиеся на сотрудников, аутсорсинговые предприятия и поставщиков, на неопределенное, достаточно большое количество людей в формате открытого запроса. Это определение отражает основную идею краудсорсинга. Однако его недостаточно в контексте рассматриваемой нами темы. Мы говорим о краудсорсинге в машинном обучении. Это значит, что мы передаем облаку исполнителей те задачи, которые связаны со сбором и разметкой данных, а также с оценкой этих данных для разного рода проектов. Разработчики используют эти данные, чтобы обучать машины, а именно модели этих машин, выполнять требуемые задачи. Поэтому в машинном обучении краудсорсинг — это дополнительный вычислительный кластер, который помогает командам создавать и улучшать их продукты. Один из первых проектов, который задействовал краудсорсинг в получении данных для обучения модели, — проект Distributed Proofreaders (с англ. — «Распределенные корректоры»). Его главная цель — цифровизация печатных книг с помощью программы для оптического распознавания символов (OCR). Вовлекая тысячи волонтёров, проект Distributed Proofreaders оцифровывает печатные книги и улучшает программу для распознавания текстов. Чем больше данных модель этой программы получает от волонтеров, тем лучше она считывает текст с отсканированных страниц книг. Соответственно, чем лучше становится эта модель, тем меньше времени и усилий человека требуется для того, чтобы находить и исправлять ее ошибки. Рассмотрим этот проект подробнее: Добровольцам предлагают сравнить отсканированное изображение страницы и текст этой страницы, распознанный с помощью программного обеспечения для оптического распознавания символов (OCR). Поскольку программа оптического распознавания текста не справляется с задачей в полном объеме, в тексте часто появляются ошибки. Задача добровольца — исправить ошибки OCR и загрузить файл обратно на сайт. Выполненная работа передается второму добровольцу, он проверяет ее, исправляет ошибки. Книга аналогичным образом проходит третий этап корректуры и два этапа форматирования с использованием одного и того же веб-интерфейса. После того, как все страницы книги прошли через несколько этапов проверки, постпроцессор собирает их в электронную книгу и отправляет в архив проекта «Гутенберг». Отредактированные страницы книг в дальнейшем используются разработчиками, как данные для обучения OCR. Модель программы обучается на данных и в дальнейшем совершает меньше ошибок при распознавании текста на изображениях. Другой пример использования краудсорсинга в ML — сервис reCaptcha. Он",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 2,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "добровольца — исправить ошибки OCR и загрузить файл обратно на сайт. Выполненная работа передается второму добровольцу, он проверяет ее, исправляет ошибки. Книга аналогичным образом проходит третий этап корректуры и два этапа форматирования с использованием одного и того же веб-интерфейса. После того, как все страницы книги прошли через несколько этапов проверки, постпроцессор собирает их в электронную книгу и отправляет в архив проекта «Гутенберг». Отредактированные страницы книг в дальнейшем используются разработчиками, как данные для обучения OCR. Модель программы обучается на данных и в дальнейшем совершает меньше ошибок при распознавании текста на изображениях. Другой пример использования краудсорсинга в ML — сервис reCaptcha. Он был запущен учеными Университета Карнеги-Меллона в 2007 году и стал продолжением проекта Captcha, появившегося в 2000 году. Напомним, что Captcha — это программа, которая защищает сайты от интернет-ботов. Посещая сайт и совершая на нем определенные действия, пользователь получает просьбу заполнить веб-форму. Его задача — вписать в эту форму буквы и цифры, которые он видит на изображении. Люди с хорошим зрением могут легко распознать эти символы, а боты не могут. Так сервис определяет, кто из посетителей сайта человек, а кто — бот. Ботам доступ к сайтам закрывается, так как они наносят вред сайтам. Создатели проекта Captcha пошли дальше. Они подсчитали, что у каждого человека уходит примерно 10 секунд на ввод одной капчи. А у человечества (10 умножаем на 200 млн) — 500 000 часов. Тогда появилась идея о том, что время, потраченное на ввод капчи, можно использовать с пользой для людей. Это стало началом проекта reCaptcha. Отличие этого проекта от проекта Captcha состоит в том, что вы не только печатаете капчу и подтверждаете, что вы человек, но и одновременно делаете минимальное полезное усилие. В 2007 году таким усилием была оцифровка книг, а с 2012 года reCaptcha стали использовать для распознавания изображений из онлайн-карт. Мы расскажем про инициативу, вошедшую в историю под девизом Stop Spam, Read Books. В чем она заключалась? Каждая страница книги сканируется. Компьютер расшифровывает слова на каждом отсканированном изображении. Для этого используется технология OCR — та, же технология, что и в первом проекте. При распознавании текста OCR допускает ошибки. Их особенно много в распознанных текстах старых книг, поскольку в некоторых местах чернила выцвели и страницы пожелтели. Например, в книгах, написанных более 50 лет назад, компьютер не может распознать более 30% слов. Все нераспознанные слова направляются людям, чтобы они их распознали, когда вводят капчу в интернете. Задача добровольцев — ввести слова, взятые из отсканированных книг, которые компьютер не смог распознать. Добровольцу необходимо распознать два слова из книги. Почему именно два? Одно из слов взято из книги, и оно неизвестно компьютеру. Соответственно, проверить ответ добровольца компьютер не может. Поэтому волонтер получает второе слово — его компьютер знает. Мы не говорим, какое из слов известно компьютеру, и просим добровольца ввести оба. Если доброволец вводит известное слово правильно, система получает подтверждение, что он — человек, а также получает уверенность в правильности ввода другого слова. Одно и то же слово, которое неизвестно компьютеру, направляется десяти участникам проекта. Если все они вводят его одинаково, то есть их ответы совпадают, то это слово отправляется в книгу.",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 3,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Добровольцу необходимо распознать два слова из книги. Почему именно два? Одно из слов взято из книги, и оно неизвестно компьютеру. Соответственно, проверить ответ добровольца компьютер не может. Поэтому волонтер получает второе слово — его компьютер знает. Мы не говорим, какое из слов известно компьютеру, и просим добровольца ввести оба. Если доброволец вводит известное слово правильно, система получает подтверждение, что он — человек, а также получает уверенность в правильности ввода другого слова. Одно и то же слово, которое неизвестно компьютеру, направляется десяти участникам проекта. Если все они вводят его одинаково, то есть их ответы совпадают, то это слово отправляется в книгу. Как и в случае с первым проектом, данные, полученные от добровольцев, используются для обучения технологии OCR. Инициативой проекта reCaptcha впечатлилось множество владельцев сайтов. Новый сервис взамен традиционной Captcha установили такие сайты, как Tiketmaster, Facebook, Twitter и примерно 350 000 других сайтов. Каждый день на этих сайтах вплоть до 2012 года люди оцифровывали примерно 100 млн слов в день. Это 2,5 млн книг в год. В результате, в течение пяти лет с момента его запуска в проекте по оцифровке книг поучаствовали минимум 750 млн людей (это 10% всего населения). Книги, оцифрованные в рамках этого проекта сегодня представлены на сайте books.google.com. Подводя итоги вышесказанного, сформулируем определение краудсорсинга в ML. Краудсорсинг в ML — это способ сбора данных, которые необходимы разработчикам, чтобы обучать машины выполнять необходимые действия. С помощью краудсорсинга разработчики вовлекают в процесс выполнения задач обычных людей, которые не владеют определенными навыками и экспертизой. В рамках четко заданных инструкций они выполняют нужное количество заданий. Результаты этих заданий — собранные, размеченные или оцененные данные — входят в те датасеты, которые используются для обучения машин. Ключевые принципы краудсорсинга в ML Применение краудсорсинга в машинном обучении значительно ускорило процесс развития AI продуктов. Беспилотные автомобили, голосовые помощники, поисковые системы, онлайн-карты, машинный перевод появились и развиваются во многом благодаря данным, полученным с помощью краудсорсинга. Например, чтобы поисковая система смогла точно отвечать на вопросы пользователей, нужно проделать большую работу по разметке данных: проанализировать запросы и поведение пользователя, оценить возможные результаты на соответствие запросу, сравнить разные варианты поисковых выдач и выбрать лучший. Все эти данные ложатся в основу моделей, которые учатся искать лучшие ответы, опираясь на размеченные людьми образцы. Такие задачи, на первый взгляд, кажутся трудозатратными и продолжительными по времени. Но если воспользоваться возможностями краудсорсинга и подойти к ним, как к инженерной проблеме, эти сложности будут преодолены. В этом тезисе содержится основная идея краудсорсинга для AI и машинного обучения: чтобы решить задачу по разметке данных для обучения или оценки качества модели, нужно подойти к ней как к инженерной проблеме. Это значит, что нужно организовать выполнение задачи таким образом, чтобы конечный результат зависел от качества самого процесса, а не от добросовестности или экспертности отдельных исполнителей. Такой подход требует соблюдения ряда правил. Прежде всего, чтобы проект был доступен максимальному количеству исполнителей и не зависел от редких компетенций, его необходимо разделить на сценарии или небольшие задачи. Принцип деления сложной задачи на несколько микрозадач называется декомпозицией. Это основополагающий принцип для каждого краудсорсингового проекта, создаваемого для задач машинного обучения. Каждую микрозадачу необходимо детально продумать.",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 4,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "чтобы решить задачу по разметке данных для обучения или оценки качества модели, нужно подойти к ней как к инженерной проблеме. Это значит, что нужно организовать выполнение задачи таким образом, чтобы конечный результат зависел от качества самого процесса, а не от добросовестности или экспертности отдельных исполнителей. Такой подход требует соблюдения ряда правил. Прежде всего, чтобы проект был доступен максимальному количеству исполнителей и не зависел от редких компетенций, его необходимо разделить на сценарии или небольшие задачи. Принцип деления сложной задачи на несколько микрозадач называется декомпозицией. Это основополагающий принцип для каждого краудсорсингового проекта, создаваемого для задач машинного обучения. Каждую микрозадачу необходимо детально продумать. Определить элементы, которые будут ее сопровождать. Некоторые из них (например, инструкции или интерфейсы) обязательно должны присутствовать в проекте. Другие — такие как предварительная фильтрация исполнителей или отслеживание их поведения в проекте — используются в случае необходимости. Все эти элементы решают вопрос качества данных: чем лучше продуман проект, чем эффективнее он «сопровождает» исполнителя во время разметки, тем меньше остается пространства для ошибок или недобросовестного поведения. Детальную схему проекта, состоящую из цепочки микрозадач и сопровождающих их элементов, называют пайплайном (от англ. pipeline — «линия, очередь»). Его создают на этапе планирования проекта и обращаются к нему как к «дорожной карте». ML-задачи, где используется разметка Краудсорсинг помогает решить разнообразный спектр ML-задач. Разделим их на две основные группы — разметка и сбор данных. Разметка данных К этой группе относится целый ряд задач, в рамках которых пользователю краудсорсинговой платформы необходимо выполнить некоторое действие с уже полученными данными. Например, его могут попросить перевести записи из аудио в текст (транскрипция аудио) или выделить в запросе пользователя в поисковой системе определенные смысловые части, такие как тип продукта, цвет, бренд (NLP-задания). Также в эту группу входят задачи по проверке автоматического перевода, модерации контента, разметке видео или сегментации объектов на изображениях. В качестве примера рассмотрим задачи по сегментации изображений. Как правило, они нужны для обучения алгоритмов компьютерного зрения. Они используются, например, для создания беспилотного транспорта, который должен распознавать всевозможные препятствия на дорогах: людей, светофоры, разметку, дорожные знаки, дома, заборы, искусственные неровности и т. д. Чтобы эти модели были качественными и могли без труда распознавать любые объекты на своем пути, им нужно показать большое количество изображений и в, более сложных случаях, видео с выделенными на них объектами разных классов. Выделением этих объектов занимаются пользователи краудсорсинговых платформ. На 2D и 3D изображениях, а также видео, снятых во время движения с помощью камер, радаров и лидаров, они находят нужные объекты и обводят их. Изображения и видео, размеченные по требованиям инструкции, используются для обучения моделей компьютерного зрения. Самый простой пайплайн задачи по сегментации изображений для беспилотных автомобилей состоит из трех проектов (рис. 1). В первом проекте исполнители отвечают на вопрос, есть ли на фото нужные объекты (например, дорожные знаки). Те изображения, на которых эти объекты есть, перенаправляются в проект номер два. В нем вторая группа исполнителей обводит дорожные знаки с помощью прямоугольников. Эту разметку проверяет еще одна группа исполнителей в следующем проекте, третьем по счету. Далее включается схема так называемой отложенной приёмки заданий. В случае отклонения задание отправляется на повторную разметку. Верно выполненная работа включается",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 5,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и видео, размеченные по требованиям инструкции, используются для обучения моделей компьютерного зрения. Самый простой пайплайн задачи по сегментации изображений для беспилотных автомобилей состоит из трех проектов (рис. 1). В первом проекте исполнители отвечают на вопрос, есть ли на фото нужные объекты (например, дорожные знаки). Те изображения, на которых эти объекты есть, перенаправляются в проект номер два. В нем вторая группа исполнителей обводит дорожные знаки с помощью прямоугольников. Эту разметку проверяет еще одна группа исполнителей в следующем проекте, третьем по счету. Далее включается схема так называемой отложенной приёмки заданий. В случае отклонения задание отправляется на повторную разметку. Верно выполненная работа включается в итоговый датасет. Пайплайн проекта по разметке данных для обучения модели компьютерного зрения Подобные пайплайны, но еще более многоступенчатые, используются для обучения моделей компьютерного зрения Яндекса. В январе 2020 года инженерам компании удалось продемонстрировать одну из моделей на конференции Consumer Electronics в Лас-Вегасе. Беспилотные автомобили со встроенной моделью проследовали по маршруту с разными дорожными сценариями: нерегулируемыми перекрестками, сложными поворотами со встречным разъездом, пешеходными переходами и многополосными участками. Всего эти автомобили преодолели более 7 тысяч км. Сбор данных Суть задач, связанных со сбором контента, заключается в поиске материалов (изображений, фотографий, фактов), необходимых для решения проблемы. Например, используя краудсорсинг, инженеры собирают фразы для обучения голосового помощника (рис. 2). Пайплайн такого проекта выглядит довольно просто: исполнители записывают необходимую фразу, например, «Привет, Алиса», и загружают ее в интерфейс задания на краудсорсинговой платформе. Далее другая группа исполнителей проверяет эти записи на предмет ошибок и других требований: если запись соответствует инструкции, вторая группа подтверждает ее, а если в записи допущены ошибки, отклоняет. В следующем проекте еще одна группа исполнителей записывает недостающие фразы, затем они вновь проходят проверку. Этот процесс повторяется по кругу, пока не будет собрано достаточное количество фраз нужного качества. Пайплайн проекта по сбору данных для обучения модели распознавания голоса Краудсорсинговые платформы Масштабируемость и скорость выполнения задач по разметке данных напрямую зависят от доступа заказчика к большому облаку исполнителей. Залог успеха здесь — использование открытых краудсорсинговых платформ, которые позволяют постоянно пополнять это облако и, следовательно, масштабировать процессы сбора или разметки данных. Открытые краудсорсинговые платформы — например, Amazon Mechanical Turk или Толока — работают по принципу маркетплейсов. Заказчик может создать на такой платформе свой проект, найти для него нужных исполнителей, обучить их и поручить им выполнение задания, контролируя качество результата. Пользователи открытой платформы, в свою очередь, могут выбрать интересующий их проект, выполнить задания и получить за проделанную работу вознаграждение. Свой выбор проекта они могут сделать как на основе рейтинга проекта, так и с учетом итогового вознаграждения — либо просто потому, что какая-то задача им интересна больше других. Открытые краудсорсинговые платформы — инструмент для тех, кто планирует самостоятельно контролировать разметку данных. А это, как правило, большинство проектов в сфере AI и машинного обучения. Для ML-разработчиков крайне важно, чтобы кропотливая работа по написанию инструкций, проектированию интерфейсов, отбору и обучению участников, настройке контроля качества была выполнена в точности так, как это запланировано в пайплайне проекта. Все эти шаги напрямую влияют на качество тренировочных данных, а от них в немалой степени зависит успех продукта. При выборе краудсорсинговой платформы важно учесть",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 6,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на основе рейтинга проекта, так и с учетом итогового вознаграждения — либо просто потому, что какая-то задача им интересна больше других. Открытые краудсорсинговые платформы — инструмент для тех, кто планирует самостоятельно контролировать разметку данных. А это, как правило, большинство проектов в сфере AI и машинного обучения. Для ML-разработчиков крайне важно, чтобы кропотливая работа по написанию инструкций, проектированию интерфейсов, отбору и обучению участников, настройке контроля качества была выполнена в точности так, как это запланировано в пайплайне проекта. Все эти шаги напрямую влияют на качество тренировочных данных, а от них в немалой степени зависит успех продукта. При выборе краудсорсинговой платформы важно учесть и то, какими инструментами они располагают. Например, с готовыми шаблонами можно быстрее спроектировать интерфейс задания, а инструменты контроля качества помогут отсеять роботов и недобросовестных исполнителей. Кроме того, выбор платформы во многом определит то, с какими исполнителями будет вестись работа. Изучение их характеристик даст понимание, в каких странах они проживают, на каких языках разговаривают и, что немаловажно, сталкивались ли они с проектами, подобными тому, над которым планируется работа. Альтернативой платформам-маркетплейсам могут стать проекты, которые предлагают готовые датасеты и помощь в разметке данных для проекта. Это, например, Scale AI, Hive Data, Alegion. Такие платформы подойдут не всем — выше уже шла речь о том, что некоторые проекты (как, например, обучение алгоритмов компьютерного зрения) нуждаются в специфическом контексте для сбора датасета. Кроме того, построенные по общим принципам краудсорсинга проекты могут запускаться и на внутреннее облако исполнителей, связанных с компанией какими-либо договорными отношениями. Это важно в случаях, если речь идет о разметке чувствительных данных. Однако такой процесс тяжело поддается масштабированию, потому что требует больших ресурсов для сопровождения сотрудников. Границы применимости краудсорсинга Несмотря на все многообразие задач, которые можно решить с помощью краудсорсинга, есть случаи, когда его применение затруднено либо просто нецелесообразно. Во-первых, необходимо оценить затраты, сопутствующие запуску проекта. Создание и настройка эффективного пайплайна для сбора или обработки данных требуют времени и квалификации высокоуровневого специалиста. Потраченный им ресурс может не окупиться, если требуется лишь один раз разметить небольшое количество данных. Облаку исполнителей с трудом поддаются задачи, требующие серьезного включения и поддержания контекста. Секрет краудсорсинга — в создании небольших автономных заданий, каждое из которых может быть решено согласно несложной инструкции. Если исполнителю требуется учитывать большой объем сопутствующей информации, чтобы выполнить задачу верно — скорее всего, ее лучше выполнять без использования краудсорсинга. Например, облако исполнителей вряд ли сможет осуществить перевод книги: ее не стоит разбивать на отдельные предложения, ведь перевод должен быть последовательным и согласованным. В то же время, краудсорсинг может помочь при переводе отдельных фраз в конечном контексте, например, отдельных реплик для голосового ассистента. Наконец, если задача требует крайне специфических навыков, то поиск или обучение подходящего исполнителя на краудсорсинговой платформе сравнится с наймом эксперта. В таких случаях стоит оценить возможность декомпозиции задачи так, чтобы она оказалась разбита на ряд менее сложных действий. Если сделать это невозможно (например, для выполнения задания требуется знание редкого языка), оптимальным способом поиска исполнителя могут стать профессиональные сообщества. Этапы создания краудсорсингового проекта Типичная краудсорсинговая задача состоит из шести этапов: Декомпозиция; Инструкция и интерфейс; Контроль качества; Отбор и обучение исполнителей; Выбор схемы оплаты",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 7,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "же время, краудсорсинг может помочь при переводе отдельных фраз в конечном контексте, например, отдельных реплик для голосового ассистента. Наконец, если задача требует крайне специфических навыков, то поиск или обучение подходящего исполнителя на краудсорсинговой платформе сравнится с наймом эксперта. В таких случаях стоит оценить возможность декомпозиции задачи так, чтобы она оказалась разбита на ряд менее сложных действий. Если сделать это невозможно (например, для выполнения задания требуется знание редкого языка), оптимальным способом поиска исполнителя могут стать профессиональные сообщества. Этапы создания краудсорсингового проекта Типичная краудсорсинговая задача состоит из шести этапов: Декомпозиция; Инструкция и интерфейс; Контроль качества; Отбор и обучение исполнителей; Выбор схемы оплаты и бонусирования; Агрегация ответов. Разберем каждый из этапов на примере уже упомянутого проекта по сбору данных для обучения беспилотных автомобилей. Мы запустим этот проект на краудсорсинговой платформе «Толока». Декомпозиция В качестве исходных данных возьмем объемный набор фотографий с изображением улиц. После запуска краудсорсингового проекта мы должны получить те же изображения, но с выделенными на них дорожными знаками. Наша задача — выделить прямоугольниками дорожные знаки на каждой фотографии. Пример того, как должен выглядеть итоговый датасет с выделенными на них объектами приведен на рисунке 3. Изображение с выделенными на нем дорожными знаками с помощью полигонов Можем ли мы поручить нашу задачу участникам краудсорсинговой платформы напрямую? В данном случае — нет. Изображения для разметки могут полностью не соответствовать нашему запросу. Например, на изображениях может не быть нужных объектов. Некоторые фотографии могут не загрузиться в интерфейсе (появится ошибка). Чтобы избежать подобных ситуаций, нам нужно отобрать фотографии с подходящими объектами. Отбор фото или их фильтрация станет первой микрозадачей или первым пулом (так называется набор заданий в рамках проекта на платформе «Толока») нашего проекта. Что дальше? Когда мы получили фотографии с дорожными знаками, мы сможем запустить проект по выделению объектов на изображениях. Наша задача — выделить на фотографиях все дорожные знаки прямоугольниками. Чтобы создать подобное задание на краудсорсинговой платформе «Толока», можно воспользоваться готовым шаблоном. Он предусматривает специальный инструмент, «полигон», который с легкостью позволяет выполнять подобные задания. На этом мы могли бы остановиться. Получили изображения с выделенными объектами — задача выполнена. Однако для данного проекта потребуется запустить еще одно микрозадание. Фотографии с выделенными объектами необходимо проверить. Кто-то из исполнителей может пропустить некоторые знаки или выделить их неверно. Таким образом, проверка размеченных изображений в конкретном проекте необходима. Но специфика задачи такова, что мы не можем просто сравнить работу отдельного исполнителя с заведомо верным примером: выделенные области могут отличаться на несколько пикселей, но это не будет означать, что ответ неверен. Итак, что мы делаем? Мы создаем новый пул заданий, в котором спрашиваем «Верно ли выделены объекты на фото?». Участники отвечают на вопрос, после чего фото с верно отмеченными объектами отправляются в итоговый датасет и оплачиваются. Фото с неверно выделенными объектами отклоняются и не оплачиваются. Все фотографии, которые не проходят проверку, отправляются на переразметку (т. е. размечаются повторно). Какие выводы мы можем сделать по итогу разбора декомпозиции проекта? Самый главный вывод — решение о декомпозиции задачи следует принимать, исходя из типа задачи и данных, которые есть на входе — это могут быть изображения, видео, ссылки, точки на карте, координаты этих",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 8,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "неверен. Итак, что мы делаем? Мы создаем новый пул заданий, в котором спрашиваем «Верно ли выделены объекты на фото?». Участники отвечают на вопрос, после чего фото с верно отмеченными объектами отправляются в итоговый датасет и оплачиваются. Фото с неверно выделенными объектами отклоняются и не оплачиваются. Все фотографии, которые не проходят проверку, отправляются на переразметку (т. е. размечаются повторно). Какие выводы мы можем сделать по итогу разбора декомпозиции проекта? Самый главный вывод — решение о декомпозиции задачи следует принимать, исходя из типа задачи и данных, которые есть на входе — это могут быть изображения, видео, ссылки, точки на карте, координаты этих точек. Также следует различать типичные случаи, в которых декомпозиция особенно рекомендована для проекта. Речь идет об объемных проектах, многослойных задачах, задачах со множеством вариантов ответов и объемных процессах: Объемные проекты. Если в рамках проекта нужно ответить на несколько вопросов, то лучше сделать это поочередно или в выбранной последовательности. Многослойные задачи. Если в рамках одной задачи нужно выполнить более одного действия (например, отнести объект к определенной группе и ответить на вопрос, предназначен ли он только для взрослых), то лучше сделать это поочередно или в выбранной последовательности. Задачи со множеством вариантов ответов. Если в задании есть один вопрос и 10 и более вариантов ответа, то лучшим решением будет группировка ответов по темам, а затем создание отдельного проекта для каждой группы ответов. Объемные процессы. Если задача включает сложные механизмы контроля качества и отложенную проверку, необходимо создать отдельный проект, в котором одна группа исполнителей будет проверять другую. Есть ли случаи, когда декомпозировать задачу не нужно? Да. Нет необходимости разбивать задачу на части, если соблюдаются два критерия: инструкции к задаче помещаются на половине листа бумаги формата А4, или задача выполнена с помощью одного действия, например, выбора из нескольких категорий. Инструкция После декомпозиции нашего проекта нам необходимо создать для него инструкцию. Инструкция потребуется для каждой микрозадачи. В нашем случае нам необходимо создать три инструкции. Какие пункты мы обязательно в них укажем? Первым пунктом инструкции станет описание задачи. В нем мы объясним участнику, что предстоит сделать и где будет использован результат этой работы. Например: Вашему вниманию представлен проект, результаты которого помогут сделать беспилотные автомобили безопасным транспортом. Ваша задача — определить, есть ли дорожные знаки на изображении. Выберите ответ «Да», если изображение содержит дорожные знаки. Выберите ответ «Нет», если на изображении дорожных знаков нет. На изображении, представленном ниже, есть несколько дорожных знаков. Значит, правильный ответ — «Да». Picture Далее, мы подробно опишем условия входа в задание: расскажем, будет ли обучение и экзамен, с каким качеством его нужно пройти, есть ли в проекте повторный экзамен для тех, кто не прошел испытание с первого раза. Также опишем ценообразование. Например: Чтобы выполнить это задание, вам потребуется пройти обучение на тренировочном пуле. В тренировочный пул войдут задания аналогичные тем, что будут в основном проекте. После обучения мы предложим вам пройти экзамен. В экзамен войдут 5 изображений. Следующий элемент инструкции — технические нюансы. Здесь мы расскажем, с какого устройства потребуется выполнить задание — со смартфона или с компьютера — и какие дополнительные настройки браузера будут необходимы. Этот пункт в особенности важен для второго",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 9,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ли обучение и экзамен, с каким качеством его нужно пройти, есть ли в проекте повторный экзамен для тех, кто не прошел испытание с первого раза. Также опишем ценообразование. Например: Чтобы выполнить это задание, вам потребуется пройти обучение на тренировочном пуле. В тренировочный пул войдут задания аналогичные тем, что будут в основном проекте. После обучения мы предложим вам пройти экзамен. В экзамен войдут 5 изображений. Следующий элемент инструкции — технические нюансы. Здесь мы расскажем, с какого устройства потребуется выполнить задание — со смартфона или с компьютера — и какие дополнительные настройки браузера будут необходимы. Этот пункт в особенности важен для второго задания в рамках нашего проекта. Разметить дорожные знаки прямоугольниками участники смогут только с компьютера: Мы рекомендуем выполнять это задание с персонального компьютера. Это необходимо, чтобы вы смогли корректно выделить все необходимые объекты на изображении. Краткое описание интерфейса задания — еще один важный пункт в инструкции. Для большей наглядности мы сделаем скриншот с комментариями о том, для чего нужны те или иные блоки и кнопки. Если в задании простой интерфейс, эту часть можно пропустить. Например: Используйте желтый квадрат («полигон») в левой части экрана, чтобы выделять дорожные знаки на изображении. Теперь о самом задании. Чтобы избежать ошибок, мы пошагово опишем все частые сценарии, которые могут случиться при выполнении наших задач. Также мы укажем, что делать с нестандартными случаями. Добавим примеры: несколько кейсов сделают теорию намного понятнее. Справочные материалы — глоссарий, faq — важное дополнение к этим сценариям. Наконец, мы расскажем, куда направлять вопросы по заданию или проекту в целом. На что мы обратим внимание при написании текста? Первое, за чем стоит проследить — сам язык, которым написана инструкция. Мы откажемся от профессионального сленга и не будем использовать терминологию. Некоторые термины, например, «полигоны», мы объясним или заменим синонимами — «прямоугольники». Наша задача — сделать инструкцию простой и понятной для большого числа участников. Следуя этой же задаче, мы упростим стиль и синтаксис (одна мысль = одно предложение; одна тема = один абзац), не будем использовать пояснения в скобках и сделаем форматирование единообразным. Готовый текст инструкции мы обязательно проверим, выполнив некоторое количество заданий. Такое упражнение быстро покажет, какие случаи еще не описаны в инструкции, а какие описаны мало. Кроме того, оно позволит проверить как выглядит наше задание на разных устройствах: умещаются ли все картинки и скриншоты на экранах мобильного телефона, планшета и компьютера. В итоге каждая инструкция не займет больше двух экранов. Это максимальное количество пространства для инструкции, за пределы которого лучше не выходить. Если инструкция все же не вписывается в такой объем, вероятно, задача слишком многосоставная и ее нужно декомпозировать. Агрегация результатов Представим, что мы запустили наш проект и получили необходимые данные. В краудсорсинговых проектах данные обычно собираются в перекрытии (мнения большинства) — это один из распространенных механизмов контроля качества исполнителей и улучшения качества итогового набора данных. Но как выбрать из нескольких оценок финальную? В данном случае нам помогут механизмы агрегации данных. Что они делают? Они обрабатывают файлы с ответами исполнителей и выбирают из нескольких ответов тот, который с наибольшей вероятностью окажется верным. Рассмотрим принцип работы механизмов агрегации данных на примере первого пула с",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 10,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "инструкция все же не вписывается в такой объем, вероятно, задача слишком многосоставная и ее нужно декомпозировать. Агрегация результатов Представим, что мы запустили наш проект и получили необходимые данные. В краудсорсинговых проектах данные обычно собираются в перекрытии (мнения большинства) — это один из распространенных механизмов контроля качества исполнителей и улучшения качества итогового набора данных. Но как выбрать из нескольких оценок финальную? В данном случае нам помогут механизмы агрегации данных. Что они делают? Они обрабатывают файлы с ответами исполнителей и выбирают из нескольких ответов тот, который с наибольшей вероятностью окажется верным. Рассмотрим принцип работы механизмов агрегации данных на примере первого пула с заданиями (см. рис. 4). У нас есть набор изображений, и наша цель — отнести каждое изображение к группе «изображения с дорожными знаками» или к группе «изображения без дорожных знаков». В соответствии с принципом краудсорсинга задание должно быть распределено между несколькими исполнителями, каждый из которых размечает некое подмножество изображений. В результате для каждого изображения у нас есть несколько результатов разметки. Цель метода агрегации — объединить эти результаты в один качественный ответ. Агрегация данных, полученных с помощью краудсорсинга Мнение большинства Алгоритм агрегации данных «Мнение большинства» основан на предположении, что правильный ответ — этот тот, который выбирают большинство исполнителей (рис. 5). Самый популярный ответ становится финальным ответом. Агрегация данных по методу, основанному на мнении большинства Практика показывает, что при помощи метода, основанного на мнении большинства, можно получить достойные результаты. Поэтому этот метод с успехом применяется во многих проектах. Также одно из преимуществ этого метода заключается в том, что он весьма нагляден и логика его работы понятна. Однако в проектах краудсорсинга существуют определенные временные и бюджетные ограничения. Наша цель в том, чтобы собрать минимальный объем данных, необходимый для достижения желаемой точности. С этой точки зрения, метод, основанный на мнении большинства, далеко не всегда будет оптимальным выбором. Чтобы осознать слабые стороны метода, рассмотрим его модель. Модель Модель, лежащая в основе метода, проста. Есть N изображений и M исполнителей. Каждое изображение j∈1,...,N подразумевает некий неизвестный ответ («изображения с дорожными знаками» или «изображения без дорожных знаков» в нашем случае). При использовании модели, основанной на мнении большинства, предполагается, что если исполнитель i i разметил изображение j j, его ответ является правильным с некоторой вероятностью Исполнитель i отвечает на вопрос j верно ) = p P(Исполнительiотвечаетнавопросjверно)=p При этом вероятность правильного ответа полагается одинаковой для каждого исполнителя и вопроса. Допущение, что p > 1 / 2 p>1/2 учитывает, что для каждого исполнителя вероятность правильного ответа выше, чем неправильного. В таком случае, поскольку число разметок для каждого изображения достаточно велико, мнение большинства с высокой вероятностью даст истинные ответы. Ограничения В силу своей простоты, метод основанный на мнении большинства имеет ряд ограничений: **Однородность исполнителей.**Во-первых, данный метод предполагает, что все исполнители обладают одинаковыми способностями. Иными словами,для каждого конкретного вопроса вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех исполнителей. Однако на практике пул исполнителей на краудсорсинговых платформах чрезвычайно разнообразен: кто-то из них очень аккуратно и скрупулезно выполняет задачи, а кто-то небрежен и чаще допускает ошибки. Таким образом, одно из направлений совершенствования модели, основанной на мнении большинства, — это учет различия в",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 11,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "случае, поскольку число разметок для каждого изображения достаточно велико, мнение большинства с высокой вероятностью даст истинные ответы. Ограничения В силу своей простоты, метод основанный на мнении большинства имеет ряд ограничений: **Однородность исполнителей.**Во-первых, данный метод предполагает, что все исполнители обладают одинаковыми способностями. Иными словами,для каждого конкретного вопроса вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех исполнителей. Однако на практике пул исполнителей на краудсорсинговых платформах чрезвычайно разнообразен: кто-то из них очень аккуратно и скрупулезно выполняет задачи, а кто-то небрежен и чаще допускает ошибки. Таким образом, одно из направлений совершенствования модели, основанной на мнении большинства, — это учет различия в способностях исполнителей в рамках модели. **Однородность вопросов.**Во-вторых, модель, основанная на мнении большинства, предполагает, что вопросы имеют одинаковую сложность. Другими словами, вероятность того, что исполнитель правильно ответит на вопрос, одинакова для всех вопросов. Однако некоторые вопросы в рамках проекта могут быть сложнее других. Таким образом, еще одно направление по улучшению модели на основании мнения большинства — это учесть в модели разную степень сложности вопросов. Далее мы рассмотрим оба направления развития модели и расскажем о других алгоритмах, учитывающих особенности краудсорсинговых заданий. Агрегация с учетом способностей исполнителей Рассмотрим модель, которая учитывает неоднородность исполнителей при агрегации ответов. Модель Естественный способ учесть различия в способностях исполнителей — ввести параметр качества для каждого исполнителя. Если есть M M исполнителей, то мы можем связать каждого исполнителя i∈1,...,M с неизвестным параметром качества ∈[0,1]. Чем выше параметр качества исполнителя, тем больше вероятность того, что исполнитель ответит на вопрос правильно: P ( Исполнитель i отвечает на вопрос j верно ) = p i P(Исполнительiотвечаетнавопросjверно)=p i Другими словами, вероятность того, что исполнитель правильно ответит на вопрос, своя для каждого исполнителя (но от вопроса она все еще не зависит). Методы В ситуации, когда у исполнителей разные способности, логично присваивать больший вес ответам более сильных исполнителей и меньший вес — ответам более слабых. Однако проблема в том, что параметры качества для исполнителей априори нам не известны. Основная идея двух методов модели агрегации данных с учетом способностей исполнителей заключается том, чтобы одновременно оценить параметры качества для исполнителей и ответы на поставленные вопросы. Рассмотрим каждый их них. Использование большого объема контрольных заданий Контрольные вопросы (также honeypots, golden sets) — это задания, на которые заказчик заранее знает правильные ответы. На практике мы часто добавляем в набор данных определенное количество контрольных вопросов, чтобы контролировать качество работы исполнителей. Когда этих вопросов достаточно много, мы можем использовать их для оценки качества работы. Предположим, что у нас есть G G контрольных вопросов и некий исполнитель i i, который правильно ответил на k i k i вопросов из G G контрольных вопросов. Тогда мы можем оценить параметр качества для исполнителя следующим образом: Теперь, когда у нас есть оценка параметра качества, мы можем оценить ответ каждого исполнителя по-разному. Эта идея подводит нас к концепции взвешенного мнения большинства (от англ. Weighted majority vote). Идея этого метода проиллюстрирована на рисунке ниже (рис. 6). Предположим, что у нас есть нестандартное изображение, на котором столб похож на дорожный знак. В этом случае модель, основанная на простом мнении большинства, не делает отличия между ответами исполнителей с меньшими",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 12,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "контрольных вопросов и некий исполнитель i i, который правильно ответил на k i k i вопросов из G G контрольных вопросов. Тогда мы можем оценить параметр качества для исполнителя следующим образом: Теперь, когда у нас есть оценка параметра качества, мы можем оценить ответ каждого исполнителя по-разному. Эта идея подводит нас к концепции взвешенного мнения большинства (от англ. Weighted majority vote). Идея этого метода проиллюстрирована на рисунке ниже (рис. 6). Предположим, что у нас есть нестандартное изображение, на котором столб похож на дорожный знак. В этом случае модель, основанная на простом мнении большинства, не делает отличия между ответами исполнителей с меньшими способностями (первых двух исполнителей) и ответами исполнителя-эксперта (последнего исполнителя) и допускает ошибку. Напротив, модель взвешенного мнения большинства дополнительно взвешивает каждый ответ полученным коэффициентом качества исполнителя. Такая модель приводит к правильному ответу, поскольку мнение исполнителя-эксперта в таком случае перевешивает мнения двух других исполнителей. Агрегация данных по методу, основанному на взвешенном мнении большинства Когда контрольных вопросов не так много Метод взвешенного мнения большинства подходит для тех случаев, когда в проекте есть достаточное количество контрольных заданий, необходимых для оценки качества работы исполнителя. Однако зачастую контрольных заданий в проекте не хватает, в связи с чем оценки могут быть довольно неточными. Кроме того, исполнители могут коллективно выявить контрольные вопросы и начать обманывать систему, давая правильные ответы на контрольные вопросы и случайные ответы на другие. В этом случае, чтобы оценить параметры качества исполнителей при ответе на неизвестные вопросы, мы можем использовать метод Дэвида — Скина: Метод Дэвида-Скина (Dawid, Skene, 1979) Метод Дэвида-Скина одновременно находит значения качества исполнителей и ответы на вопросы, которые согласуются с наблюдаемыми данными в наибольшей степени. Мы имеем в качестве данных — количество раз, при которых разметчик u ∈ U u∈U поставил класс k ∈ K k∈K объекту i ∈ I i∈I (возможно, разметчик видел этот объект несколько раз). Обозначим через объект i класса k } , Y ik =I{объект i класса k}, это наши латентные величины. В качестве параметров имеем — вероятность того, что разметчик u u поставил класс ℓ ℓ вместо правильного класса — вероятность класса k k. Примем также обозначения: по всем u и k для объекта i } N i ={n ik u по всем u и k для объекта i}, для разметчика u и объекта i } N i u ={n ik u для разметчика u и объекта i}, по всем k для объекта i } Y i ={Y ik по всем k для объекта i}. Поймём, какой будет функция неполного правдоподобия в этой задаче. Прежде всего, π,p (N,Y)= i∈I ∏ p(N i ,Y i ), Если k k – номер класса i i-го объекта, то объект i класса объект i класса k ) p(N p(объект i класса k) p(N i ∣объект i класса k) (значения Y i t Y it однозначно определяются номером истинного класса, поэтому справа Y i Y i пропадает). Далее, мы считаем, что разметчики действуют независимо, поэтому p ( N i ∣ объект i класса объект i класса k ) . p(N i ∣объект i класса k)= u∈U ∏ p(N i u ∣объект i",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 13,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "будет функция неполного правдоподобия в этой задаче. Прежде всего, π,p (N,Y)= i∈I ∏ p(N i ,Y i ), Если k k – номер класса i i-го объекта, то объект i класса объект i класса k ) p(N p(объект i класса k) p(N i ∣объект i класса k) (значения Y i t Y it однозначно определяются номером истинного класса, поэтому справа Y i Y i пропадает). Далее, мы считаем, что разметчики действуют независимо, поэтому p ( N i ∣ объект i класса объект i класса k ) . p(N i ∣объект i класса k)= u∈U ∏ p(N i u ∣объект i класса k). Разберёмся с величиной p ( N i ∣ объект i класса k ) p(N i ∣объект i класса k). Она отвечает за то, какие классы u u-й разметчик ставил i i-му объекту. Мы считаем, что встречи разметчика с объектом упорядочены по времени, тогда p ( u -й разметчик отнёс i -й объект к классам объект i класса k ) = p(u-й разметчик отнёс i-й объект к классам k 1 ′ ,…,k r ′ ∣объект i класса k)= -ю встречу с i -м объектом u -й разметчик отнёс его к классу k s ′ ∣ объект i класса k ) =∏ s p(в s-ю встречу с i-м объектом u-й разметчик отнёс его к классу k s ′ ∣ объект i класса k) Эту вероятность можно переписать в виде ℓ∈K а итоговое неполное правдоподобие предстаёт в виде π,p (N,Y)= i∈I ∏ k∈K ∏ (ρ k u∈U ∏ ℓ∈K Его нам нужно максимизировать по π π и ρ ρ Пояснение к формуле: Вне больших скобок фиксируются объект и его класс, сама скобка возводится в степень 1, если рассматривается правильный класс объекта, и в степень 0 иначе. Внутри сначала записана вероятность того, что объект имеет данный класс, а затем — перебор по всем пользователям и всем классам, которые мог поставить данный пользователь. Наконец, записывается вероятность того, что пользователь нашему объекту поставил некоторый класс, которая возводится в степень того, сколько раз он поставил этот класс. Например, если пользователь видел изображение котика 5 раз, при этом 3 раза он сказал, что котик, а два раза — песик, то вероятность cat,cat u для данного котика учтется 3 раза, а вероятность cat,dog u — 2 раза. Рассмотрим концепцию метода Дэвида-Скина на простом примере (рис. 7). Предположим, что у нас есть только N = 4 N=4 вопросов и M = 3 M=3 исполнителей. Каждый исполнитель отвечает на все вопросы. В этом случае наблюдаемые данные — это ответы исполнителей на вопросы. Агрегация данных по методу Дэвида-Скина Давайте разберемся в том, каким образом метод Дэвида — Скина позволяет найти параметры качества для исполнителей и те ответы на вопросы, которые лучше всего соответствуют наблюдаемым данным. Для этого рассмотрим два варианта, показанные на картинках ниже (см. рис. 7.1). Каждая картинка предполагает свой набор параметров. Посмотрим, какой из предложенных вариантов лучше соответствует наблюдаемым данным. Picture Агрегация данных по методу Дэвида-Скина Во-первых, обратите внимание, что на обоих изображениях предложенные ответы согласуются с ответами исполнителя, у которого, по оценкам, высокий параметр качества. Но какой",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 14,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "все вопросы. В этом случае наблюдаемые данные — это ответы исполнителей на вопросы. Агрегация данных по методу Дэвида-Скина Давайте разберемся в том, каким образом метод Дэвида — Скина позволяет найти параметры качества для исполнителей и те ответы на вопросы, которые лучше всего соответствуют наблюдаемым данным. Для этого рассмотрим два варианта, показанные на картинках ниже (см. рис. 7.1). Каждая картинка предполагает свой набор параметров. Посмотрим, какой из предложенных вариантов лучше соответствует наблюдаемым данным. Picture Агрегация данных по методу Дэвида-Скина Во-первых, обратите внимание, что на обоих изображениях предложенные ответы согласуются с ответами исполнителя, у которого, по оценкам, высокий параметр качества. Но какой выбор параметров подходит данным лучше всего? Чтобы ответить на этот вопрос, обратите внимание, что ответы второго и третьего исполнителей полностью совпадают. Если параметры качества для этих исполнителей соответствуют первой картинке 0.5 =0.5, тогда, если верить этой модели, эти два исполнителя отвечают наугад. В таком случае высокая степень согласия между исполнителями нас бы скорее удивила, поскольку отвечая наугад, они должны время от времени расходиться в своих ответах. Напротив, если исполнители 2 и 3 — эксперты, как на втором изображении =1, тогда мы ожидаем, что у них будет высокая степень согласия, и это то, что мы видим в данных. Интуитивно, второй набор параметров лучше согласуется с наблюдаемыми данными. Приведенный простой пример показывает, что концепция согласованности между потенциальными параметрами и наблюдаемыми данными позволяет нам исключить те варианты, которые плохо согласуются с наблюдаемыми данными. Оба метода — взвешенное мнение большинства и агрегация по методу Дэвида — Скина — входят в стандартный функционал Толоки. В двух наших пулах, в первом и третьем, мы будем использовать метод Дэвида — Скина. Он позволит нам получить наиболее точные данные для нашего проекта. Подробнее узнать о том, как получить агрегированные результаты из размеченного пула, можно в документации. Агрегация с учетом сложности вопросов Метод Дэвида-Скина и метод, основанный на мнении взвешенного большинства, — основа современного краудсорсинга. Многие создатели проектов повышают качество данных, используя эти методы агрегации. Однако существуют и другие современные подходы. Например, есть группа подходов, которые учитывают сложность вопроса при агрегировании ответов. Параметрический подход Аналогично тому, как мы замеряли качество для каждого исполнителя, вводя параметр качества p i p i , точно так же для каждого исполнителя мы можем ввести параметр сложности d j d j для каждого вопроса. Тем не менее, главная проблема заключается в том, как описать взаимодействие между качеством исполнителя и сложностью вопроса, и в результате рассчитать вероятность того, что конкретный исполнитель правильно ответит на выбранный вопрос. В работе Уайтхилла с соавторами (2009) предлагается следующее решение. Во-первых, параметр качества для исполнителя, который раньше мерился в диапазоне [ 0 , 1 ] [0,1], теперь задается в интервале (−∞,∞). В частности, возможно нулевое качество p = 0 p=0, которое соответствует ситуации, когда исполнитель отвечает на все вопросы наугад. Положительные значения качества подразумевают, что работник с большей вероятностью даст правильный ответ, а отрицательные значения означают, что исполнитель настроен враждебно и с большей вероятностью даст неправильный ответ. Во-вторых, для параметра сложности каждого вопроса d∈(0,∞) также может быть дана интуитивная интерпретация: низкая сложность вопроса ( d ≈ 0 ) (d≈0) означает",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 15,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вопрос. В работе Уайтхилла с соавторами (2009) предлагается следующее решение. Во-первых, параметр качества для исполнителя, который раньше мерился в диапазоне [ 0 , 1 ] [0,1], теперь задается в интервале (−∞,∞). В частности, возможно нулевое качество p = 0 p=0, которое соответствует ситуации, когда исполнитель отвечает на все вопросы наугад. Положительные значения качества подразумевают, что работник с большей вероятностью даст правильный ответ, а отрицательные значения означают, что исполнитель настроен враждебно и с большей вероятностью даст неправильный ответ. Во-вторых, для параметра сложности каждого вопроса d∈(0,∞) также может быть дана интуитивная интерпретация: низкая сложность вопроса ( d ≈ 0 ) (d≈0) означает что вопрос настолько прост, что любой исполнитель ответит на него правильно с вероятностью, близкой к 1. Чем выше уровень сложности, тем меньше вероятность того, что конкретный исполнитель ответит на вопрос правильно. Объединив эти параметры, модель предполагает, что вероятность для конкретного исполнителя i i при ответе на конкретный вопрос j j может быть корректно описана следующим параметрическим выражением: P ( Исполнитель i отвечает на вопрос j верно ) = 1 exp P(Исполнительiотвечаетнавопросjверно)= exp(− Следует заметить, что в таком случае вероятность является функцией и самого исполнителя, и вопроса, на который исполнитель отвечает. Как только мы выбрали параметрическое уравнение для описания взаимосвязи между уровнем качества исполнителя и сложностью вопроса, с одной стороны, и вероятностью правильного ответа, с другой, мы можем применять все те же принципы, что и для расчета параметров по модели Дэвида – Скина. Таким образом мы можем оценить не только параметры модели, но и полученные ответы на вопросы. Более подробно об этом можно почитать в статье. Несмотря на то, что параметрические модели позволяют делать весьма эффективные выводы, в них неизбежно заложены сильные допущения о когнитивных процессах, присущих исполнителям при ответе на вопросы. Эти допущения обычно невозможно проверить, поэтому неясно, насколько хорошо они согласуются с реальностью. Соответственно, если допущения параметрической модели неверны, то и методы, используемые такой моделью, могут дать неожиданные результаты. Это подводит нас к идее непараметрического подхода, где можно попробовать избежать сильных допущений о мыслительных процессах. Непараметрический подход Непараметрический подход предложил Нихар Б. Шах с коллегами в 2016 году. Вместо моделирования вероятностей, что исполнитель i i верно ответит на вопрос j j, считается, что между этими вероятностями есть взаимосвязь. При этом модель использует два ключевых допущения: Во-первых, предполагается, что исполнителей можно выстроить в ряд в порядке возрастания способностей. Если исполнитель i 1 i 1 занимает в этом ряду более высокую позицию, чем исполнитель i 2 i 2 , то при ответе на каждый вопрос исполнитель i 1 i 1 с большей вероятностью даст правильный ответ, чем исполнитель i 2 i 2 . Во-вторых, предполагается, что вопросы можно выстроить в ряд в зависимости от их сложности. Если вопрос j 1 j 1 сложнее вопроса j 2 j 2 , то любой исполнитель совершит ошибку при ответе на вопрос j 1 j 1 с не меньшей вероятностью, что и отвечая на вопрос j 2 j 2 . Стоит заметить, что эти допущения гораздо слабее, чем в параметрической модели. В самом деле, параметрическая модель не только предполагает существование таких упорядоченных рядов, но и",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 16,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ответе на каждый вопрос исполнитель i 1 i 1 с большей вероятностью даст правильный ответ, чем исполнитель i 2 i 2 . Во-вторых, предполагается, что вопросы можно выстроить в ряд в зависимости от их сложности. Если вопрос j 1 j 1 сложнее вопроса j 2 j 2 , то любой исполнитель совершит ошибку при ответе на вопрос j 1 j 1 с не меньшей вероятностью, что и отвечая на вопрос j 2 j 2 . Стоит заметить, что эти допущения гораздо слабее, чем в параметрической модели. В самом деле, параметрическая модель не только предполагает существование таких упорядоченных рядов, но и задает все вероятности. С другой стороны, непараметрический подход делает всего лишь естественное предположение о существовании последовательных рядов, но не ограничивает набор когнитивных механизмов, характерных для исполнителей. Было показано, что в некоторых случаях непараметрическая модель позволяет лучше делать выводы. Более подробно об этом можно почитать в полном тексте статьи. Как мы уже говорили, эти подходы еще достаточно новые и не успели стать классикой краудсорсинга. Если сложность вопросов в вашем проекте существенно варьируется, мы рекомендуем более основательно изучить упомянутые методы и лежащие в их основе допущения, а затем опробовать их на практике. Использованная литература Jeff Howe, The Rise of Crowdsourcing, The Wired, 2006. Джефф Хау, Краудсорсинг: Коллективный разум как инструмент развития бизнеса, Альпина Паблишер, 2012. Omar Alonso, The Practice of Crowdsourcing, 2019. «Cамая богатая часть планеты работает бесплатно во время перерывов на кофе»: редактор Wired Джефф Хау о краудсорсинге, T&P, 2012. Р. А. Долженко, А. В. Бакаленко, Краудсорсинг как инструмент мобилизации интеллектуальных ресурсов: опыт использования в Сбербанке России, Российский журнал менеджмента, Том 14, №3, 2016, С. 77–102. Беспилотные автомобили Яндекса на CES 2020: 7 тысяч км без водителя за рулём по улицам Лас-Вегаса, Новости Яндекса, 2020. Метод Дэвида и Скина Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 11.1. Обучение с подкреплением Следующий параграф 12.1. Bias-variance decomposition Классический взгляд на то, почему слишком сложные модели переобучаются",
    "metadata": {
      "title": "Краудсорсинг",
      "url": "https://education.yandex.ru/handbook/ml/article/kraudsorsing",
      "course": "ml",
      "chapter": "11. Взаимодействие со средой",
      "chapter_id": "11.2",
      "part": 17,
      "total_parts": 17,
      "source_file": "11.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Классический взгляд на то, почему слишком сложные модели переобучаются В данном параграфе мы изучим инструмент, который позволяет анализировать ошибку алгоритма в зависимости от некоторого набора факторов, влияющих на итоговое качество его работы. Этот инструмент в литературе называется bias-variance decomposition — разложение ошибки на смещение и разброс. В разложении, на самом деле, есть и третья компонента — случайный шум в данных, но ему не посчастливилось оказаться в названии. Данное разложение оказывается полезным в некоторых теоретических исследованиях работы моделей машинного обучения, в частности, при анализе свойств ансамблевых моделей. Некоторые картинки в тексте кликабельны. Это означает, что они были заимствованы из какого-то источника и при клике вы сможете перейти к этому источнику. Вывод разложения bias-variance для MSE Рассмотрим задачу регрессии с квадратичной функцией потерь. Представим также для простоты, что целевая переменная y y — одномерная и выражается через переменную x x как: y=f(x)+ε, где f f — некоторая детерминированная функция, а ε ε — случайный шум со следующими свойствами: Eε=0,Varε=Eε 2 =σ 2 . В зависимости от природы данных, которые описывает эта зависимость, её представление в виде точной f ( x ) f(x) и случайной ε ε может быть продиктовано тем, что: данные на самом деле имеют случайный характер; измерительный прибор не может зафиксировать целевую переменную абсолютно точно; имеющихся признаков недостаточно, чтобы исчерпывающим образом описать объект, пользователя или событие. Функция потерь на одном объекте x x равна MSE=(y(x)−a(x)) 2 Однако знание значения MSE только на одном объекте не может дать нам общего понимания того, насколько хорошо работает наш алгоритм. Какие факторы мы бы хотели учесть при оценке качества алгоритма? Например, то, что выход алгоритма на объекте x x зависит не только от самого этого объекта, но и от выборки X X, на которой алгоритм обучался: X=((x 1 ,y 1 ),…,(x a(x)=a(x,X) Кроме того, значение y y на объекте x x зависит не только от x x, но и от реализации шума в этой точке: y(x)=y(x,ε) Наконец, измерять качество мы бы хотели на тестовых объектах x x — тех, которые не встречались в обучающей выборке, а тестовых объектов у нас в большинстве случаев более одного. При включении всех вышеперечисленных источников случайности в рассмотрение логичной оценкой качества алгоритма a a кажется следующая величина: Q(a)=E x E X,ε [y(x,ε)−a(x,X)] 2 Внутреннее матожидание позволяет оценить качество работы алгоритма в одной тестовой точке x x в зависимости от всевозможных реализаций X X и ε ε, а внешнее матожидание усредняет это качество по всем тестовым точкам. Замечание. Запись E X , ε E X,ε в общем случае обозначает взятие матожидания по совместному распределению X X и ε ε. Однако, поскольку X X и ε ε независимы, она равносильна последовательному взятию матожиданий по каждой из переменных: X,ε =E X E ε , но последний вариант выглядит несколько более громоздко. Попробуем представить выражение для Q ( a ) Q(a) в более удобном для анализа виде. Начнём с внутреннего матожидания: X,ε [y(x,ε)−a(x,X)] 2 =E X,ε [f(x)+ε−a(x,X)] не зависит от множители независимы X,ε [ не зависит от ε (f(x)−a(x,X)) 2 + множители независимы 2ε⋅(f(x)−a(x,X)) [(f(x)−a(x,X)) 2 ]+2 =0 E ε [ε] ⋅E X",
    "metadata": {
      "title": "Bias-variance decomposition",
      "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition",
      "course": "ml",
      "chapter": "12. Теория ML",
      "chapter_id": "12.1",
      "part": 1,
      "total_parts": 5,
      "source_file": "12.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "X , ε E X,ε в общем случае обозначает взятие матожидания по совместному распределению X X и ε ε. Однако, поскольку X X и ε ε независимы, она равносильна последовательному взятию матожиданий по каждой из переменных: X,ε =E X E ε , но последний вариант выглядит несколько более громоздко. Попробуем представить выражение для Q ( a ) Q(a) в более удобном для анализа виде. Начнём с внутреннего матожидания: X,ε [y(x,ε)−a(x,X)] 2 =E X,ε [f(x)+ε−a(x,X)] не зависит от множители независимы X,ε [ не зависит от ε (f(x)−a(x,X)) 2 + множители независимы 2ε⋅(f(x)−a(x,X)) [(f(x)−a(x,X)) 2 ]+2 =0 E ε [ε] ⋅E X (f(x)−a(x,X))+E [(f(x)−a(x,X)) 2 ]+σ 2 Из общего выражения для Q ( a ) Q(a) выделилась шумовая компонента σ 2 σ 2 . Продолжим преобразования: [(f(x)−a(x,X)) 2 ]=E X [(f(x)−E X [a(x,X)]+E X [a(x,X)]−a(x,X)) не зависит от не зависит от X [(f(x)−E X [a(x,X)]) 2 ] + =Var X [a(x,X)] E X [(a(x,X)−E X [a(x,X)]) не зависит от +2E X [ не зависит от X (f(x)−E X [a(x,X)]) ⋅(E X [a(x,X)]−a(x,X))]= bias bias X a(x,X) f(x)−E X [a(x,X)] ) 2 +Var X [a(x,X)]+2(f(x)−E X [a(x,X)])⋅ =0 (E X [a(x,X)]−E X [a(x,X)]) = = bias =bias X 2 a(x,X)+Var X [a(x,X)] Таким образом, итоговое выражение для Q ( a ) Q(a) примет вид bias Q(a)=E x E X,ε [y(x,ε)−a(x,X)] 2 =E x bias X 2 a(x,X)+E x Var X [a(x,X)]+σ 2 , где bias bias X a(x,X)=f(x)−E X [a(x,X)] — смещение предсказания алгоритма в точке x x, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости Var X [a(x,X)]=E X [a(x,X)−E X [a(x,X)]] 2 — дисперсия (разброс) предсказаний алгоритма в зависимости от обучающей выборки [y(x,ε)−f(x)] 2 — неустранимый шум в данных. Смещение показывает, насколько хорошо с помощью данного алгоритма можно приблизить истинную зависимость f f, а разброс характеризует чувствительность алгоритма к изменениям в обучающей выборке. Например, деревья маленькой глубины будут в большинстве случаев иметь высокое смещение и низкий разброс предсказаний, так как они не могут слишком хорошо запомнить обучающую выборку. А глубокие деревья, наоборот, могут безошибочно выучить обучающую выборку и потому будут иметь высокий разброс в зависимости от выборки, однако их предсказания в среднем будут точнее. На рисунке ниже приведены возможные случаи сочетания смещения и разброса для разных моделей: Источник Синяя точка соответствует модели, обученной на некоторой обучающей выборке, а всего синих точек столько, сколько было обучающих выборок. Красный круг в центре области представляет ближайшую окрестность целевого значения. Большое смещение соответствует тому, что модели в среднем не попадают в цель, а при большом разбросе модели могут как делать точные предсказания, так и довольно сильно ошибаться. Полученное нами разложение ошибки на три компоненты верно только для квадратичной функции потерь. Для других функций потерь существуют более общие формы этого разложения (Domigos, 2000, James, 2003) с похожими по смыслу компонентами. Это позволяет предполагать, что для большинства основных функций потерь имеется некоторое представление в виде смещения, разброса и шума (хоть и, возможно, не в столь простой аддитивной форме). Пример расчёта оценок bias и variance Попробуем вычислить разложение на смещение и разброс на каком-нибудь практическом примере. Наши",
    "metadata": {
      "title": "Bias-variance decomposition",
      "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition",
      "course": "ml",
      "chapter": "12. Теория ML",
      "chapter_id": "12.1",
      "part": 2,
      "total_parts": 5,
      "source_file": "12.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "что модели в среднем не попадают в цель, а при большом разбросе модели могут как делать точные предсказания, так и довольно сильно ошибаться. Полученное нами разложение ошибки на три компоненты верно только для квадратичной функции потерь. Для других функций потерь существуют более общие формы этого разложения (Domigos, 2000, James, 2003) с похожими по смыслу компонентами. Это позволяет предполагать, что для большинства основных функций потерь имеется некоторое представление в виде смещения, разброса и шума (хоть и, возможно, не в столь простой аддитивной форме). Пример расчёта оценок bias и variance Попробуем вычислить разложение на смещение и разброс на каком-нибудь практическом примере. Наши обучающие и тестовые примеры будут состоять из зашумлённых значений целевой функции f ( x ) f(x), где f ( x ) f(x) определяется как sin ⁡ x f(x)=xsinx В качестве шума добавляется нормальный шум с нулевым средним и дисперсией σ 2 σ 2 , равной во всех дальнейших примерах 9. Такое большое значение шума задано для того, чтобы задача была достаточно сложной для классификатора, который будет на этих данных учиться и тестироваться. Пример семпла из таких данных: 20 Посмотрим на то, как предсказания деревьев зависят от обучающих подмножеств и максимальной глубины дерева. На рисунке ниже изображены предсказания деревьев разной глубины, обученных на трёх независимых подвыборках размера 20 (каждая колонка соответствует одному подмножеству): 20 Глядя на эти рисунки, можно выдвинуть гипотезу о том, что с увеличением глубины дерева смещение алгоритма падает, а разброс в зависимости от выборки растёт. Проверим, так ли это, вычислив компоненты разложения для деревьев со значениями глубины от 1 до 15. Для обучения деревьев насемплируем 1000 случайных подмножеств train =(x train ,y train ) размера 500, а для тестирования зафиксируем случайное тестовое подмножество точек test также размера 500. Чтобы вычислить матожидание по ε ε, нам нужно несколько экземпляров шума ε ε для тестовых лейблов: test =y(x test , ε ^ )=f(x test )+ ε ^ Положим количество семплов случайного шума равным 300. Для фиксированных train =(x train ,y train ) и test =(x test ,y test ) квадратичная ошибка вычисляется как MSE=(y test −a(x test ,X train )) 2 Взяв среднее от M S E MSE по train test и ε ε, мы получим оценку для Q ( a ) Q(a), а оценки для компонент ошибки мы можем вычислить по ранее выведенным формулам. На графике ниже изображены компоненты ошибки и она сама в зависимости от глубины дерева: 20 По графику видно, что гипотеза о падении смещения и росте разброса при увеличении глубины подтверждается для рассматриваемого отрезка возможных значений глубины дерева. Правда, если нарисовать график до глубины 25, можно увидеть, что разброс становится равен дисперсии случайного шума. То есть деревья слишком большой глубины начинают идеально подстраиваться под зашумлённую обучающую выборку и теряют способность к обобщению: 20 Код для подсчёта разложения на смещение и разброс, а также код отрисовки картинок можно найти в данном ноутбуке. Bias-variance trade-off: в каких ситуациях он применим В книжках и различных интернет-ресурсах часто можно увидеть следующую картинку: Источник Она иллюстрирует утверждение, которое в литературе называется bias-variance trade-off: чем выше сложность обучаемой модели, тем меньше её",
    "metadata": {
      "title": "Bias-variance decomposition",
      "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition",
      "course": "ml",
      "chapter": "12. Теория ML",
      "chapter_id": "12.1",
      "part": 3,
      "total_parts": 5,
      "source_file": "12.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "разброса при увеличении глубины подтверждается для рассматриваемого отрезка возможных значений глубины дерева. Правда, если нарисовать график до глубины 25, можно увидеть, что разброс становится равен дисперсии случайного шума. То есть деревья слишком большой глубины начинают идеально подстраиваться под зашумлённую обучающую выборку и теряют способность к обобщению: 20 Код для подсчёта разложения на смещение и разброс, а также код отрисовки картинок можно найти в данном ноутбуке. Bias-variance trade-off: в каких ситуациях он применим В книжках и различных интернет-ресурсах часто можно увидеть следующую картинку: Источник Она иллюстрирует утверждение, которое в литературе называется bias-variance trade-off: чем выше сложность обучаемой модели, тем меньше её смещение и тем больше разброс, и поэтому общая ошибка на тестовой выборке имеет вид U U-образной кривой. С падением смещения модель всё лучше запоминает обучающую выборку, поэтому слишком сложная модель будет иметь нулевую ошибку на тренировочных данных и большую ошибку на тесте. Этот график призван показать, что существует оптимальная сложность модели, при которой соблюдается баланс между переобучением и недообучением и ошибка при этом минимальна. Существует достаточное количество подтверждений bias-variance trade-off для непараметрических моделей. Например, его можно наблюдать для метода k k ближайших соседей при росте k k и для ядерной регрессии при увеличении ширины окна σ σ (Geman et al., 1992): 20 Чем больше соседей учитывает k k-NN, тем менее изменчивым становится его предсказание, и аналогично для ядерной регрессии, из-за чего сложность этих моделей в некотором смысле убывает с ростом k k и σ σ. Поэтому традиционный график bias-variance trade-off здесь симметрично отражён по оси x x. Однако, как показывают последние исследования, непременное возрастание разброса при убывании смещения не является абсолютно истинным предположением. Например, для нейронных сетей с ростом их сложности может происходить снижение и разброса, и смещения. Одна из наиболее известных статей на эту тему — статья Белкина и др. (Belkin et al., 2019), в которой, в частности, была предложена следующая иллюстрация: 20 Слева — классический bias-variance trade-off: убывающая часть кривой соответствует недообученной модели, а возрастающая — переобученной. А на правой картинке — график, называемый в статье double descent risk curve. На нём изображена эмпирически наблюдаемая авторами зависимость тестовой ошибки нейросетей от мощности множества входящих в них параметров ( H H). Этот график разделён на две части пунктирной линией, которую авторы называют interpolation threshold. Эта линия соответствует точке, в которой в нейросети стало достаточно параметров, чтобы без особых усилий почти идеально запомнить всю обучающую выборку. Часть до достижения interpolation threshold соответствует «классическому» режиму обучения моделей: когда у модели недостаточно параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев. В качестве вывода к этому разделу хочется сформулировать два основных тезиса: Bias-variance trade-off нельзя считать непреложной истиной, выполняющейся для всех моделей и обучающих данных. Разложение на смещение и разброс не влечёт",
    "metadata": {
      "title": "Bias-variance decomposition",
      "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition",
      "course": "ml",
      "chapter": "12. Теория ML",
      "chapter_id": "12.1",
      "part": 4,
      "total_parts": 5,
      "source_file": "12.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "параметров, чтобы сохранить обобщающую способность при почти полном запоминании обучающей выборки. А часть после достижения interpolation threshold соответствует «современным» возможностям обучения моделей с огромным числом параметров. На этой части графика ошибка монотонно убывает с ростом количества параметров у нейросети. Авторы также наблюдают похожее поведение и для «древесных» моделей: Random Forest и бустинга над решающими деревьями. Для них эффект проявляется при одновременном росте глубины и числа входящих в ансамбль деревьев. В качестве вывода к этому разделу хочется сформулировать два основных тезиса: Bias-variance trade-off нельзя считать непреложной истиной, выполняющейся для всех моделей и обучающих данных. Разложение на смещение и разброс не влечёт немедленного выполнения bias-variance trade-off и остаётся верным и для случая, когда все компоненты ошибки (кроме неустранимого шума) убывают одновременно. Этот факт может оказаться незамеченным из-за того, что в учебных пособиях часто разговор о разложении дополняется иллюстрацией с U U-образной кривой, благодаря чему в сознании эти два факта могут слиться в один. Список литературы Блог-пост про bias-variance от Йоргоса Папахристудиса Блог-пост про bias-variance от Скотта Фортмана-Роу Статьи от Домингоса (2000) и Джеймса (2003) про обобщённые формы bias-variance decomposition Блог-пост от Брейди Нила про необходимость пересмотра традиционного взгляда на bias-variance trade-off Статья Гемана и др. (1992), в которой была впервые предложена концепция bias-variance trade-off Статья Белкина и др. (2019), в которой был предложен double-descent curve Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 11.2. Краудсорсинг Следующий параграф 13.1. Введение в теорию глубокого обучения",
    "metadata": {
      "title": "Bias-variance decomposition",
      "url": "https://education.yandex.ru/handbook/ml/article/bias-variance-decomposition",
      "course": "ml",
      "chapter": "12. Теория ML",
      "chapter_id": "12.1",
      "part": 5,
      "total_parts": 5,
      "source_file": "12.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Центральной теоретической проблемой обучения с учителем является проблема обобщающей способности. В самом деле, как можно гарантировать, что модель, обученная на некотором наборе данных, будет показывать хорошие результаты на данных, которых в обучении не было? Для классических моделей было доказано много содержательных результатов, которые, может быть, не давали ответы на все вопросы, но позволяли многое понять о работе моделей. Что же касается нейросетей, для них теория ещё только создаётся, и в этом разделе мы познакомим вас с рядом направлений развития этой науки. Нейронная сеть фиксированной архитектуры реализует некоторый класс моделей F F. Например, разные элементы этого класса могут соответствовать различным наборам весов. Когда такой класс фиксирован, мы обычно решаем задачу минимизации некоторой функции ошибки или, как чаще говорят в теории ML, задачу минимизации эмпирического риска (f)=E x,y∈S m r(y,f(x)) среди моделей f f из класса F F, где S m S m – обучающий датасет из m m примеров, выбранных независимо из распределения данных D D, а r(y, y ^ ) – функция риска, например, I[y  = y ^ ]. Наша цель, однако, минимизировать не эмпирический, а истинный риск, то есть R(f)=E x,y∼D r(y,f(x)), где математическое ожидание берётся по распределению данных, а не по выборке (математическое ожидание риска на всех мыслимых данных, не только на выборке). К сожалению, в рамках задачи обучения с учителем доступа к истинному распределению данных у нас нет, поэтому минимизировать истинный риск напрямую не удаётся, но мы можем попробовать его оценить. Часто для этого используют риск на валидации, но в этом разделе учебника мы постараемся получить теоретические оценки. Пусть – модель из класса F F, которую мы построили исходя из выборки S m S m . Интересно оценить, насколько её истинный риск отличается от эмпирического, то есть оценить разность В случае нейронных сетей очень трудно предсказать, к какой именно модели сойдётся наш метод обучения (например, градиентный спуск) на данной выборке S m S m . Тем не менее, разницу рисков всегда можно оценить сверху супремумом по всем моделям из класса: sup f∈F sup (R(f)− R ^ m (f)). В этом случае риск можно оценить сверху величиной sup f∈F sup (R(f)− R ^ m (f)). Такие оценки называются равномерными (uniform bounds); мы рассмотрим их подробно в соответствующем параграфе. Понятно, что подобная оценка становится бесполезной (vacuous), если в классе содержится модель, которая идеально работает на фиксированной выборке (f)=0), но на какой-либо другой (потенциально тестовой) выборке из тех же данных работает плохо ( R ( f ) R(f) велик). Так, известно, что модели класса VGG способны выучить ImageNet даже с перемешанными метками классов (см. статью Understanding deep learning requires rethinking generalization). Понятно, что истинный риск у такой модели будет близок к риску случайного угадывания. «Плохую» модель можно построить следующим образом. Пусть A A – наш исходный алгоритм обучения, например, градиентный спуск. Он принимает на вход выборку и выдаёт обученную модель. Возьмём датасет, составленный из двух частей: S m S m – это самая обычная выборка, в которой объекты насэмплированы из распределения – выборка, объекты которой сгенерированы из того же распределения, но метки перепутаны. Рассмотрим модель m,M =A(S m ∪",
    "metadata": {
      "title": "Введение в теорию глубокого обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/teoriya-glubokogo-obucheniya-vvedenie",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.1",
      "part": 1,
      "total_parts": 4,
      "source_file": "13.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Так, известно, что модели класса VGG способны выучить ImageNet даже с перемешанными метками классов (см. статью Understanding deep learning requires rethinking generalization). Понятно, что истинный риск у такой модели будет близок к риску случайного угадывания. «Плохую» модель можно построить следующим образом. Пусть A A – наш исходный алгоритм обучения, например, градиентный спуск. Он принимает на вход выборку и выдаёт обученную модель. Возьмём датасет, составленный из двух частей: S m S m – это самая обычная выборка, в которой объекты насэмплированы из распределения – выборка, объекты которой сгенерированы из того же распределения, но метки перепутаны. Рассмотрим модель m,M =A(S m ∪ S ~ M ). Чем больше M M будет по сравнению с m m, тем ближе будет построенная модель к случайному угадыванию. При этом, если суммарный размер двух выборок не слишком велик, то наша модель сможет запомнить их обе, в частности, S m S m . Таким образом, эмпирический риск ) такой модели окажется мал, а истинный – велик. Интуитивно понятно, что чем «сложнее» класс F F, тем больше шансов найти в нём подобную модель. Одной из классических мер сложности класса моделей является размерность Вапника-Червоненкиса, или VC-размерность, предложенная в 1971 году в статье В. Н. Вапника и А. Я. Червоненкиса «О равномерной сходимости частот появления событий к их вероятностям». Она даёт следующую равномерную оценку: sup log f∈F sup (R(f)− R ^ m (f))≤O( m VC(F)logVC(F) ). Как и следовало ожидать, правая часть растёт со сложностью модели и падает с размером выборки. Известно, что для полносвязных сетей VC-размерность растёт как Θ ( n N ) Θ(nN), где n n – ширина сети (число нейронов в слое), а N N – общее число параметров; см. статью Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks. Рассмотрим полносвязную сеть с одним скрытым слоем. Тогда общее число параметров сети пропорционально ширине, а значит, VC-размерность пропорциональна квадрату ширины. Соответствующая оценка на истинный риск принимает вид: (n)≤ R ^ m (n)+ m Cn , где C C – константа из равномерной оценки разницы рисков с помощью VC-размерности, а (n) – эмпирический риск сети ширины n n, обученной на данной выборке S m S m . Как правило, эмпирический риск монотонно убывает с ростом ширины, пока не достигнет нуля (в самом деле, чем больше ширина, тем сложнее класс моделей и тем больше шансов обнаружить в нём модель, запоминающую фиксированную выборку). В результате (n) может вести себя немонотонно: у этой величины может обнаружиться минимум строго левее точки, где (n) впервые достигает нуля: Схематическое изображение изменения эмпирического риска $\\hat R_m$ (синяя кривая) и предсказанного риска $\\tilde R$ (красная кривая) в зависимости от ширины сети. Правее минимума предсказанный риск монотонно растёт. Но оказывается, что реальный истинный риск, напротив, убывает, выходя на асимптоту: Зависимость эмпирического риска $\\hat R_m$ (бирюзовая кривая) и истинного риска $R$ (синяя кривая) для полносвязной сети с одним скрытым слоем, обученной на наборе данных CIFAR10, в зависимости от ширины сети; подробности см. в работе B. Neyshabur, R. Tomioka, N. Srebro, In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning. Возникает",
    "metadata": {
      "title": "Введение в теорию глубокого обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/teoriya-glubokogo-obucheniya-vvedenie",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.1",
      "part": 2,
      "total_parts": 4,
      "source_file": "13.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "где (n) впервые достигает нуля: Схематическое изображение изменения эмпирического риска $\\hat R_m$ (синяя кривая) и предсказанного риска $\\tilde R$ (красная кривая) в зависимости от ширины сети. Правее минимума предсказанный риск монотонно растёт. Но оказывается, что реальный истинный риск, напротив, убывает, выходя на асимптоту: Зависимость эмпирического риска $\\hat R_m$ (бирюзовая кривая) и истинного риска $R$ (синяя кривая) для полносвязной сети с одним скрытым слоем, обученной на наборе данных CIFAR10, в зависимости от ширины сети; подробности см. в работе B. Neyshabur, R. Tomioka, N. Srebro, In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning. Возникает вопрос: если у истинного риска есть асимптота, то как ведут себя нейронные сети в пределе бесконечной ширины? Мы остановимся на этом вопросе в соответствующем параграфе. Упомянутый выше эксперимент с перемешиванием меток классов на части обучающей выборки можно рассматривать как модификацию не данных, а алгоритма обучения. А именно, давайте представим алгоритм, который, получая на вход выборку S m S m , сделает с ней следующее: Каким-то образом делит её на две части: Заменяет в метки на случайные. Обучает модель на объединении и «испорченной» Такой «испорченный» алгоритм обучения приводит к модели, которая запоминает и поэтому на исходной обучающей выборке S m S m работает не так уж и плохо. Но на тестовых данных он показывает качество, сравнимое со случайным угадыванием. Работающие на практике алгоритмы, например, градиентный спуск, тоже могут обучить модель, которая «запомнила» обучающую выборку. Тем не менее, на тестовой выборке обученная модель будет давать нормальное качество (см. опять же вторую картинку в начале параграфа). Возникает вопрос: почему так? Почему среди всех конфигураций весов, для которых риск на обучающей выборке равен нулю, градиентный спуск не выбирает те, для которых истинный риск сравним со случайным угадыванием? Явление, при котором среди всех эквивалентных по эмпирическому риску решений алгоритм выбирает определённые, называется implicit bias, и будет рассмотрен в соответствующем параграфе. Если даже известно, какие конфигурации весов «предпочитает» наш алгоритм обучения, это никак не повлияет на равномерную оценку разницы рисков. В параграфе про PAC-байесовские оценки будет рассмотрен класс оценок, которые позволяют учесть предпочтения алгоритма. А именно, пусть обученная модель случайна (это действительно так из-за случайности инициализации весов и, например, шума стохастического градиентного спуска), то есть алгоритм строит распределение на моделях – назовём его апостериорным распределением. Пусть дано другое распределение, не зависящее от выборки, назовём его априорным. Тогда роль сложности в наших оценках будет играть расстояние Кульбака-Лейблера (KL-дивергенция) между апостериорным и априорным распределениями на моделях. Если априорное распределение покрывает «предпочтительные» решения и не покрывает остальные, то KL-дивергенция мала и оценка разницы рисков невелика. Из-за внешней схожести некоторых величин, возникающих в этой теории, с объектами из байесовской статистики, такие оценки называется PAC-байесовскими (PAC-bayesian bounds, от probably approximately correct). Выше мы негласно предполагали, что используемый алгоритм обучения успешно решает задачу минимизации эмпирического риска. Для нейронных сетей наиболее популярный алгоритм – градиентный спуск или его разновидности. Если бы функция потерь была выпуклой как функция от весов сети, то это гарантировало бы сходимость в глобальный минимум. В общем случае теория оптимизации не даёт таких гарантий. Тем не менее, для сетей реалистичного",
    "metadata": {
      "title": "Введение в теорию глубокого обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/teoriya-glubokogo-obucheniya-vvedenie",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.1",
      "part": 3,
      "total_parts": 4,
      "source_file": "13.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "распределение покрывает «предпочтительные» решения и не покрывает остальные, то KL-дивергенция мала и оценка разницы рисков невелика. Из-за внешней схожести некоторых величин, возникающих в этой теории, с объектами из байесовской статистики, такие оценки называется PAC-байесовскими (PAC-bayesian bounds, от probably approximately correct). Выше мы негласно предполагали, что используемый алгоритм обучения успешно решает задачу минимизации эмпирического риска. Для нейронных сетей наиболее популярный алгоритм – градиентный спуск или его разновидности. Если бы функция потерь была выпуклой как функция от весов сети, то это гарантировало бы сходимость в глобальный минимум. В общем случае теория оптимизации не даёт таких гарантий. Тем не менее, для сетей реалистичного размера градиентный спуск успешно сходится в глобальный минимум, что толкает нас на предположение о том, что все локальные минимумы таких сетей глобальны. Это предположение действительно можно доказать в некоторых частных случаях; см. параграф про ландшафт функции потерь. В качестве необходимого дополнения следует также гарантировать, что градиентный спуск (или его разновидности) не сходится в возможные седловые точки. При определённых условиях на минимизируемую функцию можно доказать, что для сходимости в седловую точку необходимо инициализировать функцию на множестве меры ноль. Более подробно вы можете почитать в работе Gradient descent only converges to minimizers или в её обобщениях Gradient Descent Only Converges to Minimizers: Non-Isolated Critical Points and Invariant Regions и On the almost sure convergence of stochastic gradient descent in non-convex problems. Впрочем, гарантии сходимости, как правило, формулируются для фиксированной архитектуры сети и ничего не говорят о скорости сходимости. Хотелось бы также иметь гарантии на то, что какой бы широкой или глубокой сеть не была, градиентный спуск сойдётся в минимум за разумное время. А для этого необходимо понимать, что на самом первом шаге градиентного спуска градиент не будет гаснуть или «взрываться» с ростом ширины или глубины. Известен ряд эвристик для инициализации весов, помогающих с этим бороться: например, инициализации Глоро (Xavier Glorot) и Хе (Kaiming He). Подробнее про них вы можете прочитать в параграфе про тонкости обучения нейросетей. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 12.1. Bias-variance decomposition Классический взгляд на то, почему слишком сложные модели переобучаются Следующий параграф 13.2. Обобщающая способность – классическая теория",
    "metadata": {
      "title": "Введение в теорию глубокого обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/teoriya-glubokogo-obucheniya-vvedenie",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.1",
      "part": 4,
      "total_parts": 4,
      "source_file": "13.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как мы уже видели во введении, мы не можем напрямую оптимизировать истинный риск модели f ( x ) f(x) R(f)=E x,y∼D r(y,f(x)), так как нам недоступно полное распределение данных D D. Поэтому вместо задачи минимизации истинного риска, мы будем минимизировать эмпирический риск (f)=E x,y∈S m r(y,f(x)) на доступном нам наборе данных S m S m . Классическая теория предлагает оценивать разность эмпирического и истинного равномерно, что даёт sup f∈F sup (R(f)− R ^ m (f)). Такая оценка не зависит от алгоритма обучения; она зависит лишь от класса моделей F F, в котором происходит поиск. Так, в случае нейронных сетей в качестве класса F F можно взять класс всех нейронных сетей фиксированной архитектуры, отличающихся только весами. Если класс F F настолько велик, что для большинства наборов данных размера m m содержит модель f f, у которой эмпирический риск мал, а истинный велик, то оценка выше теряет смысл. Работа Understanding deep learning requires rethinking generalization показала, что именно это и происходит в нейронных сетях, применяемых на практике, на реальных наборах данных. А именно, пусть A A – алгоритм, применяемый для обучения сети, например, градиентный спуск. Предположим, что с высокой вероятностью (A(S m ))=0, если только размер выборки m m не слишком велик. Пусть S m S m – наша выборка, а – датасет, в котором примеры берутся из выборки, а разметка случайна. По предположению, модель m,m ′ =A(S m ∪S m ′ ′ ), обученная на объединённом датасете, имеет нулевой риск на «настоящем» датасете S m S m , если только m + m ′ m+m ′ не слишком велик. С другой стороны, если ≫m, то m,m ′ ≈A(S m ′ ′ ) – истинный риск такой модели близок к риску случайного угадывания. Тем не менее, если в качестве F F взять не все модели, реализуемые данной архитектурой нейронной сети, а лишь реализуемые данным алгоритмом обучения на наборах данных из распределения с высокой вероятностью, то можно надеятся, что равномерная оценка окажется осмысленной. Мы говорим «с высокой вероятностью» для того, чтобы исключить «нереалистичные» наборы данных, обучение на которых ведёт к плохим результатам, а также ничтожно-редкие случаи реализации шума в алгоритме обучения, при котором последний сходится в «плохие» решения. Подробнее о том, какие модели реализуются градиентным спуском, мы обсудим в параграфе про implicit bias. Оценка супремума Попробуем оценить супремум разницы рисков. Будем считать, что выборка S m S m выбирается случайным (и равновероятным) образом из распределения данных D D. Некоторые из выборок могут быть катастрофически плохими, поэтому мы будем рассматривать оценки, которые верны не обязательно всегда, а просто с достаточно большой вероятностью. Предположим сначала, что класс моделей F F конечен. Тогда P ( sup f∈F sup (R(f)− R ^ m (f))≥ϵ)= =P(∃f∈F:(R(f)− R ^ m (f))≥ϵ)≤ f∈F ∑ P(R(f)− R ^ m (f)≥ϵ)≤ ≤ ∣ F ∣ sup ≤∣F∣ f∈F sup P(R(f)− R ^ m (f)≥ϵ)∀ϵ>0 Заметим, что R(f)=E (f). Поэтому при фиксированном f f разницу рисков (f)−R(f) можно оценить с помощью неравенства Хёффдинга. Неравенство Хёффдинга (Hoeffding's inequality). Пусть ,…,X m – независимые одинаково распределённые случайные величины со значениями в [ 0 , 1",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 1,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "катастрофически плохими, поэтому мы будем рассматривать оценки, которые верны не обязательно всегда, а просто с достаточно большой вероятностью. Предположим сначала, что класс моделей F F конечен. Тогда P ( sup f∈F sup (R(f)− R ^ m (f))≥ϵ)= =P(∃f∈F:(R(f)− R ^ m (f))≥ϵ)≤ f∈F ∑ P(R(f)− R ^ m (f)≥ϵ)≤ ≤ ∣ F ∣ sup ≤∣F∣ f∈F sup P(R(f)− R ^ m (f)≥ϵ)∀ϵ>0 Заметим, что R(f)=E (f). Поэтому при фиксированном f f разницу рисков (f)−R(f) можно оценить с помощью неравенства Хёффдинга. Неравенство Хёффдинга (Hoeffding's inequality). Пусть ,…,X m – независимые одинаково распределённые случайные величины со значениями в [ 0 , 1 ] [0,1]. Тогда для всех ϵ > 0 ϵ>0 имеют место неравенства i=1 ∑ m X i −E i=1 ∑ m X i ≥ϵ)≤e P(E i=1 ∑ m X i − i=1 ∑ m X i ≥ϵ)≤e − m 2ϵ 2 . Как следствие неравенства Хёффдинга, получаем, что для любого ϵ > 0 ϵ>0 и для любой f ∈ F f∈F. P(R(f)− R ^ m (f)≥ϵ)≤e −2mϵ 2 Заметим, что тогда для любой f ∈ F f∈F, log ⁡ 1 δ с вероятностью R(f)− R ^ m (f)≤ 2m 1 log δ 1 с вероятностью ≥1−δ по S m . Несмотря на то, что эта оценка является оценкой на обобщающую способность, она не имеет смысла, так как модель f f в ней задана априори и не зависит от S m S m . Другими словами, она верна для необученных моделей f f. Возвращаясь к нашей оценке, получаем: P ( sup f∈F sup (R(f)− R ^ m (f))≥ϵ)≤∣F∣e −2mϵ 2 ∀ϵ>0, где ∣ F ∣ ∣F∣ – мощность класса F F. Следовательно, sup log ⁡ 1 δ + log ⁡ ∣ F ∣ ) с вероятностью f∈F sup (R(f)− R ^ m (f))≤ 2m 1 (log δ 1 +log∣F∣) с вероятностью ≥1−δ по S m . В случае бесконечного F F используем следующее обобщение неравенства Хёффдинга: Неравенство МакДайармида (McDiarmid's inequality). Пусть ,…,X m – независимые одинаково распределённые случайные величины, g g – скалярная функция с m m аргументами, такая что sup ,…,x m , x ~ i sup ∣g(x 1 ,…,x i ,…,x m )−g(x 1 ,… x ~ i ,…x m )∣≤c i ∀i=1,…,m для некоторых c i c i . Тогда для любого ϵ > 0 ϵ>0 имеет место неравенство P(g(X 1 ,…,X m )−Eg(X 1 ,…,X m )≥ϵ)≤e − ∑ i=1 Применяя теорему к sup g({(x i ,y i )} i=1 m )=sup f∈F (R(f)− R ^ m (f)), получаем: P S m ( sup sup f∈F sup (R(f)− R ^ m (f))−E S m ′ f∈F sup (R(f)− R ^ m ′ (f))≥ϵ)≤e −2mϵ 2 , из чего следует: sup sup log ⁡ 1 δ с вероятностью f∈F sup (R(f)− R ^ m (f))≤E S m ′ f∈F sup (R(f)− R ^ m ′ (f))+ 2m 1 log δ 1 с вероятностью ≥1−δ по S m , где (f) – эмпирический риск на выборке . В следующем подразделе мы постараемся оценить жёлтое слагаемое. Симметризация и сложность Радемахера",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 2,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i )} i=1 m )=sup f∈F (R(f)− R ^ m (f)), получаем: P S m ( sup sup f∈F sup (R(f)− R ^ m (f))−E S m ′ f∈F sup (R(f)− R ^ m ′ (f))≥ϵ)≤e −2mϵ 2 , из чего следует: sup sup log ⁡ 1 δ с вероятностью f∈F sup (R(f)− R ^ m (f))≤E S m ′ f∈F sup (R(f)− R ^ m ′ (f))+ 2m 1 log δ 1 с вероятностью ≥1−δ по S m , где (f) – эмпирический риск на выборке . В следующем подразделе мы постараемся оценить жёлтое слагаемое. Симметризация и сложность Радемахера Оценим сверху матожидание супремума: E S m ′ sup sup f∈F sup (R(f)− R ^ m ′ (f))=E S m ′ f∈F sup (f)− R ^ m ′ (f))≤ sup f∈F sup ( R ^ m ′′ (f)− R ^ m ′ (f))= sup f∈F sup ( m 1 i=1 ∑ m (r(y i ′′ ,f(x i ′′ ))−r(y i ′ ,f(x i ′ )))). Этот шаг называется «симметризация»: теперь выражение выше зависит от двух равнозначных обучающих выборок . Ниже для краткости будем обозначать (f)=r(y i ′ ,f(x i ′ )) и (f)=r(y i ′′ ,f(x i ′′ )). Как оценить сверху супремум разности рисков? Наивная оценка, супремум суммы, слишком слаба: в самом деле, при фиксированном наборе данных вполне вероятно может существовать модель, имеющая большой риск на нём (достаточно взять модель, обученную на тех же данных, но с «неправильными» метками), поэтому матожидание супремума эмпирического риска может быть велико. Для обхода этой сложности заметим, что выражение выше симметрично относительно перестановки местами двух выборок: sup sup f∈F sup ( m 1 i=1 ∑ m (r i ′′ (f)−r i ′ (f)))=E f∈F sup ( m 1 i=1 ∑ m (r i ′ (f)−r i ′′ (f))). Более того, так как элементы обеих выборок выбираются независимо, значение выражения не меняется и при перестановке местами отдельно i i-ых элементов двух выборок. А именно, для любого набора ,…,σ m ∈{−1,1} sup f∈F sup ( m 1 i=1 ∑ m (r i ′′ (f)−r i ′ (f)))= sup f∈F sup ( m 1 i=1 (f)−r i ′ (f))). Будем выбирать σ i σ i независимо и равновероятно из {−1,1}. Такие случайные величины называются переменными Радемахера. Поскольку оценки выше были верны для любых сигм, они верны и в среднем по переменным Радемахера, выбранным независимо от выборки: sup f∈F sup ( m 1 i=1 ∑ m (r i ′′ (f)−r i ′ (f)))= sup 1:m f∈F sup ( m 1 i=1 (f)−r i ′ (f))). После введения переменных Радемахера оценка супремума разницы рисков через сумму супремумов становится не такой плохой. В самом деле, рассмотрим бинарную классификацию с помощью линейной модели. Если данные хорошо разделяются плоскостью, то E S m sup sup f∈F ( m 1 ∑ i=1 m r i (f)) будет большим, так как в качестве f f можно взять линейную модель с противоположно ориентированной разделяющей плоскостью для S m S m . В то же время для того, чтобы sup 1:m E S m sup f∈F",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 3,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "′′ (f)−r i ′ (f)))= sup 1:m f∈F sup ( m 1 i=1 (f)−r i ′ (f))). После введения переменных Радемахера оценка супремума разницы рисков через сумму супремумов становится не такой плохой. В самом деле, рассмотрим бинарную классификацию с помощью линейной модели. Если данные хорошо разделяются плоскостью, то E S m sup sup f∈F ( m 1 ∑ i=1 m r i (f)) будет большим, так как в качестве f f можно взять линейную модель с противоположно ориентированной разделяющей плоскостью для S m S m . В то же время для того, чтобы sup 1:m E S m sup f∈F ( m 1 ∑ i=1 m σ i r i (f)) было большим, необходимо, чтобы существовала модель, отвечающая правильно на тех примерах, где =−1, и неправильно, где =1; для линейной модели это невозможно при большинстве конфигураций сигм. Величина sup Rad D,m (H)=E z 1:m ∼D m E σ 1:m ∼U({−1,1} m ) h∈H sup ( m 1 i=1 ∑ m σ i h(z i )). называется сложностью Радемахера класса функций H : X → R H:X→R (для распределения D D на X X и длины выборок m m). Она велика, если в классе H H содержатся функции, принимающие большие значения с заданными знаками на любом наборе данных фиксированного размера. Другими словами, сложность Радемахера измеряет, насколько выходы функций из класса H H могут коррелировать со случайным шумом. Для нас актуальна сложность Радемахера классов вида H = r ∘ F H=r∘F, то есть композиций моделей из класса F F и функции риска r r. Если F F – класс линейных моделей в пространстве размерности меньшей, чем m m, то сложность Радемахера невелика. В то же время если F F – множество всех возможных решающих деревьев, то, если только наборы данных непротиворечивы, она равна единице. В самом деле, решающее дерево способно запомнить всю обучающую выборку, то есть добиться единичной корреляции с любым случайным шумом. Вернёмся к оценке разницы рисков: E S m ′ sup sup f∈F sup (R(f)− R ^ m ′ (f))≤E f∈F sup ( m 1 i=1 ∑ m (r i ′′ (f)−r i ′ (f)))= sup 1:m f∈F sup ( m 1 i=1 (f)−r i ′ (f)))≤ sup sup 1:m ( f∈F sup ( m 1 i=1 (f))+ f∈F sup ( m 1 i=1 (f)))= sup =2E S m ′ E σ 1:m f∈F sup ( m 1 i=1 ∑ m σ i r(y i ′ ,f(x i ′ )))= =2Rad D,m (r∘F). Оценка для «0/1-риска» Сложность Радемахера зависит от функции риска. Рассмотрим задачу бинарной классификации с классами + 1 +1 и − 1 −1. Возьмём в качестве функции риска индикатор ошибки бинарной классификации, или «0/1-риск»: r(y,z)=r 0/1 (y,z)=I[yz<0]. Название «0/1-риск» обусловлено тем, что риск принимает значения 0 0 и 1 1. Заметим следующее: E σ 1 : m sup max 1:m f∈F sup ( m 1 i=1 (f))=E σ 1:m f∈F S m max ( m 1 i=1 (f)), где – класс эквивалентности функций из F F, в котором две функции считаются эквивалентными тогда и только тогда, когда их",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 4,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "=2Rad D,m (r∘F). Оценка для «0/1-риска» Сложность Радемахера зависит от функции риска. Рассмотрим задачу бинарной классификации с классами + 1 +1 и − 1 −1. Возьмём в качестве функции риска индикатор ошибки бинарной классификации, или «0/1-риск»: r(y,z)=r 0/1 (y,z)=I[yz<0]. Название «0/1-риск» обусловлено тем, что риск принимает значения 0 0 и 1 1. Заметим следующее: E σ 1 : m sup max 1:m f∈F sup ( m 1 i=1 (f))=E σ 1:m f∈F S m max ( m 1 i=1 (f)), где – класс эквивалентности функций из F F, в котором две функции считаются эквивалентными тогда и только тогда, когда их образы на выборке S m S m имеют одинаковые знаки. Другими словами, среди всех функций, принимающих одни и те же знаки на S m S m , мы выберем по одной и сформируем из них множество . Заметим, что это множество конечно: ∣≤2 m . Нам понадобится следующая Лемма. Пусть X X – случайная величина со значениями в [ a , b ] [a,b] и нулевым средним. Тогда для любых s > 0 s>0 имеет место неравенство (b−a) 2 s 2 . С её помощью получаем: E σ 1 : m sup max 1:m f∈F sup ( m 1 i=1 (f))=E σ 1:m f∈F S m max ( m 1 i=1 (f))= = 1 m s log ⁡ exp max logexp(sE σ 1:m f∈F S m max ( i=1 (f)))≤ ≤ 1 m s log exp ⁡ ( s max logE σ 1:m exp(s f∈F S m max ( i=1 (f)))≤ ≤ 1 m s log exp log f∈F S m ∑ E σ 1:m exp(s i=1 (f))= = 1 m s log log f∈F S m ∑ i=1 (f) )≤ ≤ 1 m s log log log f∈F log(∣F log log∣F Эта оценка верна для любого s > 0 s>0. Минимизируем её по s s. Легко видеть, что оптимальное s s равняется ( 2 / m ) log (2/m)log∣F S m ∣ ; подставляя его, получаем: log Rad D,m (r∘F)≤E S m m 2 log∣F S m ∣ .(1) Определим функцию роста класса F F как max (m)= S m max ∣F S m ∣. Эта функция показывает, сколько различных разметок класс функций F F может породить на наборе данных, в зависимости от размера этого набора. Очевидно, что (m)≤2 m и монотонно не убывает. Например, для линейной модели на d d-мерном пространстве признаков (m)=2 m при m ≤ d + 1 m≤d+1 (любое подмножество d + 1 d+1 точек в общем положении в d d-мерном пространстве всегда можно отделить гиперплоскостью), но строго меньше этого числа при m > d + 1 m>d+1 (например, если точки – углы квадрата на плоскости, его диагонали нельзя разделить прямой). Когда ∣=2 m , будем говорить, что « F F разделяет S m S m ». Определим размерность Вапника-Червоненкиса (или VC-размерность) как максимальное m m, при котором семейство F F разделяет любой датасет max VC(F)=max{m∣Π F (m)=2 m }. Таким образом, VC-размерность линейной модели равна d + 1 d+1. Следующая лемма даёт связь между",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 5,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "+ 1 m≤d+1 (любое подмножество d + 1 d+1 точек в общем положении в d d-мерном пространстве всегда можно отделить гиперплоскостью), но строго меньше этого числа при m > d + 1 m>d+1 (например, если точки – углы квадрата на плоскости, его диагонали нельзя разделить прямой). Когда ∣=2 m , будем говорить, что « F F разделяет S m S m ». Определим размерность Вапника-Червоненкиса (или VC-размерность) как максимальное m m, при котором семейство F F разделяет любой датасет max VC(F)=max{m∣Π F (m)=2 m }. Таким образом, VC-размерность линейной модели равна d + 1 d+1. Следующая лемма даёт связь между размерностью Вапника-Червоненкиса и функцией роста: Лемма (Sauer–Shelah, см. подробнее здесь) (m)≤ k=0 ∑ VC(F) ( k m ) Изучим асимптотическое поведение сложности Радемахера при m → ∞ m→∞. Обозначим D=VC(F). Для m ≤ D m≤D имеем (m)=2 m , а для m > D m>D: (m)≤ k=0 ∑ D ( k m )≤( D m ) D k=0 k=0 (1+ Подставляя это выражение в (1), получаем окончательную оценку на сложность Радемахера: log ⁡ m − log Rad D,m (r∘F)≤ m 2 VC(F)(1+logm−log(VC(F))) log m→∞ ( VC(F) m logm ). Соответствующая оценка на истинный риск тогда примёт вид: log log ⁡ m m ) с вероятностью log δ 1 +Θ m→∞ ( VC(F) m logm )с вероятностью ≥1−δ по S m . Для того, чтобы эта оценка была осмыслена, необходимо гарантировать log ⁡ m ) VC(F)<m/(2logm). Для линейных моделей, при условии m ≫ d m≫d (данных намного больше, чем признаков), оценки действительно получаются осмысленными. К сожалению, для нейронных сетей это подчас неверно. В работе Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks показано, что если F F обозначает класс моделей, реализуемых полносвязной сетью ширины n n с N N параметрами, то VC(F)=Θ(nN). Таким образом, наша оценка на сложность Радемахера становится бесполезной в реалистичных сценариях, когда число весов сети N N много больше числа примеров в обучающей выборке m m. Если априори известно, что результат обучения лежит в некотором классе F B F B , то в оценке сложности Радемахера можно использовать именно этот класс, а не полный класс моделей F F. Очевидно, что сложность F B F B , лежащего в F F, не больше сложности F F. Так, в работе Spectrally-normalized margin bounds for neural networks получены оценки для сложности полносвязной сети с липшицевыми функциями активации при условии, что нормы весов ограничены; см. также полный конспект лекций. В этом случае под F B F B будем понимать класс сетей с весами нормы не больше B B. Обозначим соответствующую оценку через B B: sup с вероятностью f∈F B sup (R(f)− R ^ m (f))≤B(B,δ)с вероятностью ≥1−δ по S m . К сожалению, нет гарантий, что градиентный спуск всегда сходится в решение с нормой меньше какого-то числа. Чтобы обойти это ограничение, используют следующую технику. Возьмём последовательность ограничений B j B j , такую что j+1 и j=1 ⋃ ∞ F B j =F. Также возьмём последовательность δ j δ j , монотонно убывающую к нулю и суммирующуюся в δ δ.",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 6,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "этом случае под F B F B будем понимать класс сетей с весами нормы не больше B B. Обозначим соответствующую оценку через B B: sup с вероятностью f∈F B sup (R(f)− R ^ m (f))≤B(B,δ)с вероятностью ≥1−δ по S m . К сожалению, нет гарантий, что градиентный спуск всегда сходится в решение с нормой меньше какого-то числа. Чтобы обойти это ограничение, используют следующую технику. Возьмём последовательность ограничений B j B j , такую что j+1 и j=1 ⋃ ∞ F B j =F. Также возьмём последовательность δ j δ j , монотонно убывающую к нулю и суммирующуюся в δ δ. Тогда для любого j ≥ 1 j≥1 sup с вероятностью f∈F B j sup (R(f)− R ^ m (f))≤B(B j ,δ j )с вероятностью ≥1−δ j по S m . А значит, sup с вероятностью f∈F B j sup (R(f)− R ^ m (f))≤B(B j ,δ j )∀j≥1с вероятностью ≥1−∑ j=1 ∞ δ j =1−δ по S m . Из этого следует, что с вероятностью )≤B(B )с вероятностью ≥1−δ по S m , где – минимальное j j, при котором Такая техника используется, например, в работах Spectrally-normalized margin bounds for neural networks и A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. Фундаментальная проблема равномерных оценок Пример модели (розовая кривая), имеющей малый истинный риск, но большой эмпирический на заданном наборе данных (кружочки). Данные одного класса лежат на желтом круге, другого – на голубом; оптимальная разделяющая поверхность обозначена пунктиром. Имея набор из кружочков, мы строим противоположный набор, обозначенный крестиками; заметим, что он мог прийти из того же распределения. Если алгоритм обучения старается отодвинуть границу классов как можно дальше от примеров, то результатом обучения на наборе крестиков может стать розовая кривая. Пример взят из работы Uniform convergence may be unable to explain generalization in deep learning. Напомним, что построение равномерных оценок проходило в несколько шагов: Оценка супремумом sup f∈F sup (R(f)− R ^ m (f)) Применение неравенства макДайармида: sup sup log ⁡ 1 δ с вероятностью f∈F sup (R(f)− R ^ m (f))≤E S m ′ f∈F sup (R(f)− R ^ m ′ (f))+ 2m 1 log δ 1 с вероятностью ≥1−δ по S m , Оценка матожидания супремума (жёлтое слагаемое выше) с помощью симметризации с дальнейшим выходом на сложность Радемахера: E S m ′ sup sup sup f∈F sup (R(f)− R ^ m ′ (f))=E S m ′ f∈F sup (f)− R ^ m ′ (f))≤E f∈F sup ( R ^ m ′′ (f)− R ^ m ′ (f)). На каждом шаге предыдущая величина оценивается сверху, и потенциально каждое из этих неравенств может оказаться слишком слабым и привести к бессмысленной оценке. Давайте это проиллюстрируем. Выше мы уже отмечали, что если класс F F содержит модель, для которой (f) мал, а R ( f ) R(f) велик, то равномерная оценка становится бессмысленной. По этой причине, имеет смысл выбирать класс F F как можно более маленьким. Самым лучшим из возможных классов мог бы быть класс моделей, к которым сходится наш алгоритм обучения с высокой вероятностью. Рассмотрим случай, близкий к идеальному: тот, в",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 7,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "^ m ′′ (f)− R ^ m ′ (f)). На каждом шаге предыдущая величина оценивается сверху, и потенциально каждое из этих неравенств может оказаться слишком слабым и привести к бессмысленной оценке. Давайте это проиллюстрируем. Выше мы уже отмечали, что если класс F F содержит модель, для которой (f) мал, а R ( f ) R(f) велик, то равномерная оценка становится бессмысленной. По этой причине, имеет смысл выбирать класс F F как можно более маленьким. Самым лучшим из возможных классов мог бы быть класс моделей, к которым сходится наш алгоритм обучения с высокой вероятностью. Рассмотрим случай, близкий к идеальному: тот, в котором существует ϵ > 0 ϵ>0, для которого при любых f ∈ F f∈F имеем R(f)<ϵ. Иными словами, предположим, что все модели класса F F хорошо обобщают. В этом случае оценка выше близка к идеальной: с вероятностью )≤ϵс вероятностью ≥1−δ по S m . Но что будет, если мы начнём честно воспроизводить процесс построения равномерных оценок? После второго шага мы получаем оценку вида sup log ⁡ 1 δ с вероятностью f∈F sup (R(f)− R ^ m (f))≤ϵ+ 2m 1 log δ 1 с вероятностью ≥1−δ по S m , которая не сильно хуже предыдущей, но в которой всё равно появилось лишнее слагаемое. Но допустим, что мы хотим честно проделать третий шаг процедуры получения равномерных оценок. Для этого нам необходимо было оценить матожидание супремума, которое после симметризации получает вот такую верхнюю оценку: sup f∈F sup ( R ^ m ′′ (f)− R ^ m ′ (f)). Таким образом, малость истинного риска не гарантирует малость эмпирического риска на любом наборе данных. Так, авторы статьи Uniform convergence may be unable to explain generalization in deep learning предъявили пример, в котором для любого существует модель ∈F, такая что )≈1, но при этом ) и ) малы. Иллюстрация такой ситуации приведена в начале параграфа. Тогда sup sup f∈F ( R ^ m ′′ (f)− R ^ m ′ (f)) велик, и оценки теряют смысл. К счастью, даже эта фундаментальная проблема не ставит крест на равномерных оценках. Так, работы Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting и Stability and Deviation Optimal Risk Bounds with Convergence Rate O(1/n) рассматривают равномерную оценку в классе интерполирующих моделей, то есть, имеющих нулевой эмпирический риск: sup f∈F: R ^ m (f)=0 sup R(f). Для таких моделей контрпример выше не работает. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.1. Введение в теорию глубокого обучения Следующий параграф 13.3. PAC-байесовские оценки риска",
    "metadata": {
      "title": "Обобщающая способность – классическая теория",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshayushaya-sposobnost-klassicheskaya-teoriya",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.2",
      "part": 8,
      "total_parts": 8,
      "source_file": "13.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В предыдущем параграфе рассматривались равномерные оценки разницы истинного и эмпирического рисков. Если в рассматриваемом классе моделей есть «плохие», то равномерные оценки становятся слишком пессимистичными. Часто нельзя гарантировать, что что наш алгоритм обучения их никогда не выбирает, поэтому класс моделей F F для равномерной оценки не получится сузить до класса только «хороших» моделей. Но можно надеяться, что плохие выучиваются не слишком часто. Например, известно, что градиентный спуск обычно сходится к хорошим моделям (об этом мы ещё поговорим в параграфе про implicit bias). В этом параграфе мы разберём элегантный способ учесть «предпочтения» алгоритма обучения в оценке разницы рисков. Вспомним равномерную оценку для конечного F F: P ( sup f∈F sup (R(f)− R ^ m (f))≥ϵ)=P(∃f∈F:(R(f)− R ^ m (f))≥ϵ)≤ f∈F ∑ P(R(f)− R ^ m (f)≥ϵ)≤∣F∣e −2mϵ 2 ∀ϵ>0, где ∣ F ∣ ∣F∣ – мощность класса F F. Эта оценка формально верна и для бесконечного F F, но смысл её теряется. Давайте попробуем исправить это. Пусть F F не более, чем счётно. Для каждого f ∈ F f∈F возьмём своё ϵ ( f ) ϵ(f). Если взять ϵ ( f ) ϵ(f) таким, чтобы f∈F e −2mϵ 2 (f) было конечным, то приходим к осмысленной оценке: P(∃f∈F:(R(f)− R ^ m (f))≥ϵ(f))≤ f∈F ∑ P(R(f)− R ^ m (f)≥ϵ(f))≤ f∈F ∑ e −2mϵ 2 (f) . Рассмотрим теперь некоторое вероятностное распределение P ( f ) P(f) на F F. В качестве ϵ ( f ) ϵ(f) возьмём −2mϵ 2 (f) =P(f)e −2m ϵ ~ 2 , где . Из этого уравнения получаем следующее выражение для ϵ ( f ) ϵ(f): log ϵ(f)= log P(f) 1 . В итоге, для любого >0 получаем оценку: log P(∃f∈F:(R(f)− R ^ m (f))≥ log P(f) 1 )≤e −2m ϵ ~ 2 . Или, что то же самое, с вероятностью ≥ 1 − δ ≥1−δ по S m S m для любого f ∈ F f∈F: log ⁡ 1 δ + log R(f)− R ^ m (f)≤ 2m 1 (log δ 1 +log P(f) 1 ) Заметим, что если F F конечно, а P ( f ) P(f) – равномерное распределение, то оценка выше совпадает с равномерной оценкой. Если же наш алгоритм обучения предпочитает выбирать модели, для которых P ( f ) P(f) велико, то оценка улучшается по сравнению с равномерной. Таким образом, распределение P ( f ) P(f) «кодирует» наши представления о предпочтениях алгоритма. Будем называть P ( f ) P(f) «априорным распределением». Как обобщить оценку выше на несчётные классы моделей? В первую очередь, предположим, что наш алгоритм обучения A A стохастичен, а значит, на выходе даёт не одну модель, а распределение: =A(S m ). Будем называть это распределение «апостериорным». Такое рассуждение осмысленно, например, для стохастического градиентного спуска: очевидно, что результат его работы на невыпуклой функции потерь недетерминирован (он может сходиться в разные локальные минимумы). Заметим, что главное отличие апостериорного распределения от априорного в том, что первое зависит от данных, а второе – нет. Важно понимать при этом, что, несмотря на названия, эти два распределения не связаны между собой никаким вариантом формулой Байеса. Сходство с байесовским",
    "metadata": {
      "title": "PAC-байесовские оценки риска",
      "url": "https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.3",
      "part": 1,
      "total_parts": 4,
      "source_file": "13.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "выше на несчётные классы моделей? В первую очередь, предположим, что наш алгоритм обучения A A стохастичен, а значит, на выходе даёт не одну модель, а распределение: =A(S m ). Будем называть это распределение «апостериорным». Такое рассуждение осмысленно, например, для стохастического градиентного спуска: очевидно, что результат его работы на невыпуклой функции потерь недетерминирован (он может сходиться в разные локальные минимумы). Заметим, что главное отличие апостериорного распределения от априорного в том, что первое зависит от данных, а второе – нет. Важно понимать при этом, что, несмотря на названия, эти два распределения не связаны между собой никаким вариантом формулой Байеса. Сходство с байесовским подходом скорее внешнее. Поэтому слова «априорное» и «апостериорное» имеет смысл писать в кавычках, но для экономии места мы их будем в дальнейшем опускать. Оценки разности рисков, о которых речь пойдёт ниже, называются PAC-байесовскими (PAC-bayesian, где PAC – probably approximately correct). Сформулируем одну из классических оценок из этого класса: Теорема Макаллестера. Пусть F F – множество моделей и P P – распределение на F F. Тогда для любого δ∈(0,1) с вероятностью ≥ 1 − δ ≥1−δ по S m S m имеем: log 2m−1 1 (log δ 4m +KL( Q ^ m ∣∣P)) , где R(Q)=E f∼Q R(f) и (Q)=E f∼Q R ^ m (f). Видим, что оценка тем лучше, чем ближе апостериорное распределение к априорному. Здесь работает следующая интуиция. Если для большинства обучающих наборов данных апостериорное распределение близко к априорному, то оно почти не зависит от данных, а значит, истинный риск и риск на обучающей выборке должны быть близки с высокой вероятностью. Если же апостериорное зависит от данных сильно, то, скорее всего, модель сильно переобучается, а значит, оценка не может быть хорошей; в нашем случае она велика из-за большой KL-дивергенции. Для доказательства теоремы нам понадобятся две леммы: Лемма 1. Для любого распределения P P на F F и для любого δ∈(0,1) с вероятностью ≥ 1 − δ ≥1−δ по S m S m имеем: f∼P e (2m−1)(Δ m (f)) 2 ≤ δ 4m , где (f)=∣R(f)− R ^ m (f)∣. Лемма 2 (лемма Донскера-Вередана, Donsker-Varadhan). Пусть P P и Q Q – вероятностные распределения на множестве X X. Тогда для любого h : X → R h:X→R log x∼Q h(x)≤logE x∼P e h(x) +KL(Q∣∣P). Теорема Макаллестера – не единственная из возможных пак-байесовских оценок. Например, несколько улучшенную версию той же оценки можно найти в работе Bounds for averaging classifiers. Другие оценки подобного типа можно найти в монографии PAC-Bayesian supervised classification: the thermodynamics of statistical learning. Применение пак-байесовских оценок к детерминированным алгоритмам обучения Выше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделей F F всё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит",
    "metadata": {
      "title": "PAC-байесовские оценки риска",
      "url": "https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.3",
      "part": 2,
      "total_parts": 4,
      "source_file": "13.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "thermodynamics of statistical learning. Применение пак-байесовских оценок к детерминированным алгоритмам обучения Выше были рассмотрены две PAC-байесовские оценки: одна для не более, чем счётного множества моделей, другая – для произвольного. За возможность использования несчётных классов моделей мы заплатили тем, что алгоритм обучения должен быть недетерминированным (для детерминированных алгоритмов KL-дивергенция в Теореме Макаллестера может вырождаться в бесконечность; например, это так, если априорное распределение гауссово). Чаще всего класс моделей F F всё-таки несчетён: например, если это класс всех сетей фиксированной архитектуры, то он индексируется весами, которых несчётное множество. При этом, хотя используемый алгоритм обучения и в самом деле недетерминирован (стохастический градиентный спуск зависит от случайного выбора батчей и от инициализации весов) и теорема Макаллестера выполняется, финальное распределение моделей очень сложно охарактеризовать, и из-за этого непонятно, как считать KL-дивергенцию. Предположим, что алгоритм обучения всё-таки детерминирован; этого можно добиться, зафиксировав сид генератора случайных чисел при обучении. Как получить осмысленную PAC-байесовскую оценку для детерминированного алгоритма на несчётном множестве моделей? Мы рассмотрим два способа. Первый способ – добавить известный шум в финальную модель, выданную детерминированным алгоритмом. Так, для нейронных сетей, результатом работы алгоритма обучения является набор весов. Если добавить в этот набор гауссовский шум, а также в качестве априорного распределения взять гауссовское, то KL-дивергенцию в теореме Макаллестера можно будет посчитать аналитически. Дисперсию шума в апостериорном распределении тоже можно обучить с помощью градиентного спуска одновременно с весами, тем самым минимизируя правую часть оценки из вышеупомянутой теоремы. Если в найденную модель удастся добавить шум так, чтобы KL-дивергенция значительно уменьшилась, но при этом риск на обучающей выборке не сильно вырос, то оценка на истинный риск получится хорошей. Это рассуждение связывает PAC-байесовские оценки и гипотезу о том, что «плоские» («широкие») минимумы хорошо обобщают. В самом деле, если минимум «плоский», то в модель из него можно добавить много шума, не испортив качество на обучении. Оценки, основанные на этом принципе, можно найти в работах Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data и A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. Второй способ состоит в том, чтобы взять дискретное кодирование c c и применить дискретную PAC-байесовскую оценку к закодированной модели вместо оригинальной. Обозначим закодированную модель f f через f c f c . Следуя работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach, возьмём априорное распределение с массой, убывающей с ростом длины кода: m(∣f c ∣)2 −∣f c ∣ . Здесь ∣ – длина кода модели m(k) – некоторое вероятностное распределение на N N, а Z Z – нормализующая константа. Тогда KL-дивергенция примет следующий вид: log log ⁡ 2 − log KL(δ f c ∣∣P c )=logZ+∣f c ∣log2−log(m(∣f c ∣)). Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на",
    "metadata": {
      "title": "PAC-байесовские оценки риска",
      "url": "https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.3",
      "part": 3,
      "total_parts": 4,
      "source_file": "13.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "распределение на N N, а Z Z – нормализующая константа. Тогда KL-дивергенция примет следующий вид: log log ⁡ 2 − log KL(δ f c ∣∣P c )=logZ+∣f c ∣log2−log(m(∣f c ∣)). Для того, чтобы KL-дивергенция выше была как можно меньше, необходимо, чтобы наш алгоритм обучения на реалистичных данных сходился в модели с маленькой длиной кода. Для этого будем применять наше кодирование не к оригинальной модели, а к сжатой с помощью некоторого алгоритма сжатия. Здесь мы предполагаем, что модели, к которым сходится наш алгоритм обучения, можно сжать с малыми потерями до моделей с малой длиной кода. Другими словами, мы опираемся на предположение, что обученные модели в некоторым смысле «простые». Если модель параметризована весами θ θ, типичный алгоритм сжатия выдаст набор (S,Q,C), где dim ⁡ θ ] S=s 1:k ⊂[dimθ] – позиции ненулевых весов; C=c 1:r ⊂R – «словарь» весов; Q=q 1:k ∈[r] ∀i∈[k] – квантизованные значения весов. Выход алгоритма будет выглядеть как C(θ) i =c q j , если i = s j i=s j , иначе 0 0. Тогда наивное 32-битное кодирование даст следующую длину: log ⁡ dim ⁡ θ + log ∣C(θ)∣ c =∣S∣ c +∣Q∣ c +∣C∣ c ≤k(logdimθ+logr)+32r. В работе Non-vacuous Generalization Bounds at the ImageNet Scale: a PAC-Bayesian Compression Approach описанный выше способ применяется к модели MobileNet (свёрточной сети, сконструированной специально для мобильных устройств), обученной на наборе данных ImageNet, и получают верхнюю оценку на истинный риск, равную 96.5 % 96.5% (риск случайного угадывания – 99.9 % 99.9%). Хотя такой результат и выглядит очень скромным, но это первая осмысленная оценка обобщающей способности реально используемой нейронной сети на реалистичном наборе данных. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.2. Обобщающая способность – классическая теория Следующий параграф 13.4. Сети бесконечной ширины",
    "metadata": {
      "title": "PAC-байесовские оценки риска",
      "url": "https://education.yandex.ru/handbook/ml/article/pac-bajesovskie-ocenki-riska",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.3",
      "part": 4,
      "total_parts": 4,
      "source_file": "13.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Во введении обсуждалось, что истинный риск нейронной сети выходит на асимптоту при стремлении ширины сети (то есть числа нейронов в слое) к бесконечности. Это намекает нам на то, что существует предельная модель, «бесконечно широкая сеть». В этом параграфе мы обсудим подходы к анализу её поведения. Динамика обучения нейронной сети описывается эволюцией в пространстве весов – например, правилом обновления весов в градиентном спуске. Но в каком виде можно записать эволюцию бесконечно широкой сети, в которой весов бесконечно много? Есть два способа это сделать. Первый способ – ввести меру в пространстве весов В качестве примера рассмотрим нейронную сеть с одним скрытым слоем, скалярным выходом и скалярным входом: f(x)= n 1 i=1 ∑ n a i ϕ(w i x). Это выражение можно представить в виде f(x)=∫ R 2 aϕ(wx)dμ n (a,w), где мера сосредоточена в весах, ассоциированных с каждым из нейронов скрытого слоя: (a,w)= n 1 i=1 ∑ n δ a i (a)δ w i (w). Здесь δ x δ x – мера, сосредоточенная в x x. При стремлении ширины n n к бесконечности μ n μ n может иметь предел. Так, если все веса насемплированы независимо из стандартного нормального распределения N(0,1), предельная мера принимает вид двумерного стандартного нормального распределения N(0,I 2×2 ), а предсказание предельной сети можно записать в виде f(x)=E a,w∼N(0,1) aϕ(wx). Заметим, что множитель 1 / n 1/n в определении модели выше принципиально важен для того, чтобы предельная мера и представление предельной сети в виде интеграла по мере были определены. Такая параметризация носит название mean-field parameterization. Динамику эволюции весов также можно представить в виде эволюции меры. В самом деле, в случае конечной ширины градиентный спуск говорит нам о том, как за один шаг оптимизации меняются веса, ассоциированные с каждым из нейронов, или, что то же самое, как меняется мера μ n μ n . Заменив в этом выражении меру μ n μ n на предельную, можно получить эволюцию предельной меры. К сожалению, представление эволюции предельной сети в виде эволюции меры не даёт сказать много о свойствах предельной модели. Так, известно, что предельная модель всегда сходится в глобальный минимум на обучающей выборке, см статью On the global convergence of gradient descent for over-parameterized models using optimal transport, но мало что известно о её обобщающей способности. Более того, лишь сети с одним скрытым слоем допускают простую формулировку в форме эволюции меры в пределе бесконечной ширины, см. статьи Mean field analysis of neural networks: A central limit theorem, On the global convergence of gradient descent for over-parameterized models using optimal transport и Trainability and accuracy of neural networks: An interacting particle system approach. Для сетей с большим числом слоёв подобная формулировка также возможна, см. статьи A mean-field limit for certain deep neural networks и A rigorous framework for the mean field limit of multilayer neural networks, но анализ усложняется. Так, сходимость в глобальный минимум для сети с двумя скрытыми слоями была доказана лишь совсем недавно в работе Global convergence of three-layer neural networks in the mean field regime; для более глубоких сетей подобные результаты пока неизвестны. Второй способ – вместо эволюции весов рассматривать",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 1,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "gradient descent for over-parameterized models using optimal transport и Trainability and accuracy of neural networks: An interacting particle system approach. Для сетей с большим числом слоёв подобная формулировка также возможна, см. статьи A mean-field limit for certain deep neural networks и A rigorous framework for the mean field limit of multilayer neural networks, но анализ усложняется. Так, сходимость в глобальный минимум для сети с двумя скрытыми слоями была доказана лишь совсем недавно в работе Global convergence of three-layer neural networks in the mean field regime; для более глубоких сетей подобные результаты пока неизвестны. Второй способ – вместо эволюции весов рассматривать эволюцию предсказаний модели в каждой точке x x Для простоты рассмотрим задачу минимизации квадратичной функции потерь на наборе данных ) размера min ⁡ θ . 2 1 j=1 ∑ m (y j −f(x j ;θ)) 2 → θ min . Будем оптимизировать эту функцию потерь градиентным спуском с шагом k+1 −θ k =−η∇ θ ( 2 1 j=1 ∑ m (y j −f(x j=1 ∑ m (y j −f(x j ;θ t ))∇ θ f(x j ;θ k ). Ниже нам будет удобнее рассматривать градиентный спуск с непрерывным временем вместо дискретного: j=1 ∑ m (y j −f(x j ;θ t ))∇ θ f(x j ;θ t ). Переход к непрерывному времени соответствует устремлению к нулю шага η η, если при этом число шагов растёт как k=[t/η], где округление применяется в любую сторону. Обозначим через (x) предсказание в точке x x модели в момент времени t t. Оно зависит от времени следующим образом: (x)= (x)= j=1 ))∇ (x). Введём обозначение: (x,x ′ )=∇ θ T f t (x)∇ С помощью него уравнение выше можно записать более коротко: (x)= Θ ^ t (x, )).(1) Здесь и дальше мы будем считать, что (x, x ) имеет размерность 1 × m 1×m. Функция (x,x ′ ) называется эмпирическим нейрокасательным ядром (Neural Tangent Kernel, NTK); подробнее о ядрах мы поговорим ниже в параграфе про ядровые методы. Заметим, что в уравнении ( 1 ) (1) вся информация о весах содержится в ядре, которое является отображением из X × X X×X в R R. Как мы увидим ниже, при определённых условиях, при стремлении ширины сети к бесконечности ядро имеет предел и он не зависит от t t. Обозначив этот предел через Θ Θ, мы приходим к следующему виду эволюции предсказаний бесконечно широкой сети: (x)=Θ(x, )).(2) Здесь и далее будем называть Θ Θ (не эмпирическим) нейрокасательным ядром или NTK. Такой термин был введён в оригинальной работе Neural tangent kernel: Convergence and generalization in neural networks. В этом случае динамика предсказаний интегрируется следующим образом. На обучающей выборке )=Θ( )), что даёт )=f 0 ( x )−(I−e −Θ( x , x )t )(f Подставляя решение в ( 2 ) (2), получаем (x)=Θ(x, x )e −Θ( )), и, наконец, (x)=f 0 (x)−Θ(x, )(I−e −Θ( x , x )t )(f 0 ( x )− y ).(3) Прежде, чем доказывать сходимость ядра, мы обсудим, как может применяться предельное ядро и представление эволюции предсказаний в форме (1). Применение NTK-анализа NTK как математический аппарат Нам",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 2,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "NTK. Такой термин был введён в оригинальной работе Neural tangent kernel: Convergence and generalization in neural networks. В этом случае динамика предсказаний интегрируется следующим образом. На обучающей выборке )=Θ( )), что даёт )=f 0 ( x )−(I−e −Θ( x , x )t )(f Подставляя решение в ( 2 ) (2), получаем (x)=Θ(x, x )e −Θ( )), и, наконец, (x)=f 0 (x)−Θ(x, )(I−e −Θ( x , x )t )(f 0 ( x )− y ).(3) Прежде, чем доказывать сходимость ядра, мы обсудим, как может применяться предельное ядро и представление эволюции предсказаний в форме (1). Применение NTK-анализа NTK как математический аппарат Нам удалось проинтегрировать динамику предсказаний в явном виде. Что это даёт? Во-первых, мы получаем достаточное условие на сходимость в глобальный минимум на обучающей выборке. Таким условием является положительная определённость матрицы Грама ядра: )≥λ 0 для некоторого >0. В самом деле, в этом случае, )=−( ))≤−λ что даёт при →0при t→∞. Во-вторых, раз явное решение известно, можно написать оценку на обобщающую способность. Оба этих результата опираются на то, что ядро постоянно. Как мы покажем ниже, постоянство нейрокасательного ядра нейронной сети можно гарантировать лишь в пределе бесконечной ширины. Тем не менее, если сеть конечна, но достаточно широка, можно показать, что её ядро достаточно близко к предельному, и оценки сохраняют силу. Например, для обоснования сходимости в глобальный минимум достаточно показать, что наименьшее собственное значение эмпирического ядра с высокой вероятностью остаётся отделённым от нуля в течение обучения: ∀t≥0 с вероятностью ≥ 1 − δ ≥1−δ для n≥n ∗ (δ). В самом деле, из этого следует, что )=−( ))≤− а значит, при t/2 →0при t→∞. Формальное доказательство вы можете найти в работе Gradient Descent Provably Optimizes Over-parameterized Neural Networks, а также в конспекте лекций автора этого параграфа. Вот ещё несколько результатов, полученных в этом направлении: улучшенные оценки на минимальную ширину в работе Quadratic suffices for over-parametrization via matrix chernoff bound; оценки для случая глубоких сетей в работе Gradient descent finds global minima of deep neural networks; оценки на обобщающую способность, полученные через близость ядра к предельному, в работе Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks. Определение патологий обучения Как мы увидим позже, NTK реальных, стандартно параметризованных, имеющих конечную ширину сетей может меняться за время обучения существенным образом: см. эмпирическую работу Deep learning versus kernel learning и теоретический анализ для сетей с одним скрытым слоем Dynamically Stable Infinite-Width Limits of Neural Classifiers. Тем не менее, ядро в инициализации может выявить определённые патологии соответствующей нейронной сети. Рассмотрим один из примеров применения. В некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения. Необходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или,",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 3,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нейронной сети. Рассмотрим один из примеров применения. В некоторых состоящих из однородных блоков архитектурах (скажем, ResNet) можно увеличивать (и даже устремлять к бесконечности) число слоёв или блоков, и логично задаться вопросом о том, как при этом будет вести себя процесс обучения. Необходимым условием обучаемости является хороший первый шаг обучения. Если он исчезающе мал, то сеть не обучится ни на первом, ни на каком-либо другом шаге. Если он слишком велик, то обучение разойдётся на первом же шаге. Как мы увидим ниже, индикатором проблем является плохая обусловленность NTK в инициализации. Например, его собственные значения могут с ростом глубины стремиться к нулю или, наоборот, к бесконечности. В первом случае какие-то из компонент выборки никогда не выучатся, во втором обучение невозможно ни при каком конечном темпе обучения. Чтобы в этом убедиться, рассмотрим разложение матрицы Грама ядра по собственным векторами: j=1 где ≥…≥λ m ≥0, а векторы ,…, v m образуют ортонормированный базис. Разложим предсказание сети по этому базису: )=∑ j=1 m u t,j v j . Так как базис ортонормированный, каждая из компонент эволюционирует независимо от других. В самом деле, для дискретного градиентного спуска с шагом η η имеем k+1,j =u k,j +ηλ t,j ). Таким образом, если =0, то u t , j u t,j никогда не сойдётся к Кроме того, для того, чтобы процесс сходился, шаг η η должен убывать обратно пропорционально наибольшему собственному числу λ 1 λ 1 . Если последнее стремится к бесконечности, то η η стремится к нулю, а значит, η λ j ηλ j будем мало для всех j j, для которых λ j λ j конечен; соответствующие компоненты также никогда не сойдутся. Подробности см. в работе Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping, а также в более ранних работах Exponential expressivity in deep neural networks through transient chaos, Deep information propagation, Resurrecting the sigmoid in deep learning through dynamical isometry, Dynamical isometry and a mean field theory of cnns, в которых использовалась похожая идея, но не использовалось понятие NTK явно. См. также главу про инициализацию в конспекте лекций. NTK и Ядровые методы Предельное NTK нейронной сети можно использовать в любом ядровом методе, например, в SVM. Обсудим это поподробнее и заодно разберёмся, почему NTK вообще называют ядром. Рассмотрим задачу линейной регрессии: θ ^ λ = argmin =argmin θ∈R d j=1 ∑ m L(y j ,θ T x j )+λ∥θ∥ 2 2 (4) Эту же задачу можно эквивалентно переписать следующим образом: f ^ λ = argmin =argmin f∈H j=1 ∑ m L(y j ,f(x j ))+λ∥f∥ H 2 ,(5) где H H – пространство линейных отображений (x)=θ T x с некоторой нормой ∥ f ∥ H ∥f∥ H на нём. Сделаем линейное пространство H H евклидовым, введя на нём следующее скалярное произведение. Для f(x)=θ T x и (x)= θ ~ T x определим ⟨f, f ~ ⟩=θ T θ ~ . Это скалярное произведение порождает норму ∥f∥ H =∥θ∥ 2 , что и делает формулировку (5) эквивалентной формулировке (4). Пространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 4,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "образом: f ^ λ = argmin =argmin f∈H j=1 ∑ m L(y j ,f(x j ))+λ∥f∥ H 2 ,(5) где H H – пространство линейных отображений (x)=θ T x с некоторой нормой ∥ f ∥ H ∥f∥ H на нём. Сделаем линейное пространство H H евклидовым, введя на нём следующее скалярное произведение. Для f(x)=θ T x и (x)= θ ~ T x определим ⟨f, f ~ ⟩=θ T θ ~ . Это скалярное произведение порождает норму ∥f∥ H =∥θ∥ 2 , что и делает формулировку (5) эквивалентной формулировке (4). Пространство линейных моделей слишком узко, однако ничто не мешает нам рассмотреть задачу вида (5), в которой H H будет произвольным нормированным пространством функций. Наиболее хорошо изучен случай, когда функции из H H являются линейными моделями в некотором (возможно, бесконечномерном) гильбертовом пространстве признаков: f(x)=⟨Φ(x),θ⟩, где Φ Φ отображает x x в это пространство. Если последнее всё же конечномерно, то мы можем использовать матричную запись f(x)=θ T Φ(x); элементы θ θ в этой записи обычно называют первичными переменными (primal variables). Пространство функций H H также оказывается гильбертовым: соответствующее скалярное произведение имеет вид =⟨θ,θ ′ ⟩ Таким образом, =⟨f θ ,f θ ⟩ H =⟨θ,θ⟩, если (x)=⟨Φ(x),θ⟩. Любому отображению Φ Φ можно сопоставить симметричную положительно-определённую функцию K(x,x ′ )=⟨Φ T (x),Φ(x ′ )⟩; функции такого вида называются ядрами. В силу фундаментальной теоремы о представителе любое решение задачи (4) принимает вид f(x)= j=1 ∑ m α j K(x,x j )=K(x, x ) α . В отличие от θ θ, вектор α α всегда конечномерен: его размерность равна размеру обучающей выборки. Элементы α α называют двойственными (dual) переменными. Упомянутый результат позволяет перейти от минимизации f f в бесконечномерном пространстве функций (или, что то же самое, минимизации θ θ в бесконечномерном пространстве признаков), к минимизации в конечномерном пространстве двойственных переменных: α ⃗ = argmin =argmin α ∈R m j=1 ∑ m L(y j ,K(x j , x ) α )+λ .(6) Если в качестве функции потерь взять квадратичную L(y,z)= 2 1 (y−z) 2 , то получим ядровую регрессию; если же взять hinge loss L(y,z)=[1−yz] + , то SVM. Заметим, что двойственная задача полностью сформулирована в терминах ядра K K: отображение в потенциально бесконечное пространство признаков Φ Φ более нигде не возникает. Поэтому мы можем использовать в качестве K K любую симметричную положительно определённую функцию двух переменных, не думая о том, для какого пространства признаков оно будет ядром (есть теорема, что такие функции всегда являются ядрами). Это может быть очень полезно. Так, если для эмпирического NTK в инициализации (x,x ′ ) имеем Φ(x)=∇ θ f(x;θ 0 ), но совершенно неочевидно, какое отображение Φ Φ соответствует предельному NTK: lim Θ(x,x ′ )=lim n→∞ Θ ^ 0 (x,x ′ ). Таким образом, мы можем использовать Θ Θ в качестве ядра K K в двойственной задаче (6) наряду с линейным K(x,x ′ )=x T x ′ или гауссовским ядром K(x,x ′ )=e − 2σ 2 1 ∥x−y∥ 2 2 . Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 5,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "быть очень полезно. Так, если для эмпирического NTK в инициализации (x,x ′ ) имеем Φ(x)=∇ θ f(x;θ 0 ), но совершенно неочевидно, какое отображение Φ Φ соответствует предельному NTK: lim Θ(x,x ′ )=lim n→∞ Θ ^ 0 (x,x ′ ). Таким образом, мы можем использовать Θ Θ в качестве ядра K K в двойственной задаче (6) наряду с линейным K(x,x ′ )=x T x ′ или гауссовским ядром K(x,x ′ )=e − 2σ 2 1 ∥x−y∥ 2 2 . Такой подход привлекателен тем, что обучение ядровых методов более устойчиво и имеет меньше гиперпараметров. При этом можно надеяться, что результат обучения ядрового метода с NTK в качестве ядра будет близок к результату обучения соответствующей нейронной сети. Основная проблема ядровых методов в том, что они требуют вычисления матрицы Грама ядра на обучающем наборе данных ). Её размер m × m m×m (где m m – размер выборки), так что применение ядровых методов на больших данных сильно усложняется. Более того, наивное вычисление динамики f t f t из формулы (3) требует обращения матрицы Грама, которое занимает O ( m 3 ) O(m 3 ) времени. Тем не менее, определённые оптимизации существуют. Так например, в работе Kernel methods through the roof предлагается способ приближённого вычисления ( f t f t ) за log ⁡ m ) O(m 3/2 logm) памяти и времени. Другие подходы см. в работах Fast Finite Width Neural Tangent Kernel и Neural tangents: Fast and easy infinite neural networks in python. Так или иначе, на малых наборах данных выражение (3) можно вычислить точно, см. результаты в работе Harnessing the power of infinitely wide deep nets on small-data tasks. Существуют также примеры задач, в которых матрицу Грама ядра достаточно посчитать только для малых m m, см., например, Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. Ещё одна проблема использования NTK в ядровых методах состоит в том, что явный подсчёт предельного NTK доступен только для сетей, состоящих из слоёв из определённого класса. В этот класс входят полносвязные и свёрточные слои, average pooling, ряд нелинейностей с одним аргументом (включая, например, ReLU и erf), layer norm, но не входят max pooling и batch norm, часто используемые в реальных архитектурах. Явный подсчёт предельного NTK для «хороших» сетей реализован в библиотеке NeuralTangents; часть явных формул для подсчёта можно найти в статье On exact computation with an infinitely wide neural net. Тем не менее, даже в тех случаях, когда посчитать предельное NTK не представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации (x,x ′ )=∇ θ T f(x;θ 0 )∇ θ f(x ′ ;θ 0 ) Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работе Neural tangents: Fast and easy infinite neural networks in python. NTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро K(x,x ′ )=Ef(x)f(x ′ ) – это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работах Deep Neural Networks as",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 6,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "представляется возможным, в качестве ядра для ядрового метода можно использовать эмпирическое NTK в инициализации (x,x ′ )=∇ θ T f(x;θ 0 )∇ θ f(x ′ ;θ 0 ) Такое ядро можно рассматривать как шумную и смещённую оценку предельного; для уменьшения шума можно использовать Монте-Карло оценку матожидания. Некоторые оптимизации подсчёта эмпирического ядра см. в работе Neural tangents: Fast and easy infinite neural networks in python. NTK не единственное ядро, которое можно сопоставить нейронной сети. Так, NNGP-ядро K(x,x ′ )=Ef(x)f(x ′ ) – это ядро гауссовского процесса, реализуемого сетью в пределе бесконечной ширины. Подробнее можно почитать в работах Deep Neural Networks as Gaussian Processes, Wide neural networks of any depth evolve as linear models under gradient descent, Random neural networks in the infinite width limit as Gaussian processes или в конспекте лекций. Можно показать, что оно соответствует NTK-ядру для сети, в которой учится лишь выходной слой. Так как, в отличие от NTK, для подсчёта NNGP-ядра не требуется обратный проход (backward pass), последнее более вычислительно эффективно; Towards nngp-guided neural architecture search – пример работы, в которой предпочтение отдаётся NNGP-ядру именно по этой причине. Сходимость эмпирического ядра Вы этом параграфе мы покажем, что при определённой параметризации эмпирическое NTK не зависит ни от времени, ни от инициализации. Мы начнём с иллюстративного примера, прежде чем формулировать строгую теорему. Рассмотрим сеть с одним скрытым слоем, скалярным выходом и гауссовской инициализацией весов; вход для простоты тоже положим скалярным: f(x;a 1:n ,w 1:n )= i=1 ∑ n a i ϕ(w i x),a 1:n ∼N(0,n −1 I),w 1:n ∼N(0,I). Здесь n n – ширина скрытого слоя. Следуя одной из стандартных схем инициализации из статьи Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, дисперсия каждого слоя выбирается обратно пропорционально числу входных нейронов (подробнее см. в параграфе про тонкости обучения нейросетей). Назовём описанную выше параметризацию стандартной. Для сходимости ядра нам придётся несколько её видоизменить: f(x;a 1:n ,w 1:n )= n 1 i=1 ∑ n a i ϕ(w i x),a 1:n ∼N(0,I),w 1:n ∼N(0,I). Назовём новую параметризацию NTK-параметризацией. Отметим, что распределение выходов нейронов в инициализации остаётся неизменным при переходе от стандартной к NTK-параметризации. Что меняется – это динамика градиентного спуска: j=1 ∑ m ϕ(w k x j )(y j −f t (x j )), j=1 )). При t = 0 t=0 приращения весов для такой параметризации имеют порядок O(n −1/2 ), в то время как сами веса имеют порядок O ( 1 ) O(1) при t = 0 t=0. Поэтому (t)→a k (0) и (t)→w k (0) при n → ∞ n→∞ для любого данного k ∈ N k∈N и t ∈ R + t∈R + . Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке. Сравним с градиентным спуском в стандартной параметризации: j=1 ∑ m ϕ(w k x j )(y j −f t (x j )), j=1 В этом случае веса выходного слоя имеют порядок O(n −1/2 ) при t = 0 t=0, но получают приращения порядка O ( 1 ) O(1) в этот момент времени, в",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 7,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "k (0) и (t)→w k (0) при n → ∞ n→∞ для любого данного k ∈ N k∈N и t ∈ R + t∈R + . Другими словами, с ростом размера скрытого слоя градиент будет стремиться к нулю, и каждый из весов в пределе останется в начальной точке. Сравним с градиентным спуском в стандартной параметризации: j=1 ∑ m ϕ(w k x j )(y j −f t (x j )), j=1 В этом случае веса выходного слоя имеют порядок O(n −1/2 ) при t = 0 t=0, но получают приращения порядка O ( 1 ) O(1) в этот момент времени, в то время как веса входного слоя имеют порядок O ( 1 ) O(1) при t = 0 t=0, но получают в этот момент времени приращения порядка O(n −1/2 ). В новой параметризации эмпирическое NTK выглядит следующим образом: (x,x ′ )= i=1 ∑ n (∂ a i f(x)∂ a i f(x ′ )+∂ w i f(x)∂ w i f(x ′ ))= i=1 ∑ n (ϕ(w i (t)x)ϕ(w i (t)x ′ )+a i 2 (t)ϕ ′ (w i (t)x)ϕ ′ (w i (t)x ′ )xx ′ ). Так как (t)→a k (0) и (t)→w k (0) при n → ∞ n→∞ для любых заданных k ∈ N k∈N и t ∈ R + t∈R + , выражение выше асимптотически эквивалентно (x,x ′ )= n 1 i=1 ∑ n (ϕ(w i (0)x)ϕ(w i (0)x ′ )+a i 2 (0)ϕ ′ (w i (0)x)ϕ ′ (w i (0)x ′ )xx ′ ), а значит, сходится к Θ(x,x ′ )=E a,w∼N(0,1) (ϕ(wx)ϕ(wx ′ )+a 2 ϕ ′ (wx)ϕ ′ (wx ′ )xx ′ ) при n → ∞ n→∞ в силу закона больших чисел. Предельное ядро Θ(x,x ′ ) не зависит ни от времени t t, ни от инициализации. Мы будем называть это ядро нейрокасательным или просто NTK (его не стоит путать с эмпирическим NTK Ещё раз подчеркнём, что это работает для NTK-параметризации, но не для стандартной. Для стандартной параметризации эмпирическое NTK в инициализации расходится с шириной: (x,x ′ )= i=1 ∑ n (ϕ(w i (0)x)ϕ(w i (0)x ′ )+a i 2 (0)ϕ ′ (w i (0)x)ϕ ′ (w i (0)x ′ )xx ∼n⋅E w∼N(0,1) ϕ(wx)ϕ(wx ′ ). Подробнее мы поговорим об этом в одном из следующих параграфов. Для NTK-параметризации сходимость эмпирического ядра выполняется не только для сетей с одним скрытым слоем. Так, рассмотрим полносвязную сеть с L L слоями: f(x)=h L (x),h l (x)= n l−1 1 W l x l−1 (x),x l−1 (x)=ϕ(h l−1 (x)),x 0 (x)=x. Здесь 1×n L−1 l−1 для всех остальных l l. Положим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь =−∇ θ ( j=1 ∑ m L(y j ,f(x j ;θ t )))= j=1 ∑ m ∂z ∂L(y j ,z) z=f(x f(x j ;θ t ), где θ θ – объединение всех весов W 1 : L W 1:L сети. Теорема ниже доказана в оригинальной работе по NTK: Теорема. В предположениях выше, если ϕ ϕ из C 2 C 2 и липшицева и",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 8,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n l−1 1 W l x l−1 (x),x l−1 (x)=ϕ(h l−1 (x)),x 0 (x)=x. Здесь 1×n L−1 l−1 для всех остальных l l. Положим, что веса инициализируются из стандартного нормального распределения. Поставим задачу оптимизации дифференцируемой функции потерь =−∇ θ ( j=1 ∑ m L(y j ,f(x j ;θ t )))= j=1 ∑ m ∂z ∂L(y j ,z) z=f(x f(x j ;θ t ), где θ θ – объединение всех весов W 1 : L W 1:L сети. Теорема ниже доказана в оригинальной работе по NTK: Теорема. В предположениях выше, если ϕ ϕ из C 2 C 2 и липшицева и L L из C 1 C 1 и липшицева, то (x,x ′ ) сходится к Θ(x,x ′ ) по вероятности при 1:L−1 →∞ последовательно ∀x,x ∀t≥0. Оказывается, что эта теорема верна не только для полносвязных сетей с гладкими активациями. Определим тензорную программу как начальный набор переменных определённых типов и последовательность команд. Каждая команда порождает новую переменную, действуя на уже имеющиеся. Переменные бывают трёх типов: A A: n × n n×n матрицы с независимыми элементами из N(0,1); G G: вектора размера n n с асимптотически независимыми нормальными элементами; H H: образы G G-переменных относительно поэлементных нелинейностей. Для переменной W W запись W : A W:A будет означать, что W W имеет тип A A. Команды бывают следующие: trspop: W:A→W T :A (перевести переменную типа A A со значением W W в переменную типа A A со значением W T W T ); matmul: (W:A, x:H)→ n 1 Wx:G; lincomb: ({x i :G,a i ∈R} i=1 k )→∑ i=1 k a i x i :G; nonlin: ({x i :G} i=1 k ,ϕ:R k →R)→ϕ(x 1:k ):H (здесь мы несколько выходных векторов x i x i агрегируем в один с помощью покоординатной, возможно, нелинейной функции). Формализм тензорных программ позволяет представить прямой и обратный проход широкого класса нейронных архитектур, который включает свёрточные сети, рекуррентные сети, сети с residual слоями. Хотя и ни одна из операций выше не может порождать новые A A-переменные (веса), любое наперёд заданное число шагов градиентного спуска можно представить в рамках одной тензорной программы (посредством «развёртывания» шагов градиентного спуска). Назовём величину n n шириной тензорной программы. Основная «предельная» теорема тензорных программ представлена ниже: Master theorem (G. Yang, Tensor programs III: Neural matrix laws). Рассмотрим тензорную программу с M M G G-величинами, удовлетворяющую определённым начальным условиям. Пусть все нелинейности ϕ ϕ и функция ψ:R M →R полиномиально ограничены. Тогда α=1 ∑ n ψ(g α 1 ,…,g α M )→E Z∼N(μ,Σ) ψ(Z) почти наверное при n → ∞ n→∞, где μ μ и Σ Σ могут быть вычислены по некоторым рекурентным правилам. Оказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в виде α=1 n ψ(g α 1 ,…,g α M ) для некоторой функции ψ ψ, см. Tensor programs II: Neural tangent kernel for any architecture.Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 9,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ψ(g α 1 ,…,g α M )→E Z∼N(μ,Σ) ψ(Z) почти наверное при n → ∞ n→∞, где μ μ и Σ Σ могут быть вычислены по некоторым рекурентным правилам. Оказывается, что если тензорная программа выражает прямой и обратной проход в некоторой нейронной сети, то NTK сети в инициализации всегда можно представить в виде α=1 n ψ(g α 1 ,…,g α M ) для некоторой функции ψ ψ, см. Tensor programs II: Neural tangent kernel for any architecture.Таким образом, теорема выше доказывает существование и детерминированность предельного ядра в инициализации, а также даёт способ его вычисления. Более того, это верно и для ядра в любой фиксированный момент времени, см. Tensor Programs IIb. В качестве иллюстрации обратимся вновь к сети с одним скрытым слоем. Рассмотрим тензорную программу, вычисляющую прямой и обратный проходы на входах x x и x ′ x ′ . Такая программа порождает следующие G G-величины: =w(0)x, =w(0)x =a(0)x и =a(0)x ′ . Напомним, что эмпирическое NTK равно (x,x ′ )= n 1 i=1 ∑ n (ϕ(w i (0)x)ϕ(w i (0)x ′ )+a i 2 (0)ϕ ′ (w i (0)x)ϕ ′ (w i (0)x ′ )xx ′ ). Положив ψ(g α 1 ,…,g α 4 )=ϕ(g α 1 )ϕ(g α 2 )+ϕ получим выражение как раз в виде, требуемом Master Theorem. Стандартная параметризация и эволюция ядра Как было отмечено в предыдущем параграфе, эмпирическое NTK двухслойной сети расходится с шириной при стандартной параметризации. (x,x ′ )= i=1 ∑ n (ϕ(w i (t)x)ϕ(w i (t)x ′ )+a i 2 (t)ϕ ′ (w i (t)x)ϕ ′ (w i (t)x ′ )xx ′ ). При t = 0 t=0, так как w i w i независимы и имеют порядок O ( 1 ) O(1), сумма расходится пропорционально n n.Так как для квадратичной функции потерь (x)= Θ ^ t (x, )), предсказание модели в любой точке x x получает приращение порядка O ( n ) O(n) на первом же шаге обучения; для задачи регрессии такая модель теряет смысл. Однако для классификации величина предсказаний не играет роли: для бинарной классификации важен лишь знак, а для многоклассовой – индекс максимального логита. Таким образом, в этом случае, несмотря на расходящееся ядро, предел при бесконечной ширине имеет смысл, см. Dynamically Stable Infinite-Width Limits of Neural Classifiers. Рассмотрим нормализованное эмпирическое NTK (x,x ′ )= Θ ^ t (x,x ′ )/n. Его предел в инициализации равен w∼N(0,1) ϕ(wx)ϕ(wx ′ ). Назовём этот предел нормализованным NTK и обозначим (x,x ′ ). В отличие от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени: (x,x ′ ) = n 1 i=1 ∑ n (ϕ(w i (t)x)ϕ ′ (w i (t)x (t)x)ϕ(w i (t)x ′ )x) dt dw i (t) (x,x ′ ) + n 1 i=1 ∑ n a i 2 (t)xx ′ (ϕ ′ (w i (t)x)ϕ ′′ (w i (t)x (t)x)ϕ ′ (w i (t)x ′ )x) dt dw i (t) (x,x ′ ) + n 1 i=1 ∑ n 2a i (t)ϕ ′ (w i (t)x)ϕ ′ (w i (t)x ′ )xx ′ dt da i (t) . Напомним,",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 10,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "от ядра в NTK-параметризации, нормализованное NTK при стандартной параметризации зависит от времени: (x,x ′ ) = n 1 i=1 ∑ n (ϕ(w i (t)x)ϕ ′ (w i (t)x (t)x)ϕ(w i (t)x ′ )x) dt dw i (t) (x,x ′ ) + n 1 i=1 ∑ n a i 2 (t)xx ′ (ϕ ′ (w i (t)x)ϕ ′′ (w i (t)x (t)x)ϕ ′ (w i (t)x ′ )x) dt dw i (t) (x,x ′ ) + n 1 i=1 ∑ n 2a i (t)ϕ ′ (w i (t)x)ϕ ′ (w i (t)x ′ )xx ′ dt da i (t) . Напомним, как выглядит градиентный спуск в стандартной параметризации: (t) = j=1 ∑ m ϕ(w k (t)x j ), dt w k (t) = j=1 ∑ m a k (t)ϕ ′ (w k (t)x j )x j . При t = 0 t=0, =O(1), в то время как =O(n −1/2 ). Так как (0)=O(n −1/2 ) и (0)=O(1), для любого t > 0 t>0, не зависящего от (t)=O(1), (t)=O(1), (t)=O(1) и (t)=O(1). Наивная оценка сумм даёт (x,x ′ ) =O(1)+O(1)+O(1)=O(1) для любого t > 0 t>0, не зависящего от n n. Таким образом, нормализованное ядро зависит от времени даже в пределе бесконечной ширины. Экспериментальный анализ эволюции ядра реальной нейронной сети в стандартной параметризации см. в работе Deep learning versus kernel learning. Преимущество нейронных сетей над ядровыми методами, в том числе с NTK, может быть связано, в частности, с зависимостью предельного ядра от времени. В самом деле, ядро измеряет «похожесть» в некотором пространстве признаков. Для NTK это пространство фиксировано, в то время как нейронная сеть меняет своё ядро по ходу обучения, возможно, делая его более подходящим для задачи. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.3. PAC-байесовские оценки риска Следующий параграф 13.5. Ландшафт функции потерь",
    "metadata": {
      "title": "Сети бесконечной ширины",
      "url": "https://education.yandex.ru/handbook/ml/article/seti-beskonechnoj-shiriny",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.4",
      "part": 11,
      "total_parts": 11,
      "source_file": "13.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ставится как задача минимизации эмпирического риска (θ)=E x,y∈S m r(y,f θ (x)), где S m S m – выборка размера m m, а r r – функция риска, например, r(y, y ^ )=I[y  = y ^ ]. Часто интересующая нас функция риска не дифференцируема по второму аргументу, что делает градиентную оптимизацию неприменимой. По этой причине вместо исходной функции риска r r вводят её дифференцируемый выпуклый суррогат, то есть некоторую выпуклую и дифференцируемую по второму аргументу функцию ℓ ≥ r ℓ≥r. Новый функционал эмпирического риска имеет вид (θ)=E x,y∈S m ℓ(y,f θ (x)). Если f θ f θ дифференцируема по θ θ, то из дифференцируемости ℓ ℓ следует дифференцируемость , что делает возможной градиентную оптимизацию. А если f θ f θ выпукла по θ θ, то из выпуклости ℓ ℓ следует выпуклость , что даёт гарантии на сходимость градиентного спуска в глобальный минимум. Увы, в общем случае нейронные сети не выпуклы как функции своих весов. Это можно увидеть на простом примере. Пусть (x)=uvx, где u u, v v и x x – скаляры, а θ=(u,v). Гессиан f f как функции θ θ в любой точке равен ); его собственные числа равны x x и ( − x ) (−x), что и означает, что для любого ненулевого x x функция f f не выпукла. Таким образом, даже для выпуклой ℓ ℓ функция потерь нейронной сети не обязана быть выпуклой функцией весов. У невыпуклых функций могут быть минимумы, не являющиеся глобальными, в которых может «застревать» градиентный спуск. Тем не менее, на практике часто оказывается, что градиентный спуск всегда находит точку со сколь угодно близким к глобальному минимуму значением функции потерь. Это наблюдение приводит к гипотезе, что, хотя поверхность функции потерь не обязана быть выпуклой, все её минимумы глобальны для используемых нами сетей и тех наборов данных, на которых мы их обучаем. Известны два случая, для которых эту гипотезу удаётся доказать. Первый – это линейные сети. Второй – это достаточно широкие нелинейные сети (ширина одного из слоёв не меньше числа примеров в выборке). К сожалению, оба примера нереалистичны: выразительная способность линейных сетей не выше, чем у обыкновенной линейной модели, а ширина реальных нейронных сетей не настолько велика (порядка 1 0 3 10 3 нейронов против 1 0 6 10 6 примеров в ImageNet), причём улучшить оценку на ширину в общем случае невозможно, см. Q. Nguyen A note on connectivity of sublevel sets in deep learning. Возможно, для получения лучших оценок исследователям предстоит научиться учитывать структуру данных обучающей выборки. Исторически первое доказательство глобальности всех локальных минимумов линейной сети содержится в работе Deep learning without poor local minima. Более простое доказательство в немного более общем случае можно найти в работе Depth creates no bad local minima. Ещё более простое доказательство есть в работе Deep linear networks with arbitrary loss: All local minima are global, но оно подходит только для сетей без боттлнеков ( n l ≥ min ≥min(n 0 ,n L+1 ∀l∈1,…,L). Последнее подробно разобрано в конспекте лекций автора этого параграфа. Здесь мы разберём только второй случай (достаточно широкие нелинейные сети) как потенциально более перспективный.",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 1,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "учитывать структуру данных обучающей выборки. Исторически первое доказательство глобальности всех локальных минимумов линейной сети содержится в работе Deep learning without poor local minima. Более простое доказательство в немного более общем случае можно найти в работе Depth creates no bad local minima. Ещё более простое доказательство есть в работе Deep linear networks with arbitrary loss: All local minima are global, но оно подходит только для сетей без боттлнеков ( n l ≥ min ≥min(n 0 ,n L+1 ∀l∈1,…,L). Последнее подробно разобрано в конспекте лекций автора этого параграфа. Здесь мы разберём только второй случай (достаточно широкие нелинейные сети) как потенциально более перспективный. Все минимумы достаточно широкой нелинейной сети глобальны Рассмотрим нейронную сеть с одним скрытым слоем: f(x)=W 1 x 1 (x)∈R n 2 ,x 1 (x)=ϕ(h 1 (x))∈R n 1 ,h 1 (x)=W 0 x∈R n 1 ,x∈R n 0 , где функция активации ϕ ϕ применяется поэлементно. Рассмотрим набор данных ( X , Y ) (X,Y) размера m m, где X∈R n 0 ×m , а Y∈R n 2 ×m . Применяя соотношения выше к этому набору, получим следующие значения выходов слоёв: =ϕ(H 1 )∈R X∈R n 1 ×m ,X∈R n 0 ×m . Поставим задачу оптимизации квадратичной функции потерь min L(W 0:1 )=∣∣Y− 0:1 min , где ∣∣⋅∣∣ F – норма Фробениуса. Теорема 1 (On the local minima free condition of backpropagation learning) Если ϕ ϕ аналитична, ограничена и не тождественно равна нулю, ширина скрытого слоя n 1 n 1 не меньше m m и все столбцы матрицы X X различны, то все локальные минимумы L(W 0:1 ) глобальны. Доказательство. Пусть 0:1 ∗ – локальный минимум L(W 0:1 ), и пусть – соответствующие ему скрытые представления. Тогда – локальный минимум )=∣∣Y−W Задача оптимизации ) выпуклая, поэтому – глобальный минимум Если rkX 1 ∗ =m, то система 1,i X 1 ∗ , где W 1 , i W 1,i – неизвестная матрица, гарантировано имеет решение для каждого i∈1…,n 2 . Следовательно, min L(W 0:1 ∗ )=L )=minL W 0 ∗ (W 1 )=0, а значит, 0:1 ∗ – глобальный минимум L(W 0:1 ). Заметим, что для выполнения равенства rkX 1 ∗ =m необходимо ≥m. Пусть теперь rkX 1 ∗ <m. Если тем не менее min minL )=0, то 0:1 ∗ – по-прежнему глобальный минимум L(W 0:1 ). Пусть min L(W 0,1 ∗ )=L )=minL W 0 ∗ (W 1 )>0. Докажем, что 0,1 ∗ не может быть локальным минимумом L L до тех пор, пока выполнены условия следующей леммы, которую мы докажем позже: Лемма 1. Если ≥m, функция ϕ ϕ аналитична, ограничена и не тождественно равна нулю, а все столбцы матрицы X X различны, то лебегова мера множества :rkX 1 <m} равна нулю. Так как L(W 0,1 ∗ )>0 и L L – непрерывна как функция от W 0 , 1 W 0,1 , существует ϵ > 0 ϵ>0, для которого 0,1 ∈B ϵ (W 0,1 ∗ ):L(W 0,1 )>0, где через 0,1 ∗ ) мы обозначили ϵ ϵ-окрестность точки 0,1 ∗ в пространстве весов. Из леммы 1 следует, что для",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 2,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "до тех пор, пока выполнены условия следующей леммы, которую мы докажем позже: Лемма 1. Если ≥m, функция ϕ ϕ аналитична, ограничена и не тождественно равна нулю, а все столбцы матрицы X X различны, то лебегова мера множества :rkX 1 <m} равна нулю. Так как L(W 0,1 ∗ )>0 и L L – непрерывна как функция от W 0 , 1 W 0,1 , существует ϵ > 0 ϵ>0, для которого 0,1 ∈B ϵ (W 0,1 ∗ ):L(W 0,1 )>0, где через 0,1 ∗ ) мы обозначили ϵ ϵ-окрестность точки 0,1 ∗ в пространстве весов. Из леммы 1 следует, что для любого δ > 0 δ>0 найдётся ), для которого rkX 1 ′ =m. Возьмём δ∈(0,ϵ). Для соответствующего имеем L(W 0 ′ ,W 1 ∗ )>0; при этом rkX 1 ′ =m. Как было отмечено выше, задача минимизации ) выпуклая, и оптимум её равен нулю, так как rkX 1 ′ =m. Поэтому градиентный спуск, применённый к и стартующий в , сойдётся в некоторую точку ∗,′ , для которой ∗,′ )=0. Мы знаем, что в нашей эпсилон-окрестности функция потерь положительна, значит, найденная точка находится вне её: ∗,′ )∈ / B ϵ (W 0,1 ∗ ). Таким образом, найдётся ϵ > 0 ϵ>0 такое, что для любых δ∈(0,ϵ) существует пара )∈B δ (W 0,1 ∗ ) такая, что градиентный спуск, примененный к L L, стартующий в ) и действующий только на W 1 W 1 , сходится в точку ∗,′ )∈ / B ϵ (W 0,1 ∗ ). Очевидно, что если «для любых δ∈(0,ϵ)» заменить на «для любых δ > 0 δ>0», утверждение выше останется верным. Это означает, что динамика градиентного спуска, действующего только на W 1 W 1 , не устойчива по Ляпунову в точке 0,1 ∗ . Следовательно, 0,1 ∗ не может быть точкой минимума (иначе градиентный спуск был бы устойчив), а значит, условие L(W 0,1 ∗ )>0 невыполнимо в условиях леммы 1. Таким образом, все локальные минимумы L L глобальны. Теорема 1 доказана. Доказательство леммы 1. Пусть I m I m – наборов индексов из 1,…,n 1 длины m m. Рассмотрим 1,I m ∈R m×m – подматрицу матрицы X 1 X 1 , состоящую из строк X 1 X 1 , проиндексированных набором I m I m . В терминах I m I m условие rk X 1 < m rkX 1 <m эквивалентно det detX 1,I Так как ϕ ϕ аналитична, а определитель – аналитическая функция элементов матрицы, det detX 1,I m – аналитическая функция от W 0 W 0 для любого I m I m . Нам понадобится следующая лемма, доказательство которой вы можете найти в The loss surface of deep and wide neural networks (лемма 4.3): Лемма 2. В условиях леммы 1, найдётся W 0 W 0 , для которого rk X 1 = m rkX 1 =m. Из леммы 2 и эквивалентности выше следует, что найдётся W 0 W 0 , такой что для некоторого I m I m имеет место неравенство det detX 1,I m  =0. Так как определитель X 1 , m X",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 3,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "detX 1,I m – аналитическая функция от W 0 W 0 для любого I m I m . Нам понадобится следующая лемма, доказательство которой вы можете найти в The loss surface of deep and wide neural networks (лемма 4.3): Лемма 2. В условиях леммы 1, найдётся W 0 W 0 , для которого rk X 1 = m rkX 1 =m. Из леммы 2 и эквивалентности выше следует, что найдётся W 0 W 0 , такой что для некоторого I m I m имеет место неравенство det detX 1,I m  =0. Так как определитель X 1 , m X 1,m – аналитическая функция W 0 W 0 , а всякая не тождественно нулевая аналитическая функция принимает значение ноль лишь на множестве меры ноль по Лебегу, то лебегова мера множества { W 0 : det :detX 1,I m =0} равна нулю. Таким образом, лемма 1 доказана. Обобщения При доказательстве теоремы 1 мы воспользовались следующими условиями: Все обучающие примеры (столбцы матрицы X X) различны; Число скрытых слоёв L L равно одному; Ширина (последнего) скрытого слоя не меньше числа примеров: ≥m; Функция активации ϕ ϕ аналитична, ограничена и не тождественно равна нулю; Функция ошибки квадратична. Можем ли мы ослабить какие-то из них? Если какие-то из примеров совпадают и соответствующие метки также одинаковы, теорема обобщается тривиально. Если же метки не совпадают, то нулевая ошибка, вообще говоря, недостижима. Тем не менее, доказательство меняется по большому счёту лишь в том, что вместо m m будет фигурировать число различных примеров. Рассмотрим сеть с L L скрытыми слоями, действующую на набор данных X 0 X 0 размера L+1 ×m ,X l =ϕ(H l )∈R l−1 X l−1 ∈R n l ×m ∀l∈[L],X Для обобщения теоремы 1 на глубокие сети с широким последним скрытым слоем, достаточно обобщить лемму 1. Например, можно воспользоваться следующим результатом (лемма 4.4 из The loss surface of deep and wide neural networks) Лемма 3. Пусть ϕ ϕ аналитична, ограничена и не тождественно равна нулю, и пусть l∈1,…,L. Тогда если ≥m и все строки матрицы X 0 X 0 различны, то лебегова мера множества 0:l−1 :rkX l <m} равна нулю. Третье предположение можно попытаться ослабить с двух сторон. Во-первых, можно требовать меньшего числа нейронов в скрытом слое. В общем случае этот подход не работает: в статье A note on connectivity of sublevel sets in deep learning доказывается, что m m – это наименьшая ширина, при которой теорема выполняется для набора данных общего вида с различными примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки. Во-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных: ≥m для некоторого l < L l<L. Но тогда задача 0:l−1 ∗ (W l:L ) не выпукла, а значит, из rkX l ∗ =m не следует, что L(W 0:L",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 4,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "примерами. Тем не менее, для реальных нейронных сетей градиентный спуск нередко находит глобальный минимум, хотя их ширина часто гораздо меньше размера набора данных, на которых они обучаются. Возможно, оценки на минимальную ширину удастся улучшить, если учесть структуру данных: например, если все примеры разбиваются на подмножества с элементами, находящимися близко друг к другу и имеющими одинаковые метки. Во-вторых, можно предположить, что самым широким является не последний скрытый слой, а один из промежуточных: ≥m для некоторого l < L l<L. Но тогда задача 0:l−1 ∗ (W l:L ) не выпукла, а значит, из rkX l ∗ =m не следует, что L(W 0:L ∗ )=0, и градиентный спуск, действующий на W l : L W l:L , не обязан сходиться в точку, в которой L = 0 L=0 (он может застрять в локальном минимуме). Тем не менее, поставив ряд дополнительных условий, теорему 1 можно обобщить: Теорема 2. Пусть 0:L ∗ – локальный минимум L(W 0:L )=∥ Y ^ −Y∥ F 2 и выполнены следующие условия: ϕ ϕ аналитична, ограничена, не тождественно равна нулю; производная ϕ ϕ нигде не обращается в ноль; ≥m; rkW ∈{l+1,…,L}; det det(∇ W l+1:L 2 L(W 0:L ∗ ))  =0. Тогда 0:L ∗ – глобальный минимум L(W 0:L ). Условие 4 необходимо, чтобы из rkX l ∗ =m следовало L(W 0:L ∗ )=0. Отметим, что из условия 4 также следует, что l+1 >L, то есть нейронная сеть должна сужаться, начиная со следующего после самого широкого слоя. Условие 5 необходимо, чтобы в случае rkX l ∗ <m построить малое возмущение минимума 0:L ∗ , которое снова является минимумом, но для которого rk X l = m rkX l =m; невырожденный гессиан позволяет применить для этого теорему об обратной функции. Если функция активации ϕ ϕ не аналитична, то лемма 3 неверна. В самом деле, для однородной ϕ ϕ (например, для ReLU или leaky ReLU) паттерны активаций, ), не меняются при малом возмущении весов. Значит, мы, вообще говоря, не можем найти такое малое возмущение, для которого ранг X 1 X 1 будет полным. Вместо малого возмущения в работе On Connected Sublevel Sets in Deep Learning явно строятся пути в пространстве весов, на которых функция потерь не возрастает и достигает нуля. Если такой путь можно построить из произвольной точки, то все (строгие) локальные минимумы глобальны. Оказывается, что для построения такого пути аналитичность функции активации не требуется. Более того, при определённых условиях можно доказать, что из любых двух точек в пространстве весов можно построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, что множество подуровня ((−∞,E)) связно при любом E > 0 E>0 – эффект, впервые эмпирически обнаруженный в работах Loss surfaces, mode connectivity, and fast ensembling of DNNs и Essentially No Barriers in Neural Network Energy Landscape. Связность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня E > 0 E>0, то для достаточно малого ϵ > 0 ϵ>0 ему бы соответствовала отдельная связная компонента множества подуровня E + ϵ E+ϵ. С другой стороны, если все локальные минимумы глобальны, но изолированы,",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 5,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "построить соответствующие пути так, чтобы они сходились в одной точке. Это значит, что множество подуровня ((−∞,E)) связно при любом E > 0 E>0 – эффект, впервые эмпирически обнаруженный в работах Loss surfaces, mode connectivity, and fast ensembling of DNNs и Essentially No Barriers in Neural Network Energy Landscape. Связность множеств подуровня сильнее глобальности всех строгих минимумов. В самом деле, если бы существовал строгий локальный минимум уровня E > 0 E>0, то для достаточно малого ϵ > 0 ϵ>0 ему бы соответствовала отдельная связная компонента множества подуровня E + ϵ E+ϵ. С другой стороны, если все локальные минимумы глобальны, но изолированы, то множество подуровня ϵ ϵ несвязно для достаточно малого ϵ > 0 ϵ>0. Вместо того, чтобы строить пути, на которых функция потерь достигает нуля, можно строить пути, на которых функция потерь достигает сколь угодно малого значения ϵ > 0 ϵ>0. Это позволяет обобщить результат на функции потерь ℓ(y, y ^ ), для которых минимум по второму аргументу, ответу сети, не достигается. Пример такой функции – кросс-энтропия. Так мы приходим к следующей теореме: Теорема (On Connected Sublevel Sets in Deep Learning). Пусть выполнены следующие условия: ϕ(R)=R, ϕ ϕ строго монотонна и не найдётся ненулевых i=1 =j, таких что ∀ x ∈ R ∀x∈R ϕ(x)=∑ i=1 p λ i ϕ(x−a ℓ(y, y ^ ) выпукла по второму аргументу и inf inf y ^ ℓ(y, y ^ )=0 для любого y y; Существует l∈{1,…,L}, для которого rk X l = m rkX l =m и для всех ∈{l+1,…,L}. Тогда если L(W 0:L )=∑ j=1 m ℓ(y j ,f W 0:L (x j )), то Для каждого ϵ > 0 ϵ>0 найдутся веса W 0 : L W 0:L , для которых L(W 0:L )<ϵ; Множество подуровня ((−∞,E)) связно для каждого E > 0 E>0. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.4. Сети бесконечной ширины Следующий параграф 13.6. Implicit bias",
    "metadata": {
      "title": "Ландшафт функции потерь",
      "url": "https://education.yandex.ru/handbook/ml/article/landshaft-funkcii-poter",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.5",
      "part": 6,
      "total_parts": 6,
      "source_file": "13.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Под термином implicit bias мы будем понимать явление, состоящее в том, что алгоритм обучения среди всех возможных моделей с нулевым эмпирическим риском выбирает определённые. Это явление можно наблюдать уже на очень простом примере. Рассмотрим задачу линейной регрессии с квадратичной функцией потерь. Пусть имеется m m обучающих примеров в евклидовом пространстве размерности N > m N>m. В этом случае наша задача min ⁡ w 2 1 ∥Xw−y∥ 2 2 →min w недоопределена: семейство решений составляет линейное многообразие в пространстве весов. Предположим, что матрица объекты-признаки X∈R m×N имеет полный ранг по строкам: rkX=m<N. Рассмотрим динамику градиентного спуска с шагом η η и нулевой инициализацией весов: k+1 =w k +ηX T (y−Xw k ),w 0 =0. Нам будет удобнее вместо дискретного градиентного спуска рассматривать его непрерывный аналог (y−Xw),w(t)=0, переход к которому соответствует стремлению η η к нулю и выбору параметризации по t t, для которой k=[t/η] (округление берётся в любую сторону). Рассмотрим сингулярное разложение X=UΣV T , где U U и V V ортогональны, а Σ Σ – прямоугольная диагональная матрица. Тогда X=VΣ T ΣV T ∈R N×N rk(X T X)=m. Обозначая w и y=Σ T U T y, получаем: (0)=0. В координатном виде имеем следующее: (0)=0∀i∈[1:m] =0, w ~ i (0)=0∀i∈[m+1:N] так как Σ T Σ Σ T Σ – диагональная матрица, у которой лишь первые m m элементов на диагонали не равны нулю, и у вектора y тоже лишь первые m m координат ненулевые. Так как все σ i σ i для i=1,…,m ненулевые, при t → ∞ t→∞ получаем следующее решение: (∞)= ∀i∈{1,…,m}, w ~ i (∞)=0∀i∈{m+1,…,N}. Это можно записать в эквивалентном матричном виде: (∞)=(Σ y, где «+» обозначает взятие псевдообратной матрицы. Значит, w(∞)=VΣ + U T y=X + y=X T (XX T ) −1 y в силу того, что матрица X X имеет полный ранг по строкам. Покажем теперь, что это частное решение, найденное градиентным спуском с нулевой инициализацией, имеет наименьшую евклидову норму среди всех минимумов функции потерь ∣∣Xw−y∣∣ 2 2 . Рассмотрим соответствующую функцию Лагранжа: L(w;λ)= 2 ∥w∥ 2 2 +λ T (y−Xw). Здесь λ ∈ R m λ∈R m . Покажем, что пара (w(∞),λ ∗ =(XX T ) −1 y) является критической точкой этой функции: L(w(∞);λ ∗ )=y−Xw(∞)=y−XX T (XX T ) −1 y=0; L(w(∞);λ ∗ )=w(∞)−X T λ ∗ =X T ((XX T ) −1 y−(XX T ) −1 y)=0. В силу выпуклости функции Лагранжа эта критическая точка является точкой глобального минимума. Случай линейных сетей Каков implicit bias нейронных сетей? Сходится ли градиентный спуск в решение наименьшей нормы и если да, то о какой норме идёт речь? Частичный ответ на этот вопрос удаётся получить для линейных сетей. Следуя работе Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, рассмотрим линейную сеть с одним скрытым слоем: f(x;W 1:2 )=W 2 W 1 x, где – матрицы n × n n×n. Поставим задачу многомерной (метка y y – вектор) регрессии с квадратичной функцией потерь: min ⁡ W L(W 1:2 )=E x,y ∣∣y−f(x;W 1:2 )∣∣ 2 2 → W min Шаг градиентного спуска выглядит следующим",
    "metadata": {
      "title": "Implicit bias",
      "url": "https://education.yandex.ru/handbook/ml/article/implicit-bias",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.6",
      "part": 1,
      "total_parts": 3,
      "source_file": "13.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "implicit bias нейронных сетей? Сходится ли градиентный спуск в решение наименьшей нормы и если да, то о какой норме идёт речь? Частичный ответ на этот вопрос удаётся получить для линейных сетей. Следуя работе Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, рассмотрим линейную сеть с одним скрытым слоем: f(x;W 1:2 )=W 2 W 1 x, где – матрицы n × n n×n. Поставим задачу многомерной (метка y y – вектор) регрессии с квадратичной функцией потерь: min ⁡ W L(W 1:2 )=E x,y ∣∣y−f(x;W 1:2 )∣∣ 2 2 → W min Шаг градиентного спуска выглядит следующим образом: =ηE x,y W 2 T (yx =ηE x,y (yx где точка над W i W i означает производную по времени (то есть по t t). Это нелинейная система матричных дифференциальных уравнений второго порядка; чтобы проинтегрировать её аналитически, нам придётся сделать ряд предположений. Определим ковариационную матрицу входов =Exx T и матрицу ковариации меток со входами =Eyx T . Предположим, что данные декоррелированы: =I; этого можно добиться, заменив входы x x на x. Что касается матрицы ковариации меток со входами, рассмотрим её сингулярное разложение: 2,0 V 0 T = r=1 Назовём s r s r силой моды с индексом r r ковариации между метками и входами. Сделаем замену координат: В новых координатах градиентный спуск принимает вид: 2,0 =η(S 2,0 Пусть =[a 1 ,…,a n ] и =[b 1 ,…,b n ] T . Тогда в терминах векторов a a и γ=1 )=(s α −(b α T a α ))b γ=1 =(s α −(a α T b α ))a Получилась система векторных дифференциальных уравнений порядка 2 n 2n, всё ещё нелинейная. К счастью, при определённом предположении об инициализации эта система распадается на n n независимых систем порядка 2 2. Предположим, что существует ортогональная матрица R=[r 1 ,…,r n ], такая что при всех α α имеет место равенство (0)= a ~ (0)r (0)= b ~ (0)r α для некоторых скалярных величин (0) и (0). Нетрудно заметить, что в этом случае при всех α α и в любой момент времени t t имеем (t)= a ~ (t)r (t)= b ~ (t)r α для некоторых скалярных величин (t) и (t) . Тогда для различных α α выражения выше становятся независимыми друг от друга: =η(s− =η(s− Теперь это система нелинейных дифференциальных уравнений второго порядка. Если в начальный момент, то это верно и в любой момент времени. Тогда система выше превращается в одно уравнение первого порядка. В самом деле, обозначив , получаем: =2η(s−u)u.(1) Это уравнение задаёт следующую динамику градиентного спуска для функции потерь E(u)= 2 1 (s−u) 2 (её глобальный минимум – это u = s u=s). Перед тем, как интегрировать уравнение (1), напомним, как из u u перейти обратно к исходным . Имеем для ,…, Аналогично для ,…, Теперь проинтегрируем уравнение (1) в предположении, что u(0)=u u(t)=u f для выбранного 2u(s−u) du du= 2sη s−u du )du= 2sη 1 (ln( u 0 u f )−ln( ))= 2sη 1 ln( u 0 (u f −s) u f (u 0 −s) ). Рассмотрим время, необходимое, чтобы выучить",
    "metadata": {
      "title": "Implicit bias",
      "url": "https://education.yandex.ru/handbook/ml/article/implicit-bias",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.6",
      "part": 2,
      "total_parts": 3,
      "source_file": "13.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В самом деле, обозначив , получаем: =2η(s−u)u.(1) Это уравнение задаёт следующую динамику градиентного спуска для функции потерь E(u)= 2 1 (s−u) 2 (её глобальный минимум – это u = s u=s). Перед тем, как интегрировать уравнение (1), напомним, как из u u перейти обратно к исходным . Имеем для ,…, Аналогично для ,…, Теперь проинтегрируем уравнение (1) в предположении, что u(0)=u u(t)=u f для выбранного 2u(s−u) du du= 2sη s−u du )du= 2sη 1 (ln( u 0 u f )−ln( ))= 2sη 1 ln( u 0 (u f −s) u f (u 0 −s) ). Рассмотрим время, необходимое, чтобы выучить фиксированную долю силы данной моды =ξs, где ξ∈(0,1), стартуя из точки из окрестности нуля =ϵ. Оно равняется (ξ) = 2sη 1 ln( 1−ξ ξ ϵ s−ϵ )= 2sη 1 (ln(s/ϵ−1)−ln(ξ −1 −1)). Видим, что чем сильнее мода (то есть чем больше s s), тем быстрее она сходится. Рассмотрим две моды с силами , такие что . Насколько вторая (более слабая) мода выучится к моменту, когда первая уже выучится на долю ξ ξ? Из уравнения выше имеем: −s) u f (u 0 −s) =e 2sηt =(1− u 0 s )e −2sηt . Подставляя s = s 2 s=s t=t 1 (ξ) , получаем: =(1− ϵ s 2 )e −2s 2 ηt 1 (ξ) =(1− (ln(s/ϵ−1)−ln(ξ −1 −1)) 1−ξ =−s 1−ξ Поскольку , это выражение стремится к минус бесконечности при ϵ → 0 ϵ→0, из чего следует, что u f u f стремится к нулю. Это означает, что если веса в инициализации лежат в окрестности нуля, то к моменту, когда данная мода выучивается на любую фиксированную долю ξ∈(0,1), более слабые моды не успевают выучиться вообще. Таким образом, в любой момент времени t t матрица (t)W 1 (t) является наилучшим малоранговым приближением заданного ранга матрицы корреляций Σ y x Σ yx , причём чем больше t t, тем больше ранг. Можно сказать, что градиентный спуск с фиксированным числом шагов «предпочитает» решения малого ранга. В выводе выше, мы использовали ряд предположений, в частности, что вектора a 1 : n a 1:n , образующие матрицу W 1 ‾ W 1 , ортогональны в инициализации. Эмпирически те же выводы оказываются верными и без этого предположения, см. графики в оригинальной работе Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. Можно ли их обосновать строго математически? В работах Towards resolving the implicit bias of gradient descent for matrix factorization и Deep Linear Networks Dynamics доказывается, что самая сильная мода выучивается в первую очередь. Тем не менее, на момент написания этого текста остаётся недоказанным, что все моды выучиваются последовательно от сильных к слабым. К сожалению, implicit bias градиентного спуска для нелинейных сетей пока остаётся почти неизученным. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.5. Ландшафт функции потерь Следующий параграф 14.1. Оптимизация в ML Как найти оптимум функции потерь: от градиентного спуска до Adam",
    "metadata": {
      "title": "Implicit bias",
      "url": "https://education.yandex.ru/handbook/ml/article/implicit-bias",
      "course": "ml",
      "chapter": "13. Теория глубокого обучения",
      "chapter_id": "13.6",
      "part": 3,
      "total_parts": 3,
      "source_file": "13.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как найти оптимум функции потерь: от градиентного спуска до Adam Введение Зачастую задачи машинного обучения формулируются таким образом, что «веса» модели, которую мы строим, возникают, как решение оптимизационной задачи. В качестве VIP-примера рассмотрим задачу линейной регрессии: min ⁡ w , ∥y−Xw∥ 2 2 → w min , По сути, мы получили чистейшую задачу квадратичной оптимизации. В чем особенность конкретно этой задачи? Она выпуклая. Важное свойство выпуклых функций – локальный минимум автоматически является глобальным (но не обязательно единственным!). Это позволяет избегать уродливых ситуаций, которые с теоретической точки зрения могут встретиться в невыпуклом случае, например, вот такой: Теорема (No free lunch theorem) Пусть A A – алгоритм оптимизации, использующий локальную информацию (все производные в точке). Тогда существует такая невыпуклая функция f:[0,1] d →[0,1], что для нахождения глобального минимума на квадрате [0,1] d с точностью 1 m m 1 требуется совершить хотя бы m d m d шагов. Мы видим, что в общем случае без выпуклости нас ожидает полное разочарование. Ничего лучше перебора по сетке придумать в принципе невозможно. В выпуклом случае же существуют алгоритмы, которые находят глобальный минимум за разумное время. Встречаются ли в жизни функции невыпуклые? Повсеместно! Например, функция потерь при обучении нейронных сетей, как правило, не является выпуклой. Но отсюда не следует, что любой алгоритм их оптимизации будет обязательно неэффективным: ведь «контрпример» из теоремы довольно специфичен. И, как мы увидим, оптимизировать невыпуклые функции очень даже возможно. Найти глобальный минимум невыпуклой функции – очень трудная задача, но зачастую нам хватает локального, который является, в частности, стационарной точкой: такой, в которой производная равна нулю. Все теоретические результаты в случае невыпуклых задач, как правило, касаются поиска таких точек, и алгоритмы тоже направлены на их отыскание. Этим объясняется и то, что большинство алгоритмов оптимизации, придуманных для выпуклого случая, дословно перешли в невыпуклый. Теоретическая причина в следующем: в выпуклом случае поиск стационарной точки и поиск минимума – буквально одна и та же задача, поэтому то, что хорошо ищет минимум в выпуклом случае, ожидаемо будет хорошо искать стационарные точки в невыпуклом. Практическая же причина в том, что оптимизаторы в библиотеках никогда не спрашивают, выпуклую ли им функцию подают на вход, а просто работают и работают хорошо. Внимательный читатель мог возразить на моменте подмены задачи: подождите-ка, мы ведь хотим сделать функцию как можно меньше, а не стационарную точку искать какую-то непонятную. Доказать в невыпуклом случае тут, к сожалению, ничего невозможно, но на практике мы снова используем алгоритмы изначально для выпуклой оптимизации. Почему? Причина номер 1: сойтись в локальный минимум лучше, чем никуда. Об этом речь уже шла. Причина номер 2: в окрестности локального минимума функция становится выпуклой, и там мы сможем быстро сойтись. Причина номер 3: иногда невыпуклая функция является в некотором смысле «зашумленной» версией выпуклой или похожей на выпуклую. Например, посмотрите на эту картинку (функция Леви): 21 У этой функции огромное количество локальных минимумов, но «глобально» она кажется выпуклой. Что-то отдаленно похожее наблюдается и в случае нейронных сетей. Нашей задачей становится не скатиться в маленький локальный минимум, который всегда рядом с нами, а в большую-большую ложбину, где значение функции минимально и в некотором смысле стабильно. Причина",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 1,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "локальный минимум лучше, чем никуда. Об этом речь уже шла. Причина номер 2: в окрестности локального минимума функция становится выпуклой, и там мы сможем быстро сойтись. Причина номер 3: иногда невыпуклая функция является в некотором смысле «зашумленной» версией выпуклой или похожей на выпуклую. Например, посмотрите на эту картинку (функция Леви): 21 У этой функции огромное количество локальных минимумов, но «глобально» она кажется выпуклой. Что-то отдаленно похожее наблюдается и в случае нейронных сетей. Нашей задачей становится не скатиться в маленький локальный минимум, который всегда рядом с нами, а в большую-большую ложбину, где значение функции минимально и в некотором смысле стабильно. Причина номер 4: оказывается, что градиентные методы весьма часто сходятся именно к локальным минимумам. Сразу отметим важную разницу между выпуклой и невыпуклой задачами: в выпуклом случае работа алгоритма оптимизации не очень существенно зависит от начальной точки, поскольку мы всегда скатимся в точку оптимума. В невыпуклом же случае правильно выбранная точка старта – это уже половина успеха. Теперь перейдём к разбору важнейших алгоритмов оптимизации. Градиентный спуск (GD) Опишем самый простой метод, который только можно придумать – градиентный спуск. Для того, чтобы его определить, вспомним заклинание из любого курса матанализа: «градиент – это направление наискорейшего локального возрастания функции», тогда антиградиент – это направление наискорейшего локального убывания. Тогда пусть x 0 x 0 – начальная точка градиентного спуска. Тогда каждую следующую точку мы выбираем следующим образом: k+1 =x k −α∇f(x k ), где α α – это размер шага (он же learning rate). Общий алгоритм градиентного спуска пишется крайне просто и элегантно: x = normal(0, 1) # можно пробовать и другие виды инициализации repeat S times: # другой вариант: while abs(err) > tolerance h = grad_f(x) # вычисляем направление спуска x -= alpha * h # обновляем значение в точке Эту схему в приложении к линейной регрессии можно найти в параграфе про линейные модели. После всего этого начинаются тонкости: А как вычислять градиент? А как выбрать размер шага? А есть ли какие-то теоретические оценки сходимости? Начнем разбирать вопросы постепенно. Для вычисления градиентов современный человек может использовать инструменты автоматического дифференцирования. Идейно, это вариация на тему алгоритма обратного распространения ошибки (backpropagation), ведь как правило человек задает функции, составленные из элементарных при помощи умножений/делений/сложений/композиций. Такой метод реализован во всех общих фреймворках для нейронных сетей (Tensorflow, PyTorch, Jax). Но, вообще говоря, возникает некоторая тонкость. Например, расмотрим задачу линейной регрессии. Запишем её следующим образом: f(w)= N 1 i=1 Видим, что слагаемых суммарно N N – размер выборки. При N N порядка d (это количество признаков) порядка 1 0 4 10 4 вычисление градиента за O ( N d ) O(Nd) становится жутким мучением. Но если от d d избавиться без дополнительных предположений (например, о разреженности) нельзя, то с зависимостью от N N в каком-то смысле удастся разделаться при помощи метода стохастического градиентного спуска. Хранение градиентов тоже доставит нам проблемы. У градиента столько же компонент, сколько параметров у модели, и если мы имеем дело с глубокой нейросетью, это даст значительные затраты дополнительной памяти. Хуже того, метод обратного распространения ошибки устроен так, что нам приходится помнить все промежуточные представления для вычисления",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 2,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "N порядка d (это количество признаков) порядка 1 0 4 10 4 вычисление градиента за O ( N d ) O(Nd) становится жутким мучением. Но если от d d избавиться без дополнительных предположений (например, о разреженности) нельзя, то с зависимостью от N N в каком-то смысле удастся разделаться при помощи метода стохастического градиентного спуска. Хранение градиентов тоже доставит нам проблемы. У градиента столько же компонент, сколько параметров у модели, и если мы имеем дело с глубокой нейросетью, это даст значительные затраты дополнительной памяти. Хуже того, метод обратного распространения ошибки устроен так, что нам приходится помнить все промежуточные представления для вычисления градиентов. Поэтому вычислить градиент целиком невозможно ни для какой нормальной нейросети, и от этой беды тоже приходится спасаться с помощью стохастического градиентного спуска. Теперь перейдем к размеру шага. Теория говорит о том, что если функция гладкая, то можно брать достаточно маленький размер шага, где под достаточно маленьким подразумевается , где L L – некоторая константа, которая зависит от гладкости задачи (так называемая константа Липшица). Вычисление этой константы может быть задачей сложнее, чем изначальная задача оптимизации, поэтому этот вариант нам не годится. Более того, эта оценка крайне пессимистична – мы ведь хотим размер шага как можно больше, чтобы уменьшить функцию как можно больше, а тут мы будем изменять все очень мало. Существует так называемый метод наискорейшего спуска: выбираем размер шага так, чтобы как можно сильнее уменьшить функцию: α k = arg ⁡ min =arg α≥0 min f(x k −α∇f(x k )). Одномерная оптимизация является не сильно сложной задачей, поэтому теоретически мы можем её совершать (например, методом бинарного/тернарного поиска или золотого сечения), можно этот шаг также совершать неточно. Но сразу стоит заметить, что это можно делать, только если функция f f вычислима более-менее точно за разумное время, в случае линейной регрессии это уже не так (не говоря уже о нейронных сетях). Также есть всевозможные правила Армихо/Гольдштейна/Вульфа и прочее и прочее, разработанные в давние 60-е, и для их проверки требуется снова вычислять значения функции в точке. Желающие могут посмотреть на эти условия на википедии. Про более хитрые вариации выбора шагов мы поговорим позже, но сразу стоит сказать, что эта задача довольно сложная. По поводу теории: сначала скажем что-то про выпуклый случай. В максимально общем выпуклом случае без дополнительных предположений оценки для градиентного спуска крайне и крайне пессимистичные: чтобы достичь качества ε ε, то есть ∣f(x k )−f(x ∗ )∣≤ε достаточно сделать O(R 2 /ε 2 ) шагов, где R 2 R 2 — это расстояние от . Выглядит очень плохо: ведь чтобы достичь точности , необходимо сделать порядка 1 0 4 10 4 шагов градиентного спуска. Но на практике такого не происходит, потому что на самом деле верны разные предположения, дающие более приятные свойства. Для контраста, укажем оценку в случае гладкой и сильно выпуклой в точке оптимума функции: за k k шагов будет достигнута точность O ( min ⁡ { R 2 exp O(min{R 2 exp(− }), где κ κ – это так называемое число обусловленности задачи. По сути, это число измеряет, насколько линии уровня функции вытянуты в окрестности оптимума. Морали две: Скорость",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 3,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "это расстояние от . Выглядит очень плохо: ведь чтобы достичь точности , необходимо сделать порядка 1 0 4 10 4 шагов градиентного спуска. Но на практике такого не происходит, потому что на самом деле верны разные предположения, дающие более приятные свойства. Для контраста, укажем оценку в случае гладкой и сильно выпуклой в точке оптимума функции: за k k шагов будет достигнута точность O ( min ⁡ { R 2 exp O(min{R 2 exp(− }), где κ κ – это так называемое число обусловленности задачи. По сути, это число измеряет, насколько линии уровня функции вытянуты в окрестности оптимума. Морали две: Скорость сходимости градиентного спуска сильно зависит от обусловленности задачи; Также она зависит от выбора хорошей точки старта, ведь везде входит расстояние от точки старта до оптимума. В качестве ссылки на доказательство укажем на работу Себастиана Стича, где оно довольно простое и общее. В невыпуклом же случае все куда хуже с точки зрения теории: требуется порядка O(1/ε 2 ) шагов в худшем случае даже для гладкой функции, где ε ε – желаемая точность уменьшения нормы градиента. Стохастический градиентный спуск (SGD) Теперь мы попробуем сэкономить в случае регрессии и подобных ей задач. Будем рассматривать функционалы вида f(x)= i=1 ∑ N L(x,y i ), где сумма проходится по всем объектам выборки (которых может быть очень много). Теперь сделаем следующий трюк: заметим, что это усреднение – это по сути взятие матожидания. Таким образом, мы говорим, что наша функция выглядит как f(x)=E[L(x,ξ)], где ξ ξ равномерно распределена по обучающей выборке. Задачи такого вида возникают не только в машинном обучении; иногда встречаются и просто задачи стохастического программирования, где происходит минимизация матожидания по неизвестному (или слишком сложному) распределению. Для функционалов такого вида мы также можем посчитать градиент, он будет выглядеть довольно ожидаемо: ∇f(x)=E∇L(x,ξ). Будем считать, что вычисление матожидания напрямую невозможно. Новый взгляд из статистики дает возможность воспользоваться классическим трюком: давайте подменим матожидание на его несмещенную Монте-Карло оценку. Получается то, что можно назвать стохастическим градиентом: f(x)= B 1 i=1 ∑ B ∇L(x,ξ i ). Говоря инженерным языком, мы подменили вычисление градиента по всей выборке вычислением по случайной подвыборке. Подвыборку ,…,ξ B часто называют (мини)батчем, а число B B – размером батча. По-хорошему, наука предписывает нам каждый раз независимо генерировать батчи, но это трудно с вычислительной точки зрения. Вместо этого воспользуемся следующим приёмом: сначала перемешаем нашу выборку (чтобы внести дополнительную случайность), а затем будем рассматривать последовательно блоки по B B элементов выборки. Когда мы просмотрели всю выборку – перемешиваем еще раз и повторяем проход. Очередной прогон по обучающей выборке называется эпохой. И, хотя, казалось бы, независимо генерировать батчи лучше, чем перемешивать лишь между эпохами, есть несколько результатов, демонстрирующих обратное: одна работа и вторая (более новая); главное условие успеха – правильно изменяющийся размер шага. Получаем следующий алгоритм, называемый стохастическим градиентным спуском (stochastic gradient descent, SGD): x = normal(0, 1) # инициализация repeat E times: # цикл по количеству эпох for i = 0; i <= N; i += B: batch = data[i:i+B] h = grad_loss(batch).mean() # вычисляем оценку градиента как среднее по батчу x -= alpha * h Дополнительное удобство такого",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 4,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "раз и повторяем проход. Очередной прогон по обучающей выборке называется эпохой. И, хотя, казалось бы, независимо генерировать батчи лучше, чем перемешивать лишь между эпохами, есть несколько результатов, демонстрирующих обратное: одна работа и вторая (более новая); главное условие успеха – правильно изменяющийся размер шага. Получаем следующий алгоритм, называемый стохастическим градиентным спуском (stochastic gradient descent, SGD): x = normal(0, 1) # инициализация repeat E times: # цикл по количеству эпох for i = 0; i <= N; i += B: batch = data[i:i+B] h = grad_loss(batch).mean() # вычисляем оценку градиента как среднее по батчу x -= alpha * h Дополнительное удобство такого подхода – возможность работы с внешней памятью, ведь выборка может быть настолько большой, что она помещается только на жёсткий диск. Сразу отметим, что в таком случае B B стоит выбирать достаточно большим: обращение к данным с диска всегда медленнее, чем к данным из оперативной памяти, так что лучше бы сразу забирать оттуда побольше. Поскольку стохастические градиенты являются лишь оценками истинных градиентов, SGD может быть довольно шумным: 21 Поэтому если вы обучаете глубокую нейросеть и у вас в память влезает лишь батч размером с 2-4 картинки, модель, возможно, ничего хорошего не сможет выучить. Аппроксимация градиента и поведение SGD может стать лучше с ростом размера батча B B – и обычно его действительно хочется подрастить, но парадоксальным образом слишком большие батчи могут порой испортить дело (об этом дальше в этом параграфе!). Теоретический анализ Теперь перейдем к теоретической стороне вопроса. Сходимость SGD обеспечивается несмещенностью стохастического градиента. Несмотря на то, что во время итераций копится шум, суммарно он зачастую оказывается довольно мал. Теперь приведем оценки. Сначала, по традиции, в выпуклом случае. Для выпуклой функции потерь за k k шагов будет достигнута точность порядка O ( min ⁡ { R 2 exp O(min{R 2 exp(− }), где σ 2 σ 2 – это дисперсия стохградиента, а μ μ – константа сильной выпуклости, показывающая, насколько функция является «не плоской» в окрестности точки оптимума. Доказательство в том же препринте С. Стича. Мораль в следующем: дисперсия стохастического градиента, вычисленного по батчу размера B B равна /B, где – это дисперсия одного градиента. То есть увеличение размера батча помогает и с теоретической точки зрения. В невыпуклом случае оценка сходимости SGD просто катастрофически плохая: требуется O(1/ε 4 ) шагов для того, чтобы сделать норму градиента меньше ε ε. В теории есть всевозможные дополнительные способы снижения дисперсии с лучшими теоретическими оценками (Stochastic Variance Reduced Gradient (SVRGD), Spider, etc), но на практике они активно не используются. Использование дополнительной информации о функции Методы второго порядка Основной раздел. Постараемся усовершенствовать метод стохастического градиентного спуска. Сначала заметим, что мы используем явно не всю информацию об оптимизируемой функции. Вернемся к нашему VIP-примеру линейной регресии с ℓ 2 ℓ 2 регуляризацией: min ⁡ w . ∥y−Xw∥ 2 2 +λ∥w∥ 2 2 → w min . Эта функция достаточно гладкая, и может быть неплохой идеей использовать её старшие производные для ускорения сходимости алгоритма. В наиболее чистом виде этой философии следует метод Ньютона и подобные ему; о них вы можете прочитать в соответствующем разделе. Отметим, что все такие методы,",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 5,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "они активно не используются. Использование дополнительной информации о функции Методы второго порядка Основной раздел. Постараемся усовершенствовать метод стохастического градиентного спуска. Сначала заметим, что мы используем явно не всю информацию об оптимизируемой функции. Вернемся к нашему VIP-примеру линейной регресии с ℓ 2 ℓ 2 регуляризацией: min ⁡ w . ∥y−Xw∥ 2 2 +λ∥w∥ 2 2 → w min . Эта функция достаточно гладкая, и может быть неплохой идеей использовать её старшие производные для ускорения сходимости алгоритма. В наиболее чистом виде этой философии следует метод Ньютона и подобные ему; о них вы можете прочитать в соответствующем разделе. Отметим, что все такие методы, как правило, довольно дорогие (исключая L-BFGS), и при большом размере задачи и выборки ничего лучше вариаций SGD не придумали. Проксимальные методы Основной раздел. К сожалению, не всегда функции такие красивые и гладкие. Для примера рассмотрим Lasso-регресию: min ⁡ w . ∥y−Xw∥ 2 2 +λ∥w∥ 1 → w min . Второе, не гладкое слагаемое резко ломает все свойства этой задачи: теоретически оценки для градиентного спуска становятся гораздо хуже (и на практике тоже). С другой стороны, регуляризационное слагаемое устроено очень просто, и эту дополнительную структурную особенность можно и нужно эксплуатировать. Методы решения задачи вида min ⁡ x , f(x)+h(x)→ x min , где h h – простая функция (в некотором смысле), а f f – гладкая, называются методами композитной оптимизации. Глубже погрузиться в них можно в соответствующем разделе, посвященном проксимальным методам. Использование информации о предыдущих шагах Следующая претензия к методу градиентного спуска – мы не используем информацию о предыдущих шагах, хотя, кажется, там может храниться что-то полезное. Метод инерции, momentum Начнем с физической аналогии. Представим себе мячик, который катится с горы. В данном случае гора – это график функции потерь в пространстве параметров нашей модели, а мячик – её текущее значение. Реальный мячик не застрянет перед небольшой кочкой, так как у него есть некоторая масса и уже накопленный импульс – некоторое время он способен двигаться даже вверх по склону. Аналогичный прием может быть использован и в градиентной оптимизации. В англоязычной литературе он называется Momentum. 21 С математической точки зрения, мы добавляем к градиентному шагу еще одно слагаемое: k+1 =x k −α k ∇f(x k )+β k (x k −x k−1 ). Сразу заметим, что мы немного усугубили ситуацию с подбором шага, ведь теперь нужно подбирать не только α k α k , но и β k β k . Для обычного, не стохастического градиентного спуска мы можем адаптировать метод наискорейшего и получить метод тяжелого шарика: arg ⁡ min )=arg α,β min f(x k −α∇f(x k )+β(x k −x k−1 )). Но, увы, для SGD это работать не будет. Выгода в невыпуклом случае от метода инерции довольно понятна – мы будем пропускать паразитные локальные минимумы и седла и продолжать движение вниз. Но выгода есть также и в выпуклом случае. Рассмотрим плохо обусловленную квадратичную задачу, для которой линии уровня оптимизируемой функции будут очень вытянутыми эллипсами, и запустим на SGD с инерционным слагаемым и без него. Направление градиента будет иметь существенную вертикальную компоненту, а добавление инерции как раз «погасит» паразитное направление. Получаем следующую картинку: 21",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 6,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "метод тяжелого шарика: arg ⁡ min )=arg α,β min f(x k −α∇f(x k )+β(x k −x k−1 )). Но, увы, для SGD это работать не будет. Выгода в невыпуклом случае от метода инерции довольно понятна – мы будем пропускать паразитные локальные минимумы и седла и продолжать движение вниз. Но выгода есть также и в выпуклом случае. Рассмотрим плохо обусловленную квадратичную задачу, для которой линии уровня оптимизируемой функции будут очень вытянутыми эллипсами, и запустим на SGD с инерционным слагаемым и без него. Направление градиента будет иметь существенную вертикальную компоненту, а добавление инерции как раз «погасит» паразитное направление. Получаем следующую картинку: 21 Также удобно бывает представить метод моментума в виде двух параллельных итерационных процессов: k+1 x k+1 ∇f(x k ) =x k +v k+1 . Accelerated Gradient Descent (Nesterov Momentum) Рассмотрим некоторую дополнительную модификацию, которая была предложена в качестве оптимального метода первого порядка для решения выпуклых оптимизационных задач. Можно доказать, что в сильно выпуклом и гладком случае найти минимум с точностью ε ε нельзя быстрее, чем за Ω ( R 2 exp Ω(R 2 exp(− κ k )) итераций, где κ κ – число обусловленности задачи. Напомним, что для обычного градиентного спуска в экспоненте у нас был не корень из κ κ, а просто κ κ, то есть, градиентный спуск справляется с плохой обусловленностью задачи хуже, чем мог бы. В 1983 году Ю.Нестеровым был предложен алгоритм, имеющий оптимальную по порядку оценку. Для этого модифицируем немного моментум и будем считать градиент не в текущей точке, а как бы в точке, в которую мы бы пошли, следуя импульсу: k+1 x k+1 ∇f(x k+1 Сравним с обычным momentum: 21 Комментарий: иногда упоминается, что Nesterov Momentum «заглядывает в будущее» и исправляет ошибки на данном шаге оптимизации. Конечно, никто не заглядывает в будущее в буквальном смысле. В работе Нестерова были предложены конкретные (и довольно магические) константы для импульса, которые получаются из некоторой еще более магической последовательности. Мы приводить их не будем, поскольку мы в первую очередь заинтересованы невыпуклым случаем. Nesterov Momentum позволяет значительно повысить устойчивость и скорость сходимости в некоторых случаях. Но, конечно, он не является серебряной пулей в задачах оптимизации, хотя в выпуклом мире и является теоретически неулучшаемым. Также отметим, что ускоренный метод может напрямую примениться к проксимальному градиентному спуску. В частности, применение ускоренного метода к проксимальному алгоритму решения ℓ 1 ℓ 1 регрессии (ISTA) называется FISTA (Fast ISTA). Общие выводы: Добавление momentum к градиентному спуску позволяет повысить его устойчивость и избегать маленьких локальных минимумов/максимумов; В выпуклом случае добавление моментного слагаемого позволяет доказуемо улучшить асимптотику и уменьшить зависимость от плохой обусловленности задачи. Идея ускорения применяется к любым около-градиентным методам, в том числе и к проксимальным, позволяя получить, например, ускоренный метод для ℓ 1 ℓ 1 -регрессии. Адаптивный подбор размера шага Выше мы попытались эксплуатировать свойства градиентного спуска. Теперь же пришел момент взяться за больной вопрос: как подбирать размер шага? Он максимально остро встаёт в случае SGD: ведь посчитать значение функции потерь в точке очень дорого, так что методы в духе наискорейшего спуска нам не помогут! Нужно действовать несколько хитрее. Adagrad Рассмотрим первый алгоритм, который является адаптацией",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 7,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "выпуклом случае добавление моментного слагаемого позволяет доказуемо улучшить асимптотику и уменьшить зависимость от плохой обусловленности задачи. Идея ускорения применяется к любым около-градиентным методам, в том числе и к проксимальным, позволяя получить, например, ускоренный метод для ℓ 1 ℓ 1 -регрессии. Адаптивный подбор размера шага Выше мы попытались эксплуатировать свойства градиентного спуска. Теперь же пришел момент взяться за больной вопрос: как подбирать размер шага? Он максимально остро встаёт в случае SGD: ведь посчитать значение функции потерь в точке очень дорого, так что методы в духе наискорейшего спуска нам не помогут! Нужно действовать несколько хитрее. Adagrad Рассмотрим первый алгоритм, который является адаптацией стохастического градиентного спуска. Впервые он предложен в статье в JMLR 2011 года, но она написана в очень широкой общности, так что читать её достаточно сложно. Зафиксируем α α – исходный learning rate. Затем напишем следующую формулу обновления: k+1 x k+1 =G k +(∇f(x k+1 +ε α ∇f(x k ). Возведение в квадрат и деления векторов покомпонентные. По сути, мы добавляем некоторую квазиньютоновость и начинаем динамически подбирать размер шага для каждой координаты по отдельности. Наш размера шага для фиксированной координаты – это какая-то изначальная константа α α (learning rate), деленная на корень из суммы квадратов координат градиентов плюс дополнительный параметр сглаживания ε ε, предотвращающий деление на ноль. Добавка ε ε на практике оставляется дефолтными 1e-8 и не изменяется. Идея следующая: если мы вышли на плато по какой-то координате и соответствующая компонента градиента начала затухать, то нам нельзя уменьшать размер шага слишком сильно, поскольку мы рискуем на этом плато остаться, но в то же время уменьшать надо, потому что это плато может содержать оптимум. Если же градиент долгое время довольно большой, то это может быть знаком, что нам нужно уменьшить размер шага, чтобы не пропустить оптимум. Поэтому мы стараемся компенсировать слишком большие или слишком маленькие координаты градиента. Но довольно часто получается так, что размер шага уменьшается слишком быстро и для решения этой проблемы придумали другой алгоритм. RMSProp Модифицируем слегка предыдущую идею: будем не просто складывать нормы градиентов, а усреднять их в скользящем режиме: k+1 x k+1 =γG k +(1−γ)(∇f(x k+1 +ε α ∇f(x k ). Такой выбор позволяет все еще учитывать историю градиентов, но при этом размер шага уменьшается не так быстро. Общие выводы: Благодаря адаптивному подбору шага в современных оптимизаторах не нужно подбирать последовательность α k α k размеров всех шагов, а достаточно выбрать всего одно число – learning rate α α, всё остальное сделает за вас сам алгоритм. Но learning rate все еще нужно выбирать крайне аккуратно: алгоритм может либо преждевременно выйти на плато, либо вовсе разойтись. Пример приведен на иллюстрации ниже. 21 Объединяем все вместе... Adam Теперь покажем гвоздь нашей программы: алгоритм Adam, который считается решением по умолчанию и практически серебряной пулей в задачах стохастической оптимизации. Название Adam = ADAptive Momentum намекает на то, что мы объединим идеи двух последних разделов в один алгоритм. Приведем его алгоритм, он будет немного отличаться от оригинальной статьи отсутствием коррекций смещения (bias correction), но идея останется той же самой: k+1 G k+1 x k+1 =β 1 v k +(1−β 1 )∇f(x +(1−β 2",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 8,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "rate все еще нужно выбирать крайне аккуратно: алгоритм может либо преждевременно выйти на плато, либо вовсе разойтись. Пример приведен на иллюстрации ниже. 21 Объединяем все вместе... Adam Теперь покажем гвоздь нашей программы: алгоритм Adam, который считается решением по умолчанию и практически серебряной пулей в задачах стохастической оптимизации. Название Adam = ADAptive Momentum намекает на то, что мы объединим идеи двух последних разделов в один алгоритм. Приведем его алгоритм, он будет немного отличаться от оригинальной статьи отсутствием коррекций смещения (bias correction), но идея останется той же самой: k+1 G k+1 x k+1 =β 1 v k +(1−β 1 )∇f(x +(1−β 2 )(∇f(x k+1 +ε α v k+1 . Как правило, в этом алгоритме подбирают лишь один гиперпараметр α α – learning rate. Остальные же: ε – оставляют стандартными и равными 0.9, 0.99 и 1e-8 соответственно. Подбор α α составляет главное искусство. Зачастую, при начале работы с реальными данными начинают со значения learning rate равного 3e-4. История данного значения достаточно забавна: в 2016 году Андрей Карпатый (Andrej Karpathy) опубликовал шутливый пост в Twitter. 21 После чего сообщество подхватило эту идею (до такой степени, что иногда число 3e-4 называют Karpathy constant). Обращаем ваше внимание, что при работе с учебными данными зачастую полезно выбирать более высокий (на 1-2 порядка) начальный learning rate (например, при классификации MNIST, Fashion MNIST, CIFAR или при обучении языковой модели на примере поэзии выбранного поэта). Также стоит помнить, что Adam требует хранения как параметров модели, так и градиентов, накопленного импульса и нормировочных констант (cache). Т.е. достижение более быстрой (с точки зрения количества итераций/объема рассмотренных данных) сходимости требует больших объемов памяти. Кроме того, если вы решите продолжить обучение модели, остановленное на некоторой точке, необходимо восстановить из чекпоинта не только веса модели, но и накопленные параметры Adam. В противном случае оптимизатор начнёт сбор всех своих статистик с нуля, что может сильно сказаться на качестве дообучения. То же самое касается вообще всех описанных выше методов, так как каждый из них накапливает какие-то статистики во время обучения. Интересный факт: Adam расходится на одномерном контрпримере, что совершенно не мешает использовать его для обучения нейронных сетей. Этот факт отлично демонстрирует, насколько расходятся теория и практика в машинном обучении. В той же работе предложено исправление этого недоразумения, но его активно не применяют и продолжают пользоваться «неправильным» Adamом потому что он быстрее сходится на практике. AdamW А теперь давайте добавим ℓ 2 ℓ 2 -регуляризацию неявным образом, напрямую в оптимизатор и минуя адаптивный размер шага: k+1 G k+1 x k+1 =β 1 v k +(1−β 1 )∇f(x +(1−β 2 )(∇f(x k+1 +ε α v k+1 +λx k ). Это сделано для того, чтобы эффект ℓ 2 ℓ 2 -регуляризации не затухал со временем и обобщающая способность модели была выше. Оставим ссылку на одну заметку про этот эффект. Отметим, впрочем, что этот алгоритм особо не используется. Практические аспекты Расписания Часто learning rate понижают итеративно: каждые условные 5 эпох (LRScheduler в Pytorch) или же при выходе функции потерь на плато. При этом лосс нередко ведет себя следующим схематичным образом: 21 Помимо этого используют другие варианты «расписаний» для learning rate. Из",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 9,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "x k+1 =β 1 v k +(1−β 1 )∇f(x +(1−β 2 )(∇f(x k+1 +ε α v k+1 +λx k ). Это сделано для того, чтобы эффект ℓ 2 ℓ 2 -регуляризации не затухал со временем и обобщающая способность модели была выше. Оставим ссылку на одну заметку про этот эффект. Отметим, впрочем, что этот алгоритм особо не используется. Практические аспекты Расписания Часто learning rate понижают итеративно: каждые условные 5 эпох (LRScheduler в Pytorch) или же при выходе функции потерь на плато. При этом лосс нередко ведет себя следующим схематичным образом: 21 Помимо этого используют другие варианты «расписаний» для learning rate. Из часто применяемых неочевидных лайфхаков: сначала сделать warmup, то есть увеличивать learning rate, а затем начать постепенно понижать. Использовалось в известной статье про трансформеры. В ней предложили следующую формулу: 0.5 ⋅ min 0.5 1.5 ) . lr=d model −0.5 ⋅min(step_num −0.5 ,step_num⋅warmup_steps −1.5 ). По сути, первые warmup_steps шагов происходит линейный рост размера шага, а затем он начинает уменьшаться как 1 / t 1/ t , где t t — число итераций. Есть и вариант с косинусом из отдельной библиотеки для трансформеров. 21 В этой же библиотеке можно также почерпнуть идею рестартов: с какого-то момента мы снова включаем warmup, увеличивая размер шага. Большие батчи Представим ситуацию, что мы хотим обучить свою нейронную сеть на нескольких GPU. Одно из решений выглядит следующим образом: загружаем на каждую видеокарту нейронную сеть и свой отдельный батч, вычисляем стохастические градиенты, а затем усредняем их по всем видеокартам и делаем шаг. Что плохого может быть в этом? По факту, эта схема в некотором смысле эквивалентна работе с одним очень большим батчем. Хорошо же, нет разве? На самом деле существует так называемый generalization gap: использование большого размера батча может приводить к худшей обобщающей способности итоговой модели. О причине этого эффекта можно поспекулировать, базируясь на текущих знаниях о ландшафтах функций потерь при обучении нейронных сетей. Больший размер батча приводит к тому, что оптимизатор лучше «видит» ландшафт функции потерь для конкретной выборки и может скатиться в маленькие «узкие» паразитные локальные минимумы, которые не имеют обобщающий способности — при небольшом шевелении этого ландшафта (distributional shift c тренировочной на тестовую выборку) значение функции потерь резко подскакивает. В свою очередь, широкие локальные минимумы дают модель с лучшей обобщающей способностью. Эту идею можно увидеть на следующей картинке: 21 Иными словами, большие батчи могут приводить к переобучению, но это можно исправить правильным динамическим подбором learning rate, как будет продемонстрировано далее. Сразу отметим, что совсем маленькие батчи – это тоже плохо, с ними ничего не получится выучить, так как каждая итерация SGD знает слишком мало о ландшафте функции потерь. LARS Мы рассмотрим нестандартный оптимизатор для обучения нейронных сетей, которого нет в Pytorch по умолчанию, но который много где используется: Layer-wise Adaptive Rate Scaling (LARS). Он позволяет эффективно использовать большие размеры батчей, что очень важно при вычислении на нескольких GPU. Основная идея заключена в названии – нужно подбирать размер шага не один для всей сети или каждого нейрона, а отдельный для каждого слоя по правилу, похожему на RMSProp. По сравнению с оригинальным RMSProp подбор learning rate для",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 10,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "батчи – это тоже плохо, с ними ничего не получится выучить, так как каждая итерация SGD знает слишком мало о ландшафте функции потерь. LARS Мы рассмотрим нестандартный оптимизатор для обучения нейронных сетей, которого нет в Pytorch по умолчанию, но который много где используется: Layer-wise Adaptive Rate Scaling (LARS). Он позволяет эффективно использовать большие размеры батчей, что очень важно при вычислении на нескольких GPU. Основная идея заключена в названии – нужно подбирать размер шага не один для всей сети или каждого нейрона, а отдельный для каждого слоя по правилу, похожему на RMSProp. По сравнению с оригинальным RMSProp подбор learning rate для каждого слоя дает большую стабильность обучения. Теперь рассмотрим формулу пересчета: пусть w l w l – это веса слоя l l, l < L l<L. Параметры алгоритма: базовый learning rate η η (на который запускается расписание), коэффициент инерции m m, коэффециент затухания весов β β (как в AdamW). for l in range(L): # Цикл по слоям g_l = stochgrad(w_prev)[l] # Вычисляем стохградиент из батча для текущего слоя lr = eta * norm(w[l]) / (norm(g_l) + beta * norm(w[l])) # Вычислеяем learning rate для текущего слоя v[l] = m * v[l] + lr * (g_l + beta * w[l]) # Обновляем momentum w[l] -= v[l] # Делаем градиентный шаг по всему слою сразу w_prev = w # Обновляем веса LAMB Этот оптимизатор введен в статье Large Batch Optimization For Deep Learning и является идейным продолжателем LARS, более приближенным к Adam, чем к обычному RMSProp. Его параметры – это параметры Adam η,β 1 ,β 2 ,ε, которые берутся как в Adam, а также параметр λ λ, который отвечает за затухание весов ( β β в LARS). for l in range(L): # Цикл по слоям g_l = stochgrad(w_prev)[l] # Вычисляем стохградиент из батча для текущего слоя m[l] = beta_1 * m[l] + (1 - beta_1) * g_l # Вычисляем моментум v[l] = beta_2 * v[l] + (1 - beta_2) * g_l # Вычисляем новый размер шага m[l] /= (1 - beta_1**t) # Шаг для уменьшения смещения из Adam v[l] /= (1 - beta_2**t) r[l] = m[l] / sqrt(v[l] + eps) # Нормируем моментум как предписывает Adam lr = eta * norm(w[l]) / norm(r[l] + llambda * w[l]) # Как в LARS w[l] = w[l] - lr * (r[l] + llambda * w[l]) # Делаем шаг по моментуму w_prev = w # Обновляем веса Усреднение Теперь снова заглянем в теорию: на самом деле, все хорошие теоретические оценки для SGD проявляются, когда берётся усреднение по точкам. Этот эффект при обучении нейронных сетей был исследован в статье про алгоритм SWA. Суть очень проста: давайте усреднять веса модели по каждой c c-й итерации; можно считать, что по эпохам. В итоге, веса финальной модели являются усреднением весов моделей, имевших место в конце каждой эпохи. В результате такого усреднения сильно повышается обобщающая способность модели: мы чаще попадаем в те самые широкие локальные минимумы, о которых мы говорили в разделе про большие батчи. Вдохновляющая картинка из статьи прилагается: 21 На второй и третьей картинке изображено сравнение SGD и SWA при",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 11,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "хорошие теоретические оценки для SGD проявляются, когда берётся усреднение по точкам. Этот эффект при обучении нейронных сетей был исследован в статье про алгоритм SWA. Суть очень проста: давайте усреднять веса модели по каждой c c-й итерации; можно считать, что по эпохам. В итоге, веса финальной модели являются усреднением весов моделей, имевших место в конце каждой эпохи. В результате такого усреднения сильно повышается обобщающая способность модели: мы чаще попадаем в те самые широкие локальные минимумы, о которых мы говорили в разделе про большие батчи. Вдохновляющая картинка из статьи прилагается: 21 На второй и третьей картинке изображено сравнение SGD и SWA при обучении нейронной сети (Preactivation ResNet-164 on CIFAR-100) при одной и той же инициализации. На первой же картинке изображено, как идеологически должен работать SWA. Также мы видим тут демонстрацию эффекта концентрации меры: после обучения стохастический градиентный спуск становится случайным блужданием по области в окрестности локального минимума. Если, например, предположить, что итоговая точка – это нормальное распределение с центром в реальном минимуме в размерности d > 1 0 6 d>10 6 , то все эти точки с большой вероятности будут находиться в окрестности сферы радиуса d d . Интуитивную демонстрацию многомерного нормального распределения можно увидеть на следующей картинке из книги Р.Вершинина \"High-Dimensional Probability\" (слева в размерности 2, справа в большой размерности): 21 Поэтому, чтобы вычислить центральную точку этой гауссианы, усреднение просто необходимо, по такому же принципу работает и SWA. Предобуславливание Теперь мы снова обратимся к теории: скорость сходимости градиентного спуска (даже ускоренного) очень сильно зависит от числа обусловленности задачи. Разумной идеей будет попытаться использовать какие-то сведения о задаче и улучшить этот показатель, тем самым ускорив сходимость. В теории, здесь могут помочь техники предобуславливания. Но, к сожалению, попытки наивно воплотить эту идею приводят к чему-то, похожему на метод Ньютона, в котором нужно хранить большую-большую матрицу для обучения больших моделей. Способ обойти эту проблему рассмотрели в статье о методе Shampoo, который использует то, что веса нейронной сети зачастую удобно представлять как матрицу или даже многомерный тензор. Таким образом, Shampoo можно рассматривать как многомерный аналог AdaGrad. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 13.6. Implicit bias Следующий параграф 14.2. Проксимальные методы Как оптимизировать функции потерь с $L_1$-регуляризацией",
    "metadata": {
      "title": "Оптимизация в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.1",
      "part": 12,
      "total_parts": 12,
      "source_file": "14.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как оптимизировать функции потерь с $L_1$-регуляризацией В этом разделе мы поговорим о том, как оптимизировать негладкие функции в ситуациях, когда «плохую» составляющую удаётся локализовать и она сравнительно несложная. Проксимальная минимизация Для того, чтобы подступиться к проксимальным методам, посмотрим на градиентный спуск с другой стороны. Для простоты рассмотрим константный размер шага α α. Перепишем шаг градиентного спуска следующим образом: k+1 −x k =−∇f(x k ). Посмотрим на это уравнение по-другому. Рассмотрим функцию x ( t ) x(t), равную x k x k при (k−1)α<t≤αk ( t t мы будем воспринимать, как некоторый временной параметр). Тогда при t = α k t=αk: x(t+α)−x(t) =−∇f(x(t)). Теперь слева не что иное, как аппроксимация производной! Если мы устремим α α к нулю, то получится так называемое уравнение градиентного потока: =−∇f(x). Эта динамика в случае выпуклой функции f f сходится к точке минимума x ∗ x ∗ из любой начальной точки при t → + ∞ t→+∞. Сравнение между динамикой градиентного спуска и градиентного потока можно увидеть на следующем изображении: Proksimalnye Первый состоит из дискретных шагов, второй же представляет из себя непрерывный процесс. Нетрудно осознать физический смысл динамики =−∇f(x): маленькое тело скатывается по склону графика функции так, что в любой момент её скорость совпадает с антиградиентом, то есть оно катится по направлению наискорейшего спуска. Теперь представим, что мы сейчас занимается не машинным обучением, а численными методами. Перед нами есть обыкновенное дифференциальное уравнение (ОДУ), и его надо решить. Одним из численных методов решения ОДУ (более стабильным, чем обычная схема Эйлера) является обратная схема Эйлера (backward Euler scheme): k+1 −x k =−∇f(x k+1 ). В обратной схеме Эйлера мы делаем градиентный спуск, только градиент смотрим не в текущей точке (как было бы в обычной схеме Эйлера), а буквально в будущей. Занятная идея, только вот напрямую выразить x k + 1 x k+1 из этого уравнения не получится. Нужно поступить чуть хитрее. Заметим, что k+1 (x−x k ) i 2 x k+1 Это позволяет нам сказать, что весь вектор k+1 −x k является градиентом функции g(u)= 2α k 1 ∥u−x k ∥ 2 , посчитанном в точке x k + 1 x k+1 . Тогда получаем, что x k + 1 x k+1 удовлетворяет следующему условию: ∇(g(u)+f(u))(x k+1 )=0. Если функция f ( x ) f(x) выпуклая, то f(x)+g(x) тоже выпуклая, и её стационарная точка будет точкой минимума. Стало быть, x k + 1 x k+1 можно высчитывать по формуле x k + 1 = arg ⁡ min k+1 =arg u min {f(u)+ 2α k 1 ∥u−x k ∥ 2 }. Определим прокс-оператор следующим образом: arg ⁡ min prox f (x)=argmin{f(u)+ 2 1 ∥u−x∥ 2 }. Тогда, поскольку умножение на >0 внутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему: x k + 1 = arg ⁡ min k+1 =argmin{α k (f(u)+ 2α k 1 ∥u−x∥ 2 )}= arg ⁡ min argmin{α k f(u)+ 2 1 ∥u−x∥ 2 }=prox Итеративный процесс k+1 =prox α k f (x k ) называется методом проксимальной минимизации. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы",
    "metadata": {
      "title": "Проксимальные методы",
      "url": "https://education.yandex.ru/handbook/ml/article/proksimalnye-metody",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.2",
      "part": 1,
      "total_parts": 3,
      "source_file": "14.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "=arg u min {f(u)+ 2α k 1 ∥u−x k ∥ 2 }. Определим прокс-оператор следующим образом: arg ⁡ min prox f (x)=argmin{f(u)+ 2 1 ∥u−x∥ 2 }. Тогда, поскольку умножение на >0 внутри арг-минимума не влияет на саму точку минимума, получаем следующую итеративную схему: x k + 1 = arg ⁡ min k+1 =argmin{α k (f(u)+ 2α k 1 ∥u−x∥ 2 )}= arg ⁡ min argmin{α k f(u)+ 2 1 ∥u−x∥ 2 }=prox Итеративный процесс k+1 =prox α k f (x k ) называется методом проксимальной минимизации. Вы можете спросить себя: зачем он нужен? Ведь теперь на каждом шаге мы должны решать задачу оптимизации: min min f(u)+ 2α k 1 ∥u−x k ∥ 2 Если f f выпуклая, нам есть, что ответить: наличие второго слагаемого гарантирует сильную выпуклость задачи, то есть она решается достаточно эффективно. Но если f f не является выпуклой, то мы ничего не достигли этой модификацией. Композитная оптимизация, проксимальный градиентный метод (PGM) Чтобы понять, зачем нам понадобилась проксимальная оптимизация, рассмотрим оптимизацию функций вида min min {f(x)=g(x)+h(x)}, где g ( x ) g(x) – это гладкая функция, а h ( x ) h(x) – это функция, для которой прокс-оператор считается аналитически. Воспользуемся следующим трюком: по g g мы совершим градиентный шаг, а по h h – проксимальный. Получаем следующую итеративную процедуру: k+1 =prox ∇g(x k )); Эта процедура определяет так называемый проксимальный градиентный метод (Proximal Gradient Method, PGM), который может использоваться, например, для решения задачи регрессии с ℓ 1 ℓ 1 -регуляризацией. ISTA (Iterative Shrinkage-Thresholding Algorithm) Теперь решим конкретную задачу ℓ 1 ℓ 1 -регрессии. Она выглядит следующим образом: min ⁡ w . ∥y−Xw∥ 2 2 +λ∥w∥ 1 → w min . Мы хотим применить PGM к этой задаче, для этого нужно научиться вычислять прокс-оператор для ℓ 1 ℓ 1 -нормы. Проделаем эту операцию: arg ⁡ min prox α∥⋅∥ 1 (x)=arg u min {∥u∥ 1 + 2α 1 ∥u−x∥ 2 2 }= = arg ⁡ min =arg u min { i=1 Заметим, что каждое слагаемое зависит только от одной координаты. Это значит, что каждую координату мы можем прооптимизировать отдельно и получить d d одномерных задач минимизации вида arg ⁡ min arg u i min {∣u Решение такой одномерной задачи записывается в виде функции soft thresholding: prox α∥⋅∥ 1 (x) ∣≤α x i ≤−α Тогда мы получаем следующий алгоритм для ℓ 1 ℓ 1 -регрессии, которые называются Iterative Shrinkage-Thresholding Algorithm (ISTA): w = normal(0, 1) # инициализация repeat S times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание delta = f - y # посчитать отклонение предсказания grad = 2 * X.T.dot(delta) / n # посчитать градиент w_prime = w - alpha * grad # считаем веса, которые отправим в прокс for i in range(d): w[i] = soft_threshold(w_prime[i], alpha * llambda) # вычисляем прокс Заметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат. Конкретно этот метод",
    "metadata": {
      "title": "Проксимальные методы",
      "url": "https://education.yandex.ru/handbook/ml/article/proksimalnye-metody",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.2",
      "part": 2,
      "total_parts": 3,
      "source_file": "14.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание delta = f - y # посчитать отклонение предсказания grad = 2 * X.T.dot(delta) / n # посчитать градиент w_prime = w - alpha * grad # считаем веса, которые отправим в прокс for i in range(d): w[i] = soft_threshold(w_prime[i], alpha * llambda) # вычисляем прокс Заметим одну крутую особенность этого алгоритма -- мы явно видим, что решение получается разреженное, ведь какие-то координаты будут явно зануляться при применении soft threshold! Причем чем больше размер и шага, и параметра регуляризации, тем больше прореживается координат. Конкретно этот метод не применяется на практике, но используются его вариации. Например, статья, которая указана в параграфе про линейные модели о том, как работало предсказание CTR в google в 2012 году, также базируется на вычислении soft threshold как прокс-оператора. Общие выводы Подытожим все вышесказанное: Проксимальные методы – теоретически интересная идея для выпуклой оптимизации, которая должна давать более численно стабильные алгоритмы. Проксимальные методы позволяют достаточно эффективно решать задачи композитной оптимизации, в частности, ℓ 1 ℓ 1 -регуляризованную задачу регрессии. Более того, используемые на практике решения задачи ℓ 1 ℓ 1 -регуляризованной регрессии так или иначе базируются на идее ISTA. Также есть попытки использовать проксимальные методы для более сложных моделей. Например, статья о применении их в нейросетях. Кроме того, имеются применения проксимальных методов для построения распределенных алгоритмов. Все подробности можно найти в монографии Neal Parikh и Stephen Boyd, мы же только привели применение этих идей в машинном обучении. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 14.1. Оптимизация в ML Как найти оптимум функции потерь: от градиентного спуска до Adam Следующий параграф 14.3. Методы второго порядка От метода Ньютона до LBFGS",
    "metadata": {
      "title": "Проксимальные методы",
      "url": "https://education.yandex.ru/handbook/ml/article/proksimalnye-metody",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.2",
      "part": 3,
      "total_parts": 3,
      "source_file": "14.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом разделе мы сконцентрируемся сначала на методах, которые используют информацию о гессиане функции, а затем рассмотрим, как, сохраняя высокоуровневую идею метода Ньютона, обойтись без гессиана. Метод Ньютона Итак, наша задача – безусловная оптимизация гладкой функции f ( x ) → min f(x)→ x∈R d min . Как и при оптимизации методом градиентного спуска, мы будем искать направление уменьшения функционала. Но в этот раз мы будем использовать не линейное приближение, а квадратичное: f(x+Δx)≈f(x)+⟨∇f(x),Δx⟩+ 2 1 ⟨Δx,B(x)Δx⟩. Формула Тейлора говорит нам брать B(x)=∇ 2 f(x). Приравняв к нулю градиент этой квадратичной аппроксимации, мы получаем направление спуска для метода Ньютона: Δx=[B(x)] −1 ∇f(x). Обозначим =B(x k ),H k =B k −1 . В таком случае мы можем записать итеративный алгоритм спуска: k+1 ∇f(x k ). В литературе методом Ньютона называется такой метод при =1, при другом размере шаге ∈(0,1) этот метод называют дэмпированным (damped) методом Ньютона. Обсудим, в чем главная особенность метода Ньютона и в чем заключается выигрыш по сравнению с классическим градиентным спуском. Таких особенностей две. Скорость сходимости метода Ньютона Первая связана со скоростью его сходимости. А именно – в окрестности решения он сходится квадратично. Теорема. Пусть функция f f имеет достаточно гладкий гессиан и сильно выпукла в точке оптимума x ∗ x ∗ . Тогда ∃ r > 0 ∃r>0, что для всякого :∥x 0 −x ∗ ∥≤r для метода Ньютона с =1 верно k+1 −x ∗ ∥≤c∥x k −x ∗ ∥ 2 для константы c c зависящей только от f f. Метод Ньютона и плохо обусловленные задачи Второе приятное свойство заключается в устойчивости метода Ньютона к плохой обусловленности задачи (в отличие от метода градиентного спуска). Разберёмся, что это значит. Когда мы говорим о плохой обусловленности задачи, мы имеем в виду, что гессиан в точке оптимума плохо обусловлен, то есть отношение максимального и минимального собственных чисел является большим числом. Геометрически это значит, что линии уровня функции вблизи оптимума похожи на очень вытянутые эллипсоиды; мы уже обсуждали, что в такой ситуации градиентный спуск может работать медленно. А как справится метод Ньютона? Оказывается, намного лучше. И связано это с его инвариантностью к линейным преобразованиям. А именно, рассмотрим функцию (y)=f(Ay) для некоторой невырожденной матрицы A A. Обозначим x = A y x=Ay. Посмотрим, как связаны градиент и гессиан новой функции с градиентом и гессианом старой. Воспользуемся производной сложной функции: ∇f, Рассмотрим теперь траекторию ,…,x K метода Ньютона, запущенного из точки x 0 x 0 для поиска минимума функции f f, и траекторию ,…,y K метода Ньютона, запущенного для поиска минимума функции f ^ f ^ . Если =Ay 0 , то для всех k k будет верно =Ay k , то есть траектории получаются одна из другой при помощи этого линейного преобразования, другими словами, траектории исходной и новой функции подобны. Вернёмся теперь к плохо обусловленной задаче минимизации функции f f. Рассмотрим линейное преобразование A=(∇ x ∗ 2 f) −1/2 и функцию (x)=f(Ax). Тогда для функции f ^ f ^ число обусловленности гессиана в точке оптимума равно в точность единице (проверьте это!), а траектории для этой новой, хорошо обусловленной функции, и старой, плохо обусловленной,",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 1,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "K метода Ньютона, запущенного для поиска минимума функции f ^ f ^ . Если =Ay 0 , то для всех k k будет верно =Ay k , то есть траектории получаются одна из другой при помощи этого линейного преобразования, другими словами, траектории исходной и новой функции подобны. Вернёмся теперь к плохо обусловленной задаче минимизации функции f f. Рассмотрим линейное преобразование A=(∇ x ∗ 2 f) −1/2 и функцию (x)=f(Ax). Тогда для функции f ^ f ^ число обусловленности гессиана в точке оптимума равно в точность единице (проверьте это!), а траектории для этой новой, хорошо обусловленной функции, и старой, плохо обусловленной, подобны. В частности, метод Ньютона не будет, как градиентный спуск, долго метаться где-то на задворках вытянутой эллиптической «ямки» вокруг оптимума, а быстро ринется к центру. Можно сказать, что метод Ньютона правильно улавливает кривизну линий уровня функции и это позволяет ему быстрее сходиться к оптимуму. Эту идею стоит запомнить, она появляется в некоторых вдохновлённых методами второго порядка модификациях SGD. Также еще можно заметить, что свойства, которые мы требуем от функции в теореме о квадратичной сходимости, вообще говоря, не сохраняются при линейных преобразованиях: могут поменяться константы липшицевости и сильной выпуклости. Это простое замечание побудило исследователей ввести класс самосогласованных функций, более широкий и линейно инвариантный, для которого метод Ньютона также сходится. Подробнее об этом можно узнать в разделе 9.6 книги S. Boyd & L. Vandenberghe, Convex Optimization. Слабости метода Ньютона От хорошего переходим к плохому: к слабостям метода Ньютона. Во-первых, мы имеем квадратичную скорость сходимости только в окрестности оптимума. А если мы стартуем из произвольно удалённой точки, то нам, как и в случае градиентного спуска, требуется подбор шага α k α k при помощи линейного поиска (что нам вряд ли по карману). Если подбирать шаг не хочется, можно прибегнуть к интересному теоретическому методу получения гарантий на глобальную сходимость – добавлению кубической регуляризации. Другая проблема кроется в формуле пересчета следующей итерации: вычисление и обращение гессиана. Конечно, вместо обращения гессиана можно честно решать систему линейных уравнений, но асимптотика остается прежней: O ( d 3 ) O(d 3 ), а от затрат памяти на хранение матрицы O ( d 2 ) O(d 2 ) вообще некуда деться. А это значит, что, например, решать линейную регрессию с ~10000 признаками методом Ньютона попросту невозможно. Есть и третья, малозаметная проблема: дословно метод Ньютона не работает для невыпуклых задач, поскольку f(x) не будет положительно опредленной и Δ x Δx перестанет быть направлением спуска. Для решения этой проблемы можно немного «подпортить» нашу аппроксимацию и рассмотреть матрицу вида f(x k )+Δ k , такую что B k B k станет положительно определенной, и уже её подставлять в нашу квадратичную модель. Идея подмены гессиана на что-то более подходящее – это главная идея квазиньютоновских методов, обсуждаемых далее. Итак, общие выводы: Метод Ньютона – теоретически оптимальный метод, который автоматически улавливает кривизну функции в окрестности оптимума. Для размерности d > 1000 d>1000 он уже не является эффективным, поскольку требует вычисления и хранения гессиана, а также решения системы линейных уравнений с его участием (что может быть в общем случае очень дорого). Квазиньютоновские методы Чтобы придумать, как бороться",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 2,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нашу аппроксимацию и рассмотреть матрицу вида f(x k )+Δ k , такую что B k B k станет положительно определенной, и уже её подставлять в нашу квадратичную модель. Идея подмены гессиана на что-то более подходящее – это главная идея квазиньютоновских методов, обсуждаемых далее. Итак, общие выводы: Метод Ньютона – теоретически оптимальный метод, который автоматически улавливает кривизну функции в окрестности оптимума. Для размерности d > 1000 d>1000 он уже не является эффективным, поскольку требует вычисления и хранения гессиана, а также решения системы линейных уравнений с его участием (что может быть в общем случае очень дорого). Квазиньютоновские методы Чтобы придумать, как бороться с проблемами метода Ньютона, нужно посмотреть на него с другой стороны, а для этого мы обратимся ненадолго к решению задачи нахождения нуля векторной функции. Метод касательной Итак, рассмотрим совершенно новую задачу. Пусть дана функция g:R n →R n и нужно найти её ноль, то есть такое x ∗ x ∗ , что g(x ∗ )=0. Связь с оптимизацией (по крайней мере в выпуклом случае) довольно проста: если взять g(x)=∇f(x), то корень уравнения g(x)=0 и будет точкой оптимума. Сначала рассмотрим одномерный случай d = 1 d=1. Как найти ноль функции с помощью итеративной процедуры? Логично поступить следующим образом: проводим касательную y=g ′ (x n )(x−x n )+g(x n ) к графику функции и находим точку x n + 1 x n+1 , в которой линейная аппроксимация обнуляется: 0=g ′ (x n )(x n+1 −x n )+g(x n ), откуда получаем формулу пересчета n+1 g(x n ) . 23 Известно, что этот метод обладает квадратичной скоростью сходимости в одномерном мире, что очень перекликается с методом Ньютона для оптимизации – и не просто так. Если рассмотреть многомерный случай, то вычисление производной заменяется на вычисление якобиана векторнозначной функции g g. В случае g = ∇ f g=∇f наш якобиан становится гессианом и получаем в точности обычный метод Ньютона для оптимизации: n+1 =x n −[∇ Метод секущей и общая схема квазиньютоновских методов Пусть мы хотим найти такую точку x ∗ x ∗ , что g(x ∗ )=0. В одномерном случае мы можем подменить вычисление ) вычислением её приближения g(x n )−g(x n−1 )/(x n −x n−1 ). Откуда получаем формулу пересчета: n+1 =x n − g(x n )−g(x n−1 ) x n −x n−1 g(x n ) Графически, этот метод выглядит следующим образом: 23 Скорость сходимости этого метода несколько ниже, чем у метода Ньютона (линейная, а не квадратичная), но зато мы теперь не должны вычислять производную! В текущем виде, используя просто подмену градиента на его конечно-разностную аппроксимацию, не очевидно, как обобщить этот метод на произвольную размерность. Но, если посмотреть на название метода и на картинку, как он работает, мы видим, что мы по сути проводим через два предыдущих приближения секущую, а затем выбираем ноль этой секущей в качестве следующей точки. В многомерном случае мы можем выписать соответствующее ей уравнение y=B k (x−x k )+g(x k ), где B k B k – матрица размера d × d d×d, которая должна удовлетворять так называемому уравнению секущей (secant equation): k−1 )=g(x k )−g(x k−1 ). Теперь, чтобы",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 3,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "производную! В текущем виде, используя просто подмену градиента на его конечно-разностную аппроксимацию, не очевидно, как обобщить этот метод на произвольную размерность. Но, если посмотреть на название метода и на картинку, как он работает, мы видим, что мы по сути проводим через два предыдущих приближения секущую, а затем выбираем ноль этой секущей в качестве следующей точки. В многомерном случае мы можем выписать соответствующее ей уравнение y=B k (x−x k )+g(x k ), где B k B k – матрица размера d × d d×d, которая должна удовлетворять так называемому уравнению секущей (secant equation): k−1 )=g(x k )−g(x k−1 ). Теперь, чтобы выбрать следующую точку, нужно найти ноль секущей, то есть k+1 −x k )+g(x k )=0⟺x k+1 =x k −B k −1 g(x k ). А теперь рассмотрим g(x)=∇f(x) и добавим в итеративную схему выше размер шага. Тогда мы получаем общую итеративную схему квазиньютоновских методов: k+1 ∇f(x k ). При этом необходимо выбирать такие B k B k , чтобы они (а) были симметричными и положительно определенными и (б) удовлетворяли уравнению секущей k−1 )=∇f(x k )−∇f(x k−1 ) Первое требование восходит к двум соображениям. Первое – B k B k должно приближать гессиан, а он в идеале в окрестности точки минимума как раз является симметричным и положительно определенным. Второе соображение проще: в противном случае =−B k −1 ∇f(x k ) попросту не будет направлением спуска. Несмотря на эти два свойства, выбор по прежнему остается достаточно широким, откуда возникает большое разнообразие квазиньютоновских методов. Мы рассмотрим один классический и широко известный метод BFGS (Broyden, Fletcher, Goldfarb, Shanno). BFGS Сначала заметим, что в самом алгоритме в первую очередь используется обратная матрица к B k B k , которую мы обозначим . Тогда выбирать B k B k – это тоже самое, что выбирать H k H k . Введем еще два стандартных обозначения, чтобы можно было проще записывать все последующие формулы: k+1 =∇f(x k+1 )−∇f(x k ). В их терминах уравнение секущей для H k H k выглядит максимально просто: k−1 =s k−1 . Теперь введем некоторое искусственное требование, которое гарантирует единственность H k + 1 H k+1 – выберем ближайшую подходящую матрицу к H k H k , удовлетворяющую описанным выше условиям: H k + 1 = argmin k+1 =argmin H { 2 1 ∥H−H k ∥ Н=H ⊤ , Hy k =s k } Вообще говоря, при выборе разных норм ∥ ⋅ ∥ ∥⋅∥ мы будем получать разные квазиньютоновские алгоритмы. Рассмотрим один достаточно общий класс норм (аналог взвешенных ℓ 2 ℓ 2 норм в матричном мире): ∥A∥:=∥W 1/2 AW 1/2 ∥ F , где ∥ ⋅ ∥ F ∥⋅∥ F – это Фробениусова норма ∥C∥ F 2 =⟨C,C⟩ F =tr(C ⊤ C)= i,j W – некоторая симметричная и положительно определенная матрица весов, которую мы выберем таким образом, что она будет сама по себе удовлетворять уравнению секущей Сразу уточним, что матрица весов в таком случае меняется на каждой итерации и, по сути, на каждой итерации мы имеем разные задачи оптимизации, само же предположение задает дополнительную похожесть на обратный гессиан, поскольку можно взять в",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 4,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "достаточно общий класс норм (аналог взвешенных ℓ 2 ℓ 2 норм в матричном мире): ∥A∥:=∥W 1/2 AW 1/2 ∥ F , где ∥ ⋅ ∥ F ∥⋅∥ F – это Фробениусова норма ∥C∥ F 2 =⟨C,C⟩ F =tr(C ⊤ C)= i,j W – некоторая симметричная и положительно определенная матрица весов, которую мы выберем таким образом, что она будет сама по себе удовлетворять уравнению секущей Сразу уточним, что матрица весов в таком случае меняется на каждой итерации и, по сути, на каждой итерации мы имеем разные задачи оптимизации, само же предположение задает дополнительную похожесть на обратный гессиан, поскольку можно взять в качестве весов усредненый гессиан =[∫ 0 1 ∇ 2 f(x k +τα k p k )dτ] Решив описанную выше оптимизационную задачу, мы получаем матрицу H k + 1 H k+1 , не зависящую явным образом от матрицы весов: Эта формула как раз является ключевой в алгоритме BFGS. Чтобы заметить одно крайне важное свойство этой формулы, раскроем скобки: k+1 )+ρ )+ρ Отсюда мы видим, что нам в этой формуле достаточно умножать матрицу на вектор и складывать матрицы, что можно делать за O ( d 2 ) O(d 2 ) операций! То есть мы победили один из самых страшных минусов метода Ньютона. Воспользовавшись тем, что 1/ρ – числа, перепишем формулу в более computational friendly стиле: k+1 =H k +ρ k 2 (1/ρ )(s k s k ⊤ )−ρ Общие выводы: Итерации BFGS вычислительно проще итераций метода Ньютона и не требуют вычисления гессиана; По скорости сходимости BFGS уступает методу Ньютона, но все равно является достаточно быстрым; По прежнему требуется O ( d 2 ) O(d 2 ) памяти, что по-прежнему вызывает проблемы при большой размерности ( −10 5 ). Время выполнения итерации O ( d 2 ) O(d 2 ) гораздо лучше, чем O ( d 3 ) O(d 3 ) метода Ньютона, но всё ещё оставляет желать лучшего. Казалось бы, избавиться от O ( d 2 ) O(d 2 ) нельзя принципиально, ведь нужно как-то взаимодействовать с матрицей H k H k размера O ( d 2 ) O(d 2 ), а она не факт что разреженная. Но и в этом случае можно добиться улучшения до линейной сложности (как у градиентных методов!). L-BFGS При взаимодействии с матрицами существует два основных способа хранить их дешевле, чем «по-честному». Первый способ – пользоваться разреженностью матрицы, а второй – низкоранговыми разложениями или чем-то близким. Поскольку сейчас мы не хотим добавлять предположений на задачу, которую мы решаем, то единственный выход – это пользоваться структурой H k H k , возникающей в BFGS. Если внимательно взглянуть на формулы обновления, то их можно переписать в следующем виде: k+1 =V(s V(s k ,y k )+U(s V(s k ,y k )=I−ρ , U(s k ,y k )=ρ Для того, чтобы перейти от k+1 , можно хранить не матрицу H k H k , а набор пар из k пар i=1,…,k и начальное приближение H 0 H 0 (например, =γI для некоторого γ > 0 γ>0), чтобы «восстановить» H k H k . Пользуясь такой структурой, мы можем хранить матрицу H k",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 5,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "то единственный выход – это пользоваться структурой H k H k , возникающей в BFGS. Если внимательно взглянуть на формулы обновления, то их можно переписать в следующем виде: k+1 =V(s V(s k ,y k )+U(s V(s k ,y k )=I−ρ , U(s k ,y k )=ρ Для того, чтобы перейти от k+1 , можно хранить не матрицу H k H k , а набор пар из k пар i=1,…,k и начальное приближение H 0 H 0 (например, =γI для некоторого γ > 0 γ>0), чтобы «восстановить» H k H k . Пользуясь такой структурой, мы можем хранить матрицу H k + 1 H k+1 при помощи лишь (k+1)⋅2d+1 чисел, а не d 2 d 2 . К сожалению, такая структура имеет довольно простую проблему: при k > d / 2 k>d/2 затраты памяти становятся только выше. Возникает простая идея – а давайте хранить только последние m = const m=const обновлений! Таким образом, мы получаем алгоритм L-BFGS, который имеет уже линейные O ( m d ) O(md) затраты памяти и, что немаловажно, такие же линейные затраты O ( m d ) O(md) на итерацию, ведь умножение матриц V V и U U на вектор может осуществляться за линейное время. Общие выводы: L-BFGS обладает линеной сложностью итерации, линейными требованиями по дополнительной памяти и к тому же требует вычислять только градиенты! Производительность сильно зависит от константы m m, отвечающей за точность аппроксимации гессиана; Как и все методы из этого раздела, требует точного, а не стохастического вычисления градиентов. Практические аспекты Из всех перечисленных в этом разделе методов важнее всего отметить L-BFGS как самый практичный. Он реализован в любой* библиотеке, которая имеет дело с оптимизацией чего-либо и может быть эффективным, если удаётся вычислить градиенты (и значения функций для линейного поиска размера шага). К сожалению, это получается не всегда: при больших размерах датасета вычисление честного градиента и значения для функционалов вида суммы L(X,Y)= i=1 ∑ N L(x i ,y i ) не представляется возможным за разумное время. В таком случае мы вынуждены вернуться в мир стохастического градиентного спуска. Общая идея более тонкого учёта геометрии линий уровня функции потерь, в чём-то напоминающая происходящее в методе Ньютона, находит применение и в ряде вариаций SGD, но, конечно, порождает совершенно другие методы. Что же касается самого метода Ньютона, его можно несколько оптимизировать, если смириться с тем, что всё вычисляется неточно. Во-первых, обратную матрицу к гессиану матрицу на самом деле не нужно ни хранить, ни даже вычислять. Давайте разберёмся, почему. Умножить на вектор v v – это то же самое, что решить систему с левой частью ∇ 2 f ∇ 2 f и правой частью v v, а для решения систем уравнений существуют эффективные итеративные методы, не меняющие левой части системы, а требующие лишь уметь умножать её на разные векторы. При этом умножать гессиан на вектор можно при помощи автоматического дифференцирования. Кроме того, можно на кажом шаге неточно решать систему, получая таким образом неточный метод Ньютона. Теория предписывает решать систему все точнее с ростом номера итерации, но на практике нередко используют фиксированное и небольшое число шагов итеративных методов решения систем линейных",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 6,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на вектор v v – это то же самое, что решить систему с левой частью ∇ 2 f ∇ 2 f и правой частью v v, а для решения систем уравнений существуют эффективные итеративные методы, не меняющие левой части системы, а требующие лишь уметь умножать её на разные векторы. При этом умножать гессиан на вектор можно при помощи автоматического дифференцирования. Кроме того, можно на кажом шаге неточно решать систему, получая таким образом неточный метод Ньютона. Теория предписывает решать систему все точнее с ростом номера итерации, но на практике нередко используют фиксированное и небольшое число шагов итеративных методов решения систем линейных уравнений. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 14.2. Проксимальные методы Как оптимизировать функции потерь с $L_1$-регуляризацией Следующий параграф 14.4. Сходимость SGD Почему он всё-таки сходится",
    "metadata": {
      "title": "Методы второго порядка",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-vtorogo-poryadka",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.3",
      "part": 7,
      "total_parts": 7,
      "source_file": "14.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Стохастический Градиентный Спуск (SGD) имеет достаточно простую запись: k+1 Здесь g k g k — это некоторая аппроксимация градиента целевой функции ∇f(x k ) в точке x k x k , называемая стохастическим градиентом (или просто стох. градиентом), >0 — это размер шага (stepsize, learning rate) на итерации k k. Для простоты мы будем считать, что =α>0 для всех k ≥ 0 k≥0. Обычно предполагается, что стох. градиент является несмещённой оценкой ∇f(x k ) при фиксированном E(g k ∣x k )=∇f(x k ). Доказательство сходимости Зададимся следующим вопросом: с какой скоростью и в каком смысле SGD сходится к решению и сходится ли? Во-первых, как и во многих работах по стохастической оптимизации, нас будет интересовать сходимость метода в среднем, т.е. оценки на ∣∣x k −x ∗ ∣∣ 2 ) или E(f(x k )−f(x ∗ )), где x ∗ x ∗ — решение задачи (для простоты будем считать, что оно единственное). Во-вторых, чтобы SGD сходился в указанном смысле, необходимо ввести дополнительные предположения. Действительно, например, если дисперсия стох. градиента не ограничена ∣∣g k −∇f(x k )∣∣ 2 ∣x k )=∞, то ∣∣x k −x ∗ ∣∣ 2 )=∞ и никаких разумных гарантий доказать не удаётся. Поэтому дополнительно к несмещённости часто предполагается, что дисперсия равномерно ограничена: предположим, что существует такое число σ ≥ 0 σ≥0, что для всех k ≥ 0 k≥0 выполнено ∣∣g k −∇f(x k )∣∣ 2 x k )≤σ 2 . Данное предположение выполнено, например, для задачи логистической регрессии (поскольку в данной задаче норма градиентов слагаемых ограничена), но в то же время является весьма обременительным. Его можно заменить на более реалистичные предположения, что мы немного затронем далее. Однако при данном предположении анализ SGD является очень простым и полезным для дальнейших обобщений и рассуждений. Для простоты везде далее мы будем считать, что функция f f является L L-гладкой и μ μ-сильно выпуклой, т.е. для всех x,y∈R d выполнены неравенства ∣∣∇f(x)−∇f(y)∣∣≤L∣∣x−y∣∣, f(y)≥f(x)+⟨∇f(x),y−x⟩+ 2 μ ∣∣y−x∣∣ 2 . Теорема. Предположим, что f f является L L-гладкой и μ μ-сильно выпуклой, стох. градиент g k g k имеет ограниченную дисперсию, и размер шага удовлетворяет 0<α≤1/L. Тогда для всех k ≥ 0 k≥0 выполняется неравенство ∣∣x k −x ∗ ∣∣ 2 )≤(1−αμ) k ∣∣x Доказательство. Используя выражение для x k + 1 x k+1 , мы выводим ∣∣x k+1 −x ∗ ∣∣ 2 =∣∣x k −x ∗ −αg k ∣∣ 2 =∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,g k ⟩+α 2 ∣∣g k ∣∣ 2 Далее мы берём условное матожидание E(⋅∣x k ) от левой и правой частей и получаем: ∣∣x k+1 )=∣∣x k −x ∗ ∣∣ 2 −2αE(⟨x k −x ∗ ,g k ⟩∣x k )+α 2 E( 4 1 ∣∣g =∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,E(g k ∣x k )⟩+α 2 E( 4 1 ∣∣g =∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+α 2 E( 4 1 ∣∣g Следующий шаг в доказательстве состоит в оценке второго момента ∣∣g k ∣∣ 2 ∣x k ). Используя предположение об ограниченности дисперсии стох.",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 1,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "k ∣∣ 2 Далее мы берём условное матожидание E(⋅∣x k ) от левой и правой частей и получаем: ∣∣x k+1 )=∣∣x k −x ∗ ∣∣ 2 −2αE(⟨x k −x ∗ ,g k ⟩∣x k )+α 2 E( 4 1 ∣∣g =∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,E(g k ∣x k )⟩+α 2 E( 4 1 ∣∣g =∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+α 2 E( 4 1 ∣∣g Следующий шаг в доказательстве состоит в оценке второго момента ∣∣g k ∣∣ 2 ∣x k ). Используя предположение об ограниченности дисперсии стох. градиента, мы выводим: ∣∣g k ∣∣ 2 x k )=E( 4 1 ∣∣∇f(x k )+g k −∇f(x k )∣∣ 2 x k ) =∣∣∇f(x k )∣∣ 2 +E(⟨∇f(x k ),g k −∇f(x k )⟩∣x k )+E( 4 1 ∣∣g k −∇f(x k )∣∣ 2 x k ) =∣∣∇f(x k )∣∣ 2 +⟨∇f(x k ),E(g k −∇f(x k )∣x k )⟩+E( 4 1 ∣∣g k −∇f(x k )∣∣ 2 x k ) =∣∣∇f(x k )∣∣ 2 +E( 4 1 ∣∣g k −∇f(x k )∣∣ 2 x k ) ≤∣∣∇f(x k )∣∣ 2 +σ 2 Чтобы оценить сверху ∣∣∇f(x k )∣∣ 2 , мы используем следующий факт, справедливый для любой выпуклой L L-гладкой функции f f (см. книгу Ю. Е. Нестерова \"Методы выпуклой оптимизации\", 2010): ∣∣∇f(x)−∇f(y)∣∣ 2 ≤2L(f(x)−f(y)−⟨∇f(y),x−y⟩). Беря в этом неравенстве x = x k x=x y=x ∗ и используя ∇f(x ∗ )=0, получаем ∣∣g k ∣∣ 2 x k )≤2L(f(x k )−f(x ∗ ))+σ 2 . Далее мы подставляем эту оценку в выражение для E(∣∣x k+1 ∣∣x k+1 )≤∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+2α 2 L(f(x k )−f(x ∗ ))+α 2 σ 2 . Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции f f: из f(x ∗ )≥f(x k )+⟨∇f(x k ),x ∣∣x ∗ −x k ∣∣ 2 следует ⟨∇f(x k ),x k −x ∗ ⟩≥f(x k )−f(x ∗ )+ 2 μ ∣∣x Используя это неравенство в выведенной ранее верхней оценке на E(∣∣x k+1 ), мы приходим к следующему неравенству: ∣∣x k+1 )≤(1−αμ)∣∣x k −x ∗ ∣∣ 2 −2α(1−αL)(f(x k )−f(x ∗ ))+α 2 σ 2 ≤(1−αμ)∣∣x где в последнем неравенстве мы воспользовались неотрицательностью 2α(1−αL)(f(x k )−f(x ∗ )), что следует из 0<α≤1/L и f(x k )≥f(x ∗ ). Чтобы получить результат, заявленный в теореме, нужно взять полное мат. ожидание от левой и правой частей полученного неравенства (воспользовавшись при этом крайне полезным свойством условного мат. ожидания — tower property: E(E(⋅∣x k ))=E(⋅)) ∣∣x k+1 −x ∗ ∣∣ 2 )≤(1−αμ)E( 4 1 ∣∣x k −x ∗ ∣∣ 2 )+α 2 σ 2 , а затем, применяя это неравенство для ∣∣x ∣∣x k−1 … , ∣∣x 1 −x ∗ ∣∣ 2 ), получим ∣∣x k+1 −x ∗ ∣∣ 2 )≤(1−αμ) k+1 ∣∣x t=0 ∑ k (1−αμ) t ≤(1−αμ) k+1 ∣∣x t=0 ∑ ∞ (1−αμ) t =(1−αμ) k+1 ∣∣x что и требовалось доказать. Данный результат утверждает, что SGD с",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 2,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "взять полное мат. ожидание от левой и правой частей полученного неравенства (воспользовавшись при этом крайне полезным свойством условного мат. ожидания — tower property: E(E(⋅∣x k ))=E(⋅)) ∣∣x k+1 −x ∗ ∣∣ 2 )≤(1−αμ)E( 4 1 ∣∣x k −x ∗ ∣∣ 2 )+α 2 σ 2 , а затем, применяя это неравенство для ∣∣x ∣∣x k−1 … , ∣∣x 1 −x ∗ ∣∣ 2 ), получим ∣∣x k+1 −x ∗ ∣∣ 2 )≤(1−αμ) k+1 ∣∣x t=0 ∑ k (1−αμ) t ≤(1−αμ) k+1 ∣∣x t=0 ∑ ∞ (1−αμ) t =(1−αμ) k+1 ∣∣x что и требовалось доказать. Данный результат утверждает, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален . Отметим, что чем больше размер шага α α, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован здесь. Теорема выше доказана при достаточно обременительных предположениях: мы предположили, что функция является сильно выпуклой, L L-гладкой и стох. градиент имеет равномерно ограниченную дисперсию. В практически интересных задачах данные условия (в данном виде) выполняются крайне редко. Тем не менее, выводы, которые мы сделали из доказанной теоремы, справедливы для многих задач, не удовлетворяющих введённым предположениям (во многом потому, что указанные свойства важны лишь на некотором компакте вокруг решения задачи, что в свою очередь не так и обременительно). Более того, если мы сделаем немного другое предположение о стохастических градиентах, то сможем покрыть некоторые случаи, когда дисперсия не является равномерно ограниченной на всём пространстве. Предположим теперь, что =∇f ξ k (x k ), где ξ k ξ k просэмплировано из некоторого распределения D D независимо от предыдущих итераций, f(x)=E ξ∼D (f ξ (x)) и (x) является выпуклой и L ξ L ξ -гладкой для всех ξ ξ (данное предположение тоже можно ослабить, но для простоты изложения остановимся именно на такой формулировке). Будем называть данные условия предположением о выпуклых гладких стохастчиеских реализациях. Они выполнены, например, для задач линейно регрессии и логистической регрессии. В таком случае, для точек, сгенерированных SGD, справедливо, что SGD с потоянным шагом сходится линейно к окрестности решения, радиус которой пропорционален . Отметим, что чем больше размер шага α α, тем быстрее SGD достигает некоторой окрестности решения, в которой продолжает осциллировать. Однако чем больше размер шага, тем больше эта окрестность. Соответственно, чтобы найти более точное решение, необходимо уменьшать размер шага в SGD. Этот феномен хорошо проиллюстрирован здесь. Теорема. Предположим, что f f является L L-гладкой и μ μ-сильно выпуклой, стохастчиеские реализации являются выпуклыми и гладкими, и размер шага удовлетворяет max ⁡ 0<α≤1/2L max , где L max ⁡ = max max =max ξ∼D L ξ . Тогда для всех k ≥ 0 k≥0 выполняется неравенство ∣∣x k −x ∗ ∣∣ 2 )≤(1−αμ) k ∣∣x 2ασ ∗ 2 , где ξ∼D ∣∣∇f ξ (x ∗ )∣∣ 2 . Доказательство. Аналогично предыдущей доказательству предыдущей теоремы, получаем ∣∣x k+1 )=∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+α 2 E( 4 1 ∣∣g Поскольку (x) является",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 3,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Теорема. Предположим, что f f является L L-гладкой и μ μ-сильно выпуклой, стохастчиеские реализации являются выпуклыми и гладкими, и размер шага удовлетворяет max ⁡ 0<α≤1/2L max , где L max ⁡ = max max =max ξ∼D L ξ . Тогда для всех k ≥ 0 k≥0 выполняется неравенство ∣∣x k −x ∗ ∣∣ 2 )≤(1−αμ) k ∣∣x 2ασ ∗ 2 , где ξ∼D ∣∣∇f ξ (x ∗ )∣∣ 2 . Доказательство. Аналогично предыдущей доказательству предыдущей теоремы, получаем ∣∣x k+1 )=∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+α 2 E( 4 1 ∣∣g Поскольку (x) является выпуклой и L ξ L ξ -гладкой, имеем (см. книгу Ю. Е. Нестерова \"Методы выпуклой оптимизации\", 2010): ∣∣∇f ξ (x)−∇f ξ (y)∣∣ 2 ≤2L ξ (f ξ (x)−f ξ (y)−⟨∇f ξ (y),x−y⟩). Применяя это неравенство для x = x k x=x y=x ∗ , получаем max max ∣∣g k ∣∣ 2 x k )=E ξ k ∼D (∣∣∇f ξ k (x k )−∇f ξ k (x ∗ )+∇f ξ k (x ∗ )∣∣ 2 ) ≤2E ξ k ∼D (∣∣∇f ξ k (x k )−∇f ξ k (x ∗ )∣∣ 2 )+2E ξ k ∼D (∣∣∇f ξ k (x ∗ )∣∣ (4L )−f ξ k (x ∗ )−⟨∇f ξ k (x ∗ ),x k −x ∗ ⟩))+2E ξ∼D (∣∣∇f ξ k (x ∗ )∣∣ 2 ) ≤4L max (f(x k )−f(x ∗ )−⟨∇f(x ∗ ),x k −x ∗ ⟩)+2σ ∗ 2 =4L max (f(x k )−f(x ∗ ))+2σ ∗ 2 , где во втором переходе мы воспользовались стандартным фактом: ∣∣a+b∣∣ 2 ≤∣∣a∣∣ 2 +∣∣b∣∣ 2 для любых a,b∈R n . Подставим полученное неравенство в выражение для E(∣∣x k+1 ), доказанное ранее: max ∣∣x k+1 )=∣∣x k −x ∗ ∣∣ 2 −2α⟨x k −x ∗ ,∇f(x k )⟩+4L max α 2 (f(x k )−f(x ∗ ))+2α 2 σ ∗ 2 . Остаётся оценить скалярное произведение в правой части неравенства. Это можно сделать, воспользовавшись сильной выпуклостью функции f f: из f(x ∗ )≥f(x k )+⟨∇f(x k ),x ∣∣x ∗ −x k ∣∣ 2 следует ⟨∇f(x k ),x k −x ∗ ⟩≥f(x k )−f(x ∗ )+ 2 μ ∣∣x Используя это неравенство в выведенной ранее верхней оценке на E(∣∣x k+1 ), мы приходим к следующему неравенству: max ∣∣x k+1 )≤(1−αμ)∣∣x k −x ∗ ∣∣ 2 −2α(1−2αL max )(f(x k )−f(x ∗ ))+2α 2 σ 2 ≤(1−αμ)∣∣x k −x ∗ ∣∣ 2 +2α 2 σ ∗ 2 , где в последнем неравенстве мы воспользовались неотрицательностью max 2α(1−2αL max )(f(x k )−f(x ∗ )), что следует из max ⁡ 0<α≤1/2L max f(x k )≥f(x ∗ ). Действуя по аналогии с доказательством предыдущей теоремы, получаем требуемый результат. Выводы, которые можно сделать из данной теоремы, очень похожи на те, что мы уже сделали из прошлой теоремы. Главные отличия заключаются в том, что L max ⁡ L max может быть гораздо больше L L, т.е. максимальный допустимый размер шага α α в данной теореме может быть гораздо меньше, чем в предыдущей. Однако размер окрестности теперь зависит от дисперсии",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 4,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "σ ∗ 2 , где в последнем неравенстве мы воспользовались неотрицательностью max 2α(1−2αL max )(f(x k )−f(x ∗ )), что следует из max ⁡ 0<α≤1/2L max f(x k )≥f(x ∗ ). Действуя по аналогии с доказательством предыдущей теоремы, получаем требуемый результат. Выводы, которые можно сделать из данной теоремы, очень похожи на те, что мы уже сделали из прошлой теоремы. Главные отличия заключаются в том, что L max ⁡ L max может быть гораздо больше L L, т.е. максимальный допустимый размер шага α α в данной теореме может быть гораздо меньше, чем в предыдущей. Однако размер окрестности теперь зависит от дисперсии стох. градиента в решении , что может быть значительно меньше σ 2 σ 2 . Рассмотрим важный частный случай — задачи минимизации суммы функций: min x∈R d min {f(x)= n 1 i=1 ∑ n f i (x)}. Обычно (x) имеет смысл функции потерь на i i-м объекте датасета. Предположим, что (x) — выпуклая и L i L i -гладкая функция. Тогда выполняется предположение о выпуклых гладких стохастчиеских реализациях: действительно, достаточно задать ξ ξ как случайное число из {1,2,…,n}, имеющее равномерное распределение. Тогда справедлив результат предыдущей теоремы с L max ⁡ = max max =max i∈1,…,n) i=1 n ∣∣∇f i (x ∗ )∣∣ 2 . Для любого K ≥ 0 K≥0 можно выбрать шаг в SGD следующим образом: если K ≤ 2 L max max ⁡ , если K > 2 L max max ⁡ , если K > 2 L max max если K≤ μ 2L max ,γ k = 2L max 1 , если K> μ 2L max и k<k 0 ,γ k = 2L max 1 , если K> μ 2L max и k≥k 0 ,γ k = 4L max +μ(k−k 0 ) 1 , где =⌈K/2⌉. Тогда из доказанного выше результата следует (см. Лемму 3 из статьи С. Стиха), что после K K итераций max exp ⁡ ( − μ L max ∣∣x K −x ∗ ∣∣ 2 )=O( μ L max ∣∣x 0 −x ∗ ∣∣ 2 exp(− L max μ K)+ Таким образом, чтобы гарантировать ∣∣x K −x ∗ ∣∣ 2 )≤ε, SGD требуется O ( L max ⁡ μ log ⁡ ( L max max log( με L max ∣∣x итераций/подсчётов градиентов слагаемых. Чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать O ( n L μ log O(n μ L log( ε ∣∣x подсчётов градиентов слагаемых, поскольку каждая итерация GD требует n n подсчётов градиентов слагаемых (нужно вычислять полный градиент ∇f(x)= n 1 ∑ i=1 n ∇f i (x)). Можно показать, что L ≤ L max ⁡ ≤ n L L≤L max ≤nL, поэтому в худшем случае полученная оценка для SGD заведомо хуже, чем для GD. Однако в случае, когда L max max =O(L), однозначного вывода сделать нельзя: при большом ε ε может доминировать первое слагаемое в оценке сложности SGD, поэтому в таком случае SGD будет доказуемо быстрее, чем GD (если пренебречь логарифмическими множителями). Иными словами, чтобы достичь не очень большой точности решения, выгоднее использовать SGD, чем GD. В ряде ситуаций небольшой",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 5,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n подсчётов градиентов слагаемых (нужно вычислять полный градиент ∇f(x)= n 1 ∑ i=1 n ∇f i (x)). Можно показать, что L ≤ L max ⁡ ≤ n L L≤L max ≤nL, поэтому в худшем случае полученная оценка для SGD заведомо хуже, чем для GD. Однако в случае, когда L max max =O(L), однозначного вывода сделать нельзя: при большом ε ε может доминировать первое слагаемое в оценке сложности SGD, поэтому в таком случае SGD будет доказуемо быстрее, чем GD (если пренебречь логарифмическими множителями). Иными словами, чтобы достичь не очень большой точности решения, выгоднее использовать SGD, чем GD. В ряде ситуаций небольшой точности вполне достаточно, но так происходит не всегда. Поэтому возникает ествественный вопрос: можно ли так модифицировать SGD, чтобы полученный метод сходился линейно асимптотически к точному решению (а не к окрестности как SGD), но при этом стоимость его итераций была сопоставима со стоимостью итераций SGD? Оказывается, что да и соответствующие методы называются методами редукции дисперсии. Методы редукции дисперсии Перед тем, как мы начнём говорить о методах редукции дисперсии, хотелось бы раскрыть подробнее причину того, что SGD не сходится линейно асимптотически к точному решению. Мы рассмотрели анализ SGD в двух предположениях, и в обоих случаях нам требовалось вывести некоторую верхнюю оценку на второй момент стох. градиента, т.е. на E(∣∣g k ∣∣ 2 ∣x k ). В обоих случаях эта оценка содержала некоторый константный член ( σ 2 σ 2 или — зависит от рассматриваемого предположения), который потом возникал и в финальной оценке на ∣∣x k −x ∗ ∣∣ 2 ), препятствуя тем самым линейно сходимости метода. Конечно, это рассуждение существенно опирается на конкретный способ анализа метода, а потому не является строгим объяснением, почему SGD не сходится линейно. Однако важно отметить, что оценка на E(∣∣g k ∣∣ 2 ∣x k ) достаточно точно отражает поведение метода вблизи решения: даже если точка x k x k оказалась по какой-то причине близка к решению x ∗ x ∗ (или даже просто совпала с решением), то E(∣∣g k ∣∣ 2 ∣x k ) и, в частности, E(∣∣g k −∇f(x k )∣∣ 2 ∣x k ) будут порядка σ 2 σ 2 или . Следовательно, при следующем шаге метод с большой вероятностью отдалится от/выйдет из решения, поскольку ∣∣x k+1 −x k ∣∣ 2 )=α 2 E( 4 1 ∣∣g k ∣∣ 2 )∼α 2 σ 2 или Из приведённых выше рассуждений видно, что дисперсия стох. градиента мешает методу сходится линейно к точному решению. Поэтому хотелось бы как-то поменять правило вычисления стох. градиента, чтобы выполнялись 3 важных свойства: (1) новый стох. градиент должен быть не сильно дороже в плане вычислений, чем подсчёт стох. градиента в SGD (градиента слагаемого), (2) новый стох. градиент должен быть несмещённой оценкой полного градиента ∇f(x k ), и (3) дисперсия нового стох. градиента должна уменьшаться в процессе работы метода. Например, можно рассмотреть следующий стох. градиент: =∇f j k (x k )+s k , где j k j k выбирается случайно равновероятно из множества {1,2,…,n} и E(s k ∣x k )=0. В таком случае, будет выполнено свойство (2) из списка выше. Чтобы достичь желаемой цели,",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 6,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "поменять правило вычисления стох. градиента, чтобы выполнялись 3 важных свойства: (1) новый стох. градиент должен быть не сильно дороже в плане вычислений, чем подсчёт стох. градиента в SGD (градиента слагаемого), (2) новый стох. градиент должен быть несмещённой оценкой полного градиента ∇f(x k ), и (3) дисперсия нового стох. градиента должна уменьшаться в процессе работы метода. Например, можно рассмотреть следующий стох. градиент: =∇f j k (x k )+s k , где j k j k выбирается случайно равновероятно из множества {1,2,…,n} и E(s k ∣x k )=0. В таком случае, будет выполнено свойство (2) из списка выше. Чтобы достичь желаемой цели, необходимо как-то специфицировать выбор случайного вектора s k s k . Исторически одним из первых способов выбора s k s k был =−∇f j k (w k )+∇f(w k ), где точка w k w k обновляется раз в m ∼ n m∼n итераций: k+1 ={ w k , x k+1 , if k+1modm  =0, if k+1modm=0. Данный метод называется Stochastic Variance Reduced Gradient (SVRG). Данный методы был предложен и проанализирован в NeurIPS статье Джонсона и Жанга в 2013 году. Теперь же убедимся, что метод удовлетворяет всем трём отмеченным свойствам. Начнём с несмещённости: =∇f j k (x k )−∇f j k (w k )+∇f(w E(g i=1 ∑ n (∇f i (x k )−∇f i (w k )+∇f(w k ))=∇f(x k ). Далее, вычисление g k g k подразумевает 2 подсчёта градентов слагаемых при kmodm  =0 и n + 2 n+2 подсчёта градентов слагаемых при kmodm  =0. Таким образом, за m m последователльных итераций SVRG происходит вычисление 2(m−1)+n+2=2m+n градиентов слагаемых, в то время как SGD требуется m m подсчётов градиентов слагаемых. Если m = n m=n (стандартный выбор параметра m m), то n n итераций SVRG лишь в 3 раза дороже, чем n n итераций SGD. Иными словами, в среднем итерация SVRG не сильно дороже итерации SGD. Наконец, если мы предположим, что метод сходится ∣∣x k −x ∗ ∣∣ 2 )→0 (а он действительно сходится, см., например, доказательство вот тут), то получим, что ∣∣w k −x ∗ ∣∣ 2 )→0, а значит ∣∣x k −w k ∣∣ 2 )→0 и ∣∣∇f(w k )∣∣ 2 )→0. Но тогда в силу Липшицевости градиентов f i f i для всех i=1,…,n имеем: max ∣∣g k ∣∣ 2 )=E( 4 1 ∣∣∇f j k (x k )−∇f j k (w k )+∇f(w k )∣∣ 2 ) ≤2E( 4 1 ∣∣∇f j k (x k )−∇f j k (w k )∣∣ 2 )+2E( 4 1 ∣∣∇f(w k )∣∣ 2 ) ≤2L max E( 4 1 ∣∣x k −w k ∣∣ 2 )+2E( 4 1 ∣∣∇f(w k )∣∣ 2 )→0, а значит, дисперсия g k g k стремится к нулю. Приведённые выше рассуждения не являются формальным доказательством сходимости метода, но частично объясняют, почему метод сходится и, самое главное, объясняют интуицию позади формул, задающих метод. Строгое доказательство можно прочитать вот тут. Мы же здесь приведём результат о сходимости немного другого метода — Loopless Stochastic Variance Reduced Gradient (L-SVRG), который был предложен в 2015 году и",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 7,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(x k )−∇f j k (w k )∣∣ 2 )+2E( 4 1 ∣∣∇f(w k )∣∣ 2 ) ≤2L max E( 4 1 ∣∣x k −w k ∣∣ 2 )+2E( 4 1 ∣∣∇f(w k )∣∣ 2 )→0, а значит, дисперсия g k g k стремится к нулю. Приведённые выше рассуждения не являются формальным доказательством сходимости метода, но частично объясняют, почему метод сходится и, самое главное, объясняют интуицию позади формул, задающих метод. Строгое доказательство можно прочитать вот тут. Мы же здесь приведём результат о сходимости немного другого метода — Loopless Stochastic Variance Reduced Gradient (L-SVRG), который был предложен в 2015 году и переоткрыт в 2019 году. Основное отличие от SVRG состоит в том, что точка w k w k теперь обновляется на каждой итерации с некоторой маленькой вероятностью p ∼ 1 / n p∼1/n: с вероятностью с вероятностью p . w k+1 с вероятностью 1−p, с вероятностью p. Иными словами, L-SVRG имеет случайную длину цикла, в котором w k w k не обновляется. Вся интуиция и все наблюдения приведённые для SVRG выше, справедливы и для L-SVRG. Можно доказать следующий результат. Теорема. Предположим, что f f является L L-гладкой, μ μ-сильно выпуклой и имеет вид суммы, функции f i f i являются выпуклыми и L i L i -гладкими для всех i=1,…,n, и размер шага удовлетворяет max ⁡ 0<α≤1/6L max , где L max ⁡ = max max =max i∈1,…,n L i . Тогда для всех k ≥ 0 k≥0 для итераций L-SVRG выполняется неравенство min ∣∣x k −x ∗ ∣∣ 2 )≤(1−min{αμ, где =∣∣x i=1 n ∣∣∇f i (x 0 )−∇f i (x ∗ )∣∣. Замечание. В частности, если max ⁡ α=1/6L max p=1/n, то min ⁡ { μ 6 L max ∣∣x k −x ∗ ∣∣ 2 )≤(1−min{ 6L max Следовательно, чтобы гарантировать ∣∣x k −x ∗ ∣∣ 2 )≤ε, L-SVRG требуется max ⁡ μ ) log O((n+ μ L max )log( ε V 0 )) итераций/подсчётов градиентов слагаемых (в среднем). Напомним, что чтобы гарантировать то же самое, градиентному спуску (GD) необходимо сделать O ( n L μ log O(n μ L log( με L∣∣x подсчётов градиентов слагаемых, поскольку каждая итерация GD требует n n подсчётов градиентов слагаемых (нужно вычислять полный градиент ∇f(x)= n 1 ∑ i=1 n ∇f i (x)). Можно показать, что L ≤ L max ⁡ ≤ n L L≤L max ≤nL, поэтому в худшем случае полученная оценка для L-SVRG не лучше, чем для GD. Однако в случае, когда L max max =O(L), L-SVRG имеет сложность значительно лучше, чем GD (если пренебречь логарифмическими множителями). В заключение этого раздела, хотелось бы отметить, что существуют и другие методы редукции дисперсии. Одним из самых популярных среди них является SAGA. В отличие от SVRG/L-SVRG, в методе SAGA хранится набор градиентов ),∇f 2 (w k 2 ),…,∇f n (w k n ). Здесь точка обозначает точку, в которой в последний раз был подсчитан градиент функции i i до итерации k k. Формально это можно записать следующим образом: =…=w =∇f j k (x k )−∇f i=1 для всех k+1 j k =x k ,w",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 8,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "когда L max max =O(L), L-SVRG имеет сложность значительно лучше, чем GD (если пренебречь логарифмическими множителями). В заключение этого раздела, хотелось бы отметить, что существуют и другие методы редукции дисперсии. Одним из самых популярных среди них является SAGA. В отличие от SVRG/L-SVRG, в методе SAGA хранится набор градиентов ),∇f 2 (w k 2 ),…,∇f n (w k n ). Здесь точка обозначает точку, в которой в последний раз был подсчитан градиент функции i i до итерации k k. Формально это можно записать следующим образом: =…=w =∇f j k (x k )−∇f i=1 для всех k+1 j k =x k ,w k+1 i =w k i для всех i k+1 =x k −αg k . Основное преимущество SAGA состоит в том, что не требуется вычислять полный градиент всей суммы по ходу работы метода, однако в начале требуется посчитать градиенты всех слагаемых (отмечаем здесь, что эта операция может быть гораздо дороже по времени, чем вычисление полного градиента) и, более того, требуется хранить n n векторов, что может быть недопустимо для больших датасетов. В плане теоретических гарантий SAGA и L-SVRG не отличимы. Ниже приведён график с траекториями SGD (с постоянным шагом), L-SVRG и SAGA при решении задачи логистической регрессии. Как можно видеть из графика, SGD достаточно быстро достигает не очень высокой точности и начинает осциллировать вокруг решения. В то же время, L-SVRG и SAGA достигают той же точности медленнее, но зато не осциллируют вокруг решения, а продолжают сходится (причём линейно). Сравнение работы SGD, L-SVRG и SAGA при решении задачи логистической регрессии на датасете gisette из библиотеки LIBVSM. Shodimost Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 14.3. Методы второго порядка От метода Ньютона до LBFGS Следующий параграф 15.1. Введение в онлайн-обучение",
    "metadata": {
      "title": "Сходимость SGD",
      "url": "https://education.yandex.ru/handbook/ml/article/shodimost-sgd",
      "course": "ml",
      "chapter": "14. Оптимизация в ML",
      "chapter_id": "14.4",
      "part": 9,
      "total_parts": 9,
      "source_file": "14.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "О чём раздел про онлайн-обучение, кому и зачем его читать? Во многих случаях обучение ML-модели ― это однократный процесс, после которого она не меняется и только используется для предсказания. А что, если к нам постоянно поступает новая информация и мы должны её учитывать? Тогда модель должна уметь обновляться при поступлении нового объекта или батча объектов. Грубо говоря, этим и занимается онлайн-оптимизация. Можно заметить, что обновление модели на батче объектов проходит и в процессе стохастической оптимизации, ― и это сходство не случайно. Оказывается, что все известные вам методы стохастической оптимизации первого порядка ― такие как SGD, AdaGrad, Adam, AMSgrad и другие ― являются в первую очередь алгоритмами онлайн-обучения. Чтобы в этом убедиться, достаточно открыть эти статьи и увидеть, для какой задачи выводятся гарантии на сходимость. Постановка задачи онлайн-обучения является одновременно математически простой и очень общей, соединяя три больших темы: «Классическое» онлайн обучение. Стохастическую оптимизацию на фиксированном датасете. Мы покажем, что любой алгоритм онлайн обучения можно переформулировать, как алгоритм стохастической оптимизации; при этом из гарантий на сходимость, полученных для онлайн обучения, автоматически будет следовать сходимость на фиксированном датасете. Adversarial обучение. Данный текст является в первую очередь систематизирующим. Мы постараемся достичь следующих целей: Подведем единую математическую базу, необходимую для вдумчивого чтения статей по оптимизации. Это будет полезно ML-теоретикам. Покажем, как исторически развивались методы оптимизации, как из одного метода получался другой, какие проблемы они решали и ― главное ― актуальны ли эти проблемы сейчас. Разберём все «именные» методы оптимизации на набор базовых концепций и покажем, как вы можете самостоятельно их сочетать, создавая оптимальный метод для решения своей задачи. Спойлер: базовых концепций намного меньше, чем наименований методов. Эти знания будут полезны ML-инженерам. Пройдемся по относительно нишевым темам, таким как разреженные методы регуляризации 1/2 , и рассмотрим наилучшие методы оптимизации для них. Такие методы невозможно получить в стандартной постановке стохастической оптимизации. Эти знания будут полезны ML-инженерам, занимающимся рекомендательными системами. В параграфе «Введение в онлайн-обучение», которую вы читаете сейчас, вы познакомитесь с общей постановкой задачи онлайн-обучения, а также с семейством алгоритмов Follow the Regularized Leader (FTRL), которое включает в себя все методы первого порядка. Кроме того, вы узнаете, как сводить задачи стохастической оптимизации к задачам онлайн-обучения и увидите, что этот переход позволяет строить более эффективные методы стохастической оптимизации, особенно для разреженных регуляризаторов вроде L 1 L 1 . В параграфе «Адаптивный FTRL» вы узнаете, как улучшить сходимость алгоритмов стохастической оптимизации с помощью регуляризаторов и каковы гарантии сходимости для регуляризованных задач. Это позволит вывести AdaGrad как наилучший адаптивный метод для онлайн-оптимизации. В параграфе «Регуляризация в онлайн-обучении» мы снова поговорим о регуляризации, но на этот раз речь пойдёт о регуляризаторах, которые накладывают на решение определённые органичения, например, разреженность. Вы сможете с новой стороны взглянуть на разреживающие свойства L 1 L 1 -регуляризаторов. Кроме того, мы получим не достижимые с помощью обычных SGD/AdaGrad результаты для разреженных 1/2 регуляризаторов. В параграфе «Стохастическая оптимизация в Deep Learning» мы перейдём к методам оптимизации в глубоких нейросетях. Вас ждёт краткий исторический обзор и мотивация появления двух важных модификаций AdaGrad ― Adam и RMSprop. Мы покажем, что эти методы ломаются вокруг критических точек, и поговорим",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 1,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "метод для онлайн-оптимизации. В параграфе «Регуляризация в онлайн-обучении» мы снова поговорим о регуляризации, но на этот раз речь пойдёт о регуляризаторах, которые накладывают на решение определённые органичения, например, разреженность. Вы сможете с новой стороны взглянуть на разреживающие свойства L 1 L 1 -регуляризаторов. Кроме того, мы получим не достижимые с помощью обычных SGD/AdaGrad результаты для разреженных 1/2 регуляризаторов. В параграфе «Стохастическая оптимизация в Deep Learning» мы перейдём к методам оптимизации в глубоких нейросетях. Вас ждёт краткий исторический обзор и мотивация появления двух важных модификаций AdaGrad ― Adam и RMSprop. Мы покажем, что эти методы ломаются вокруг критических точек, и поговорим о том, как починить это и достичь более точной сходимости (этого эффекта можно достичь либо прямой модификацией алгоритмов (AMSgrad и RAdam), либо косвенно с помощью Learning Rate Scheduler'ов). В конце параграфа мы соберём воедино все рассмотренные концепции и покажем, как можно комбинировать лучшее из разных методов оптимизации в один новый метод. Оглавление Постановка задачи Литература. Отсюда и далее, пока явно не скажем о переходе на другие источники информации, используется материал из книги Shai Shalev-Shwartz Online Learning and Online Convex Optimization Онлайн-обучение ― это процесс предсказания ответов на последовательность вопросов с учётом знания (возможно, неполного) о предыдущих правильных ответах. Представим себе следующую игру (назовём её игра (1)). На каждом раунде игры t t мы: Получаем x t x t ― частичную информацию о текущем «вопросе»; Выбираем модель w t w t , которой будем делать прогноз; Прогнозируем Получаем истинный ответ y t y t ; Получаем обратную связь-лосс l(p t ,y t ). Лоссы обычно имеют семантику функции ошибки: больше ― хуже, меньше ― лучше. Цель любого алгоритма онлайн обучения ― минимизация суммарной ошибки прогнозов Loss(T)= t=1 ∑ T l(p t ,y t ) для любого количества раундов T T. Пока рассмотрим интуитивный пример: линейная регрессия (обозначения взяты из параграфа про линейные модели). Пусть у нас уже сыграны раунды 1,…,t−1 и есть выборка данных ,…,x t−1 и ответов ,…,y t−1 . Получаем новый x t x t . В данном случае просто получаем и пока не используем; Выбираем модель w t w t , которая наилучшим образом объясняет всю предыдущую имеющуюся выборку x 1.. t x 1..t (алгоритм обучения можем выбирать любой, какой нам нравится); Прогнозируем =<w t ,x t >; Получаем правильный ответ y t y t ; Считаем loss Действуя таким образом, мы делаем интуитивное предположение, что ответы y t y t как-то зависят от наших x t x t и что эту зависимость мы можем выучить из предыдущей выборки, улучшив прогноз на новых объектах. Предположения Теория онлайн обучения выгодно отличается от классической теории статистического обучения довольно расслабленными и гораздо более простыми (с точки зрения математических формулировок) условиями. Мы не делаем предположений о некой статистической зависимости между . Зависимость может быть детерминированной, стохастической или даже adversarial: Детерминированная: в самом начала игры делается выбор детерминированной зависимости Стохастическая: x t x t может быть реализациями случайной величины, зависящей от y t y t Adversarial: мы играем против активного противника, который может на каждом раунде игры по своему усмотрению менять зависимость и/или",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 2,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "t и что эту зависимость мы можем выучить из предыдущей выборки, улучшив прогноз на новых объектах. Предположения Теория онлайн обучения выгодно отличается от классической теории статистического обучения довольно расслабленными и гораздо более простыми (с точки зрения математических формулировок) условиями. Мы не делаем предположений о некой статистической зависимости между . Зависимость может быть детерминированной, стохастической или даже adversarial: Детерминированная: в самом начала игры делается выбор детерминированной зависимости Стохастическая: x t x t может быть реализациями случайной величины, зависящей от y t y t Adversarial: мы играем против активного противника, который может на каждом раунде игры по своему усмотрению менять зависимость и/или подбирать l(p t ,y t ), имея на руках в том числе текущий ответ y t y t , не доступный алгоритму онлайн-обучения Adversarial постановка включает в себя все остальные как частные случаи, так что сразу будем строить теорию для наиболее общего случая. Поведение алгоритма на шаге T Начнем с введения метрики качества алгоритма на некотором раунде игры T T, а затем расширим ее на все раунды игры. Если у противника нет никаких ограничений, то противник всегда выигрывает. Поскольку l(p t ,y t ) выбирается после нашего прогноза, он может выбрать любую функцию с сколь угодно большим штрафом. Чтобы такого не случалось, мы предположим, что все ответы на шаге T T должны быть сгенерированы некоторым отображением :X→Y,h ∗ ∈H, где H H ― пространство возможных решений, известное и онлайн-алгоритму, и противнику. С учетом введенного ограничения на поведение противника, введем понятие regret: Regret T (h)= t=1 ∑ T l(p t ,y t )− t=1 ∑ T l(h(x t ),y t ) Regret ― это метрика того, насколько онлайн алгоритм работает хуже, чем некоторая фиксированная модель-бейзлайн h (regret переводится как «сожаление»: насколько мы пожалели о том, что взяли онлайн алгоритм, а не модель h). Поскольку мы работаем в adversarial случае, то логично сравнивать наш онлайн алгоритм с сильнейшим возможным противником, а именно: противник всегда выбирает не «некоторую», а наилучшую модель-бейзлайн ∈H: max maxRegret(T)= h ∗ ∈H max [ t=1 ∑ T l(p t ,y t )− t=1 ∑ T l(h ∗ (x t ),y t )] Поведение алгоритма на всей последовательности раундов игры Вспомним, что вообще-то мы играем игру с бесконечным числом раундов. В таком случае, естественно будет анализировать поведение ряда maxRegret(T),T∈N,T→∞. Здесь хочется еще раз подчеркнуть, в чем заключается adversarial поведение: на каждом шаге t maxRegret будет иметь свою наилучшую модель в бейзлайне: max maxRegret(T 1 )= h ∗ ∈H max t=1 ∑ T 1 l(p t ,y t )− t=1 ∑ T 1 l(h ∗ (x t ),y t )= t=1 ∑ T 1 l(p t ,y t )− t=1 ∑ T 1 l(h T 1 ∗ (x t ),y max maxRegret(T 2 )= h ∗ ∈H max t=1 ∑ T 2 l(p t ,y t )− t=1 ∑ T 2 l(h ∗ (x t ),y t )= t=1 ∑ T 2 l(p t ,y t )− t=1 ∑ T 2 l(h T 2 ∗ (x t ),y t ) Качество онлайн алгоритма на протяжении всей игры Когда мы говорим",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 3,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "t=1 ∑ T 1 l(p t ,y t )− t=1 ∑ T 1 l(h ∗ (x t ),y t )= t=1 ∑ T 1 l(p t ,y t )− t=1 ∑ T 1 l(h T 1 ∗ (x t ),y max maxRegret(T 2 )= h ∗ ∈H max t=1 ∑ T 2 l(p t ,y t )− t=1 ∑ T 2 l(h ∗ (x t ),y t )= t=1 ∑ T 2 l(p t ,y t )− t=1 ∑ T 2 l(h T 2 ∗ (x t ),y t ) Качество онлайн алгоритма на протяжении всей игры Когда мы говорим про adversarial setting и игру с противником, мы хотим не просто как-то минимизировать кумулятивный Loss(T)= t=1 ∑ T l(p t ,y t ), но еще и хотим быть не хуже нашего противника. Потребуем, чтобы lim T→∞ lim T 1 maxRegret(T)=0 Такое условие означает, что regret должен расти медленнее чем линейно (в таком случае говорят, что алгоритм имеет сублинейный regret). Сублинейности бывают разные. Так, Regret T может быть ограничен сверху рядом с асимптотикой T T или же рядом с асимптотикой log ⁡ T logT Асимптотика log ⁡ T logT, очевидно, приводит к намного лучшей сходимости. Но достичь этого не всегда возможно. Стандартной асимптотикой regret в большинстве используемых на практике алгоритмов является T T , для этой асимптотики условия на задачу наименее жесткие. Все рассматриваемые нами ниже алгоритмы будут иметь асимптотику T T и отличаться в основном константами в оценках (но, конечно, отличия в константах при оценке Regret часто приводят к существенно разному поведению на практике). Любые более мощные асимптотики требуют условий, которые крайне редко выполняются в практических задачах Online to batch conversion В данном обзоре мы будем анализировать методы, которые гораздо чаще используются для оптимизации в классической постановке: есть фиксированный датасет i=1 N , модель (x) с обучаемыми параметрами w w и функция потерь f f, задача ― найти минимум функции i=1 ∑ N f(p w (x i ),y i ) Если представить, что все наши f(p w (x i ),y i ) ― независимые одинаково распределенные случайные величины, то можно считать, что на самом деле мы оптимизируем i=1 ∑ N f(p w (x i ),y i )≈E (x,y) f(p w (x),y) Такую постановку задачи часто называть батчевой (англ. batch). Это означает, что мы можем использовать два класса методов оптимизации: методы, которые на каждом шаге смотрят сразу на всю выборку (например, градиентный спуск или метод Ньютона); методы, которые на каждом шаге смотрят на случайное подмножество данных в надежде, что, итерируясь по таким подмножествам, мы сможем соптимизировать матожидание E f ( w ) Ef(w) (например, SGD). Такие методы называют стохастическими. Существует специальный класс методов анализа сходимости, называемый online to batch conversion. Они позволяют адаптировать алгоритм онлайн-обучения к постановке задачи стохастической оптимизации на фиксированном датасете; при этом оценка на regret транслируется в асимптотику сходимости стохастической оптимизации. Математически строгий вывод этих методов обычно довольно громоздкий и не дарит более глубокого понимания идей в современных стохастических методах, это чисто технические выкладки, поэтому мы здесь ограничимся интуитивным описанием. Строгий вывод можно найти, например, в упомянутой выше",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 4,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "смотрят на случайное подмножество данных в надежде, что, итерируясь по таким подмножествам, мы сможем соптимизировать матожидание E f ( w ) Ef(w) (например, SGD). Такие методы называют стохастическими. Существует специальный класс методов анализа сходимости, называемый online to batch conversion. Они позволяют адаптировать алгоритм онлайн-обучения к постановке задачи стохастической оптимизации на фиксированном датасете; при этом оценка на regret транслируется в асимптотику сходимости стохастической оптимизации. Математически строгий вывод этих методов обычно довольно громоздкий и не дарит более глубокого понимания идей в современных стохастических методах, это чисто технические выкладки, поэтому мы здесь ограничимся интуитивным описанием. Строгий вывод можно найти, например, в упомянутой выше книге Shai Shalev-Schwartz. Процесс стохастической оптимизации на фиксированном датасете можно представить в виде задачи онлайн обучения, если вытянуть все эпохи (проходы по датасетам) в единую последовательность. Мы получим задачу онлайн обучения, в которой ) сэмплируются из фиксированного множества ),…,(x N ,y N ). Строго говоря, тут сэмлпирование двухстадийное: Берем исходное множество функций Сэмплируем из него без возвращения, пока множество не станет пустым Как только оно стало пустым ― заново заполняем его Таким образом, деление на \"эпохи\" отчетливо видно и в вытянутой последовательности. Легко видеть, что эта задача является корректной задачей онлайн обучения. Тут мы активно пользуемся тем, что постановка задачи онлайн обучения математически простая и очень общая. Из корректности данной задачи следует, что все алгоритмы онлайн обучения будут иметь на такой последовательности сублинейный regret. Следующим шагом давайте взглянем на regret в момент смены эпохи. Обозначим за M M―число эпох, тогда: max max max maxRegret(T)= h ∗ ∈H max [ t=1 ∑ T l(p t ,y t )− t=1 ∑ T l(h ∗ (x t ),y t )]= h ∗ ∈H max [ m=1 ∑ M i=1 ∑ N l(p m,i ,y i )− m=1 ∑ M i=1 ∑ N l(h ∗ (x i ),y i )]= h ∗ ∈H max [ m=1 ∑ M i=1 ∑ N l(p m,i ,y i )−M i=1 ∑ N l(h ∗ (x i ),y i )] Из сходимости последовательности следует сходимость любой ее подпоследовательности, а значит, последовательность regret'ов в моменты смены эпох тоже ведет себя сублинейно: 1 M N max max [ m=1 ∑ M i=1 ∑ N l(p m,i ,y i )−M i=1 ∑ N l(h ∗ (x i ),y i )]→0 max max [ MN 1 m=1 ∑ M i=1 ∑ N l(p m,i ,y i )− N 1 i=1 ∑ N l(h ∗ (x i ),y i )]→0 min m=1 ∑ M i=1 ∑ N l(p m,i min [ N 1 i=1 ∑ N l(h ∗ (x i ),y i )]→0 Последнее слагаемое уже выглядит практически как постановка задачи стохастической оптимизации на фиксированном датасете! Интуиция на данный момент подсказывает нам, что разрыв между решениями, даваемыми онлайн обучением, и точным решением задачи батч-оптимизации, будет постепенно сокращаться. В этот момент интуицию можно выключать―остаются только строгие технические выкладки по ссылкам выше. Выпуклая онлайн-оптимизация Выпуклая оптимизация играет центральную роль в анализе алгоритмов онлайн-обучения и позволяет получать эффективные алгоритмы. Вот примеры задач, в которых она хорошо работает: Линейная оптимизация; Expert Advice problem; Линейная/логистическая регрессия. Для задач,",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 5,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "M i=1 ∑ N l(p m,i min [ N 1 i=1 ∑ N l(h ∗ (x i ),y i )]→0 Последнее слагаемое уже выглядит практически как постановка задачи стохастической оптимизации на фиксированном датасете! Интуиция на данный момент подсказывает нам, что разрыв между решениями, даваемыми онлайн обучением, и точным решением задачи батч-оптимизации, будет постепенно сокращаться. В этот момент интуицию можно выключать―остаются только строгие технические выкладки по ссылкам выше. Выпуклая онлайн-оптимизация Выпуклая оптимизация играет центральную роль в анализе алгоритмов онлайн-обучения и позволяет получать эффективные алгоритмы. Вот примеры задач, в которых она хорошо работает: Линейная оптимизация; Expert Advice problem; Линейная/логистическая регрессия. Для задач, возникающих в глубинном обучении, мы поступим согласно рекомендациям ведущих ученых: возьмем теоретически обоснованный алгоритм выпуклой оптимизации, воткнем в нейросеть и помолимся, чтобы он сохранил свои хорошие свойства. С методами первого порядка, как правило, работает (а здесь мы будем рассматривать только такие методы) Введём в нашу игру предположение о выпуклости, а заодно попробуем сделать вычисления менее громоздкими. Для этого определим упрощённую игру (2): Выбираем параметрическую модель w t w t ; Получаем извне выпуклую функцию потерь (w); Считаем f t f t в точке w t w t и получаем наш loss Первое упрощение состоит в том, что прогноз h t h t и бейзлайн мы теперь берём не из абстрактного функционального множества H H, а из некоторого параметризованного семейства. Говоря «модель w t w t », мы имеем в виду «модель, заданная параметрами w t w t ». Скажем, для линейной регрессии это может быть вектор весов и bias. Regret будет записываться следующим образом: maxRegret T = t=1 t=1 Второе упрощение в том, что мы не думаем о признаках x t x t и таргетах y t y t . Вся эта информация спрятана в определение функции (w). Например, для линейной регрессии (w)=(x t T w−y t ) 2 . При этом теперь у нас нет частичной информации о текущем раунде игры перед выбором новой модели w t w t : ведь мы сначала выбираем w t w t и лишь потом получаем (w). Обратите внимание: если вы попробуете себе представить онлайн алгоритм на практике, то, как правило, частичная информация о функции (w) перед выбором w t w t вам доступна. Например, рассмотрим рекомендательную систему с онлайн-дообучаемой ранжирующей моделью: Пользователь пришел, мы сразу пошли в базу данных за его историей покупок и получили признаковое описание (возможно частичное) x t x t ; С учётом этого признакового описания мы выбираем модель w t w t и с её помощью оцениваем релевантность товаров этому пользователю; Смотрим, что купил пользователь и купил ли, это даёт нам Тем не менее, в этом параграфе мы будем считать, что частичной информации нет, потому что хотим разрабатывать наиболее общий фреймворк, а не ad-hoc алгоритмы, использующие конкретный вид этой частичной информации. Если даже для какой-то узкой проблемы можно сформулировать специфический алгоритм, учитывающий частичную информацию, с высокой вероятностью он не будет работать значимо лучше стандартного решения. Если знаете контрпримеры ― напишите, добавим сюда для полноты. Follow the Leader Предположим, что мы провели t t шагов игры (2) и теперь",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 6,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "w t w t и с её помощью оцениваем релевантность товаров этому пользователю; Смотрим, что купил пользователь и купил ли, это даёт нам Тем не менее, в этом параграфе мы будем считать, что частичной информации нет, потому что хотим разрабатывать наиболее общий фреймворк, а не ad-hoc алгоритмы, использующие конкретный вид этой частичной информации. Если даже для какой-то узкой проблемы можно сформулировать специфический алгоритм, учитывающий частичную информацию, с высокой вероятностью он не будет работать значимо лучше стандартного решения. Если знаете контрпримеры ― напишите, добавим сюда для полноты. Follow the Leader Предположим, что мы провели t t шагов игры (2) и теперь выбираем модель w t + 1 w t+1 (как условились, без информации о t+1 (w)). Наиболее естественным выбором будет алгоритм, минимизирующий ошибку на всех предыдущих раундах min t+1 =arg w min s=1 ∑ t f s (w) Такой алгоритм называется Follow The Leader (FTL), потому что мы идем вплотную за наилучшим возможным алгоритмом-бейзлайном в regret (лидером), который учитывает ещё и информацию с ( t + 1 ) (t+1)-го шага: min t+1 =arg w min s=1 ∑ t+1 f s (w) К сожалению, для алгоритма в таком виде есть важные примеры выпуклых задач, когда он не работает. Допустим, наши функции потерь линейны (w)=g t T w. Вам может показаться, что линейная функция не особенно похожа на функцию потерь, но, забегая вперед, именно такие функции потерь встретятся дальше при изучении градиентных онлайн-алгоритмов ( =∇f t (w t )). Рассмотрим одномерную задачу (w)=g ∈R, ∈[−1;1]. Пусть g t = { − 0.5 −0.5 1 −1 t=1 t%2=0 t%2=1 Алгоритм FTL выглядит так: min min T+1 =arg w min t=1 ∑ T g t w=arg w min w( t=1 ∑ T g t ) Такие осциллирующие суммы коэффициентов будут заставлять FTL выбирать наихудшее возможное решение в каждом раунде. Функция потерь в каждом раунде будет равна 0.5 0.5, а кумулятивная функция потерь примет вид ∑ t = 1 T 0.5 = 0.5 T t=1 ∑ T 0.5=0.5T. При этом кумулятивная функция потерь константного решения =0 будет равна 0. Получаем линейный regret 0.5 T 0.5T относительно бейзлайна =0, алгоритм не сходится. Follow The Regularized Leader Чтобы стабилизировать алгоритм, мы добавим регуляризаторы, и назовем получившийся алгоритм Follow The Regularized Leader (FTRL): min =arg w min [ t=1 ∑ T f t (w)+R(w)] Упражнение. Проверьте, что в примере из предыдущего параграфа добавление регуляризатора стабилизирует осцилляцию решения w w. Добавка R ( w ) R(w) должна быть выпуклой и неотрицательной. При этом различный выбор R ( w ) R(w) будет приводить к различным алгоритмам и различным оценкам на regret. Первое, что приходит в голову ― это L 2 L 2 регуляризатор R(w)=∣∣w∣∣ 2 2 . Он даёт алгоритм min =arg w min [ t=1 ∑ T f t (w)+ 2λ 1 ∣∣w∣∣ 2 2 ] Adaptive FTRL Следующая идея―сделать регуляризатор зависящим от данных (то есть от f t f t ) и своим на каждом раунде T: min =arg w min [ t=1 ∑ T f t (w)+R T (w)] Забегая вперед―все современные градиентные алгоритмы Adam, RMSProp, AdaGrad и",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 7,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "неотрицательной. При этом различный выбор R ( w ) R(w) будет приводить к различным алгоритмам и различным оценкам на regret. Первое, что приходит в голову ― это L 2 L 2 регуляризатор R(w)=∣∣w∣∣ 2 2 . Он даёт алгоритм min =arg w min [ t=1 ∑ T f t (w)+ 2λ 1 ∣∣w∣∣ 2 2 ] Adaptive FTRL Следующая идея―сделать регуляризатор зависящим от данных (то есть от f t f t ) и своим на каждом раунде T: min =arg w min [ t=1 ∑ T f t (w)+R T (w)] Забегая вперед―все современные градиентные алгоритмы Adam, RMSProp, AdaGrad и т.д. попадают в это семейство data-dependent регуляризаторов и работают значительно эффективнее любых алгоритмов с константными регуляризаторами R ( w ) R(w). Обратите внимание: регуляризаторы являются частью алгоритма FTRL, они не входят в формулу для regret, которая по-прежнему имеет вид Regret T (w ∗ )= t=1 t=1 Таким образом, мы не изменили постановку решаемой нами задачи, изменили лишь метод ее решения. Обратите внимание: введение регуляризаторов влияет только на онлайн-алгоритм и выбор w t w t . Бейзлайны выбираются как и раньше: min =arg w min t=1 ∑ T f t (w) Линеаризация и вычислительно эффективный FTRL Рассмотрим пример с логистической регрессией log (w)=log(1+e ) и константным L 2 L 2 регуляризатором: ∑ t = 1 T log min ⁡ w t=1 ∑ T log(1+e ∣∣w∣∣ 2 2 ⟶ w min Классический пример использования онлайн логистической регрессии ― предсказание CTR в рекламе. Миллионы запросов в секунду => миллионы решений этой оптимизационной задачи в секунду (если разбивать на батчи ― тысячи, но сути это не меняет). Успех онлайн-алгоритма в таких задачах определяется его вычислительной эффективностью, как по памяти, так и по скорости. Увы, с этим у нашего алгоритма не всё так хорошо: Скорость: аналитически задача не решается => FAIL Память: нужно хранить все предыдущие запросы 1.. T t∈1..T => FAIL Здесь нам на помощь приходит линеаризация задачи. Если фунции (w) выпуклые (вниз) и гладкие (на негладкие посмотрим позже), то они удовлетворяют основному свойству выпуклых функций f(w)≥f(w t )+[∇f(w t )] T (w−w t ) Разложим все функции (w) в точках (w)≥f t (w t )+[∇f(w t )] T (w−w )−f t (w)≤[∇f(w t )] T (w−w t ) Просуммируем от 1 до t=1 )−f t (w))≤ t=1 ∑ T ([∇f −[∇f Теперь обозначим =∇f t (w t ) и рассмотрим выпуклую линейную задачу онлайн обучения с функцией потерь (w)=g t T w. Regret для нее выглядит как LinearizedRegret T (w ∗ )= t=1 t=1 Неравенство выше позволяет нам оценить regret исходной задачи через regret линеаризованной: Regret T (w ∗ )≤LinearizedRegret T (w ∗ ) Минимизируя правую часть неравенства, мы, безусловно, будем минимизировать и левую, так что мы можем выбирать w t w t алгоритмом, решающим линеаризованную задачу, и получать хорошо сходящийся метод для исходной задачи. Посмотрим, будет ли линеаризованный алгоритм вычислительно эффективнее. Посмотрим на линеаризацию задачи с data-depedent регуляризатором: min =arg w min [ t=1 ∑ T ∇[f t (w t )] T w+R T (w)] Линейные задачи имеют аналитическое решение для широкого",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 8,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "для нее выглядит как LinearizedRegret T (w ∗ )= t=1 t=1 Неравенство выше позволяет нам оценить regret исходной задачи через regret линеаризованной: Regret T (w ∗ )≤LinearizedRegret T (w ∗ ) Минимизируя правую часть неравенства, мы, безусловно, будем минимизировать и левую, так что мы можем выбирать w t w t алгоритмом, решающим линеаризованную задачу, и получать хорошо сходящийся метод для исходной задачи. Посмотрим, будет ли линеаризованный алгоритм вычислительно эффективнее. Посмотрим на линеаризацию задачи с data-depedent регуляризатором: min =arg w min [ t=1 ∑ T ∇[f t (w t )] T w+R T (w)] Линейные задачи имеют аналитическое решение для широкого спектра (w). Собственно, это и есть основное, что нужно помнить на практике ― выбирать регуляризатор так, чтобы эта задача решалась аналитически. Мы рассмотрим простейший случай (w)=R(w)= 2η 1 ∣∣w∣∣ min =arg w min [ t=1 ∣∣w∣∣ 2 2 ] Справа дифференцируемая функция, так что мы можем найти w T w T , приравняв к нулю градиент: =−η t=1 )=−ηz T , где t=1 ) ― это сумма векторов, которую не нужно пересчитывать заново на каждом шаге, а можно инкрементально обновлять. Благодаря этому нам больше не нужно помнить все предыдущие объекты выборки, достаточно хранить лишь некоторую статистику. Готово, мы построили наш первый вычислительно эффективный алгоритм онлайн обучения! В дальнейшем мы займемся тем, чтобы найти наилучший вычислительно эффективный алгоритм. Обратите внимание: теперь вы понимете, почему пример с линейной функцией потерь был так важен: линейные функции соответствуют линеаризованному regret. При этом, как мы уже выяснили, без регуляризатора такие линеаризованные задачи нестабильны. Обратите внимание: если переписать немного формулу для w T w T , мы получим: =−η t=1 )=w T−1 −η∇f t (w t ) Таким образом, формулы FTRL c константным регуляризатором эквивалентны формулам обычного стохастического градиентного спуска. Забегая вперед, скажем, что различия в формулах градиентного спуска и FTRL будут только в разделе Composite objective FTRL. В этих отличиях и будет заключаться преимущество FTRL перед привычным SGD. Обратите внимание: концепции FTRL и gradient descent в литературе часто называют lazy (ленивая) и greedy (жадная) соответственно. Gradient descent жадный, потому что алгоритм для обновления w t + 1 w t+1 использует только текущий w t w t и текущий градиент g t g t . Всё, что было на предыдущих шагах, алгоритм забывает. FTRL ленивый, потому что алгоритм в явном виде сохраняет всю информацию с начала обучения и рассчитывает w t + 1 w t+1 , исходя из всей истории ,…,g t , и только после этого применяет все регуляризаторы. Подробнее мы расскажем об этом в разделе «Сравнение Composite Objective FTRL-Proximal и Adaptive Gradient Descent». Субдифференциал и субградиентные методы Выше мы рассматривали гладкие функции (w). Гладкость ― сильное ограничение, и оно на самом деле необязательно, можно ослабить условие, если использовать субградиенты. Когда мы переходили от исходной задачи к линеаризованной, мы использовали основное свойство гладких выпуклых функций f(w)≥f(w t )+[∇f(w t )] T (w−w t ),∀w t Гладкость обеспечивает существование ∇f(w t ) для всех w t w t . Но нам ведь не нужно, чтобы существовал именно градиент функции. Нам достаточно, чтобы существовал какой-то вектор g",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 9,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "этого применяет все регуляризаторы. Подробнее мы расскажем об этом в разделе «Сравнение Composite Objective FTRL-Proximal и Adaptive Gradient Descent». Субдифференциал и субградиентные методы Выше мы рассматривали гладкие функции (w). Гладкость ― сильное ограничение, и оно на самом деле необязательно, можно ослабить условие, если использовать субградиенты. Когда мы переходили от исходной задачи к линеаризованной, мы использовали основное свойство гладких выпуклых функций f(w)≥f(w t )+[∇f(w t )] T (w−w t ),∀w t Гладкость обеспечивает существование ∇f(w t ) для всех w t w t . Но нам ведь не нужно, чтобы существовал именно градиент функции. Нам достаточно, чтобы существовал какой-то вектор g t g t , для которого выполнено неравенство f(w)≥f(w t )+g T (w−w t ) И в этом помогают следующие два понятия. Субдифференциалом функции f ( w ) f(w) в точке w t w t называется множество f(w)={g t ∣f(w)≥f(w t )+f T (w−w t ),∀w} Субградиентом функции f ( w ) f(w) в точке w t w t называется любой элемент множества ∂f(w t ). Потребуем, чтобы для любой точки был непустой субдифференциал, и дело в шляпе, можно вместо (w) везде подставлять субградиент g t g t и обобщить все выкладки выше на негладкий случай. Примеры. Для гладких функций субдифференциал состоит из одной точки: градиента функции, а субградиент равен градиенту. В качестве примера функции с нетривиальным субградиентом рассмотрим функцию f(x)=∣x∣, где x x ― скаляр. Субградиент в точке 0 0 ― это можество ∣x∣={λ∣∣x∣≥αx} Легко видеть, что ∣x∣ ― это отрезок [−1,1]. Замечание. На практике субдифференциал используют не так часто. Оптимизационные задачи с популярными негладкими регуляризаторами L 1 L 1 решают «в лоб», без перехода к субградиентной оценке, например, с помощью проксимальных методов. Обратите внимание. В литературе очень часто используется термин Online Mirror Descent. Mirror descent ― это оптимизационная процедура вида min t+1 =arg w min w+λψ(w)+∣∣w−w t ∣∣ 2 2 ], в которой ψ ψ ― дополнительный негладкий регуляризатор (например, тот же L 1 L 1 ), который мы как раз таки не заменяем на субградиентную оценку, а вместо этого оптимизируем всё «в лоб». Заметьте, что эти формулы идентичны формулам Proximal Gradient Descent. Если у нас нет регуляризатора ψ ψ, то формулы эквивалентны обычному gradient descent. Как вы увидите дальше, Mirror Descent ― это частный случай общего фреймворка, который мы описываем. Субградиентные методы оптимизации. Почти все градиентные методы оптимизации обобщаются на негладкие функции. Модифицируется необходимое и достаточное условие минимума для выпуклых функций: точка w ∗ w ∗ является минимумом, если субдифференциал содержит ноль: 0∈∂f(w ∗ ). Очевидно, это прямое обобщение условия для гладких функций, где субдифференциал состоит только из градиента функции. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 14.4. Сходимость SGD Почему он всё-таки сходится Следующий параграф 15.2. Адаптивный FTRL",
    "metadata": {
      "title": "Введение в онлайн-обучение",
      "url": "https://education.yandex.ru/handbook/ml/article/onlajn-obuchenie-i-stohasticheskaya-optimizaciya",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.1",
      "part": 10,
      "total_parts": 10,
      "source_file": "15.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В данном разделе мы рассмотрим широкое семейство алгоритмов, позволяющее делать улучшения в способах введения регуляризации, которые невозможно добиться в классическом градиентном спуске. Полезные ссылки Все написанное ниже (за исключением вывода AdaGrad) — сокращенный пересказ обзора H. Brendan McMahan A Survey of Algorithms and Analysis for Adaptive Online Learning. Везде, где мы обозначаем Lemma 4, Theorem 10 и т.д. — мы ссылаемся на соответствующие теоремы из этой статьи. То же самое с доказательствами: если мы что-то опускаем, подробности можно найти в обзоре Интуитивный вывод AdaGrad взят из статьи Adaptive Subgradient Methods for Online Learning and Stochastic Optimization . Вместо оригинальных оценок на метод Regularized Dual Averaging, требующих дополнительных понятий вроде двойственности по Фенхелю, мы использовали аналогичную оценку из обзора выше, сохранив все рассуждения автора. Опять же — строгое доказательство оценок на regret для AdaGrad есть в этом обзоре. Синтаксический сахар В выкладках очень часто используются суммы, и без сокращенных обозначений читать их невозможно. В литературе про онлайн-обучение приняты вот такие сокращения: 0:t (w)= s=0 ∑ t r s (w); Особо отметим обозначение 0:t (w t )= s=0 ), т.е. точка w t w t фиксирована и не меняется с индексацией в сумме; 0:t (w)=f 1:t (w)+r 0:t (w) (обычно это будет сумма функции потерь и регуляризатора); g t g t — субградиент функции (w) в точке w t w t . Аддитивные регуляризаторы В новых обозначениях описанные выше алгоритмы примут вид: Adaptive FTRL: min =arg w min [f 1:t (w)+R T (w)] Adaptive Linearized FTRL: min =arg w min [∇f 1:t (w t ) T w+R T (w)] Опишем условия, накладываемые нами на алгоритм. В обзоре они называются Setting 1. Setting 1 От функций (w) мы потребуем, чтобы они представлялись в виде: (w)= t=0 ∑ T r t (w)=r 0:T (w) Слагаемые должны удовлетворять следующим условиям: Все (w) выпуклы (вниз); (w)≥0; min =arg w min r 0 (w). Также наложим следующие требования на 1:t =f 1:t (w)+r 0:t (w): Область определения h 1 : t h 1:t — непустое множество. Это требование может показаться странным, но при желании можно придумать пример h 1 : t h 1:t с пустой областью определения: достаточно взять несколько регуляризаторов-проекций (w) на непересекающиеся выпуклые множества (подробнее о таких регуляризаторах мы расскажем в одном из следующих разделов); Субдифференциал (w) в точке w t w t непуст. Классы алгоритмов FTRL Будем рассматривать аддитивные регуляризаторы (w) из двух семейств в зависимости от того, где у них минимум: FTRL-Centered: a r g min arg w min r t (w)=w 0 ; FTRL-Proximal: a r g min arg w min r t (w)=w t ; Composite Objective: смешение первых двух семейств. Обратите внимание: название Proximal напрямую связано с проксимальным градиентным спуском (ссылка на учебник с проксимальными методами). В обоих случаях мы накладываем регуляризатор в текущей точке w t w t . Обратите внимание: для Proximal регуляризаторов зачастую требуют выполнения более сильного условия: )=0. Это не такое уж и серьёзное ограничение: все разумные Proximal регуляризаторы (например, ∣∣w−w t ∣∣ 2 ) ему удовлетворяют. Обратите внимание: у обоих семейств есть значимые высокоцитируемые статьи",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 1,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "r g min arg w min r t (w)=w 0 ; FTRL-Proximal: a r g min arg w min r t (w)=w t ; Composite Objective: смешение первых двух семейств. Обратите внимание: название Proximal напрямую связано с проксимальным градиентным спуском (ссылка на учебник с проксимальными методами). В обоих случаях мы накладываем регуляризатор в текущей точке w t w t . Обратите внимание: для Proximal регуляризаторов зачастую требуют выполнения более сильного условия: )=0. Это не такое уж и серьёзное ограничение: все разумные Proximal регуляризаторы (например, ∣∣w−w t ∣∣ 2 ) ему удовлетворяют. Обратите внимание: у обоих семейств есть значимые высокоцитируемые статьи FTRL-Centered: метод Regularized Dual Averaging. Статья получила премию Test of Time Award на NeurIPS 2021, так как огромное количество последующих громких результатов (тот же AdaGrad) напрямую основывались на этих результатах. В названии Dual Averaging под dual average имеется в виду 1:t , то есть среднее по градиентам. Кардинально других техник оценок regret там нет, обзор McMahan строго улучшает все доступные там результаты. FTRL-Proximal: самая известная статья от гугла Ad Click Prediction. Известна она скорее потому, что там выписаны формулы и объяснено, как правильно реализовывать метод для large-scale задач с результатами применения различных дополнительных инженерных идей. Это хороший инженерный обзор, а не математическая статья. Рассмотрим отдельно каждую из разновидностей алгоритмов FTRL-Centered Задача оптимизации имеет вид min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+r 0:t (w)], где (w) таковы, что a r g min arg w min r t (w)=w 0 Пример: Рассмотрим SGD с фиксированным learning rate и стартом в точке 0 0. Положим (w)= 2η 1 ∣∣w∣∣ (w)=0,t>0 min t+1 =arg w min [g 1:t T w+ 2η 1 ∣∣w∣∣ 2 2 ]. Как мы уже знаем, итеративное обновление весов будет иметь вид t+1 =w t −ηg t . FTRL-Proximal Задача имеет похожий вид min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+r 0:t (w)], (w) выбираются так, чтобы a r g min arg w min r t (w)=w t Пример: Рассмотрим SGD с убывающим learning rate: t−1 1 Подробный вывод связи мы приведём в одном из следующих разделов, а сейчас просто приведём результат: (w)=σ t ∣∣w−w min t+1 =arg w min [g 1:t T w+ s=1 ∑ t σ s ∣∣w−w t+1 Обратите внимание: как правило, на практике Proximal методы работают лучше. Интуитивно, центрирование в недавних точках вместо Composite-Objective FTRL Рассмотрим смесь центрированных и проксимальных регуляризаторов: min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+ψ 0:t (w)+r 0:t (w)], где (w) и (w) таковы, что a r g min arg w min r t (w)=w t a r g min arg w min ψ t (w)=w 0 Пример: FTRL-Proximal с L1 и L2 регуляризацией min t+1 =arg w min [g 1:t T w+λ 1,t ∣∣w∣∣ 1 +λ 2,t ∣∣w∣∣ w 2 + s=1 ∑ t σ s ∣∣w−w s ∣∣ 2 2 ] Обратите внимание: как правило, центрированные регуляризаторы в довесок к проксимальным вводят уже не для «дополнительной стабилизации» алгоритма,",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 2,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+ψ 0:t (w)+r 0:t (w)], где (w) и (w) таковы, что a r g min arg w min r t (w)=w t a r g min arg w min ψ t (w)=w 0 Пример: FTRL-Proximal с L1 и L2 регуляризацией min t+1 =arg w min [g 1:t T w+λ 1,t ∣∣w∣∣ 1 +λ 2,t ∣∣w∣∣ w 2 + s=1 ∑ t σ s ∣∣w−w s ∣∣ 2 2 ] Обратите внимание: как правило, центрированные регуляризаторы в довесок к проксимальным вводят уже не для «дополнительной стабилизации» алгоритма, а для наложения ограничений на решение w w. Обратите внимание: наиболее правильные и хорошо работающие на практике способы подбора коэффициентов λ 1 , t λ 1,t 2,t мы приведём в параграфе про учет дополнительной регуляризации. Гарантии сходимости для алгоритмов FTRL В этом разделе мы обсудим теоретические оценки на скорость сходимости алгоритма FTRL или, что то же самое, на скорость убывания maxRegret. Напомним формулу: max maxRegret(T)= w ∗ max [ t=1 )−f 1:T (w ∗ )] Чтобы делать оценки на maxRegret, нужно пытаться оценить асимптотику ряда, каждое слагаемое которого — это решение сложной оптимизационной задачи min min f 1:t (w) с произвольными функциями (w). Работать с такой сущностью крайне сложно. Наша основная цель — сделать верхнюю оценку на regret, в которой не будет этого члена.(???) Strong FTRL Lemma (Lemma 4) Пусть (w) — последовательность произвольных (не обязательно) функций; Пусть (w) — последовательность выпуклых неотрицательных регуляризаторов; Пусть также min t+1 =arg w min h 0:t (w) всегда определен (относительно слабые условия 1-2 требуют от нас это явно проговорить); Тогда алгоритм, выбирающий w t + 1 w t+1 по правилу (3), удовлетворяет неравенству Regret T (w ∗ )≤r 0:T (w ∗ )+ t=1 ∑ T [h 0:t (w t )−h 0:t (w t+1 )−r Из чего состоит эта лемма? Слагаемое 0:T (w ∗ ) — это суммарная регуляризация в точке w ∗ w ∗ . Совсем избавиться от вхождения w ∗ w ∗ не получится, но мы можем выбирать регуляризатор так, чтобы оценить сверху 0:T (w ∗ ) было не очень сложно. Каждое слагаемое суммы t=1 ∑ T [h 0:t (w t )−h 0:t (w t+1 ) 2 1 ] отражает, насколько улучшается t t-й лосс h 0 : t h 0:t при замене min t+1 =arg w min h 0:t (w). Поведение разностей 0:t (w t )−h 0:t (w t+1 ) характеризует стабильность алгоритма. Мы ожидаем, что при больших t t у хорошо сходящегося алгоритма на очередном шаге w t w t будет достаточно близок к оптимуму w t + 1 w t+1 , то есть вся сумма будет меняться всё медленнее, и её получится разумно оценить. Пример ситуации, когда это не так, мы уже видели, когда рассматривали FTL без регуляризации для линейной функции потерь (там всё было максимально нестабильно и расходилось). К счастью, введение регуляризации обычно помогает добиться стабильности. Обе компоненты неразрывно связаны. Добавляя регуляризацию, мы увеличиваем первую компоненту, но улучшает стабильность алгоритма, чем уменьшаем вторую, и наоборот. Обратите внимание: в",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 3,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "стабильность алгоритма. Мы ожидаем, что при больших t t у хорошо сходящегося алгоритма на очередном шаге w t w t будет достаточно близок к оптимуму w t + 1 w t+1 , то есть вся сумма будет меняться всё медленнее, и её получится разумно оценить. Пример ситуации, когда это не так, мы уже видели, когда рассматривали FTL без регуляризации для линейной функции потерь (там всё было максимально нестабильно и расходилось). К счастью, введение регуляризации обычно помогает добиться стабильности. Обе компоненты неразрывно связаны. Добавляя регуляризацию, мы увеличиваем первую компоненту, но улучшает стабильность алгоритма, чем уменьшаем вторую, и наоборот. Обратите внимание: в условиях леммы допускаются невыпуклые (w), и это позволяет применять её в весьма общей ситуации. Впрочем, все наши последующие выкладки все-таки будут опираться на выпуклость (w). Теоретические оценки на Regret (regret bounds) Ниже мы представим теоремы 1,2 и 10 из обзора McMahan. Они дают оценки на regret в немного разных исходных предположениях и для разных типов регуляризаторов; асимптотика regret в каждом из случаев ), хотя константы будут различными. О важности констант в сходимости мы поговорим в одной из следующих параграфов, когда будем разбирать метод AdaGrad. В самом конце параграфа мы обсудим, какие оценки получаются для линеаризованного regret. А в следующем параграфе мы займёмся выводом конкретных алгоритмов FTRL для разных видов регуляризаторов. Мы не будем полностью пересказывать обзор (если вам стало интересно, рекомендуем прочитать его самостоятельно) и докажем в качестве примера теорему 2, а для остальных приведём лишь формулировки. Напоминание из выпуклого анализа Определение Выпуклая функция ψ ( x ) ψ(x) называется σ σ-сильно выпуклой по отношению к некоторой норме ∣ ∣ ⋅ ∣ ∣ ∣∣⋅∣∣, если выполнено ∀g∈∂ψ(y)ψ(x)≥ψ(y)+g T (x−y)+ 2 σ ∣∣x−y∣∣ 2 Определение Двойственной нормой ∣∣⋅∣∣ ∗ по отношению к норме ∣ ∣ ⋅ ∣ ∣ ∣∣⋅∣∣ называется sup ∣∣x∣∣ ∗ = y:∣∣y∣∣≤1 sup x T y Более подробно о σ σ-сильной выпуклости и двойственных нормах вы можете почитать, например, в книге Boyd, 2004, Convex Optimization. Теорема 1. General FTRL Bound Пусть Обновление параметров происходит по правилу min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+r 0:t (w)]; Выполнены все условия Setting 1; Регуляризатор (w) выбирается так, чтобы выражение 0:t (w)+f t+1 (w)=r 0:t (w)+f 1:t+1 (w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣ t (возможно, своей на каждом шаге). Тогда Regret T (w ∗ )≤r 0:T−1 (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ (t−1),∗ 2 , где ∣∣⋅∣∣ (t−1),∗ — норма, двойственная к норме ∣∣⋅∣∣ (t−1) . Теорема 2. FTRL-Proximal Bound Пусть Обновление параметров происходит по правилу min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+r 0:t (w)]; Выполнены все условия Setting 1; Все регуляризаторы (w) лежат в семействе FTRL-Proximal, причём )=0 для всех (w) выбирается так, чтобы выражение 0:t (w)=r 0:t (w)+f 1:t (w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣ t (возможно, своей на каждом шаге). Тогда Regret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 4,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "где ∣∣⋅∣∣ (t−1),∗ — норма, двойственная к норме ∣∣⋅∣∣ (t−1) . Теорема 2. FTRL-Proximal Bound Пусть Обновление параметров происходит по правилу min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+r 0:t (w)]; Выполнены все условия Setting 1; Все регуляризаторы (w) лежат в семействе FTRL-Proximal, причём )=0 для всех (w) выбирается так, чтобы выражение 0:t (w)=r 0:t (w)+f 1:t (w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣ t (возможно, своей на каждом шаге). Тогда Regret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2 , где ∣∣⋅∣∣ t,∗ — норма, двойственная к норме ∣∣⋅∣∣ t . Теорема 10. Composite Objective FTRL-Proximal Bound Пусть Обновление параметров происходит по правилу min min t+1 =arg w min h 0:t =arg w min [g 1:t T w+α 1:t Ψ(w)+r 0:t (w)]; Выполнены все условия Settning 1; (w)=f t (w)+α t Ψ(w)+r t (w); α t α t — неубывающая последовательность; Ψ ( w ) Ψ(w) — Centered регуляризатор с минимумом в точке (w) — Proximal регуляризаторы; (w) выбирается так, чтобы выражение 0:t ^ (w)=r 0:t (w)+α 1:t Ψ(w)+f 1:t (w) было 1-сильно выпукло по отношению к некоторой норме ∣∣⋅∣∣ t (возможно, своей на каждом шаге). Тогда Если мы рассматриваем regret относительно (w)=f t (w)+α t Ψ(w), то Regret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2 ; Если мы рассматриваем regret относительно (w), то Regret T (w ∗ )≤r 0:T (w ∗ )+α 1:t Ψ(w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2 , где ∣∣⋅∣∣ (t),∗ — норма, двойственная к норме ∣∣⋅∣∣ t . Обратите внимание. Оценки Proximal и General отличаются индексацией: до t t или до t − 1 t−1 соответственно. Это чисто техническое различие, однако именно из-за него с Proximal регуляризаторами удобнее работать как в теоретических выкладках, так и при выведении практических методов. Обратите внимание. На (w) мы не хотим накладывать ограничения сильной выпуклости, но сильную выпуклость функции 0:t (w)=f 1:t (w)+r 0:t (w) можно обеспечить за счет выбора сильно выпуклых регуляризаторов. В самом деле, сумма выпуклой и сильно выпуклой функций сильно выпукла. Если (w)≥f t (w t )+(w−w (w)≥r t (w t )+(w−w ∣∣w−w (w)+r t (w)≥f t (w t )+r t (w t )+(w−w t ) T (∇f t (w t )+∇r t (w t ))+ 2 1 ∣∣w−w t ∣∣ 2 . Обратите внимание. Норма ∣∣w∣∣ t,∗ 2 является сопряженной к норме, относительно которой 1-сильно выпукла функция 0:t (w)=f 1:t (w)+r 0:t (w). Это значит, что норму мы будем выбирать по сумме регуляризаторов 0:t (w), а не просто по (w). Доказательство на примере теоремы 2 Нам понадобится следующая чисто техническая лемма, доказательство которой мы опустим. Желающие могут прочитать Appendix B в обзоре. Lemma 7. Пусть ϕ 1 ϕ 1 — выпуклая функция →R∪{∞}, для которой существует min =arg x min ϕ 1 (x); ψ ψ — выпуклая функция; (x)=ϕ 1 (x)+ψ(x) — выпуклая функция, для которой существует min",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 5,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "t ∣∣ 2 . Обратите внимание. Норма ∣∣w∣∣ t,∗ 2 является сопряженной к норме, относительно которой 1-сильно выпукла функция 0:t (w)=f 1:t (w)+r 0:t (w). Это значит, что норму мы будем выбирать по сумме регуляризаторов 0:t (w), а не просто по (w). Доказательство на примере теоремы 2 Нам понадобится следующая чисто техническая лемма, доказательство которой мы опустим. Желающие могут прочитать Appendix B в обзоре. Lemma 7. Пусть ϕ 1 ϕ 1 — выпуклая функция →R∪{∞}, для которой существует min =arg x min ϕ 1 (x); ψ ψ — выпуклая функция; (x)=ϕ 1 (x)+ψ(x) — выпуклая функция, для которой существует min =arg x min ϕ 2 (x) и которая, кроме того, 1-сильно выпукла по норме ∣ ∣ ⋅ ∣ ∣ ∣∣⋅∣∣. Тогда, для любого элемента b b субдифференциала ψ имеет место неравенство ∣∣x 1 −x 2 ∣∣≤∣∣b∣∣ ∗ и для любого x ′ x ′ имеет место неравенство )−ϕ ∣∣b∣∣ ∗ . Доказательство теоремы 2 Рассмотрим соседние раунды t+1 . Имеем min min =arg w min h 0:t−1 =arg w min [f 1:t−1 +r 0:t−1 ] Обозначим (w)=f 1:t−1 (w)+r 0:t (w)=h 0:t−1 (w)+r t (w)=h 0:t (w)−f t (w). Поскольку w t w t одновременно минимизирует и (w) (т.к. это proximal регуляризатор), и 0:t−1 , имеем min min =arg w min [h 0:t−1 +r t (w)]=arg w min ϕ 1 (w). Далее, min min t+1 =arg w min h 0:t =arg w min [ϕ 1 (w)+f t (w)] Выпишем оценку из Strong FTRL Lemma и постараемся оценить отмеченные рыжим слагаемые t=1 )−f 1:T (w ∗ )≤r 0:T (w ∗ )+ t=1 ∑ T h 0:t (w t )−h 0:t (w t+1 )−r t (w t ) Так как по условию теоремы )=0, мы можем убрать это слагаемое: 0:t (w t )−h 0:t (w t+1 )−r t (w t )=h 0:t (w t )−h 0:t (w t+1 )+f t (w t )−(ϕ 1 (w t+1 )+f t (w t+1 )) Обозначим (w)=ϕ 1 (w)+f t (w). Применив Лемму 7, получаем )+f t (w t )−ϕ 1 (w t+1 )−f t (w t+1 )≤ 2 1 ∣∣g t ∣∣ t,∗ 2 О связи оценок на regret для обычного и линеаризованного FTRL Вспомним, что для линеаризованного FTRL имеет место неравенство: Regret T (w)≤LinearizedRegret T (w). Увы, верхняя оценка на левую часть неравенства не помогает оценить правую. Поэтому рассмотрим линеаризованный алгоритм более подробно. Он работает с последовательностью функций (w)=g t T w, где . Субдифференциал состоит из одного вектора (градиента это функции) (w) =g t Применим приведённые выше оценки на regret для исходного и для линеаризованного алгоритма: Regret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ LinearizedRegret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 t,∗ 2 =r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2 Легко убедиться, что оценки regret для обычного и линеаризованного FTRL совпадают и выполнено соотношение Regret T (w ∗ )≤LinearizedRegret T (w ∗ )≤TheoremRegret T (w ∗ ). Таким образом, для",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 6,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Субдифференциал состоит из одного вектора (градиента это функции) (w) =g t Применим приведённые выше оценки на regret для исходного и для линеаризованного алгоритма: Regret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ LinearizedRegret T (w ∗ )≤r 0:T (w ∗ )+ 2 1 t=1 t,∗ 2 =r 0:T (w ∗ )+ 2 1 t=1 ∑ T ∣∣g t ∣∣ t,∗ 2 Легко убедиться, что оценки regret для обычного и линеаризованного FTRL совпадают и выполнено соотношение Regret T (w ∗ )≤LinearizedRegret T (w ∗ )≤TheoremRegret T (w ∗ ). Таким образом, для линеаризованного варианта любого алгоритма FTRL не нужно доказывать собственные оценки. А поскольку линеаризованный FTRL намного эффективнее, в дальнейшем мы всегда будем сразу переходить от исходного алгоритма к линеаризованному. Построение эффективного адаптивного FTRL Теперь, когда мы получили теоретические оценки на качество работы адаптивного FTRL, настала пора рассмотреть несколько конкретных примеров алгоритмов из этого класса. Семейство квадратичных регуляризаторов (w) Во всех дальнейших выкладках мы сразу ограничим себя семейством квадратичных регуляризаторов: Для FTRL-Centered алгоритмов: (w)=w T D t w=∣∣w∣∣ D t 2 , Для FTRL-Proximal алгоритмов: (w)=(w−w t ) T D t (w−w t )=∣∣w−w где D t D t — некоторая симметричная положительно определённая матрица (возможно, своя для каждого шага). Помимо того, что они удобны и привычны, таки регуляризаторы позволяют достаточно просто выписывать оценки на regret. Чтобы в этом убедиться, вспомним, какие нетривиальные сущности возникают в теоремах: на каждом шаге нам нужно выбрать норму ∣∣⋅∣∣ t , по отношение к которой выражение 0:t (w)+f t+1 (w)=r 0:t (w)+f 1:t+1 (w) было бы 1-сильно выпуклым; во всех оценках участвует 0:T (w ∗ ) (или 0:T−1 (w ∗ )), и его хорошо бы уметь оценивать сверху; также в оценках фигурирует норма, двойственная к ∣∣⋅∣∣ t , и её нужно уметь выводить. Давайте разберёмся с каждым из пунктов и поймём, почему для квадратичных регуляризаторов всё довольно хорошо. Выбор нормы ∣∣⋅∣∣ t Тут всё просто: Регуляризатор ∣∣w∣∣ D является 1-сильно выпуклым относительно нормы ∣∣w∣∣ D (т.е. относительно себя же); Регуляризатор ∣∣w−w t ∣∣ D является 1-сильно выпуклым относительно той же самой нормы ∣∣w∣∣ D . Нам, впрочем, нужна 1-сильная выпуклость всей суммы 0:t (w), но легко убедиться, что r 0 : t r 0:t 1-сильно выпукло относительно суммарной нормы ∣∣⋅∣∣ D 0:t 2 . Поскольку D 0 : t D 0:t — тоже симметричная положительно определенная матрица, мы остаёмся в том же классе норм Махаланобиса. Двойственная норма 0:t (w) Оказывается, что ∣∣w∣∣ D,∗ =∣∣w∣∣ D −1 Ограничение сверху для 0:t (w ∗ ) Строго говоря, здесь никаких гарантий нет, и, например, очень плохая инициализация может всё сильно испортить. На практике, впрочем, всё работает нормально, но авторы статей не могут себе позволить надеяться на благосклонность судьбы. Поэтому в статьях часто встречается следующий костыль. Для вывода оценок на regret вводится регуляризатор (w)=I R (w), где (w)={ ∞ 0 ∣∣w∣∣>R ∣∣w∣∣≤R это проекция на шар. Тогда можно доказать, что ∣∣w ∗ ∣∣≤R. Семейство логарифмических регуляризаторов Для ряда частных задач вроде expert advice problem и оптимизаций по вероятностным распределениям используется",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 7,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "норма 0:t (w) Оказывается, что ∣∣w∣∣ D,∗ =∣∣w∣∣ D −1 Ограничение сверху для 0:t (w ∗ ) Строго говоря, здесь никаких гарантий нет, и, например, очень плохая инициализация может всё сильно испортить. На практике, впрочем, всё работает нормально, но авторы статей не могут себе позволить надеяться на благосклонность судьбы. Поэтому в статьях часто встречается следующий костыль. Для вывода оценок на regret вводится регуляризатор (w)=I R (w), где (w)={ ∞ 0 ∣∣w∣∣>R ∣∣w∣∣≤R это проекция на шар. Тогда можно доказать, что ∣∣w ∗ ∣∣≤R. Семейство логарифмических регуляризаторов Для ряда частных задач вроде expert advice problem и оптимизаций по вероятностным распределениям используется также семейство энтропийных регуляризаторов log ⁡ w i r t (w)= i=1 ∑ N w i logw i Более подробно о нём можно почитать в обзоре Shai-Shalev Schwartz, пример 2.5. Constant learning rate FTRL Простейший пример — это константный регуляризатор (w)={ 2η 1 ∣∣w∣∣ 2 2 , s=0, 0, s>0 Легко показать, что ∣∣w∣∣ 2 2 =∣∣w∣∣ 2η 1 I 2 . Соответствующий итерационный процесс оптимизации имеет вид min t+1 =arg w min [g 1:t T w+ 2η 1 ∣∣w∣∣ 2 2 ] Как мы уже наблюдали ранее, этот метод эквивалентен методу стохастического градиентного спуска с константным learning rate. А именно, шаг обновления весов можно сформулировать двумя способами: на языке FTRL: t+1 =−ηg 1:t T ; на языке градиентного спуска: t+1 =w t −ηg t . Оценка на Regret (3.1 Constant Learning Rate Online Gradient Descent). Пусть ∣∣g t ∣∣≤G; ∣∣w ∗ ∣∣≤R. Тогда, если взять , то для любого Regret T (w ∗ )≤RG T ′ В целом, такая стратегия регуляризации не самая оптимальная. Интуитивно, наш регуляризатор фиксирован вне зависимости от того, сколько мы уже сыграли раундов, и со временем может перестать компенсировать член 1:t T w, и тогда стабильность алгоритма может падать. FTRL с learning rate scheduling Чтобы исправить нестабильность алгоритма, возьмём L 2 L 2 -регуляризатор, не равный нулю на каждом шаге. Процесс оптимизации примет вид: Для FTRL-Proximal: min t+1 =arg w min [g 1:t T w+ s=0 ∑ t 2 σ s ∣∣w−w s ∣∣ 2 2 ]; Для FTRL-Centered: min t+1 =arg w min [g 1:t T w+ s=0 ∑ t 2 σ s ∣∣w∣∣ 2 2 ]. Посмотрим, какое обличье примет алгоритм FTRL-Proximal, если его изложить на языке градиентного спуска. Для этого продифференцируем и приравниваем нулю выражение, которое мы минимизируем: 0=g 1:t + s=0 ∑ t σ s (w−w s=0 1:t =σ 0:t t+1 = σ 0:t 1 s=0 0:t 1 g 1:t Попробуем получить рекуррентную формулу для выражения w t + 1 w t+1 через t+1 = σ 0:t 1 ( s=0 1:t 0:t−1 1 ( s=0 ∑ t−1 σ s w s −g 1:t−1 t+1 = σ 0:t 1 ( s=0 ∑ t−1 1:t−1 t+1 = σ 0:t 1 (σ 0:t−1 0:t 1 (σ 0:t w t −g t )=w t − σ 0:t 1 g t Если теперь положить 0:t 1 , мы получаем формулу градиентного спуска: t+1 Таким образом, темп обучения градиентного спуска равен обратной сумме коэффициентов регуляризации ftrl. Точно",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 8,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "= σ 0:t 1 s=0 0:t 1 g 1:t Попробуем получить рекуррентную формулу для выражения w t + 1 w t+1 через t+1 = σ 0:t 1 ( s=0 1:t 0:t−1 1 ( s=0 ∑ t−1 σ s w s −g 1:t−1 t+1 = σ 0:t 1 ( s=0 ∑ t−1 1:t−1 t+1 = σ 0:t 1 (σ 0:t−1 0:t 1 (σ 0:t w t −g t )=w t − σ 0:t 1 g t Если теперь положить 0:t 1 , мы получаем формулу градиентного спуска: t+1 Таким образом, темп обучения градиентного спуска равен обратной сумме коэффициентов регуляризации ftrl. Точно так же можно выразить t−1 1 В качестве классической непокоординатной последовательности learning rate обычно берут t+1 − t Оценка на Regret (3.2, Dual Averaging) Пусть ∣∣g t ∣∣≤G, ∣∣w ∗ ∣∣≤R. Тогда, если выбрать t+1 R , то Regret Как и в случае с константным learning rate, константа на практике никому не известна, так что ее подменяют на α α и перебирают руками с learning rate, равным α t + 1 t+1 α . Data-Adaptive FTRL До сих пор мы рассматривали в качестве нормы ∣ ∣ ⋅ ∣ ∣ ∣∣⋅∣∣ стандартное скалярное произведение, в которое различные компоненты вектора весов (которые, грубо говоря, соответствуют различным признакам) вносят равный вклад. Такой подход может быть слишком наивным для «боевых» задач, где геометрия оптимизации имеет форму, например, вытянутого эллипса. Нетрудно обобщить предыдущие рассуждения на случай произвольного скалярного произведения min t+1 =arg w min [g 1:t T w+ 2 1 s=0 ∑ T ∣∣w−w Коэффициенты σ s σ s в этом выражении теперь спрятались в D s D s . Найдем точку минимума: 0=g 1:t + 2 1 s=0 ∑ t (w−w s )(D s +D s T )=g 1:t + s=0 ∑ t (w−w s=0 ∑ t D s )w= s=0 1:t Но сразу возникают проблемы: Нужно хранить s=0 ∑ t D s , в общем случае это квадрат по памяти от числа параметров. Ни в какой реальной задаче мы не сможем себе этого позволить; На каждой итерации метода нужно решать гигантскую систему линейных уравнений для поиска w w. Есть все шансы состариться, так и не успев увидеть решение задачи оптимизации. Упростим себе жизнь и предположим, что все матрицы D s D s диагональны. Тогда s=1 ∑ t D s можно хранить в виде вектора диагональных элементов того же размера, что и w w, а система на каждой итерации будет решаться за линию. AdaGrad: наилучший адаптивный метод Разрешив себе брать нормы ∣∣⋅∣∣ D s с диагональными матрицами D s D s , мы сделали алгоритм более гибким, но при этом приобрели дополнительные степени свободы (выбор диагональных элементов). Попробуем ответить на два вопроса: Можно ли матрицы D s D s не угадывать, а настраивать по доступной на очередном шаге информации? Как выбирать матрицы D s D s так, чтобы минимизировать оценки на regret? В процессе поисков ответов на них мы придём к известному методу оптимизации AdaGrad. Помня, что ∣∣.∣∣ D,∗ =∣∣.∣∣ D −1 , выпишем общий вид оценки на regret: Regret T (w ∗ )≤r",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 9,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "наилучший адаптивный метод Разрешив себе брать нормы ∣∣⋅∣∣ D s с диагональными матрицами D s D s , мы сделали алгоритм более гибким, но при этом приобрели дополнительные степени свободы (выбор диагональных элементов). Попробуем ответить на два вопроса: Можно ли матрицы D s D s не угадывать, а настраивать по доступной на очередном шаге информации? Как выбирать матрицы D s D s так, чтобы минимизировать оценки на regret? В процессе поисков ответов на них мы придём к известному методу оптимизации AdaGrad. Помня, что ∣∣.∣∣ D,∗ =∣∣.∣∣ D −1 , выпишем общий вид оценки на regret: Regret T (w ∗ )≤r 0:T−1 (w ∗ )+ 2 1 t=1 ∑ T ∣∣g∣∣ (t),∗ 2 = t=1 ∑ T ∣∣w t=1 ∑ T ∣∣g t ∣∣ (D 0:T ) −1 2 Чтобы упростить выкладки, введем новую симметричную положительно определенную матрицу 0:T −1 и перепишем формулы Regret T (w ∗ )≤r 0:T−1 (w ∗ )+ 2 1 t=1 ∑ T ∣∣g∣∣ t−1,∗ 2 = t=1 ∑ T ∣∣w t=1 ∑ T ∣∣g t ∣∣ S T 2 С членом t=1 ∑ T ∣∣w явно будет очень сложно работать: чтобы им пользоваться, нужно иметь на руках оптимальное решение для всей предыдущей выборки. Более перспективным выглядит слагаемое t=1 ∑ T ∣∣g t ∣∣ S T 2 : вычислять их одно удовольствие. Идея метода AdaGrad как раз в том, чтобы не пытаться работать с первым членом и минимизировать второй, надеясь, что итоговые оценки на regret при этом тоже улучшатся. Для начала выведем диагональный AdaGrad как более простой случай. Если все D t D t диагональны, то матрица 1:T −1 тоже диагональна и представляется набором диагональных элементов (уберем индекс T T для сокращения выкладок, так как мы рассматриваем фиксированный раунд). Распишем второе слагаемое в regret t=1 ∑ T ∣∣g t=1 ∑ T i=1 ∑ N s i g t,i 2 Попробуем минимизировать его inf t=1 ∑ T i=1 ∑ N s i g t,i 2 ⟶ s inf s i ≥0 Условие ≥0 возникает из неотрицательной определенности матрицы S T S T . Решеним такой задачи, очевидно, является →+∞. Однако в этом случае член t=1 ∑ T ∣∣w из оценки на regret станет, наоборот, бесконечно большим, и нужен какой-то компромисс. Введем довольно слабое ограничение на положительные коэффициенты inf t=1 ∑ T i=1 ∑ N s i g t,i 2 ⟶ s inf s i ≥0, i=1 ∑ N s i ≤c и найдём оптимум с помощью метода множителей Лагранжа. Функция Лагранжа имеет вид L(s,λ,θ)= t=1 ∑ T i=1 ∑ N s i g t,i 2 +λ T s+θ( i=1 ∑ N s i −c) Отметим, что здесь λ λ — это вектор, а θ θ — число. Приравняем к нулю частные производные: ∂L(s,λ,θ) =− s i 1 t=1 ∑ T g t,i 2 +λ i +θ Вспомним про условия дополняющей нежесткости, требующие, чтобы =0. Так как s i s i мы нулю приравнять здесь не можем, получаем, что =0: t=1 ∑ T g t,i 2 −θ=0 t=1 ∑ T g t,i 2 Теперь вспомним про условие i=1 ∑",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 10,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Лагранжа. Функция Лагранжа имеет вид L(s,λ,θ)= t=1 ∑ T i=1 ∑ N s i g t,i 2 +λ T s+θ( i=1 ∑ N s i −c) Отметим, что здесь λ λ — это вектор, а θ θ — число. Приравняем к нулю частные производные: ∂L(s,λ,θ) =− s i 1 t=1 ∑ T g t,i 2 +λ i +θ Вспомним про условия дополняющей нежесткости, требующие, чтобы =0. Так как s i s i мы нулю приравнять здесь не можем, получаем, что =0: t=1 ∑ T g t,i 2 −θ=0 t=1 ∑ T g t,i 2 Теперь вспомним про условие i=1 ∑ N s i ≤c. Можно показать, что оптимум достигается на границе (то есть когда неравенство превращается в равенство). Тогда i=1 i=1 ∑ N t=1 ∑ T g t,i i=1 ∑ N t=1 ∑ T g t,i i=1 ∑ N t=1 ∑ T g t,i 2 c t=1 ∑ T g t,i 2 Вернемся к оценке на regret. Чему равно c c мы не знаем, поэтому мы просто констатируем, что оптимальные коэффициенты s i s i пропорциональны s=1 ∑ t g s,i t=1 ∑ T g t,i 2 Теперь S T S T — диагональная матрица с диагональными элементами . Следовательно, 0:T =(S T ) −1 - тоже диагональная матрица с диагональными элементами 0:T,i = α 1 t=1 ∑ T g t,i 2 = t=1 ∑ T α 1 t=1 ∑ T g t,i 2 − α 1 t=1 ∑ T−1 g t,i 2 +0,D 0 =0, и легко убедиться, что t,i = α 1 t=1 ∑ T g t,i 2 − α 1 t=1 ∑ T g t,i 2 Теперь вспомним, что эти формулы в точности повторяют то, что мы получили выше для соотношения t−1 1 , только вместо общего коэффициента η t η t у нас теперь покоординатные коэффициенты η t , i η t,i t,i = s=1 ∑ t g s,i 2 α Получаем формулы для метода AdaGrad в градиентной постановке: t+1,i =w t,i − s=1 ∑ t g s,i 2 α g t,i , где коэффициент α α приобретает значение learning rate. Оценка на Regret (3.4, FTRL-Proximal with Diagonal Matrix Learning Rates) Если использовать AdaGrad с покоординатными learning rate, то Regret T (w ∗ )≤2 2 R t=1 ∑ T g t 2 Отметим, что это оценка отличается от предыдущей тем, что вместо G T G T используется t=1 ∑ T g t 2 . Таким образом, если у градиента на какой-то из позиций стоит что-то большое, это повлияет лишь на одно из слагаемых под корнем вместо того, чтобы умножиться на T T . Эффективный размер шага. Предположим, что градиенты ограничены по норме ∣∣g∣∣ 2 ≤R. Перепишем наши формулы в виде s=1 ∑ T g s,i s=1 ∑ T g s,i Из этих формул следует, что в среднем learning rate в AdaGrad убывает как =O( T 1 ), то есть так же, как в предыдущем методе. Отличие состоит лишь в более правильной покоординатной нормировке, которая улучшает сходимость. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 11,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "если у градиента на какой-то из позиций стоит что-то большое, это повлияет лишь на одно из слагаемых под корнем вместо того, чтобы умножиться на T T . Эффективный размер шага. Предположим, что градиенты ограничены по норме ∣∣g∣∣ 2 ≤R. Перепишем наши формулы в виде s=1 ∑ T g s,i s=1 ∑ T g s,i Из этих формул следует, что в среднем learning rate в AdaGrad убывает как =O( T 1 ), то есть так же, как в предыдущем методе. Отличие состоит лишь в более правильной покоординатной нормировке, которая улучшает сходимость. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 15.1. Введение в онлайн-обучение Следующий параграф 15.3. Регуляризация в онлайн-обучении",
    "metadata": {
      "title": "Адаптивный FTRL",
      "url": "https://education.yandex.ru/handbook/ml/article/adaptivnyj-ftrl",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.2",
      "part": 12,
      "total_parts": 12,
      "source_file": "15.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе мы поговорим о регуляризации, но использовать мы её будем не для стабилизации обучения, а для того, чтобы накладывать ограничение на получаемое нами решение. Чтобы отличать их от стабилизирующих слагаемых, для таких регуляризаторов будем использовать обозначение (w) В теории от регуляризатора требуется только выпуклость, но на практике широко используются лишь три вида: =∣∣w∣∣ 1 и его собрат L 1 / 2 L 1/2 =∣∣w∣∣ 2 2 ; Проекция на выпуклое множество ψ(w)=I χ (w)={ ∞ 0 w  ∈χ w∈χ Классическим способом введения регуляризации является прибавление к оптимизируемому функционалу: (w)=f t (w)+ψ t (w) с последующим применением любых методов оптимизации «из коробки». Яркий пример — L 2 L 2 регуляризация: (w)=f t (w)+ 2λ 2 1 ∣∣w∣∣ 2 2 , которая не портит гладкости функционала. Идея неразложения регуляризаторов в субградиентную оценку Вспомним вывод linearized FTRL. В ходе линеаризации мы заменяли все функции (w) на их субградиентную оценку в точке w t w t . Для регуляризованного функционала (w)=f t (w)+ψ t (w) получалась бы такая оценка: (w)≥ f ^ t (w t )+(g t +∂ψ t ) T (w−w t ), где через ∂ ψ t ∂ψ t мы обозначили для краткости субградиент ψ t ψ t в точке w t w t . Теперь субградиентную оценку можно подставить в метод FTRL: min t+1 =arg w min [(g 1:t +∂ψ 1:t ) T w+ s=1 ∑ t ∣∣w−w Идея неразложения состоит в следующем: заменим на субградиентную оценку только (w), а регуляризатор будем подбирать так, чтобы задача FTRL решалась аналитически. Интуитивно, оценка (w)=f t (w)+ψ t (w)≥f t (w t )+g t T (w−w t )+ψ t (w) должна быть точнее оценки (w)=f t (w)+ψ t (w)≥f t (w t )+ψ t (w t )+(g t +∂ψ t ) T (w−w t ) а значит, и метод оптимизации будет точнее и эффективнее. Эта идея очень важна для построения регуляризованных алгоритмов онлайн-обучения. Давайте выпишем, как будут выглядеть с учётом этой идеи регуляризованные алгоритмы. Composite Objective FTRL min t+1 =arg w min [g 1:t T w+ψ 1:t (w)+ s=1 ∑ t ∣∣w−w Online Mirror Descent, Proximal Gradient Descent, (F)ISTA min t+1 =arg w min [g t T w+ψ t (w)+∣∣w−w Напомним, что три названия в заголовке соответствуют трём способам восприятия этой формулы: Online Mirror Descent — метод онлайн-обучения; Proximal Gradient Descent — метод (стохастической) батч-оптимизации. В стохастическом случае он неотличим от Mirror Descent; (F)ISTA — по сути, это название аналитического решения указанного уравнения для L 1 L 1 -регуляризации. Связь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления В этом подразделе мы будем проводить рассуждения на примере L 1 L 1 -регуляритора. для других регуляризаторов выкладки будут аналогичными. Выпишем Proximal (он же Mirror) Gradient Descent с L 1 L 1 -регуляризацией: min t+1 =arg w min g t T w+λ 1 ∣∣w∣∣ 1 + 2η t 1 ∣∣w−w t ∣∣ 2 2 Необходимым условием минимума явняется равенство нулю градиента (а в данном случае субградиента) всего выражения: 0=g t+1 −w t ) где - субградиент регуляризатора ∣∣w∣∣ 1 в",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 1,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "указанного уравнения для L 1 L 1 -регуляризации. Связь между Composite-Objective FTRL и Proximal Gradient Descent. Lazy vs Greedy представления В этом подразделе мы будем проводить рассуждения на примере L 1 L 1 -регуляритора. для других регуляризаторов выкладки будут аналогичными. Выпишем Proximal (он же Mirror) Gradient Descent с L 1 L 1 -регуляризацией: min t+1 =arg w min g t T w+λ 1 ∣∣w∣∣ 1 + 2η t 1 ∣∣w−w t ∣∣ 2 2 Необходимым условием минимума явняется равенство нулю градиента (а в данном случае субградиента) всего выражения: 0=g t+1 −w t ) где - субградиент регуляризатора ∣∣w∣∣ 1 в точке w t w t . Отсюда получаем t+1 Если же переписать формулы в духе FTRL, мы получим min t+1 =arg w min g 1:t T w+ g ^ 1:t−1 T w+λ∣∣w∣∣ 1 + 2 1 s=0 ∑ t ∣∣w−w s ∣∣ σ s 2 Получился метод, который оптимизирует L 1 L 1 -регуляризатор в явном виде только на текущей итерации t t, а для остальных использует субоптимальные субградиентные оценки. Заметим, что тем же выражением можно ограничить сверху и функционал: min t+1 =arg w min g 1:t T w+tλ∣∣w∣∣ 1 + 2 1 s=0 ∑ t ∣∣w−w s ∣∣ σ s 2 Мы получили метод FTRL с incremental L 1 L 1 — более сильным и стабильным вариантом регуляризации, чем Mirror Descent. Подробнее его анализом мы займемся в параграфе про продвинутую L 1 L 1 -регуляризацию. L 1 L 1 -регуляризация Отбор параметров разреженных моделей Предположим, что мы хотим обучить модель минимального размера и при этом как можно лучшего качества. В этом нам поможет отбор параметров. А именно, давайте постараемся оставить только те из них, которые оказывают наиболее влияние на лосс 1:T (w). Определение. Будем называть параметр w i w i разреженным, если он не используется (пропускается) при предсказании некоторых (w). «Некоторых» может означать как десятую часть, так и 0.99999 0.99999 прогнозов (w), главное — что такие объекты просто есть. Частым мы будем называть параметр, у которого частота пропусков низкая (например, 10 % 10% пропусков), а редким — тот, у которого она высокая (второй случай). Пример. Рассмотрим модель разреженной линейной регрессии (w)=(w . Обычно она применяется в ситуациях, когда элементы вектора признаков x t , i x t,i — это 0 0 или 1 1 (например, «встретилось ли i i-е слово в t t-м документе»), причем на практике доля единиц обычно бывает очень маленькой. Поэтому существенная часть параметров w i w i при прогнозе на шаге t t будет умножаться на нули и, таким образом, не будет использоваться. Обратите внимание: как правило, в литературе по онлайн-обучению говорят о разреженных параметрах, а не признаках. Впрочем, подавляющее большинство моделей на разреженных признаках устроены так, что каждому такому признаку сопоставляется некий набор параметров, поэтому определения «разреженный признак» и «разреженные параметры» взаимозаменяемы. В линейной модели, как в примере выше, каждому признаку x i x i сопоставляется параметр w i w i . В более сложных моделях признаку x i x i может сопоставляться вектор параметров w i w i — эмбеддинг этого признака. Давайте",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 2,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "w i при прогнозе на шаге t t будет умножаться на нули и, таким образом, не будет использоваться. Обратите внимание: как правило, в литературе по онлайн-обучению говорят о разреженных параметрах, а не признаках. Впрочем, подавляющее большинство моделей на разреженных признаках устроены так, что каждому такому признаку сопоставляется некий набор параметров, поэтому определения «разреженный признак» и «разреженные параметры» взаимозаменяемы. В линейной модели, как в примере выше, каждому признаку x i x i сопоставляется параметр w i w i . В более сложных моделях признаку x i x i может сопоставляться вектор параметров w i w i — эмбеддинг этого признака. Давайте теперь поймём, что означает фраза «признак влияет на лосс 1:T (w)». Оказывать влияние можно двумя способами: Качеством. Если параметр w i w i редкий, но очень хорошо прогнозирует свой небольшой набор объектов, его стоит оставить. За счет того, что мы оставим достаточное количество таких параметров, мы можем покрыть большое число объектов. Такие параметры называются memorization parameters (они как будто запоминают «свои» объекты). Количеством. Если параметр w i w i часто встречается, то он в любом случае должен остаться в модели и помогать с суммарным качеством прогноза. Убирать мы хотим только слабые и редкие параметры. Таких, как правило, больше 99 % 99%. Обратите внимание: мы не хотим убирать слабые, но часто встречающиеся параметры. Тому есть две причины: Места они много не занимают, а количества данных в large scale задачах достаточно, чтобы правильно выучить эти параметры. Они будут вносить свой, пусть и небольшой, вклад в общее качество; Частые параметры хорошо запоминают среднее поведение на всех данных, а разреженные — поведение на конкретных объектах. Если наша цель — оставить как можно меньше параметров, то выгоднее хорошо выучить среднее поведение на всех данных, а отклонения от среднего запомнить с помощью memorization parameters. Если в модели есть только супер-разреженные параметры, то из-за огромной вариативности в их возможных комбинациях в данных каждому параметру придется доучивать среднее поведение. Подробнее на этой проблеме мы остановимся в конце параграфа. Инициализация разреженных параметров В обучении разреженных моделей все параметры, на которые накладывается L 1 L 1 -регуляризация, инициализируются нулями. С точки зрения здравого смысла такая инициализация довольно естественна, однако есть и более формальное обоснование; Если параметры инициализируются нулями, то мы по мере обучения смотрим на градиенты этих параметров и в зависимости от градиентов принимаем решение, нужен нам параметр для прогноза или не нужен. Все параметры стартуют в равных условиях, и модель понемногу выходит из состояния «абсолютная разреженность», выучивая что-то содержательное. Если же параметры инициализируются случайно, то нам надо сначала доучить все параметры до какого-то более или менее разумного значения, а потом уже пытаться понять, нужен ли он нам. Момент, когда модель начинает эффективно разреживаться, тем самым очень сильно отдалается. Composite-objective FTRL с L 1 L 1 -регуляризацией Напомним формулировку задачи: min t+1 =arg w min g 1:t T w+λ 1,t ∣∣w∣∣ 1 + 2 1 s=1 ∑ T ∣∣w−w s ∣∣ σ s 2 Решение можно выписать в явном виде. Для этого введём следующие обозначения: z t z t будет аккумулировать сумму градиентов, =0, n t n t будет аккумулировать",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 3,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Если же параметры инициализируются случайно, то нам надо сначала доучить все параметры до какого-то более или менее разумного значения, а потом уже пытаться понять, нужен ли он нам. Момент, когда модель начинает эффективно разреживаться, тем самым очень сильно отдалается. Composite-objective FTRL с L 1 L 1 -регуляризацией Напомним формулировку задачи: min t+1 =arg w min g 1:t T w+λ 1,t ∣∣w∣∣ 1 + 2 1 s=1 ∑ T ∣∣w−w s ∣∣ σ s 2 Решение можно выписать в явном виде. Для этого введём следующие обозначения: z t z t будет аккумулировать сумму градиентов, =0, n t n t будет аккумулировать сумму поэлементных квадратов градиентов, =0, α α — это learning rate. Следующие формулы выписаны отдельно для каждой координаты. В них i i — индекс параметра модели, t t — номер итерации. t,i = η t,i 1 − η t−1,i t,i +g t,i 2 − n t,i t+1,i =z t,i +g t,i −σ t,i w t,i t+1,i =n t,i +g t,i t+1,i ={ 0 − n t+1,i +αλ 2,t α (z t+1,i −sgn(z t+1,i )λ 1,t ) ∣z t+1,i ∣≤λ 1,t ∣z t+1,i ∣>λ 1,t (∗) Вывод этих формул хорошо расписан в конспекте курса Д. А. Кропотова. Анализ аналитического решения При регуляризаторе ∣∣w∣∣ 1 в оптимизируемом функционале стоят коэффициенты λ 1 , t λ 1,t , которые могут как-то зависеть от t t. Обычно рассматривают три вида зависимости: Fixed: 1,t =λ. Squared incremental: 1,t = t λ Linear incremental: 1,t =tλ Их также можно комбинировать, получая коэффициенты регуляризации 1,t =λ 1,global + t λ 1,sqrt +tλ 1,incremental Напомним, что все веса w i w i мы инициализируем нулями. По формулам ( ∗ ) (∗) из нуля на шаге t t выводятся веса w i w i , для которых 1:t,i −∑σ s w s,i ∣>λ 1,t . Таким образом, начальное условие выхода параметров из нуля имеет вид 1:t,i ∣>λ 1,t . Попробуем понять физический смысл этого неравенства. Напоминание. Говорят, что функция f ( w ) f(w) имеет липшиц-непрерывный градиент с константой L L, если ∣∣∇f(x)−∇f(y)∣∣ 2 2 ≤ 2 L ∣∣x−y∣∣ 2 2 Предположим, что это выполняется (ниже мы покажем, что это не слишком обременительное ограничение). Тогда, подставив в качестве y y точку оптимума функции (w) (не путайте с глобальным из regret!), мы получим ∣∣∇f(x)∣∣ 2 2 ≤ 2 L ∣∣x−x ∗ ∣∣ 2 2 Это означает, что для достаточно хорошей функции норма градиента является оценкой снизу на расстояние до точки оптимума в пространстве параметров. Чем больше норма градиента, тем дальше мы от оптимальных параметров w w. Вернемся к выражению 1:t,i ∣>λ 1,t . Здесь мы имеем дело (а) отдельно с каждой из координат и (б) с нормой суммы градиентов (а не с суммой норм). Хорошая новость: утверждение выше верно и для функций одной переменной, то есть s,i ∣, грубо говоря, показывает, насколько мы далеки от оптимума по i i-й координате. Знак g s , i g s,i говорит о том, в какую сторону мы будем сдвигаться по i i-й координате w w на s s-м шаге. Если сдвиги были в",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 4,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "оптимума в пространстве параметров. Чем больше норма градиента, тем дальше мы от оптимальных параметров w w. Вернемся к выражению 1:t,i ∣>λ 1,t . Здесь мы имеем дело (а) отдельно с каждой из координат и (б) с нормой суммы градиентов (а не с суммой норм). Хорошая новость: утверждение выше верно и для функций одной переменной, то есть s,i ∣, грубо говоря, показывает, насколько мы далеки от оптимума по i i-й координате. Знак g s , i g s,i говорит о том, в какую сторону мы будем сдвигаться по i i-й координате w w на s s-м шаге. Если сдвиги были в основном в одну сторону, то 1:t,i будет больше, а если они всё время в разную сторону, то отдельные слагаемые могут скомпенсировать друг друга, и 1:t,i может быть малым. Отметим ещё, что абсолютная величина компоненты 1:t,i на первых итерациях может отражать прогнозирующую силу параметра w i w i : в самом деле, неверное значение важного для предсказания параметра может вести к большим ошибкам, что будет давать большие градиенты. Посмотрим теперь, как будет вести себя разреженная модель в зависимости от вида λ 1 , t λ 1,t . Linear incremental ( 1,t =tλ 1 ) Условие выхода w i w i из нуля принимает вид 1:t,i ∣>tλ 1 , что равносильно 1:t,i >λ 1 Ограничение на среднее значение компоненты градиента означает, что для выхода из нуля параметр w i w i должен иметь определённую прогнозирующую силу. Это противоречит нашему требованию о том, чтобы частые маломощные параметры все равно присутствовали в модели и выучивали среднее поведение. Обратите внимание. Выше мы показали, что проксимальный градиентный спуск с обычным min t+1 =arg w min g t T w+λ 1 ∣∣w∣∣ 1 + η t 1 ∣∣w−w t ∣∣ 2 2 в некотором смысле эквивалентен Composite-Objective FTRL с инкрементальным L 1 L 1 . Таким образом, обычная L 1 L 1 -регуляризация в классическом градиентном спуске эквивалентна именно инкрементальному L 1 L 1 , который, как мы выяснили, субоптимален. Ниже мы рассмотрим специфический для FTRL вариант L 1 L 1 -регуляризации, который лишен этих недостатков. Фиксированный ( 1,t =tλ 1 ) Это самый мощный и полезный на практике режим. Здесь мы не нормируем на 1 t t 1 (то есть не берём среднее), и это означает, что выйти из нуля может и слабый, но частый параметр, который за много итераций накопит достаточно большую сумму частных производных. Свойства фильтрации с фиксированным регуляризатором в точности совпадают с продуктовыми требованиями: Редкий параметр с мощной прогнозирующей силой на старте будет иметь большие по модулю градиенты одного знака, и он выйдет из нуля; Редкий параметр с малой прогнозной силой не выйдет из нуля; Частые параметры в любом случае выйдут из нуля. Squared incremental: ( 1,t = t λ) В этой статье было теоретически обосновано, что если параметр частый, но нерелевантный и абсолютно шумный, то дисперсия 1:t ∣ будет иметь асимптотику ). Из этого следует, что, если сделать регуляризацию порядка t t , мы лишим такой случайный шум почти любых шансов выйти из нуля. К сожалению, ни в игрушечных примерах вроде Avazu,",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 5,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "требованиями: Редкий параметр с мощной прогнозирующей силой на старте будет иметь большие по модулю градиенты одного знака, и он выйдет из нуля; Редкий параметр с малой прогнозной силой не выйдет из нуля; Частые параметры в любом случае выйдут из нуля. Squared incremental: ( 1,t = t λ) В этой статье было теоретически обосновано, что если параметр частый, но нерелевантный и абсолютно шумный, то дисперсия 1:t ∣ будет иметь асимптотику ). Из этого следует, что, если сделать регуляризацию порядка t t , мы лишим такой случайный шум почти любых шансов выйти из нуля. К сожалению, ни в игрушечных примерах вроде Avazu, ни в продакшен задачах улучшений качества прогноза или степени разреживания модели без потери качества достичь не удалось. Возможно, вам повезет больше. Полезность частых параметров для разреживания модели Рассмотрим две линейных модели (w)=w T x t +b,g t (w)=w T x t , в которых все параметры w i w i разреженные. Давайте считать, что в первой модели есть константный (и совсем даже не разреженный) признак =1, которому и соответствует параметр b b. Теперь в каждой из моделей наложим на w w регуляризацию L 1 L 1 и сравним, что получится: В модели f t f t параметрам w i w i нужно запомнить «отклонение» от среднего b b; В модели g t g t параметрам w i w i нужно запомнить абсолютное значение предсказания. Нетрудно понятно, что при наличии bias нормы градиентов в первой модели в среднем будут намного меньше, потому что мы на каждом шаге оптимизации будем стартовать с точки, которая в среднем ближе к точке оптимума (bias и есть наше среднее). Поэтому меньше весов смогут преодолеть порог по модулю суммы градиентов и выйти из нуля. Таким образом, несмотря на одинаковый оптимум без регуляризации, при введении L 1 L 1 -регуляризации модель с bias будет обладать более хорошим соотношением разреженность/качество прогноза. Эта логика легко обобщается на более сложные случаи, когда вместо bias у нас есть неразреженные контентные признаки. Вывод такой: модели, в которых есть только очень разреженные параметры, обладают гораздо худшим соотношением разреженность/качество, чем модели, в которых есть и контентные, и разреженные параметры. Убедиться в этих эффектах мы сможем в разделе с практикой на линейных моделях. L 2 L 2 регуляризация Weight decay Рассмотрим обыкновенный SGD. t+1 =w t −αg t Weight decay состоит во введение штрафа на размер текущих весов: t+1 =(1−λ)w t −αg t ,0≤λ<1 Внимательные читатели уже заметили, что в случае с SGD это эквивалентно введению L 2 L 2 -регуляризации. Давайте разберёмся, как это сделать правильно. Decoupled weight decay Попробуем заменить (w) на (w)=f t (w)+λ 2 ∣∣w∣∣ 2 2 и запустить любой адаптивный метод, например, AdaGrad. Если мы беспечно заменим на градиентную оценку всю функцию (w) (забыв, что с регуляризатором этого делать не стоит), то алгоритм примет вид t+1 где s=1 В этих формулах нехороши две вещи: Коэффициенты α α и λ 2 λ 2 нетривиальным образом взаимодействуют. Это крайне неудобно при переборе гиперпараметров: изменение learning rate α α должно влечь за собой переподбор коэффициента регуляризации λ 2 λ 2 по полной сетке; В",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 6,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "L 2 L 2 -регуляризации. Давайте разберёмся, как это сделать правильно. Decoupled weight decay Попробуем заменить (w) на (w)=f t (w)+λ 2 ∣∣w∣∣ 2 2 и запустить любой адаптивный метод, например, AdaGrad. Если мы беспечно заменим на градиентную оценку всю функцию (w) (забыв, что с регуляризатором этого делать не стоит), то алгоритм примет вид t+1 где s=1 В этих формулах нехороши две вещи: Коэффициенты α α и λ 2 λ 2 нетривиальным образом взаимодействуют. Это крайне неудобно при переборе гиперпараметров: изменение learning rate α α должно влечь за собой переподбор коэффициента регуляризации λ 2 λ 2 по полной сетке; В квадратах градиентов мы хотим видеть только адаптивность к кривизне самой функции f t f t , но теперь там ещё добавка Эта проблема была впервые замечена в Decoupled weight decay regularization. Авторы также рассматривали влияние на momentum, к этому мы вернёмся в параграфе про AdamW. Авторы статьи предлагают модифицировать метод AdaGrad следующим образом: t+1 =w t − s=1 Сразу отметим сходство с исходными формулами weight decay — его и добивались авторы. Decoupled weight decay — это адаптивный L 2 L 2 Легко видеть, что формула t+1 =w t − s=1 описывает обыкновенный покоординатный градиентный спуск с некоторым линеаризованным L 2 L 2 -регуляризатором. Давайте «проинтегрируем» это выражение обратно до аргминимума, из которого бы получились такие формулы обновления весов: min t+1 =arg w min ∣∣w−w t ∣∣ 2 2 ] Получается, что decoupled weight decay — это адаптивный L 2 L 2 -centered регуляризатор. Его можно усовершенствовать, вспомним наше важное правило не заменять регуляризатор на субградиентную оценку. Перейдём к задаче min t+1 =arg w min ∣∣w∣∣ ∣∣w−w t ∣∣ 2 2 ] Она отличается от предыдущей заменой w на w=∣∣w∣∣ 2 . Её решение имеет вид t+1 = 1+λ Поскольку мы меньше огрубляем оптимизируемый функционал, обучение может стать немного стабильнее. Обратите внимание, что в оптимизационной задаче у нас теперь стоит не просто λ 2 λ 2 , а Decoupled L 2 L 2 -регуляризация в Composite-Objective FTRL Теперь посмотрим, как decoupled weight decay будте работать с Composite-Objective FTRL. Линеаризованная задача имеет вид: min t+1 =arg w min [g 1:t T w+ 2 λ 2 ∣∣w∣∣ σ 1:s 2 + 2 1 s=1 ∑ t ∣∣w−w Перепишем её: min t+1 =arg w min [g 1:t T w+ 2 1 s=1 ∑ t (∣∣w−w ∣∣w∣∣ σ s 2 )] Нетрудно показать, что решение имеет вид t+1 t+1 =− 1+λ Для z t z t можно написать и явную формулу: 1:t −∑ s=1 Замечание. Чтобы оценить Regret такого метода, мы не сможем механически воспользоваться оценкой для AdaGrad: ведь она базированась на оценке на Regret, выведенной либо для целиком Proximal, либо для целиком Centered L 2 L 2 -регуляризаторов. Composite objective из теоремы 10 тут не годится, так как Centered регуляризатор в этом случае не поедет в оценку норм градиентов, а мы в текущем представлении рассматриваем Proximal и Centered как равноправные члены. Интуитивно, мы должны применить Lemma 7 к обоим регуляризаторам и получить точно такую же оценку с такой же двойственной нормой (напомним, что centered",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 7,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "t можно написать и явную формулу: 1:t −∑ s=1 Замечание. Чтобы оценить Regret такого метода, мы не сможем механически воспользоваться оценкой для AdaGrad: ведь она базированась на оценке на Regret, выведенной либо для целиком Proximal, либо для целиком Centered L 2 L 2 -регуляризаторов. Composite objective из теоремы 10 тут не годится, так как Centered регуляризатор в этом случае не поедет в оценку норм градиентов, а мы в текущем представлении рассматриваем Proximal и Centered как равноправные члены. Интуитивно, мы должны применить Lemma 7 к обоим регуляризаторам и получить точно такую же оценку с такой же двойственной нормой (напомним, что centered и proximal регуляризаторы имеют одинаковую двойственную норму). Двойственная норма такая же -> формулы оптимального метода AdaGrad будут такие же. Мы оставляем это читателям в качестве упражнения. (w): проекция на выпуклое множество χ χ Напоминание: множество χ χ называется выпуклым, если ∀x,y∈χ, ∀α∈[0;1]:αx+(1−α)y∈χ Проекцией на это множество называют функцию (w)={ ∞ 0 w  ∈χ w∈χ Докажем, что (w) — выпуклый регуляризатор. Для этого нам нужно проверить неравенство αI(x)+(1−α)I(y)≥I(αx+(1−α)y). Единственный шанс, когда это может быть нарушено — это I(αx+(1−α)y)=∞, I(x)=0, I(y)=0. Это значит, что x , y ∈ χ x,y∈χ, а αx+(1−α)y∈ / χ, что противоречит выпуклости χ χ. Вернемся к формулам FTRL. Здесь ситуация сильно проще — от накидывания любых последовательностей α 1 : T α 1:T на регуляризатор ничего не изменится, так что его всегда оставляют просто as is min t+1 =arg w min g 1:t T w+I χ (w)+ 2 1 s=1 ∑ T ∣∣w−w s ∣∣ σ s 2 Аналитические решения для каждого вида χ χ нужно искать отдельно. Примерно все решения получаются путем выноса (w) из оптимизируемого функционала и превращения его в ограничение, после чего можно применить метод множителей Лагранжа. Проекция на шар x:∣∣x∣∣≤c Решим аналитически задачу проекции на шар min 1:t T w+ 2 1 s=1 ∑ T ∣∣w−w s ∣∣ σ s 2 ⟶min w , ∣∣w∣∣ 2 ≤c. Функция Лагранжа будет иметь вид L(w,λ)=g 1:t T w+ 2 1 s=1 ∑ T ∣∣w−w s ∣∣ σ s 2 +λ(∣∣w∣∣ 2 −c), а её градиент равен L(w,λ)=g 1:t T + s=1 ∑ T (w−w s )⊙σ s +λ ∣∣w∣∣ 2 w , где σ s σ s - вектор, а ⊙ ⊙ — поэлементное умножение векторов. Приравнивая к нулю градиент, получаем 1:s ⊙w+λ ∣∣w∣∣ 2 w =0, где мы, как обычно, обозначили 1:t −∑ s=1 Проанализируем условие дополняющей нежесткости λ(∣∣w∣∣−c)=0. Если λ = 0 λ=0, то решение w w уже находится внутри шара и имеет вид 1:s −z t При практической реализации мы просто сначала посчитаем это выражение и проверим, не попадаем ли мы в шар. Если попадаем — отлично, если нет — то дальше говорим, что ∣∣w∣∣=c и решаем продолжаем решение 1:s ∗w+λ 1:s + c λ −z t Теперь подставим это в ∣∣w∣∣=c и получим ∣∣w∣∣= σ 1:s + c λ ∣∣z λ=∣∣z t ∣∣−σ 1:s w=c ∣∣z t ∣∣ −z t Получаем, что если мы находимся внутри шара, то мы действуем согласно обыкновенному adaptive алгоритму со всеми хорошими свойствами, иначе —",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 8,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "λ=0, то решение w w уже находится внутри шара и имеет вид 1:s −z t При практической реализации мы просто сначала посчитаем это выражение и проверим, не попадаем ли мы в шар. Если попадаем — отлично, если нет — то дальше говорим, что ∣∣w∣∣=c и решаем продолжаем решение 1:s ∗w+λ 1:s + c λ −z t Теперь подставим это в ∣∣w∣∣=c и получим ∣∣w∣∣= σ 1:s + c λ ∣∣z λ=∣∣z t ∣∣−σ 1:s w=c ∣∣z t ∣∣ −z t Получаем, что если мы находимся внутри шара, то мы действуем согласно обыкновенному adaptive алгоритму со всеми хорошими свойствами, иначе — проекция побеждает. Аналогично L 1 L 1 регуляризации, здесь тоже есть различия между lazy и greedy представлением этого регуляризатора. Однако, в классических DL задачах эти методы встречаются не слишком часто и здесь сложно привести какой-нибудь значимый успех, который мог бы улучшить качество в важной задача. Навскидку мы можем вспомнить разве что Adversatial White-Box learning, в котором можно было бы это попробовать. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 15.2. Адаптивный FTRL Следующий параграф 15.4. Методы оптимизации в Deep Learning",
    "metadata": {
      "title": "Регуляризация в онлайн-обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/regulyarizaciya-v-onlajn-obuchenii",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.3",
      "part": 9,
      "total_parts": 9,
      "source_file": "15.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В выпуклой оптимизации такая точка обязательно будет точкой глобального минимума. В невыпуклой оптимизации все сильно сложнее: Бывает много локальных минимумов Бывают седловые точки Локальный минимум — это критическая точка w ∗ w ∗ , в которой Гессиан H(w ∗ )=∇ 2 f t (w ∗ ) положительно определён. Отметим, что часто в методах глобальной оптимизации рассматривается так называемая «локальная выпуклость», для которой требуется, чтобы функция (w) была выпуклой внутри некоторого шара радиуса ϵ ϵ с центром в точке w ∗ w ∗ . Критические точки, в которых гессиан не является знакоопределённым, называются седловыми. Пример: функция f(x 1 ,x 2 )=x 1 2 −x 2 2 имеет седловую точку { 0 , 0 } {0,0}. Гессиан в точке 0 H(0,0)=( 2 0 0 −2 ) Обратите внимание: во многих современных статьях про сходимость методов оптимизации первого порядка на невыпуклых функциях (пример) в качестве критерия сходимости рассматривают сходимость по норме градиента: ∣∣∇f(w)∣∣ 2 2 <ϵ при некотором заранее фиксированном ϵ ϵ. В выпуклой оптимизации этот критерий сходимости эквивалентен двум другим: сходимости по расстоянию до оптимума в пространстве параметров: ∣∣w−w ∗ ∣∣ 2 2 <ϵ; сходимости по расстоянию до оптимума по значениям функции f(w)−f(w ∗ )<ϵ. В невыпуклой оптимизации всё не так просто и поиск глобального минимума является в общем случае NP-трудной задачей. Критерий ∣∣∇f(w)∣∣ 2 2 <ϵ даёт возможность исследовать сходимость к любой критической точке, но если речь об обучении нейронных сетях, то остается лишь надеяться, что эта критическая точка будет хорошим локальным минимумом. Скользящее среднее в знаменателе AdaGrad. Методы RMSprop и Adam Мотивация В далекие 2012-2014е в мире было не так много опыта по построению хороших нейросетевых архитектур. «Канонические» методы оптимизации нейросетей RMSprop и Adam появлялись во времена, когда ещё не придумали основополагающих вещей вроде: Residual connection и Dense connection (статьи опубликованы в 2015/2016 соответственно, во всех экспериментах используется SGD, в статье и в ссылках не упоминаются методы Adam/RMSprop), плохо решались проблемы взрывов/затуханий градиентов и т.д. Batch Normalization и Layer Normalization (2015/2016 соответственно) Также люди не умели правильно инициализировать нейросети гигантской глубины. статьи вроде 1000+ layer fully connected и 10000+ layer CNN позже. Кстати, этот цикл статей хочется особо отметить за интересную технику анализа распространения сигнала по нейронной сети. В общем, в те времена царило архитектурное средневековье со всеми родовыми проблемами нейронных сетей: Взрывы градиентов; Затухания градиентов; Взрывы-затухания сигнала на прямом проходе; Плохие начальные инициализации, нестабильный старт обучения. При попытках применять метод AdaGrad особо остро стояли проблемы 1 и 4. AdaGrad аккумулирует всю прошедшую историю 1:t 2 1 без затухания. Если в какой-то момент возникает одна из указанных проблем, знаменатель резко возрастает и больше не выправляется. Чтобы побороть проблемы 1-4, решили поработать над оптимизатором и сделать так, чтобы история в AdaGrad аккумулировалась с затуханием и метод оптимизации мог со временем забыть плохие точки. Самый популярный и простой в реализации метод — экспоненциальное скользящее среднее. RMSProp Самая первая и самая простая модификация метода AdaGrad — метод RMSprop — вместо суммы использует экспоненциальное скользящее среднее в знаменателе: =βv t−1 +(1−β)g t+1 Методу RMSprop не было посвящено ни одной специализированной статьи, равно как и",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 1,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "аккумулирует всю прошедшую историю 1:t 2 1 без затухания. Если в какой-то момент возникает одна из указанных проблем, знаменатель резко возрастает и больше не выправляется. Чтобы побороть проблемы 1-4, решили поработать над оптимизатором и сделать так, чтобы история в AdaGrad аккумулировалась с затуханием и метод оптимизации мог со временем забыть плохие точки. Самый популярный и простой в реализации метод — экспоненциальное скользящее среднее. RMSProp Самая первая и самая простая модификация метода AdaGrad — метод RMSprop — вместо суммы использует экспоненциальное скользящее среднее в знаменателе: =βv t−1 +(1−β)g t+1 Методу RMSprop не было посвящено ни одной специализированной статьи, равно как и не было никаких доказательств его сходимости даже для выпуклых задач. Adam Авторы Adam в статье Adam: A Method For Stochastic Optimization вводят два новшества по сравнению с RMSprop. Во-первых, это Momentum. Во вторых — Bias correction term. Напомним, как работает этот метод. =0,m t−1 +(1−β t−1 +(1−β 2 )g t 2 Применяем bias correction 1−β 1−β t+1 Сразу перепишем в нерекурсивной форме с зависимостью только от 1−β 2 t 1 (1−β 2 ) s=1 ∑ t β 2 t−s 1−β 1 t 1 (1−β 1 ) s=1 ∑ t β 1 t−s g s Мотивация для bias correction Авторы статьи пишут, что для правильной работы метода должны быть несмещенными оценками E [ g ] E[g] и E [ g 2 ] E[g 2 ] соответственно. Допустим, все g t g t — независимые одинаково распредёленные случайные величины. Это довольно сильное предположение, но иначе не получатся красивые формулы. Рассмотрим на примере E[v t ]=E[(1−β 2 ) s=1 ∑ t β 2 t−s g s 2 ]=(1−β 2 ) s=1 ∑ t β 2 t−s E[g =((1−β 2 ) s=1 ∑ t β 2 t−s )E[g 2 ]=(1−β 2 t )E[g 2 ] Отсюда очевидно, что исходные смещены на множитель (1−β 2 t ), поэтому авторы Adam делят на него . Так как lim t→∞ lim (1−β 2 t )=1 при 0≤β 2 <1, эффект смещения сильнее всего заметен в начале итерационного процесса. Например, при классическом β 2 = 0.999 β 2 =0.999 мы получаем смещение в 0.001 раз. В начале обучения bias correction призван уменьшить слишком большие шаги оптимизатора. Доказательство сходимости метода В оригинальной статье приводится теорема с доказательством сублинейного Regret. Доказательство содержало ошибку, в новой работе 2018 года было доказано, что для любого набора гиперпараметров Adam существует выпуклая задача, на которой он не сходится. Проблемы со сходимостью, впрочем, не являются специфичными для выпуклых задач: в нейронных сетях Adam тоже может вести себя странно, и об этом мы поговорим ниже в разделе «Как сломать адаптивные методы». Разбирать доказательство исходной статьи мы не будем, зато обратим внимание на пару неприятных фактов о различиях между «продаваемой» частью статьи и бекендом с экспериментами и доказательствами теорем. Почему Adam стали считать лучшим методом стохастической оптимизации? После успешного введения метода Adam в эксплуатацию в нейросети его окрестили «method of choice» в задачах стохастической оптимизации. Это было на 100% обусловлено его успехом в обучении нейронных сетей с нестабильными архитектурами. Структура статьи выглядит следующим образом: Выделенный в большую",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 2,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "специфичными для выпуклых задач: в нейронных сетях Adam тоже может вести себя странно, и об этом мы поговорим ниже в разделе «Как сломать адаптивные методы». Разбирать доказательство исходной статьи мы не будем, зато обратим внимание на пару неприятных фактов о различиях между «продаваемой» частью статьи и бекендом с экспериментами и доказательствами теорем. Почему Adam стали считать лучшим методом стохастической оптимизации? После успешного введения метода Adam в эксплуатацию в нейросети его окрестили «method of choice» в задачах стохастической оптимизации. Это было на 100% обусловлено его успехом в обучении нейронных сетей с нестабильными архитектурами. Структура статьи выглядит следующим образом: Выделенный в большую красивую видную рамочку алгоритм с дефолтными настройками вроде α = 0.001 α=0.001; Формулировка теоремы в разделе про доказательства; Эксперименты на нейросетях и выпуклых задачах. В пункте 1 описан алгоритм, который все нынче знают, как Adam. Мало кто знает, что в доказательствах сходимости и в экспериментах на выпуклых задачах использовался немного другой алгоритм: вместо константного α α авторы статьи взяли . Сравним эти learning rate с AdaGrad: Метод Формулы AdaGrad s=1 Adam (1−β) s=1 ∑ t β t−s g s 2 1−β t Авторы в экспериментах на логистической регрессии убили основное свойство Adam — неубывающие learning rate. Вспомним, как в разделе про вывод AdaGrad мы анализировали порядок убывания learning rate — он был =O( t 1 ). Отсюда следует, что у такого Adam learning rate убывают так же, как в AdaGrad. Словом, будьте внимательны при чтении статей: смотрите не только в описание алгоритмов, но и в их реализацию. Настоящий Adam, который в pytorch и tensorflow реализован без множителя 1 t t 1 , в выпуклой задаче разреженной логистической регрессии обычно работает намного хуже AdaGrad. Это справедливо как для чисто линейных моделей, так и для комбинированных Wide & Deep архитектур, из-за чего в одной и той же нейросети приходится использовать разные методы оптимизации для разных параметров. Промежуточный итог по Adam/RMSProp Тут нужно запомнить три идеи: Momentum Скользящее среднее в learning rate Bias correction На практике, часто почему-то рассматривают методы RMSprop и Adam как нечто отлитое в граните и не пытаются брать от них лучшее. Например, методу RMSprop обычно идет на пользу добавление bias correction от adam. Так что полезно помнить идеи, стоящие за методами оптимизации, и уметь их комбинировать. Как сломать адаптивные методы со скользящим средним Как и когда ломаются адаптивные методы Все диагональные адаптивные методы так или иначе используют покоординатный learning rate t,i = v t,i α t . Методы отличаются лишь формулировкой v t , i v t,i Метод Рекуррентные формулы v t , i v t,i Развернутые формулы v t , i v t,i α t α t AdaGrad t−1,i +g t,i t,i = s=1 ∑ t g s,i 2 α α RMSprop t−1,i +(1−β)g t,i (1−β) s=1 ∑ t β t−s g s 2 α α Adam t−1,i +(1−β)g t,i (1−β) s=1 ∑ t β t−s 1−β t Все эти методы имеют единый вид формул FTRL, аналогичный формулам FTRL-AdaGrad: min t+1 =arg w min g 1:t T w+ s=1 ∑ t ∣∣w−w s,i = v s,i s,i =",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 3,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "лишь формулировкой v t , i v t,i Метод Рекуррентные формулы v t , i v t,i Развернутые формулы v t , i v t,i α t α t AdaGrad t−1,i +g t,i t,i = s=1 ∑ t g s,i 2 α α RMSprop t−1,i +(1−β)g t,i (1−β) s=1 ∑ t β t−s g s 2 α α Adam t−1,i +(1−β)g t,i (1−β) s=1 ∑ t β t−s 1−β t Все эти методы имеют единый вид формул FTRL, аналогичный формулам FTRL-AdaGrad: min t+1 =arg w min g 1:t T w+ s=1 ∑ t ∣∣w−w s,i = v s,i s,i = α s v s,i − α s−1 v s−1,i Вспомним теоретические ограничения на (w): (w) — выпуклый; (w)≥0. Адаптивные методы с регуляризаторами (w)=∣∣w∣∣ σ t 2 будут удовлетворять этим условиям, если все s,i ≥0. В этом месте и локаются методы со скользящим средним: никто не обещал, что последовательность v t v t будет монотонно неубывать. Если же s,i < α s−1 v s−1,i ,(∗) то метод может ломаться Обратите внимание. Momentum в методе Adam никак не повлияет на справедливость наших рассуждений, поскольку в формулах для адаптивных learning rate он не используется. Адаптивные методы с такими learning rate сломаются и с momentum, и без него. Обратите внимание. Bias correction в методе Adam уменьшает learning rate в начале обучения, заставляя метод делать меньшие шаги. Все рекуррентные формулы из таблицы можно переписать в виде t−1 +C 2 g t 2 Тогда неравенство ( ∗ ) (∗) можно записать в виде t−1 t−1 v t−1 t−1 ( α t−1 Здесь мы можем подвести общую черту и сказать, что методы Adam и RMSprop дают s,i <0, когда становится меньше предыдущей накопленной истории с точностью до некоторой константы. А когда такое бывает? Уменьшение , как правило, означает приближение к критическим точкам. Добавление квадратичных регуляризаторов с отрицательным коэффициентом приводит к тому, что метод оптимизации штрафует за близость к критическим точкам, заставляя убегать от них. Это приводит к тому, что метод не может нормально сойтись к локальным минимумам (в выпуклых задачах — просто к минимумам, что намного более критично). Отметим, что по разным координатам σ s , i σ s,i могут вести себя по-разному. Таким образом, можно получить ситуацию, когда мы поощряем близость по одним координатам и штрафуем за близость по другим. Вывод условий поломок для конкретных методов AdaGrad AdaGrad невозможно сломать таким способом: для него гарантируется, что t−1 . RMSprop t,i =βv t−1,i +(1−β)g t,i 2 Подставим в условие ( ∗ ) (∗), сразу сократив константный =α: t−1,i +(1−β)g t,i 2 <v t−1,i t,i 2 <v t−1,i Adam Чисто технически, при выведении формул можно подумать, что Adam страдает от указанных эффектов гораздо сильнее RMSprop, но на самом деле это не так. Переобозначим β 2 β 2 из статьи про Adam как просто β β для общности обозначений. Распишем неравенство ( ∗ ) (∗) для метода Adam: 1−β t βv t−1 +(1−β)g t 2 < 1−β t−1 v t−1 1−β t 1−β g t 2 <v t−1 ( 1−β t−1 1 − 1−β 1−β t 1−β g t",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 4,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "в условие ( ∗ ) (∗), сразу сократив константный =α: t−1,i +(1−β)g t,i 2 <v t−1,i t,i 2 <v t−1,i Adam Чисто технически, при выведении формул можно подумать, что Adam страдает от указанных эффектов гораздо сильнее RMSprop, но на самом деле это не так. Переобозначим β 2 β 2 из статьи про Adam как просто β β для общности обозначений. Распишем неравенство ( ∗ ) (∗) для метода Adam: 1−β t βv t−1 +(1−β)g t 2 < 1−β t−1 v t−1 1−β t 1−β g t 2 <v t−1 ( 1−β t−1 1 − 1−β 1−β t 1−β g t 2 <v t−1 (1−β t−1 )(1−β t ) 1−β t −β+β 1−β t−1 1 v t−1 В отличие от RMSprop, у нас появился дополнительный множитель 1−β t−1 1 >1. С одной стороны, можно подумать, что метод строго хуже. Однако, этот множитель сильно больше нуля только во время первых шагов оптимизации, тогда как рассматриваемая нами проблема играет роль только на поздних стадиях оптимизации при приближении к критическим точкам. А к тому моменту, этот множитель будет практически равен единице и мы получим формулы выше от RMSprop. Поэтому, на самом деле, методы в одинаковой степени страдают от этих эффектов, но bias correction добавляет стабильности в начале. Интерпретации Избегание локальных минимумов или седловых точек Если представить, что нейросеть — очень плохая и жутко невыпуклая задача, то можно рассматривать подобное поведение как «защиту» от промежуточных плохих критических точек, позволяющую нам «убегать» от них. Данная интерпретация, к сожалению, имеет множество недостатков: Никто не обещал, что новая критическая точка будет лучше старой и что мы, прыгая таким образом, будем улучшать качество модели. Не каждый локальный минимум плохой. Если текущая критическая точка — хороший локальный минимум с хорошей обобщающей способностью, то мы просто нормально не сойдемся к нему и не достигнем хорошего качества модели. Общественность уже идентифицировала такое поведение как проблему и решила ее в более поздних популярных оптимизаторах (см.раздел про AMSgrad). Большинство современных рекомендаций по обучению больших неонлайновых моделей вроде GPT или картиночных моделей содержат в себе learning rate scheduler'ы как обязательный для успеха ингредиент. Эти рекомендации нивелируют проблему отрицательных регуляризаторов. Все learning rate scheduler'ы заставляют learning rate убывать, что позволяет достигать лучших результатов, чем с помощью обычных Adam и RMSprop. В параграфе про FTL мы узнали, что градиентный метод без регуляризации отвратительно работает даже на выпуклых задачах, а если мы начнём вводить отрицательную регуляризацию, да еще и на сложных невыпуклых задачах, то все может стать еще хуже. В целом, мировой опыт говорит, что полагаться на подобные интерпретации при тюнинге модели не стоит. Нестабильность в выпуклых задачах Итак, методы RMSprop и Adam плохо работают для выпуклых задач, особенно для разреженных задач, и могут приводить к субоптимальным решениям на train. Тем не менее, есть искушение заявить, что «это такая регуляризация в классическом смысле: не слишком хорошо сходимся к оптимальной точке, не слишком сильно переобучаемся под датасет и можем лучше работать на тесте». Это искушение особенно опасно потому, что подобные эффекты действительно могут иметь место, особенно в классической (не онлайновой) постановке задачи. Любая регуляризация направлена на то, чтобы сдвинуть",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 5,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "хуже. В целом, мировой опыт говорит, что полагаться на подобные интерпретации при тюнинге модели не стоит. Нестабильность в выпуклых задачах Итак, методы RMSprop и Adam плохо работают для выпуклых задач, особенно для разреженных задач, и могут приводить к субоптимальным решениям на train. Тем не менее, есть искушение заявить, что «это такая регуляризация в классическом смысле: не слишком хорошо сходимся к оптимальной точке, не слишком сильно переобучаемся под датасет и можем лучше работать на тесте». Это искушение особенно опасно потому, что подобные эффекты действительно могут иметь место, особенно в классической (не онлайновой) постановке задачи. Любая регуляризация направлена на то, чтобы сдвинуть оптимум решения исходной некорректно поставленной задачи в надежде, что точка оптимума измененной задачи будет обладать лучшей обобщающей способностью на тесте. В частности, такой эффект может иметь ранняя остановка методов оптимизации до их сходимости к точке оптимума. Однако здесь есть одно очень важное «но». Если введение регуляризации в некорректно поставленную задачу — это полностью осмысленный и контролируемый гиперпараметрами процесс, то хаотично разваливающийся вокруг точки оптимума метод оптимизации — нет. Подумайте: вдруг ваша задача фактически не является некорректно поставленной? Вдруг у вас огромный и очень репрезентативный датасет, благодаря чему оптимум на train всегда отлично работает в проде? В этом случае кривой метод оптимизации способен подпортить качество вашей модели. Нестабильность в разреженных задачах В задачах с разреженными параметрами ситуацию t,i 2 <v t−1,i получить еще легче. Допустим, у нас есть некоторый параметр w i w i , который встречается в 0.1% объектов выборки. В такой ситуации между появлениями этого объекта в выборке и очередным расчетом градиентов для него проходит значительное время. За это значительное время модель дообучалась, и за счет других, менее разреженных параметров могла научиться лучше прогнозировать очередной объект с этим параметром w i w i . Тогда ∣∣g t,i ∣∣ 2 уменьшается и, следовательно, больше шансов попасть в плохую ситуацию. Ниже мы рассмотрим метод AMSgrad и наперёд скажем, что для оптимизации разреженных параметров Adam/RMSprop добавление AMSgrad очень часто дает прибавку в качестве. Зависимость нестабильности в регуляризаторе от learning rate α α На первый взгляд, парадоксальным кажется следующий факт: чем меньше learning rate, тем в бОльшую сторону может отклониться отрицательный регуляризатор: t−1 Однако в «жадных» формулах все с точностью до наоборот: t+1 Из жадных формул очевидно, что уменьшение α α ведет к уменьшению шага и, как следствие, увеличению стабильности алгоритма. Чтобы разрешить парадокс, надо вспомнить, что в FTRL решающее значение имеет не один отдельный регуляризатор, а сумма σ 0 : t σ 0:t . В начале процесса оптимизации =0, первый регуляризатор точно не сломается. Чем меньше learning rate, тем меньшие шаги мы делаем от начальной точки и, следовательно, тем меньше должна отличаться норма градиентов. Если от шага к шагу норма градиента меняется не слишком сильно, то мы накопим огромную кумулятивную регуляризацию σ 0 : t σ 0:t к моменту, когда регуляризатор решит отклониться в отрицательную сторону. При бОльшем learning rate мы шагаем быстрее, и точки, когда ломается регуляризатор, достигаем тоже быстрее, накопив гораздо меньшую сумму σ 0 : t σ 0:t . Если теперь для очередной точки мы получили отрицательный регуляризатор, то",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 6,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "0:t . В начале процесса оптимизации =0, первый регуляризатор точно не сломается. Чем меньше learning rate, тем меньшие шаги мы делаем от начальной точки и, следовательно, тем меньше должна отличаться норма градиентов. Если от шага к шагу норма градиента меняется не слишком сильно, то мы накопим огромную кумулятивную регуляризацию σ 0 : t σ 0:t к моменту, когда регуляризатор решит отклониться в отрицательную сторону. При бОльшем learning rate мы шагаем быстрее, и точки, когда ломается регуляризатор, достигаем тоже быстрее, накопив гораздо меньшую сумму σ 0 : t σ 0:t . Если теперь для очередной точки мы получили отрицательный регуляризатор, то насколько сильно он может всё поломать? Окей, допустим, мы шагнули к критической точке. А насколько сильно может расколбасить одна плохая точка в регуляризаторе? Так, чтобы он перекрыл всю предыдущую сумму σ 0 : t σ 0:t ? Если градиенты ограничены по норме, то катастрофы, очевидно, не будет. Ограниченность градиентов по норме мы, с одной стороны, гарантировать не можем, с другой — проблемам взрыва/затухания градиентов в архитектурах уделяется столько внимания, что на практике это условие зачастую выполняется. Чиним RMSprop и Adam Мотивация Время шло, люди учились строить хорошо обучаемые архитектуры. Стали даже появляться революционные идеи вроде ReZero (не путать с аниме) с полным отказом от batchnorm/layernorm нормализаций в глубоких сетях и с улучшением качества работы и скорости сходимости. Ситуация со стабильностью обучения нейросетей кардинально изменилась. Несмотря на улучшение стабильности обучения, люди стали замечать, что при длительном процессе оптимизации Adam начинает сбоить. Авторы метода AMSgrad в статье On the Convergence of Adam and Beyond были одними из первых, кто провел почти аналогичный нашему анализ и добавили в Adam костыль, который обеспечивает выполнение условия t−1 и исключает отрицательные регуляризаторы. Обратите внимание: в разделее про Learning Rate Scheduling vs AdaGrad мы поговорим о «цикличности истории» развития методов оптимизации в deep learning. AMSgrad Авторы статьи On the Convergence of Adam and Beyond анализируют последовательность t+1 =( α t+1 v t+1 и говорят, что отрицательные значения в ней вызывают проблемы с процессом оптимизации. Их анализ в целом аналогичен приведённому выше, поэтому мы не будем его здесь дублировать. Авторы статьи не стали предлагать новых схем learning rate и просто модифицировали старую: выполнение >=v t−1 обеспечивается «в лоб» при помощи v ^ t = max =max{v t , v ^ t−1 }, v ^ 0 =0. Итоговое правило апдейта без momentum и без bias correction (оригинальный Algorithm 2 из статьи bias correction не использует): =βv t−1 +(1−β)g max =max{v t , v ^ t−1 t+1 Если нужен метод с momentum, то можно просто заменить g t g t в последней формуле на =γm t−1 +(1−γ)g t+1 Реализация без дополнительной памяти Оригинальные формулы из статьи v ^ t = max =max{v t , v ^ t−1 } предполагают, что для расчета мы держим два параметра: v t − 1 v t−1 t−1 . RMSprop и Adam хранят только один параметр v t − 1 v t−1 . Таким образом, включение метода требует дополнительных расходов памяти (х1.5 относительно RMSprop и x1.33 относительно Adam). Выше при разборе методов RMSprop/Adam мы сказали,",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 7,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "=max{v t , v ^ t−1 t+1 Если нужен метод с momentum, то можно просто заменить g t g t в последней формуле на =γm t−1 +(1−γ)g t+1 Реализация без дополнительной памяти Оригинальные формулы из статьи v ^ t = max =max{v t , v ^ t−1 } предполагают, что для расчета мы держим два параметра: v t − 1 v t−1 t−1 . RMSprop и Adam хранят только один параметр v t − 1 v t−1 . Таким образом, включение метода требует дополнительных расходов памяти (х1.5 относительно RMSprop и x1.33 относительно Adam). Выше при разборе методов RMSprop/Adam мы сказали, что на практике AMSgrad помогает разреженным параметрам. Для разреженных моделей потребление памяти — краеугольный камень, поэтому простое включение дефолтной реализации amsgrad из статьи может быть болезненным и, к сожалению, не оправданным. На практике же эвристика вида v t = max =max{v t ,v t−1 } для разреженных параметров обычно работает так же хорошо и не требует дополнительной памяти. Никаких теоретических гарантий для нее нет, но на практике она работает. Добавление bias correction Оригинальная статья (и следующие букве оригинала стандартные реализации алгоритма, например, в PyTorch) предполагает убирание bias correction. Эксперименты на разреженных данных показывают, что убирание bias correction вредит сходимости, это полезная вещь. С практической точки зрения, есть два способа реализовать bias correction в AMSgrad: Post-correction: max 1−β t 1 max{v t , v ^ t−1 }, Pre-correction: v ^ t = max =max{ 1−β t 1 v t , 1−β t−1 1 v ^ t−1 }. С точки зрения корректности метода AMSgrad, правильный вариант — pre-correction, так как он не ломает максимум. А вот эксперименты показывают, что добавление pre-correction ничего не даёт, а вот post-correction действительно помогает в том смысле, что AMSgrad + post-bias correction лучше, чем просто RMSProp/Adam с bias correction. Соединяем эвристику + bias correction Итоговые формулы можно использовать такие: max 1−β t 1 max{v t , 1−β t−1 1 v t−1 } Learning Rate Scheduling Другим способом улучшения сходимости методов RMSprop/Adam/SGD является learning rate scheduling (расписание learning rate, шедулер). Learning rate scheduler — это мета-алгоритм: они берёт любой стандартный метод оптимизации с константным параметром learning rate α α и предписывает схему изменения α t α t на каждом шаге t t, или на каждой эпохе, или на любом другом заданном периоде. Поскольку мы работаем с одним параметром α α, мы можем с ним делать всего две вещи: увеличивать или уменьшать. Эти два варианта имеют свои названия: Learning rate decay — уменьшение learning rate с течением времени с целью нивелировать осцилляцию RMSprop/Adam около критических точек. (Warm)Restart — обычно резкое увеличение learning rate. Warm — потому что мы уже сошлись в какую-то хорошую точку и сбрасываем только состояние оптимизатора в ней, но не переинициализируем сами параметры. WarmRestart может заключаться не только в увеличении α α, но и, например, в дополнительном сбросе состояния оптимизатора (обнуление momentum или v t v t ), хотя автор статьи такой подход встречали достаточно редко Существует огромное количество вариантов расписания, каждый со своим графиком изменения α t α t и со своим любовно подобранным множеством",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 8,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "rate decay — уменьшение learning rate с течением времени с целью нивелировать осцилляцию RMSprop/Adam около критических точек. (Warm)Restart — обычно резкое увеличение learning rate. Warm — потому что мы уже сошлись в какую-то хорошую точку и сбрасываем только состояние оптимизатора в ней, но не переинициализируем сами параметры. WarmRestart может заключаться не только в увеличении α α, но и, например, в дополнительном сбросе состояния оптимизатора (обнуление momentum или v t v t ), хотя автор статьи такой подход встречали достаточно редко Существует огромное количество вариантов расписания, каждый со своим графиком изменения α t α t и со своим любовно подобранным множеством задач, на которых данный метод показывает себя лучше других. Приводить здесь их список особого смысла нет, лучше просто откройте документацию любого фреймворка и наслаждайтесь разнообразием вариантов. Мы же обсудим влияние learning rate decay на осцилляцию вокруг критических точек и дадим практические рекомендации по подбору расписаний. Влияние learning rate decay на сходимость Для выпуклых задач в разделе про схемы убывания learning rate для FTRL-методов (константный регуляризатор, sqrtt 1 и AdaGrad) мы буквально на оценках на regret видели, что это важный аспект для асимптотики сходимости. В выпуклом случае, при приближении к минимуму мы должны оптимизировать решение с куда большей точностью. Норма градиентов при приближении к минимуму тоже уменьшаются, поэтому даже с константным O ( 1 ) O(1) learning rate шаги будут становиться меньше, но — как показывают и теоретические оценки на regret, и многочисленные их валидации в статьях — этого недостаточно. Уменьшение learning rate с правильной асимптотикой уменьшения дает куда более хорошие результаты. Для глубинного обучения и оптимизации к каким-то локальным минимумам эта логика тоже применима. Возвращаясь к методам Adam/RMSprop — напомним, что у них асимптотика learning rate O ( 1 ) O(1). Им в любом случае пойдет на пользу уменьшение learning rate, даже если не брать во внимание их проблемы вокруг критических точек и взять метод AMSgrad, который от этих проблем не страдает. Отсюда же очевидно, что проблемы adam/rmsprop начинают стрелять гораздо меньше. Learning rate уменьшается => от критической точки мы в плохих ситуациях шагаем на гораздо меньшее расстояние => область, вокруг которой мы будем «прыгать», сужается => мы худо-бедно, но сходимся. Практические рекомендации Как мы уже отмечали выше, шедулеров существует поистине фантастическое количество, гораздо больше, чем базовых оптимизаторов, к которым они применяются. Без структуризации подхода к ним работать становится сложно. Мы хотели бы дать вам следующие рекомендации: Выучите свою модель без learning rate scheduling со стандартными методами оптимизации и посмотрите, как ведёт себя loss для различных learning rate. Обязательно переберите learning rate на этом шаге. Начинать внедрение расписаний рекомендуем с шедулеров, которые только уменьшают learning rate. Классические варианты — ReduceOnPlateou или linear decay. Правильный подбор learning rate и темпа его уменьшения очень важны в любой задаче стохастической оптимизации. Только после того, как вы хорошенько потюните learning rate decay, можно смотреть в сторону WarmRestart. Иногда рестарты могут помочь. Автор статьи занимается в основном рекомендательными моделями и там эту технику практически никто не применяет. Learning rate scheduling vs AdaGrad У методов SGD/RMSprop/Adam последовательность ∼O(1) не является асимптотически убывающей, и для того, чтобы",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 9,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "как ведёт себя loss для различных learning rate. Обязательно переберите learning rate на этом шаге. Начинать внедрение расписаний рекомендуем с шедулеров, которые только уменьшают learning rate. Классические варианты — ReduceOnPlateou или linear decay. Правильный подбор learning rate и темпа его уменьшения очень важны в любой задаче стохастической оптимизации. Только после того, как вы хорошенько потюните learning rate decay, можно смотреть в сторону WarmRestart. Иногда рестарты могут помочь. Автор статьи занимается в основном рекомендательными моделями и там эту технику практически никто не применяет. Learning rate scheduling vs AdaGrad У методов SGD/RMSprop/Adam последовательность ∼O(1) не является асимптотически убывающей, и для того, чтобы это скомпенсировать, используется расписание learning rate. А вот у AdaGrad с η t η t и так всё в порядке. Давайте восстановим хронологию событий: Метод AdaGrad пытаются применять к нейросетям в 2012+ годах, но тогда архитектуры были нестабильны, градиенты взрывались и навсегда портили знаменатель AdaGrad, сильно уменьшая learning rate. Появляются методы RMSprop/Adam (2013/2014) со скользящим средним в знаменателе, которые могут оправиться от взрыва градиента. Развитие архитектур нейронных сетей не стоит на месте, появляются разные виды residual connection (2015), LayerNorm/BatchNorm (2015-2016), крутые методы начальной инициализации — огромное количество способов улучшения стабильности обучения. С развитием архитектур люди замечают, что RMSProp/Adam умеют застревать на одном уровне значений функции потерь, и начинают применять техники для уменьшения learning rate. В дальнейших работах метод AdaGrad часто рассматривается наравне с Adam/RSMprop и дает очень похожее, либо даже лучшее качество (см, например, статью про Shampoo). А дело в том, что архитектуры уже очень хорошо инициализируются и правильно проектируются так, чтобы не было взрывов/затуханий градиентов ни на какой стадии оптимизации. Развитие методов оптимизации в deep learning сделало небольшой круг, и мы рекомендуем об этом помнить. Порой люди могут одновременно рассуждать о бесценной пользе learning rate decay (особенно с линейным убыванием как 1 t t 1 ) и корить AdaGrad за бесконечное аккумулирование квадратов градиентов (которые убывают как 1 t t 1 ). Так что если у вас вдруг хорошо заработал шедулер с ∼O( t 1 ) — возможно, обычный AdaGrad будет лучше? SGD vs Adam В последнее время в литературе часто появляются заявления, что решения, полученные адаптивными методами в нейросетях, обладают худшей обобщающей способностью. Сразу хотим отметить, что большинство этих статей исследуют эти эффекты только на задачах Computer Vision на одних и тех же датасетах MNIST/CIFAR/ImageNet. В реальной жизни куда большее разнообразие постановок задач и датасетов, что сразу заставляет сомневаться в воспроизводимости этих эффектов. Рекомендация тут одна, как и всегда — досконально сами все проверяйте. AdamW, SGDW Данные методы предложены авторами в статье Decoupled Weight Decay Regularization, которую мы подробно разобрали в разделее про продвинутую L 2 L 2 регуляризацию. Методы AdamW и SGDW — это просто модификации методов Adam и SGD с momentum, которые используют линеаризованный decoupled L 2 L 2 . Авторы статьи изучали проблему, почему в их экспериментах SGD обобщает лучше Adam (но учится дольше и требует более аккуратной настройки). Они пришли к выводу, что дело не в магии SGD, а в том, что L 2 L 2 -регуляризация у этих двух методов работает по-разному. Добавив",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 10,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "всегда — досконально сами все проверяйте. AdamW, SGDW Данные методы предложены авторами в статье Decoupled Weight Decay Regularization, которую мы подробно разобрали в разделее про продвинутую L 2 L 2 регуляризацию. Методы AdamW и SGDW — это просто модификации методов Adam и SGD с momentum, которые используют линеаризованный decoupled L 2 L 2 . Авторы статьи изучали проблему, почему в их экспериментах SGD обобщает лучше Adam (но учится дольше и требует более аккуратной настройки). Они пришли к выводу, что дело не в магии SGD, а в том, что L 2 L 2 -регуляризация у этих двух методов работает по-разному. Добавив decoupling, авторы сумели показать, что decoupled Adam обгоняет SGD. Эти эффекты, повторимся, были уже рассмотрены ранее в разделее про продвинутую L 2 L 2 регуляризацию. Единственное, что мы не обсудили тогда — это momentum. В постановке Proximal Gradient Descent градиент заменяется на momentum t+1 ∣∣w−w =γm t−1 +(1−γ)g t+1 ∣∣w−w t+1 Покоординатные η t η t могут рассчитываться любыми методами: AdaGrad, RMSprop или Adam, не принципиально. На всякий случай напомним, что мы вывели потенциально более правильные формулы t+1 ∣∣w∣∣ ∣∣w−w t+1 = 1+λ Метод SGDW получается из формул выше, если убрать покоординатность η t η t К сожалению, здесь мы не почерпнули новых идей, так как выяснили, что это просто очередная инкарнация Proximal методов оптимизации. RAdam Этот метод заключается в том, чтобы стартовать с адаптивного метода Adam и в некоторый момент переключиться на SGD. «Некоторый момент» — это, интуитивно, момент стабилизации всех статистик в Adam, когда мы выжали все из ускоренного старта адаптивных методов и хотим получше сойтись к хорошему оптимуму в найденной им окрестности. Отметим, что позднее переключение на SGD с неубывающими learning rate автоматически починит проблемы расходимости Adam ровно там, где они чаще всего и возникают: при хорошем приближении к локальным минимумам. Мы не будем здесь подробно рассматривать их анализ, вы можете сами познакомиться с ним в статье On The Variance Of The Adaptive Learning Rate And Beyond Online RMSprop Особняком стоит метод, описанный в статье Variants of RMSProp and Adagrad with Logarithmic Regret Bounds. Авторы не придумывали очередной хотфикс, а аккуратно заново выводили формулы. Также важно, что данный метод является строгим обобщением метода AdaGrad. В работе есть два нововведения: Переформулировка метода RMSprop так, чтобы: — Осталось экспоненциальное скользящее среднее; — Не было проблемы с отрицательными регуляризаторами и взрывающимися learning rate; — Метод AdaGrad являлся частным случаем нового метода; — Чтобы все эмпирически хорошо работало в т.ч. на глубоких моделях Формулировка новых алгоритмов оптимизации SC-AdaGrad и SC-RMSprop для сильно выпуклых функций с логарифмическими гарантиями на regret. SC в названии — Strongly Convex. Пока рассмотрим только первый пункт. Авторы вводят следующий общий метод: t−1 +(1−β t+1 Нововведение здесь в том, что вместо фиксированного β β мы будем рассматривать последовательность β t β t . Авторы доказывают сублинейный regret для любых последовательностей, удовлетворяющих ≤1− 0<γ≤1 AdaGrad как частный случай Докажем, что метод Adagrad — это метод OnlineRMSprop с γ = 1 γ=1. Аналогично выводам momentum в FTRL, перепишем рекуррентное выражение для v t + 1 v t+1 s=1 ∑ t",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 11,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "глубоких моделях Формулировка новых алгоритмов оптимизации SC-AdaGrad и SC-RMSprop для сильно выпуклых функций с логарифмическими гарантиями на regret. SC в названии — Strongly Convex. Пока рассмотрим только первый пункт. Авторы вводят следующий общий метод: t−1 +(1−β t+1 Нововведение здесь в том, что вместо фиксированного β β мы будем рассматривать последовательность β t β t . Авторы доказывают сублинейный regret для любых последовательностей, удовлетворяющих ≤1− 0<γ≤1 AdaGrad как частный случай Докажем, что метод Adagrad — это метод OnlineRMSprop с γ = 1 γ=1. Аналогично выводам momentum в FTRL, перепишем рекуррентное выражение для v t + 1 v t+1 s=1 ∑ t (1−β s ) k=s+1 Подставив β=1− t 1 , получим s=1 ∑ t (1−β s )( k=s+1 s=1 k=s+1 ∏ t t t−1 s=1 k=s+1 ∏ t k k−1 = s=1 s=1 ∑ t g s 2 Далее, подставляя это в формулу , получаем s=1 s=1 Анализ OnlineRMSprop с γ < 1 γ<1 в стиле FTRL. Пригодность для выпуклых задач Докажем, что OnlineRMSprop не может сломать регуляризаторы в regret. Для этого преобразуем неравенство t−1 v t−1 <(t−1)v t−1 t(β t v t−1 +(1−β t )g t 2 )<(t−1)v t−1 t((1− t γ )v t−1 )<(t−1)v t−1 t−1 (t−1−t(1− t−1 (γ−1) Из условия 0 < γ ≤ 1 0<γ≤1 получаем, что правая часть неравенства неположительна, а левая неотрицательно. Значит, последнее неравенство невозможно, то есть все ≥0. Таким образом, регуляризаторы не сломаются, сходимость будет иметь место и данный метод можно использовать в выпуклых задачах. Строгое доказательство сходимости и оценки на Regret можно прочитать в исходной статье. Эффективный learning rate Как и ранее в методе AdaGrad, допустим, что ∣∣g∣∣ 2 <R. Тогда j=1 ∑ t (1−β j ) k=j+1 j=1 ∑ t (1−β j ) k=j+1 ∏ t β k α При ≤1− t γ выполнено lim t→∞ lim j=1 ∑ t (1−β j ) k=j+1 ∏ t β k =1 Докажем, что все элементы предела < 1. Из этого, в частности, будет следовать, что learning rate у OnlineRMSprop не меньше, чем learning rate в AdaGrad. Если все все =1, то итерационный процесс OnlineRMSprop превращается в t−1 +(1−β t ) Предположим, что ≥1. Тогда: t−1 +(1−β t )≥1 (1− t γ )v t−1 t−1 ≥1 По индукции разворачиваем вплоть до =0, получаем противоречие. Полное доказательство предела оставляем читателям. Надо бы чем-нибудь снизу подпереть, что тоже к 1 сходится. Автор сдавал матан почти 10 лет назад и ему было очень неохота откапывать все эти прекрасные пределы, поэтому ответ был получен с помощью wolfram. Вывод: learning rate у OnlineRMSprop убывает со скоростью =O( t 1 ). Мы исправили ошибку предыдущего RMSprop, изменив только перевзвешивание, но не асимптотику в η t η t . Такой RMSprop можно пробовать использовать в выпуклых задачах Momentum Попробуем расписать классический momentum с константным learning rate в стиле FTRL: t+1 =βv t +(1−β)g t+1 =w t −αv t+1 Всё, что нам нужно сделать — это взять все рекурсивные зависимости от предыдущей итерации и «размотать» их, получив явное выражение. Зависимость w t + 1 w t+1 от w t w t переписать",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 12,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "прекрасные пределы, поэтому ответ был получен с помощью wolfram. Вывод: learning rate у OnlineRMSprop убывает со скоростью =O( t 1 ). Мы исправили ошибку предыдущего RMSprop, изменив только перевзвешивание, но не асимптотику в η t η t . Такой RMSprop можно пробовать использовать в выпуклых задачах Momentum Попробуем расписать классический momentum с константным learning rate в стиле FTRL: t+1 =βv t +(1−β)g t+1 =w t −αv t+1 Всё, что нам нужно сделать — это взять все рекурсивные зависимости от предыдущей итерации и «размотать» их, получив явное выражение. Зависимость w t + 1 w t+1 от w t w t переписать довольно просто, мы это уже делали для обычного градиентного спуска: t+1 =−α i=1 ∑ t v i Теперь надо размотать =(1−β) i=1 ∑ t β t−i g i Теперь будет чуть сложнее. Подставим это и попробуем расписать, как сумму g i g i с определенными коэффициентами: t+1 =−α i=1 ∑ t v i =−α i=1 ∑ t (1−β) j=1 ∑ i β i−j g j Множитель −α(1−β) сразу выносим за сумму и пока забываем. i=1 ∑ t j=1 ∑ i β i−j g j = i=1 ∑ t j=1 ∑ t I(j≤i)β i−j j=1 ∑ t i=1 ∑ t I(j≤i)β i−j g j = j=1 ∑ t g j i=1 ∑ t I(j≤i)β i−j j=1 ∑ t g j i=j ∑ t β i−j Отлично, а теперь нам нужно получить последовательность функций. В линеаризованной задаче это фактически эквивалентно получению зависимости z t + 1 z t+1 от z t z t , где, напомним, z t z t — это сумма градиентов. t+1 −z t = j=1 ∑ t+1 g j i=j ∑ t+1 β i−j − j=1 ∑ t g j i=j ∑ t β i−j =g t+1 + j=1 ∑ t g j i=j ∑ t+1 β i−j − j=1 ∑ t g j i=j ∑ t β i−j =g t+1 + j=1 ∑ t β t+1−j g j = j=1 ∑ t+1 β t+1−j g j Теперь мы можем записать функцию, градиент которой равен t+1 −z t и онлайн-оптимизация которой эквивалентна процедуре с моментумом: (w)= j=1 ∑ t β t−j f j (w) Получаем, что для онлайн-обучения мы на самом деле каждую итерацию скармливаем экспоненциально взвешенную последовательность всех предыдущих функций исходной последовательности. В принципе, нечто такое мы и ожидали увидеть. Функции (w) ^ , очевидно, выпуклы, так что для данной измененной последовательности функций будет сублинейный regret. Nesterov Momentum Рассмотрим классический SGD с momentum, для всех adaptive методов рассуждения аналогичны. =γm t−1 +(1−γ)g t+1 =w t −αm t Градиент функции g t g t посчитан в предыдущей точке w t w t . Идея nesterov momentum в том, чтобы применить momentum на параметры w t w t до вычисления градиента: =∇f =∇f t (w t −m t−1 ) У метода много всяких «интуитивных объяснений», но изначально Nesterov Momentum был выведен сугубо аналитическими методами. Увы, попытки добавлять его в стохастическую оптимизацию «в лоб» обычно улучшением качества не заканчиваются. Анализ того, почему так нельзя и делать и как можно сделать",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 13,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Momentum Рассмотрим классический SGD с momentum, для всех adaptive методов рассуждения аналогичны. =γm t−1 +(1−γ)g t+1 =w t −αm t Градиент функции g t g t посчитан в предыдущей точке w t w t . Идея nesterov momentum в том, чтобы применить momentum на параметры w t w t до вычисления градиента: =∇f =∇f t (w t −m t−1 ) У метода много всяких «интуитивных объяснений», но изначально Nesterov Momentum был выведен сугубо аналитическими методами. Увы, попытки добавлять его в стохастическую оптимизацию «в лоб» обычно улучшением качества не заканчиваются. Анализ того, почему так нельзя и делать и как можно сделать правильно, проводится в работах Katyusha: The First Direct Acceleration of Stochastic Gradient Methods и Natasha-2 (мотивация их автора Zeyuan Allen-Zhu для выбора таких наименований доподлинно неизвестна). Katuysha правильным образом использует nesterov momentum для выпуклого случая, Natasha — для невыпуклого. Данные методы используют подход SVRG для улучшения сходимости и ускорение оптимизации происходит только при приближении к точке оптимума. Adan До недавнего времени громких историй успеха для nesterov momentum в глубоком обучении не было. Метод Natasha распространения не нашел. Наконец, авторы статьи Adan (2022) нашли способ правильной обработки Nesterov Momentum. Метод показал отличные результаты и обновил SOTA метрики на широком спектре задач. Собираем все идеи воедино Авторы данного обзора очень хотят, чтобы читатель ушел не с знанием набора наименований методов оптимизации, а с знанием набора концепций, которые тот или иной метод реализует, и при случае мог сам подстроить метод под свои нужды. Тюнинг методов оптимизации — один из главных способов улучшения качества модели на фиксированном датасете. Adaptive learning rate — автоматическое подстраивание метода под геометрию задачи оптимизации. Крайне важный класс методов для выпуклых/невыпуклых задач. Must-have для разреженных моделей. Методы: AdaGrad/RMSprop/Adam. Скользящее среднее в adaptive learning rate представлено в методах RMSprop/Adam. Не забывайте про их плохое поведение вокруг критических точек и проблемы со сходимостью на финальных этапах оптимизации. BiasCorrection: стабилизация обучения на старте для адаптивных методов со скользящим средним. Большинство экспериментов показывают, что это крайне полезная штука и стоит всегда её использовать. В том числе стоит использовать RMSprop с bias correction, если вам не нужны momentum и Adam. AMSgrad: способ починить сходимость RMSprop/Adam. Не забывайте, что стандартные реализации при использовании AMSgrad отключают bias correction, а это на самом деле может навредить, а также о том, что можно реализовать AMSgrad без дополнительной памяти, и всё будет хорошо работать. Learning rate decay: убывание learning rate зачастую является очень важной деталью в стохастической оптимизации. Помните, что можно брать как AdaGrad, в котором это есть из коробки со скоростью ) (но архитектура нейросети должна быть хорошей), так и комбинацию RMSProp/Adam + learning rate scheduler. WarmRestart: эвристика, резко увеличивающая learning rate после достижения некоторой точки в процессе оптимизации. Практически всегда идет бок о бок с learning rate decay. Где-то помогает Проксимальные методы для функций потерь с регуляризаторами: ProximalGD/AdamW/SGDW/FTRL-Proximal. Must-have для L 1 L 1 -регуляризаторов, без проксимальности они вообще не работают. FTRL-Proximal: lazy vs greedy представление. Переписываем представление любого метода оптимизации в не-жадный вид. Позволяет по-новому взглянуть на любые регуляризаторы, особенно негладкие. Must-have для L 1 L 1",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 14,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "можно брать как AdaGrad, в котором это есть из коробки со скоростью ) (но архитектура нейросети должна быть хорошей), так и комбинацию RMSProp/Adam + learning rate scheduler. WarmRestart: эвристика, резко увеличивающая learning rate после достижения некоторой точки в процессе оптимизации. Практически всегда идет бок о бок с learning rate decay. Где-то помогает Проксимальные методы для функций потерь с регуляризаторами: ProximalGD/AdamW/SGDW/FTRL-Proximal. Must-have для L 1 L 1 -регуляризаторов, без проксимальности они вообще не работают. FTRL-Proximal: lazy vs greedy представление. Переписываем представление любого метода оптимизации в не-жадный вид. Позволяет по-новому взглянуть на любые регуляризаторы, особенно негладкие. Must-have для L 1 L 1 -регуляризации. L 1 L 1 -регуляризация в FTRL-Proximal: Incremental/Fixed/SquareIncremental. Все три имеют разные свойства и разную область применения. Fixed является наилучшим для отбора разреженных признаков/эмбеддингов. L 1 / 2 L 1/2 -регуляризатор для отбора эмбеддингов или автоматического подбора размерности. Можно использовать как аналог FSTR. Крайне полезный подход для разреженных нейросетей в рекомендательных системах, для которых рекомендуется использовать адаптивную схему SquareIncremental. Heavy-ball Momentum: используется для ускорения процесса оптимизации. В выпуклых задачах имеет доказанные оценки на улучшение скорости сходимости, в нейросетях используется как эвристика (зачастую опциональная). Nesterov momentum: в выпуклом случае гораздо мощнее для batch gradient descent, чем обычный momentum, и это подверждается теоретическими гарантиями. В стохастических методах оптимизации и в онлайн обучении «в лоб» применять нельзя: для выпуклого случая подойдет Katyusha, для нейросетей — Adan. Главное, что мы хотим подчеркнуть, — эти идеи друг другу не противоречат и их можно свободно комбинировать друг с другом. Например, можно собрать себе FTRL-Proximal метод с L 1 L 1 -регуляризацией, любым momentum и RMSprop learning rate с AMSgrad. Или любую другую комбинацию. Всегда можно выбрать оптимальный набор под задачу. Пример таблицы с общими формулами Эти формулы используют все подходы выше в едином фреймворке, чтобы наглядно убедиться в том, что все можно друг с другом комбинировать. Generic FTRL-Proximal min t+1 =arg w min g ^ 1:t T w+λ 1,t ∣∣w∣∣ 1 + 2 1 ∣∣w∣∣ λ 2,t 2 + 2 1 s=1 ∑ t ∣∣w−w 1:t − s=1 t+1,i ={ 0 − 1+λ 2,t σ 1:t σ 1:t (z t −sign(z t )λ 1,t ) ∣z t,i ∣≤λ 1,t ∣z t,i ∣>λ 1,t Generic Mirror (Proximal) Gradient Descent min t+1 =arg w min g ^ t T w+λ 1,t ∣∣w∣∣ 1 + 2 1 ∣∣w∣∣ λ 2,t 2 + 2 1 ∣∣w−w t+1 ={ 0 − 1+λ 2,t −sign(z t )λ 1,t ) ∣z t ∣≤λ 1,t ∣z t ∣>λ 1,t Связь: t−1 1 Идея FTRL (lazy) Gradient Descent (greedy) Комментарии Momentum =γm t−1 +(1−γ)g То же самое Не влияет на adaptive v t v t Константный learning rate =0,t>0 Убывающий непокоординатный learning rate t−1 Обычно берут =O( t 1 ) Generic Adaptive learning rate t−1 v t−1 — векторы, везде ниже умножение означает покоординатное умножение Generic Adaptive learning rate с scheduler S(α,t) S(α t ,t) v t − S(α t−1 ,t) v t−1 S(α t ,t) Например, в Adam: 1−β t →S(α,t) 1−β t Adaptive learning rate: AdaGrad s=1 s=1 s=1 =O( t 1",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 15,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "∣≤λ 1,t ∣z t ∣>λ 1,t Связь: t−1 1 Идея FTRL (lazy) Gradient Descent (greedy) Комментарии Momentum =γm t−1 +(1−γ)g То же самое Не влияет на adaptive v t v t Константный learning rate =0,t>0 Убывающий непокоординатный learning rate t−1 Обычно берут =O( t 1 ) Generic Adaptive learning rate t−1 v t−1 — векторы, везде ниже умножение означает покоординатное умножение Generic Adaptive learning rate с scheduler S(α,t) S(α t ,t) v t − S(α t−1 ,t) v t−1 S(α t ,t) Например, в Adam: 1−β t →S(α,t) 1−β t Adaptive learning rate: AdaGrad s=1 s=1 s=1 =O( t 1 ) Adaptive learning rate: RMSprop =βv t−1 +(1−β)v То же самое =O(1), ломается у критических точек Adaptive learning rate: Online RMSprop t−1 +(1−β =1− t γ То же самое =O( t 1 ) Adaptive learning rate: Adam =βv t−1 +(1−β)v 1−β t То же самое =O(1), ломается у критических точек Adaptive learning rate: AMSgrad =βv t−1 +(1−β)v То же самое =O(1) Adaptive learning rate: RAdam Многобукв Многобукв =O(1) Классическая L 2 L 2 регуляризация 2,t 2,t =λ 2 Decoupled L 2 L 2 регуляризация 2,t = σ 0:t 2,t = η t λ 2 Инкрементальная L 1 L 1 регуляризация 1,t =tλ 1,t =λ 1 Фиксированная L 1 L 1 регуляризация 1,t =λ 1 Отсутствует Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 15.3. Регуляризация в онлайн-обучении Следующий параграф 16.1. Матричное дифференцирование Как дифференцировать матрицы и дифференцировать по матрицам: всё, что вам не рассказали про дифференцирование на матанализе",
    "metadata": {
      "title": "Методы оптимизации в Deep Learning",
      "url": "https://education.yandex.ru/handbook/ml/article/metody-optimizacii-v-deep-learning",
      "course": "ml",
      "chapter": "15. Онлайн-обучение и стохастическая оптимизация",
      "chapter_id": "15.4",
      "part": 16,
      "total_parts": 16,
      "source_file": "15.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как дифференцировать матрицы и дифференцировать по матрицам: всё, что вам не рассказали про дифференцирование на матанализе Любая задача машинного обучения — это задача оптимизации, а задачи оптимизации удобнее всего решать градиентными методами (если это возможно, конечно). Поэтому важно уметь находить производные всего, что попадается под руку. Казалось бы, в чём проблема: ведь дифференцирование — простая и понятная штука (чего не скажешь, например, об интегрировании). Зачем же как-то специально учиться дифференцировать матрицы? Да в принципе-то никаких проблем: в этом параграфе вы не узнаете никаких секретных приёмов или впечатляющих теорем. Но, согласитесь, если исходная функция от вектора x x имела вид f(x)=∣∣Ax−b∣∣ 2 (где A A — константная матрица, а b b — постоянный вектор), то хотелось бы уметь и производную выражать красиво и цельно через буквы A A, x x и b b, не привлекая отдельные координаты . Это не только эстетически приятно, но и благотворно сказывается на производительности наших вычислений: ведь матричные операции обычно очень эффективно оптимизированы в библиотеках, чего не скажешь о самописных циклах по i,j,k,s. И всё, что будет происходить дальше, преследует очень простую цель: научиться вычислять производные в удобном, векторно-матричном виде. А чтобы сделать это и не сойти с ума, мы должны ввести ясную систему обозначений, составляющую ядро техники матричного дифференцирования. Основные обозначения Вспомним определение производной для функции f:R m →R n . Функция f ( x ) f(x) дифференцируема в точке x 0 x 0 , если f(x 0 +h)=f(x 0 )+[D x 0 f](h)+ o ˉ ˉ (∣∣h∣∣), где f] — дифференциал функции f f: линейное отображение из мира x x-ов в мир значений f f. Грубо говоря, он превращает «малое приращение h = Δ x h=Δx» в «малое приращение Δ f Δf» («малые» в том смысле, что на о-малое можно плюнуть): f(x 0 +h)−f(x 0 )≈[D x 0 f](h) Отметим, что дифференциал зависит от точки x 0 x 0 , в которой он берётся: f](h). Под ∣ ∣ h ∣ ∣ ∣∣h∣∣ подразумевается норма вектора h h, например корень из суммы квадратов координат (обычная евклидова длина). Давайте рассмотрим несколько примеров и заодно разберёмся, какой вид может принимать выражение f](h) в зависимости от формы x x. Начнём со случаев, когда f f — скалярная функция. В примерах выше нам дважды пришлось столкнуться с давним знакомцем из матанализа: градиентом скалярной функции (у нескалярных функций градиента не бывает). Напомним, что градиент f функции в точке x 0 x 0 состоит из частных производных этой функции по всем координатам аргумента. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если x x — вектор-строка, то и градиент записывается вектор-строкой, а если x x — матрица, то и градиент тоже будет матрицей того же размера. Это важно, потому что для осуществления градиентного спуска мы должны уметь прибавлять градиент к точке, в которой он посчитан. Как мы уже имели возможность убедиться, для градиента скалярной функции f f выполнено равенство f](x−x 0 )=⟨∇ x 0 f,x−x 0 ⟩, где скалярное произведение — это сумма попарных произведений соответствующих координат (да-да, самое обыкновенное). Посмотрим теперь, как выглядит дифференцирование для",
    "metadata": {
      "title": "Матричное дифференцирование",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.1",
      "part": 1,
      "total_parts": 4,
      "source_file": "16.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "координатам аргумента. При этом его обычно упаковывают в ту же форму, что и сам аргумент: если x x — вектор-строка, то и градиент записывается вектор-строкой, а если x x — матрица, то и градиент тоже будет матрицей того же размера. Это важно, потому что для осуществления градиентного спуска мы должны уметь прибавлять градиент к точке, в которой он посчитан. Как мы уже имели возможность убедиться, для градиента скалярной функции f f выполнено равенство f](x−x 0 )=⟨∇ x 0 f,x−x 0 ⟩, где скалярное произведение — это сумма попарных произведений соответствующих координат (да-да, самое обыкновенное). Посмотрим теперь, как выглядит дифференцирование для функций, которые на выходе выдают не скаляр, а что-то более сложное. Простые примеры и свойства матричного дифференцирования Производная константы. Пусть f(x)=a. Тогда f(x 0 +h)−f(x 0 )=0, то есть f] — это нулевое отображение. А если f f — скалярная функция, то и f=0. Производная линейного отображения. Пусть f ( x ) f(x) — линейное отображение. Тогда f(x 0 +h)−f(x 0 )=f(x 0 )+f(h)−f(x 0 )=f(h) Поскольку справа линейное отображение, то по определению оно и является дифференциалом f]. Мы уже видели примеры таких ситуаций выше, когда рассматривали отображения умножения на матрицу слева или справа. Если f f — (скалярная) линейная функция, то она представляется в виде ⟨ a , v ⟩ ⟨a,v⟩ для некоторого вектора a a — он и будет градиентом f f. Линейность производной. Пусть f(x)=λu(x)+μv(x), где λ , μ λ,μ — скаляры, а u , v u,v — некоторые отображения, тогда f]=λ[D x 0 u]+μ[D x 0 v] Производная произведения. Пусть f(x)=u(x)v(x), где u , v u,v — некоторые отображения, тогда f]=[D x 0 u]⋅v(x 0 )+u(x 0 )⋅[D x 0 v] Это же правило сработает и для скалярного произведения: ⟨u,v⟩]=⟨[D x 0 u],v⟩+⟨u,[D x 0 v]⟩ В этом нетрудно убедиться, повторив доказательство или заметив, что в доказательстве мы пользовались лишь дистрибутивностью (= билинейностью) умножения. Производная сложной функции. Пусть f(x)=u(v(x)). Тогда f(x 0 +h)−f(x 0 )=u(v(x 0 +h))−u(v(x 0 ))≈ ≈[D v(x 0 ) u](v(x 0 +h)−v(x 0 ))≈[D v(x 0 ) u]([D x 0 v](h)) Здесь v(x 0 ) u — дифференциал u u в точке v ( x 0 ) v(x 0 ), а v(x 0 ) u](…) — это применение отображения v(x 0 ) u] к тому, что в скобках. Итого получаем: u∘v](h)=[D v(x 0 ) u]([D x 0 v](h)) Важный частный случай: дифференцирование перестановочно с линейным отображением. Пусть f(x)=L(v(x)), где L L — линейное отображение. Тогда v(x 0 ) L] совпадает с самим L L и формула упрощается: L∘v](h)=L([D x 0 v](h)) Простые примеры вычисления производной Вычислим дифференциал и градиент функции f(x)=⟨a,x⟩, где x x — вектор-столбец, a a — постоянный вектор. Вычислим производную и градиент f(x)=⟨Ax,x⟩, где x x — вектор-столбец, A A — постоянная матрица. Вычислим производную обратной матрицы: f(X)=X −1 , где X X — квадратная матрица. Вычислим градиент определителя: f ( X ) = det ( X ) f(X)=det(X), где X X — квадратная матрица. Вычислим градиент функции f(x)=∣∣Ax−b∣∣ 2 . С этой функцией мы ещё встретимся, когда будем",
    "metadata": {
      "title": "Матричное дифференцирование",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.1",
      "part": 2,
      "total_parts": 4,
      "source_file": "16.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "линейное отображение. Тогда v(x 0 ) L] совпадает с самим L L и формула упрощается: L∘v](h)=L([D x 0 v](h)) Простые примеры вычисления производной Вычислим дифференциал и градиент функции f(x)=⟨a,x⟩, где x x — вектор-столбец, a a — постоянный вектор. Вычислим производную и градиент f(x)=⟨Ax,x⟩, где x x — вектор-столбец, A A — постоянная матрица. Вычислим производную обратной матрицы: f(X)=X −1 , где X X — квадратная матрица. Вычислим градиент определителя: f ( X ) = det ( X ) f(X)=det(X), где X X — квадратная матрица. Вычислим градиент функции f(x)=∣∣Ax−b∣∣ 2 . С этой функцией мы ещё встретимся, когда будем обсуждать задачу линейной регрессии. Примеры вычисления производных сложных функций Вычислим градиент функции f ( X ) = log ( det ( X ) ) f(X)=log(det(X)). Вычислим градиент функции f(X)=tr(AX T X). Вычислим градиент функции f ( X ) = det f(X)=det(AX −1 B). Вторая производная Рассмотрим теперь не первые два, а первые три члена ряда Тейлора: f(x 0 +h)=f(x 0 )+[D x 0 f](h)+ f](h,h)+ o ˉ ˉ (∣∣h∣∣ 2 ), где f](h,h) — второй дифференциал, квадратичная форма, в которую мы объединили все члены второй степени. Вопрос на подумать. Докажите, что второй дифференциал является дифференциалом первого, то есть f](h 1 )](h 2 )=[D x 0 2 f](h 1 ,h 2 ) Зависит ли выражение справа от порядка Этот факт позволяет вычислять второй дифференциал не с помощью приращений, а повторным дифференцированием производной. Вторая производная может оказаться полезной при реализации методов второго порядка или же для проверки того, является ли критическая точка (то есть точка, в которой градиент обращается в ноль) точкой минимума или точкой максимума. Напомним, что квадратичная форма q ( h ) q(h) называется положительно определённой (соответственно, отрицательно определённой), если q(h)⩾0 (соответственно, q(h)⩽0) для всех h h, причём q(h)=0 только при h = 0 h=0. Теорема. Пусть функция f:R m →R имеет непрерывные частные производные второго порядка в окрестности точки x 0 x 0 , причём f=0. Тогда точка x 0 x 0 является точкой минимума функции, если квадратичная форма f положительно определена, и точкой максимума, если она отрицательно определена. Если мы смогли записать матрицу квадратичной формы второго дифференциала, то мы можем проверить её на положительную или отрицательную определённость с помощью критерия Сильвестра. Примеры вычисления и использования второй производной Рассмотрим задачу минимизации f(x)=∣∣Ax−b∣∣ 2 по переменной x x, где A A — матрица с линейно независимыми столбцами. Выше мы уже нашли градиент этой функции; он был равен f=2A T (Ax−b). Мы можем заподозрить, что минимум достигается в точке, где градиент обращается в ноль: =(A T A) −1 A T b. Отметим, что обратная матрица существует, так как rk(A T A)=rkA, а столбцы A A по условию линейно независимы и, следовательно, rk(A T A) равен размеру этой матрицы. Но действительно ли эта точка является точкой минимума? Давайте оставим в стороне другие соображения (например, геометрические, о которых мы упомянем в параграфе про линейные модели) и проверим аналитически. Для этого мы должны вычислить второй дифференциал функции f(x)=∣∣Ax−b∣∣ 2 . Мы нашли квадратичную форму второго дифференциала; она, оказывается, не зависит от точки (впрочем, логично:",
    "metadata": {
      "title": "Матричное дифференцирование",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.1",
      "part": 3,
      "total_parts": 4,
      "source_file": "16.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Мы можем заподозрить, что минимум достигается в точке, где градиент обращается в ноль: =(A T A) −1 A T b. Отметим, что обратная матрица существует, так как rk(A T A)=rkA, а столбцы A A по условию линейно независимы и, следовательно, rk(A T A) равен размеру этой матрицы. Но действительно ли эта точка является точкой минимума? Давайте оставим в стороне другие соображения (например, геометрические, о которых мы упомянем в параграфе про линейные модели) и проверим аналитически. Для этого мы должны вычислить второй дифференциал функции f(x)=∣∣Ax−b∣∣ 2 . Мы нашли квадратичную форму второго дифференциала; она, оказывается, не зависит от точки (впрочем, логично: исходная функция была второй степени по x x, так что вторая производная должна быть константой). Чтобы показать, что x ∗ x ∗ действительно является точкой минимума, достаточно проверить, что эта квадратичная форма положительно определена. Докажем, что функция f ( X ) = log ⁡ det ( X ) f(X)=logdet(X) является выпуклой вверх на множестве симметричных, положительно определённых матриц. Для этого мы должны проверить, что в любой точке квадратичная форма её дифференциала отрицательно определена. Для начала вычислим эту квадратичную форму. Чтобы доказать требуемое в условии, мы должны проверить следующее: что для любой симметричной матрицы X 0 X 0 и для любого симметричного (чтобы не выйти из пространства симметричных матриц) приращения H ≠ 0 H  =0 имеем [ D X 0 2 log ⁡ det logdet(X)](H,H)<0 Покажем это явно. Так как X 0 X 0 — симметричная, положительно определённая матрица, у неё есть симметричный и положительно определённый квадратный корень: 1/2 ⋅X 0 1/2 =X 0 1/2 ⋅(X 0 1/2 ) T . Тогда ⟨−X 0 −1 HX 0 −1 ,H⟩=−tr(X 0 1/2 (X 0 1/2 ) T HX 0 1/2 (X 0 1/2 −tr((X 0 1/2 ) T HX 0 1/2 (X 0 1/2 1/2 =−tr((X 0 1/2 ) T HX 0 1/2 [(X 0 1/2 ) T HX 0 1/2 =−∣∣(X 0 1/2 ) T HX 0 1/2 ∣∣ 2 , что, конечно, меньше нуля для любой ненулевой H H. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 15.4. Методы оптимизации в Deep Learning Следующий параграф 16.2. Матричная факторизация",
    "metadata": {
      "title": "Матричное дифференцирование",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnoe-differencirovanie",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.1",
      "part": 4,
      "total_parts": 4,
      "source_file": "16.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Если наш датасет таков, что все признаки целочисленные и вещественные, нет пропущенных значений и других приятных сюрпризов, то матрица объекты-признаки — это просто матрица, к которой можно пробовать применять инструменты из линейной алгебры, а среди таковых весьма полезными оказываются матричные разложения, то есть различные способы представить матрицу в виде произведения двух или более матриц, обычно специального вида. Такие разновидности, как LU-разложение, QR-разложение, разложение Холецкого вы несомненно встретите, если откроете код любой библиотеки численной линейной алгебры, но суждено ли матричным разложениям играть роль только лишь винтиков и шестерёнок, запрятанных внутри инструментов машинного обучения, или какие-то из них и сами по себе могут помочь вам анализировать данные? На этот вопрос мы попробуем ответить в данном разделе. Но прежде, чем переходить к конкретным методам, мы разберёмся, к каким моделям данных можно прийти, разложив матрицу в произведение. Итак, я разложил матрицу в произведения — и что же? Предположим, что нашу матрицу объекты-признаки X X мы представили в виде произведения (или, более общно, приблизили в каком-либо смысле таким произведением): X⁡N×D ∼ B⁡N×R ⋅ C⁡R×D N×D X ∼ N×R B ⋅ R×D C где внизу указаны размеры матриц (то есть в нашем датасете N N объектов и D D признаков). Что это может означать? Смесь признаков Мы считаем, что каждый из D D признаков нашего исходного датасета — это смесь (то есть линейная комбинация) R R скрытых (латентных) признаков: Decomp1 По сути это одна из самых простых моделей с латентными переменными, в которой исходные признаки выражаются через латентные линейным образом. Если R < D R<D, то мы получаем приближённое описание нашего датасета с помощью меньшего количества признаков. На уровне объектов каждый объект D-мерная строка) приобретает латентное представление R-мерная строка), с которой он связан соотношением C. Мы можем представлять, что наши объекты x i x i представляют из себя не D D-мерное облако, а лежат на некоторой R R-мерной плоскости; переходя к R R-мерным представлениям z i z i , мы обнажаем эту структуру. Точность аппроксимации можно измерять по-разному; наиболее популярной (в силу вычислительной простоты) является норма Фробениуса ∣A∣∗fro 2 =∑∗A ij 2 =tr(A T A) — соответствующую модель называют анализом главных компонент, или PCA (Principal Component Analysis). Понижение размерности признакового пространства Мы можем захотеть описать наш датасет меньшим чем D D количеством признаков (а может быть, и вообще каким-то весьма маленьким). У нас может быть несколько причин для этого, например: Признаков очень много, и мы боимся, что обучение на них будет занимать очень много времени или что в процессе обучения нам потребуется слишком много оперативной памяти; Мы считаем, что в данных есть шум или что часть признаков связаны соотношением приближённой линейной зависимости — иными словами, мы уверены, что значительную часть информации можно закодировать меньшим числом признаков Мы уже обсуждали, что это можно получить, построив приближённое разложение: X⁡N×D ∼ B⁡N×T ⋅ C⁡T×D N×D X ∼ N×T B ⋅ T×D C Математика помогает. Матрица имеет ранг T T тогда и только тогда, когда она представляется в виде B⁡NtimesS ⋅ C⁡S×D NtimesS B ⋅ S×D C для S = T S=T и не представляется в таком виде для меньших",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 1,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "что в процессе обучения нам потребуется слишком много оперативной памяти; Мы считаем, что в данных есть шум или что часть признаков связаны соотношением приближённой линейной зависимости — иными словами, мы уверены, что значительную часть информации можно закодировать меньшим числом признаков Мы уже обсуждали, что это можно получить, построив приближённое разложение: X⁡N×D ∼ B⁡N×T ⋅ C⁡T×D N×D X ∼ N×T B ⋅ T×D C Математика помогает. Матрица имеет ранг T T тогда и только тогда, когда она представляется в виде B⁡NtimesS ⋅ C⁡S×D NtimesS B ⋅ S×D C для S = T S=T и не представляется в таком виде для меньших S S. Доказывать это мы не будем, но подметим, что приблизить датасет линейной смесью T T признаков — это то же самое, что приблизить матрицу X X матрицей X ^ X ^ ранга T T . Качество приближения. Нам, конечно же, хочется, чтобы приближение было наилучшим — скажем, в том смысле, чтобы разность X − B C X−BC была минимальной в каком-либо смысле. Можно предложить много разных метрик; остановимся на двух: Норма Фробениуса. Представим, что матрица A=(X−BC) — это просто вектор из N × D N×D чисел, который зачем-то записали в виде прямоугольной таблицы. Тогда его норму можно записать в виде ∥A∥ fro = i,j ∑ a ij 2 = tr(A T A) Эту норму (а точнее, её квадрат) легко оптимизировать. Операторная l 2 l 2 -норма. Вычислять её тяжко, а уж оптимизировать вообще непонятно как, зато звучит круто. Идея в том, что отображения можно сравнивать в зависимости от того, как оно действует на векторы: чем больше оно умеет удлинять векторы — тем оно «больше»: ∥ A ∥ 2 = sup ∥A∥ 2 =sup{ ∣v∣ ∣Av∣ ∣v∈R D } Поскольку ∣λv∣ ∣A(λv)∣ = ∣v∣ ∣Av∣ , достаточно брать супремум только по векторам единичной длины, то есть по единичной сфере. Так как это компакт, непрерывная функция v↦∣Av∣ достигает на нём своего максимального значения, то есть мы можем переписать ∥ A ∥ 2 = sup ∥A∥ 2 =sup{∣Av∣∣v∈R D ,∣v∣=1} Смесь объектов Мы считаем, что каждый из N N объектов нашего исходного датасета — смесь (то есть линейная комбинация) R R скрытых объектов: Decomp2 Такая интерпретация может быть полезна, например, в ситуации, когда объекты — это записи с каждого из нескольких микрофонов в помещении, признаки — фреймы, а скрытые объекты — это голоса отдельных людей. Также данную модель можно интерпретировать как что-то вроде поиска типичных объектов. Отдельные представления для объектов и признаков Эту интерпретацию лучше всего пояснить на примере. Пусть объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом ( i , j ) (i,j) записана единица, если пользователь интересовался товаром, и ноль — если нет (или, в более общей ситуации, рейтинги, которые пользователи ставят товарам). Decomp3 При перемножении матриц B B и C C на ( i , j ) (i,j)-м месте произведении стоит скалярное произведение i i-й строки B B и j j-го столбца C C. Таким образом, степень релевантности товара пользователю моделируется скалярным произведением (напрашивается сравнение с косинусным расстоянием) вектора, представляющего i i-го",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 2,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Эту интерпретацию лучше всего пояснить на примере. Пусть объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом ( i , j ) (i,j) записана единица, если пользователь интересовался товаром, и ноль — если нет (или, в более общей ситуации, рейтинги, которые пользователи ставят товарам). Decomp3 При перемножении матриц B B и C C на ( i , j ) (i,j)-м месте произведении стоит скалярное произведение i i-й строки B B и j j-го столбца C C. Таким образом, степень релевантности товара пользователю моделируется скалярным произведением (напрашивается сравнение с косинусным расстоянием) вектора, представляющего i i-го пользователя, и вектора, представляющего j j-й товар. i-ый пользователь , j-ый товар +…+b ik c kj = =(i-ый пользователь,j-ый товар) Заметим ещё, что R R координат вектора, ответчающего пользователю, равно как и R R координат вектора, отвечающего товару, можно рассматривать как R R латентных признаков, которые в идеальном мире являются интерпретируемыми и характеризуют «сродство» пользователя и товара с некоторым аспектом бытия: Decomp6 Матрицы в разложении: физический смысл Но матрицы в разложении обычно не абы какие — так какие из разновидностей могут быть полезны? Во всех известных вам матричных разложениях к отдельным сомножителям предъявляются определённые требования: симметричность, треугольность, ортогональность — некоторые из них (скажем, симметричность) не имеют физического смысла ни в одной из указанных выше интерпретаций. Но одно оказывается полезным. Ковариация и дисперсия признаков Для начала — и это важно — предположим, что матрица X X центрирована по столбцам, то есть среднее в каждом из столбцов (= признаков) равно нулю (если это не так, то вычтем из каждого столбца его среднее). Теперь матрица ковариации признаков может быть с точностью до константы оценена как matrix И мы видим: i i-й и j j-й столбцы матрицы X X ортогональны тогда и только тогда, когда соответствующие признаки не коррелированы. При этом ( i , i ) (i,i)-й диагональный элемент матрицы X T X X T X — это дисперсия i i-го признака. Вывод: матрица, ортонормированная по столбцам, отвечает датасету, в котором признаки не коррелированы и имеют единичную дисперсию Сингулярное разложение С помощью сингулярного разложения можно перейти от D D исходных признаков к потенциально небольшому количеству «самых важных» , по-быстрому визуализовать данные или построить простенькую рекомендательную систему. Конечно, глубинные автоэнкодеры, TSNE или DSSM справятся с этим гораздо лучше, но если данных относительно немного или если хочется что-нибудь быстро попробовать «на коленке», старое доброе сингулярное разложение всегда подставит плечо. Математическое определение Сингулярным разложением матрицы X X называется разложение X=UΣV T , где U U и V V — матрицы, ортонормированные по столбцам, а Σ=diag(σ 1 ,σ 2 ,…) — диагональная матрица, у которой ⩾…⩾σ R >σ R+1 =0. Числа σ i σ i называются сингулярными числами сингулярными числами, а столбцы U U и V V — левыми левыми и правыми правыми сингулярными векторами соответственно (их алгебраический смысл станет ясен чуть ниже). Сингулярное разложение можно записать в полном или в усечённом виде: SVD Пара предостережений по поводу ортогональности по столбцам: U U ортогональна по столбцам U=E (элементы U T U U T U — скалярные произведения столбцов",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 3,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "называется разложение X=UΣV T , где U U и V V — матрицы, ортонормированные по столбцам, а Σ=diag(σ 1 ,σ 2 ,…) — диагональная матрица, у которой ⩾…⩾σ R >σ R+1 =0. Числа σ i σ i называются сингулярными числами сингулярными числами, а столбцы U U и V V — левыми левыми и правыми правыми сингулярными векторами соответственно (их алгебраический смысл станет ясен чуть ниже). Сингулярное разложение можно записать в полном или в усечённом виде: SVD Пара предостережений по поводу ортогональности по столбцам: U U ортогональна по столбцам U=E (элементы U T U U T U — скалярные произведения столбцов =E (не обязательно равно; элементы U U T UU T — скалярные произведения строк U U) Ясно, что хранить полное разложение нет смысла: ведь бесполезные, умножающиеся на нули, блоки будут лишь занимать память. По-английски сингулярное разложение называется SVD (singular value decomposition), и мы будем активно использовать эту аббревиатуру. Если вы не любите математику, можете пропустить. С точки зрения математики сингулярное разложение говорит следующее. Пусть X X — матрица линейного отображения φ:R D ⟶R N . Тогда найдётся ортонормированый базис ,…,v D в пространстве R D R D и ортонормированый базис ,…,u N в пространстве R N R N , в которых действие оператора записывается следующим образом: φ(v 1 ) ⋮ φ(v R ) φ(v R+1 ) ⋮ φ(v =0, =0 (знатоки функционального анализа могут узнать в этом частный случай теоремы Гильберта-Шмидта). Сингулярное разложение и операторная l2-норма. Можно показать, что ∥∥A∥∥ 2 , эта самая операторная l2-норма матрицы, равна — квадрату наибольшего сингулярного числа. Сингулярное разложение и норма Фробениуса. Можно показать, что ∣∣A∣∣ fro Есть и более тонкие, хотя и весьма частные, ситуации. Можете ли вы, например, указать несколько различных сингулярных разложений матрицы E E? Да-да, для неё сингулярное разложение максимально неоднозначно. Можете ли вы теперь придумать не скалярную матрицу, у которой были бы различные SVD, отличающиеся не только знаками столбцов матриц U U и V V? Теоретико-вероятностная интерпретация SVD Если X=UΣV T (рассмотрим сейчас не усечённое, а полное разложение, в котором матрицы U U и V V квадратные ортогональные), то X=VΣ =VΣ T ΣV T Отметим, что в рассматриваемой ситуации Σ Σ не обязательно квадратная, и поэтому нельзя написать, что Σ=Σ 2 ; тем не менее, Σ T Σ Σ T Σ — это квадратная матрица с числами ,… на диагонали. Как бы то ни было, в (ортогональном!) базисе из (ортогональных!) столбцов V V матрица X T X X T X приводится к диагональному виду с числами ,… на диагонали. Теперь представим, что наши объекты ,…,x N выбраны из D D-мерного нормального распределения p(x i )= (2π) D/2 ∣C∣ 1/2 −μ)C −1 (x i −μ) T где μ μ — вектор средних, а C C — матрица ковариации. Это, в частности, значит, что облако точек представляет из себя нечто вроде эллипсоида в D D-мерном пространстве с центром μ μ. Предположим, что μ = 0 μ=0 (все признаки центрированы); тогда оценкой матрицы ковариации признаков является матрица X. Допустим, что эта оценка точная, тогда разложение X=VΣ T ΣV T даёт нам аналогичное",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 4,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "X приводится к диагональному виду с числами ,… на диагонали. Теперь представим, что наши объекты ,…,x N выбраны из D D-мерного нормального распределения p(x i )= (2π) D/2 ∣C∣ 1/2 −μ)C −1 (x i −μ) T где μ μ — вектор средних, а C C — матрица ковариации. Это, в частности, значит, что облако точек представляет из себя нечто вроде эллипсоида в D D-мерном пространстве с центром μ μ. Предположим, что μ = 0 μ=0 (все признаки центрированы); тогда оценкой матрицы ковариации признаков является матрица X. Допустим, что эта оценка точная, тогда разложение X=VΣ T ΣV T даёт нам аналогичное разложение C=V( n 1 Σ T Σ)V T . Теперь замена координат x = z V T x=zV T (с матрицей замены V V — то есть переход происходит в базис из столбцов матрицы V V) даёт нам exp p(x i )=const⋅exp(− Обратите внимание, что стоят в формуле на непривычных местах, как будто их перепутали, но нет — просто x i x i у нас является строкой, а не столбцом. exp p(z)=const⋅exp(− (nΣ exp exp =const⋅exp(− )=const⋅exp(− +…))= =p(x i1 ′ )⋅…⋅p(x iD ′ ) Итак, если наши данные взяты из многомерного нормального распределения, после перехода к базису из столбцов V V новые координаты становятся независимыми; вместе с тем это соответствует переходу к главным осям ковариационной матрицы — и геометрически столбцы V V соответствуют главным осям эллипсоида-облака точек. SVD Использование SVD: латентные признаки Запишем X⁡N×D = UΣ⁡N×R ⋅ VT⁡R×D N×D X = N×R UΣ ⋅ R×D V T и вспомним самую первую интерпретацию матричного разложения. matrix Столбцы U Σ UΣ ортогональны (так как они пропорциональны столбцам ортогональной по столбцам матрицы U U) — то есть латентные признаки не коррелированы. При этом, поскольку длина каждого из столбцов U U равна 1, длины столбцов U Σ UΣ порпорциональны σ i σ i — а, значит, латентные признаки упорядочены по невозрастанию дисперсии (ведь с точностью до константы оценка дисперсии признака — это квадрат длины вектора его значений). Заметим, что перед применением SVD признаки лучше центрировать, иначе первая компонента будет указывать в сторону центра масс облака точек (зачем нам это?), а остальные вынуждены будут ей быть ортогональны: SVD Понижение размерности признакового пространства Мы уже обсуждали, что это можно получить, построив приближение ранга T T или, что то же самое, приближённое разложение X⁡N×D ∼ B⁡N×T ⋅ C⁡T×D N×D X ∼ N×T B ⋅ T×D C для некоторого и желательно небольшого T T. И тут SVD приходится более чем кстати. Теорема Эккарта-Янга Наилучшее по норме Фробениуса приближение ранга T T — это SVD Таким образом, если вы хотите получить T T «самых важных» признаков, то вы можете использовать SVD. Но что это за признаки? Что именно означают эти слова «самые важные»? Давайте обратимся к геометрии, которая, как мы помним, тесно связана с теорией вероятностей: SVD SVD Если применить SVD к датасету, изображённому на последней картинке, и взять два первых латентных признака, то эллипсоид превратится в эллипс; меньшая из полуосей, похожая на шум, будет забыта, останется две бOльших. Видим: самое важное для SVD — это самое масштабное.",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 5,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "приходится более чем кстати. Теорема Эккарта-Янга Наилучшее по норме Фробениуса приближение ранга T T — это SVD Таким образом, если вы хотите получить T T «самых важных» признаков, то вы можете использовать SVD. Но что это за признаки? Что именно означают эти слова «самые важные»? Давайте обратимся к геометрии, которая, как мы помним, тесно связана с теорией вероятностей: SVD SVD Если применить SVD к датасету, изображённому на последней картинке, и взять два первых латентных признака, то эллипсоид превратится в эллипс; меньшая из полуосей, похожая на шум, будет забыта, останется две бOльших. Видим: самое важное для SVD — это самое масштабное. А правда ли у нас получится хорошее приближение с помощью T T новых признаков? Посчитаем норму разности. Везде ниже U U и V V — квадратные ортогональные матрицы; в частности Σ Σ не обязательно квадратная матрица размера N × D N×D. SVD ∣∣Δ∣∣ fro 2 =tr((U )=tr(V =tr(V Σ T Σ V T )=tr( )=∣∣ Σ ∣∣ fro 2 =σ T+1 2 +…+σ R 2 Аналогичным образом ∣∣Δ∣∣ 2 =σ T+1 потому что умножение на ортогональную матрицу не меняет операторную l 2 l 2 -норму. Таким образом, если сингулярные значения убывают достаточно медленно (например, линейно), то мы вряд ли сможем приблизить исходную матрицу матрицей маленького ранга с очень хорошей точностью. Как избавиться от иллюзий. Сгенерируйте матрицу 100 × 100 100×100 с помощью np.random.rand или np.random.randn. Для какого T T вы сможете найти матрицу ранга T T, приближающую исходную с относительной точностью К счастью, в реальных датасетах сингулярные значения убывают достаточно быстро или же нам хватает довольно грубого приближения. Переход из исходного признакового пространства в новое и обратно Допустим, мы построили приближённое разложение ранга =U⋅ =:Σ Матрица — это первые T T столбцов матрицы U Σ ′ UΣ ′ , и они же первые T T столбцов матрицы UΣ=X⋅V. Таким образом, для перевода объекта x i x i в новое признаковое пространство нужно произвести ⋅V и взять первые T T столбцов или, что то же самое, ⋅V[:,:T]. Теперь пусть задана вектор-строка z i z i длины T T — латентное представление, соответствующего некоторому объекту, то есть одна из строк матрицы . Тогда точно восстановить исходный x i x i мы не сможем: ведь равенство не точное, но для приближённого восстановления x i x i мы должны произвести ⋅V[:,:T] T . На что не способно сингулярное разложение Сингулярное разложение умеет находить дающие самый существенный вклад в дисперсию линейные комбинации признаков, притом некоррелированные; в случае нормально распределённых данных эти направления оказываются главными осями эллипсоида, которым является облако данных. К сожалению, эта суперспособность SVD столь же охотно превращается в слабость, ведь: Данные не всегда распределены нормально, они могут обладать сложной геометрией, но SVD будет упрямо искать эллипсоид. Самое важное не всегда самое масштабное. Забыть привести признаки к одному масштабу — хороший способ выстрелить себе в ногу при работе с сингулярным разложением. Новые признаки не обязаны быть хорошо интерпретируемыми. Линейная комбинация возраста, стажа работы и зарплаты — это не то, что хотелось бы показывать банковскому регулятору. Выбросы почти наверняка усложнят вам жизнь, хотя, возможно, SVD",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 6,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "в случае нормально распределённых данных эти направления оказываются главными осями эллипсоида, которым является облако данных. К сожалению, эта суперспособность SVD столь же охотно превращается в слабость, ведь: Данные не всегда распределены нормально, они могут обладать сложной геометрией, но SVD будет упрямо искать эллипсоид. Самое важное не всегда самое масштабное. Забыть привести признаки к одному масштабу — хороший способ выстрелить себе в ногу при работе с сингулярным разложением. Новые признаки не обязаны быть хорошо интерпретируемыми. Линейная комбинация возраста, стажа работы и зарплаты — это не то, что хотелось бы показывать банковскому регулятору. Выбросы почти наверняка усложнят вам жизнь, хотя, возможно, SVD поможет вам их увидеть. Практические кейсы MNIST и путешествие по латентному пространству Возьмём большой датасет MNIST, состоящий из чёрно-белых изображений рукописных цифр размера 28 × 28 28×28 пикселей (его можно загрузить, к примеру, отсюда), вытянем каждое из изображений в вектор, получив тем самым матрицу размера 60000 60000×(28⋅28), и применим к этой матрице SVD. Теперь возьмём первые два латентных признака (то есть первые два столбца матрицы U Σ UΣ) — получается, что каждая рукописная цифра у нас теперь кодируется вектором из двух чисел. Нарисуем на плоскости точки, соответствующие этим векторам (скажем, по 100 из каждого класса, чтобы хоть что-нибудь было понятно): MNIST Что же мы видим? Единицы и нули оказались особенными, то есть уже первые два латентных признака хорошо их различают, правда, с середине какая-то каша. А почему? Да потому, что мы забыли центрировать данные. Давайте перед применением SVD вычтем из каждого признака (то есть из каждого пикселя) его среднее по всем картинкам, а потом нарисуем всё заново: MNIST Теперь стало получше: например, семёрки, девятки и четвёрки сгуппировались вместе с другой стороны от восьмёрок и троек (собственно говоря, это отражает тот факт, что рукописные написания семёрок, девяток и четвёрок могут быть похожи друг на друга, так и человек не сразу отличит — а вот с тройкой их спутать намного труднее). Заметим ещё вот что. В ( 28 ⋅ 28 ) (28⋅28)-мерном пространстве наборов пикселей совсем не каждая точка соответствует какой-то рукописной цифре — то, что может приходить из реального мира, лежит на некоторой хитрой поверхности в этом пространстве (если выражаться корректнее, то на подмногообразии). Если же мы попробуем нарисовать «изображения», лежащие на отрезке, соединяющем два изображения цифр, то получим нечто не слишком интересное: mnist Одно изображение просто наложилось и затем сменило другое — скучно! Но если мы сделаем то же самое в двумерном пространстве, образованном первыми двумя латентными признаками SVD, то мы будем получать, может быть, не совсем реалистичные изображения цифр, но что-то явно из мира рукописных символов: mnist Химический состав рек Посмотрим на небольшой кусок вот этого датасета, который доступен для скачивания нигде (ха-ха), и попробуем что-нибудь понять про химических состав рек европейского союза, а заодно соберём шишки, которые могут попасться при визуализации с помощью SVD. Конечно, сразу хочется нарисовать все объекты датасета в виде точек на плоскости. Мы знаем, что в этом может помочь SVD — попробуем же! Центрируем признаки — и рисуем: Rivers Ой, что-то пошло не так. Но почему же?! Наверное, надо хотя бы посмотреть на данные...",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 7,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "мы будем получать, может быть, не совсем реалистичные изображения цифр, но что-то явно из мира рукописных символов: mnist Химический состав рек Посмотрим на небольшой кусок вот этого датасета, который доступен для скачивания нигде (ха-ха), и попробуем что-нибудь понять про химических состав рек европейского союза, а заодно соберём шишки, которые могут попасться при визуализации с помощью SVD. Конечно, сразу хочется нарисовать все объекты датасета в виде точек на плоскости. Мы знаем, что в этом может помочь SVD — попробуем же! Центрируем признаки — и рисуем: Rivers Ой, что-то пошло не так. Но почему же?! Наверное, надо хотя бы посмотреть на данные... Объекты имеют вид «GBPKER0059», «GB20227», «LVV0120100» и так далее — это коды станций, измеряющих состав воды; Признаки имеют вид «1985 BOD5», «1985 Chlorophyll a», «1985 Orthophosphates» и так далее — тут указан год измерения и показатель; Посмотрев статистики, убеждаемся, что все показатели неотрицательны (то есть уж точно распределены не нормально — но может, и так сработает); при этом почти все элементы нашей матрицы находятся в пределах 1000, но три значения космически огромны, причём в одном столбце «2008 Total oxidised nitrogen» (а строки соответствуют каким-то греческим станциям, с которыми вообще всё странно), и ещё одно тоже очень большое («2005 Total organic carbon (TOC)») — вот они-то и дали нам четыре точки на графике, отличных от начала координат. Кстати говоря, если космически большие значения, по-видимому, являются результатам поломки, то по поводу четвёртого, не столь злостного, выброса есть подозрение, что это реальные значения. Посмотрев в данные, мы видим, что показатель был измерен на станции Zidlochovice, на реке Srvatka ниже Брно — а, как говорит нам википедия: As a result of water pollution by communal sewage, the reservoir suffered from an extensive amount of cyanobacteria for a long time. Так или иначе, все четыре станции мы уберём, чтобы они не портили нам SVD. Один из признаков «2002 Kjeldahl Nitrogen» принимает только нулевые значения. Уберём его, чтобы не мешался. Почистив выбросы в исходных данных, опять центрируем и рисуем: Rivers Уже лучше. Попробуем понять, что за вещества внесли вклад в первые два латентных признака. Как это сделать? Латентные признаки — это столбцы матрицы U Σ = X V UΣ=XV; линейная алгебра говорит нам, что i i-й столбец произведения X V XV — это линейная комбинация столбцов X X с коэффициентами из i i-го столбца V V. Находим номера самых больших по модулю координат V V — и оказывается, что первые два латентных признака складываются почти сплошь из насыщения воды кислородом, только за разные годы (первый за более старые, второй за чуть более свежие): First latent feature Признак элемент V V 2001 Oxygen saturation 0.27 2002 Oxygen saturation 0.27 2000 Oxygen saturation 0.26 2004 Oxygen saturation 0.26 ⋮ ⋮ ⋮ ⋮ Second latent feature Признак элемент V V 2001 Oxygen saturation -0.62 2002 Oxygen saturation -0.49 2000 Oxygen saturation -0.45 2004 Oxygen saturation -0.26 ⋮ ⋮ ⋮ ⋮ Неужели насыщение кислородом действительно так важно? Нет, просто мы не отмасштабировали признаки. Оказывается, что насыщение кислородом имеет на порядок больший масштаб, чем многое другое, и потому забивает все остальные",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 8,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "из насыщения воды кислородом, только за разные годы (первый за более старые, второй за чуть более свежие): First latent feature Признак элемент V V 2001 Oxygen saturation 0.27 2002 Oxygen saturation 0.27 2000 Oxygen saturation 0.26 2004 Oxygen saturation 0.26 ⋮ ⋮ ⋮ ⋮ Second latent feature Признак элемент V V 2001 Oxygen saturation -0.62 2002 Oxygen saturation -0.49 2000 Oxygen saturation -0.45 2004 Oxygen saturation -0.26 ⋮ ⋮ ⋮ ⋮ Неужели насыщение кислородом действительно так важно? Нет, просто мы не отмасштабировали признаки. Оказывается, что насыщение кислородом имеет на порядок больший масштаб, чем многое другое, и потому забивает все остальные признаки. Тем не менее, мы можем попробовать сделать вывод и из имеющейся картинки. По оси \"у\" что-то не очень интересное, а по оси \"х\" видим большой кластер (напомним, это меньшие значения насыщения воды кислородом в начале 2000-х), содержащий, если проверить, примерно три четверти всех точек, и ещё некоторой размазанный шлейф. Итак, на многих станциях насыщение воды кислородом в начале 2000-х было примерно в одинаковой степени мало — проверив глазами, обнаруживаем, что там просто нули. Поскольку вряд ли это так на самом деле, видимо, стоит сделать вывод, что в первой половине 2000-х насыщение кислородом измерялось из рук вон плохо. Теперь вдобавок к центрированию поделим каждый признак на его стандартное отклонение и снова нарисуем: Rivers Опять видим тесный кластер. При этом первый латентный признак складывается в основном из «Nitrate» , «pH» и «Dissolved oxygen» за разные годы, все с положительными коэффициентами, а второй — из «Total ammonium», «Total phosphorus» и «Kjeldahl Nitrogen» за разные годы, причём с отрицательными коэффициентами. В частности, справа у нас точки с высоким содержанием нитратов и высокой кислотностью. Среди этих точек: Река Тейм, про которую Википедия пишет: The Tame was once one of Britain's dirtiest rivers. Река Кёрёш, про которую тоже можно найти вот такую информацию: For some time the municipal government of Kanjiža (to which the mouth of the river belongs) protests about the extreme pollution of the Kereš's water, as it represents the single largest polluter of the Tisa river Темза (станция немного выше Лондона). Что ещё можно было бы сделать? Например, мы можем посмотреть распределения признаков и увидеть, что многие из них далеки от нормальных и в целом выиграли бы от логарифмирования — тогда, возможно, итоговая картинка стала бы красноречивей. Использование SVD: разделённые представления и рекомендательная система для бедных Мы уже обсуждали, что, вообще говоря, любое матричное разложение можно с той или иной степенью успеха использовать для построения рекомендательной системы. Основанные на этом модели называются моделями латентных факторов (Latent factor models). В 2006 году SVD-подобный алгоритм даже помог Саймону Фанку (Simon Funk; под этим псевдонимом скрывался Brandyn Webb) занять высокое место на соревновании Netflix Prize. Подход на чистом SVD Вернёмся к примеру из пункта 1.3. Пусть вновь объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом ( i , j ) (i,j) записаны рейтинги ρ i j ρ ij , которые пользователи ставят товарам. На основе этих данных мы хотим порекомендовать некоторому n n-му пользователю k k очередных товаров.",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 9,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "использовать для построения рекомендательной системы. Основанные на этом модели называются моделями латентных факторов (Latent factor models). В 2006 году SVD-подобный алгоритм даже помог Саймону Фанку (Simon Funk; под этим псевдонимом скрывался Brandyn Webb) занять высокое место на соревновании Netflix Prize. Подход на чистом SVD Вернёмся к примеру из пункта 1.3. Пусть вновь объекты нашего датасета соответствуют пользователям интернет-магазина, а признаки — товарам, причём в клетке с индексом ( i , j ) (i,j) записаны рейтинги ρ i j ρ ij , которые пользователи ставят товарам. На основе этих данных мы хотим порекомендовать некоторому n n-му пользователю k k очередных товаров. Если бы нам были известны ρ n j ρ nj для всех индексов товаров j j, задача не стоила бы выеденного яйца: мы бы просто взяли k k товаров с максимальными значениями рейтингов. Более того, мы могли бы с помощью матричного разложения построить модель и надеяться, что координаты латентных представлений пользователей и товаров окажутся интерпретируемыми (нет). Decomp31 А именно, если бы мы знали все ρ n j ρ nj , построить отдельные представления для пользователей и для товаров некоторой (подбираемой; это гиперпараметр модели) длины T T мы могли бы с помощью SVD и приближения из теоремы Эккарта-Янга: Но на деле матрица i,j обычно разреженная: в ней лишь сравнительно немного известных рейтингов, а в остальных ячейках стоят пропуски. Что же делать? Наивный вариант — заменить все пропуски нулями (то есть положить, что если пользователь не ставил рейтинг товару, то он ему вдребезги не интересен, что не всегда правдоподобно) или средними по строке/столбцу, после чего сделать SVD и радоваться жизни. В этой ситуации наша приближённая модель предсказывает рейтинг, выставленный i i-м пользователем j j-му товару, как скалярное произведение представлений пользователя и товара — то есть i i-й строки матрицы j-го столбца матрицы Теперь чтобы порекомендовать n-му пользователю k очередных товаров, мы просто берём n-ю строку матрицы X ^ X и находим номера её наибольших элементов. К сожалению, у этого метода есть как минимум две проблемы: Пропусков обычно очень много; если их все заменить какими попало значениями, оценка будет очень шумной; При таком подходе нет простого способа обновить рекомендации при добавлении новых данных — SVD придётся переучивать заново. Развиваем идею: как побороть разреженность К счастью, есть и другой путь. Давайте подумаем: чего вообще мы требуем от матриц B:= ? По сути нам нужны две вещи: B⋅C T ∼X; Обе матрицы ортогональны по столбцам. Последнее можно опустить. Ясной пользы для рекомендательной системы от этого нет; да, это давало бы нам некоррелированность латентных признаков, но мы уже видели, что интерпретируемости это не влечёт. Первое же условие удобно сформулировать в терминах векторов латентных представлений пользователей (обозначим их b i b i ; это строки B B) и товаров (обозначим их c i c i — это строки C C). А именно, нам нужно, чтобы скалярное произведение ) было как можно ближе к ρ i j ρ ij для всех пар ( i , j ) (i,j), для которых ρ i j ρ ij нам известно. Вот именно! Мы можем просто не обращать внимания на неизвестные значения,",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 10,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нет; да, это давало бы нам некоррелированность латентных признаков, но мы уже видели, что интерпретируемости это не влечёт. Первое же условие удобно сформулировать в терминах векторов латентных представлений пользователей (обозначим их b i b i ; это строки B B) и товаров (обозначим их c i c i — это строки C C). А именно, нам нужно, чтобы скалярное произведение ) было как можно ближе к ρ i j ρ ij для всех пар ( i , j ) (i,j), для которых ρ i j ρ ij нам известно. Вот именно! Мы можем просто не обращать внимания на неизвестные значения, оптимизируя только по тем клеткам X X, для которых нам что-то известно: min i,j:,ρ ij  =NA ∑ (ρ ij −(b min Но как решить эту оптимизационную задачу? Разумеется, с помощью стохастического градиентного спуска. В базовом варианте мы случайным образом перебираем пары ( i , j ) (i,j), для которых ρ i j ρ ij нам известно, и обновляем координаты векторов следующим образом: где :=b it +ηε ij c jt c jt :=c jt +ηε ij b it ,t=1,…,T,где ε ij =ρ ij −(b i ,c j ) где η η — гиперпараметр, отвечающий за темп обучения. Приятное свойство такого подхода: в нём легко добавлять новые товары/пользователей (дообучаем их векторы, заморозив остальные), а также новые оценки ρ i j ρ ij (добавляем в оптимизируемый функционал и проводим дооптимизацию). Отметим, что в ходе оптимизации мы попеременно осуществляем градиентный спуск, обновляя то B B, то C C. Эту идею можно развить следующим образом. Заметим, что при фиксированной матрице C C задача минимизации по B B выражения min i,j:,ρ ij  =NA ∑ (ρ ij −(b min превращается по сути в обычный метод наименьших квадратов, для которого можно даже выписать «точное» решение (а вы можете это сделать?). Точно так же и при фиксированном B B легко находится минимум по C C. Чередуя эти два шага, мы будем сходиться к решению быстрее и надёжнее, чем с помощью SGD. Данный алгоритм носит название Alternating Least Squares (ALS). Развиваем идею: как ещё усовершенствовать модель Можно ввести много дополнительных эвристик и предположений, которые уведут нас совсем далеко от старого доброго SVD. Например: Рейтинг не всегда является продуктом чистого взаимодействия пользователя с товаром. Бывают товары, которые сами по себе ужасно популярны (скажем, человек купит туалетную бумагу даже если не очень интересуется товарами для дома) или так ужасны, что даже интересующийся данной «латентной категорией» покупатель не станет их высоко оценивать. Это можно промоделировать, добавив к скалярному произведению члены, зависящие только от пользователя и только от товара соответственно: +(b i ,c j ) Тогда наша задача оптимизации примет вид: min i,j:ρ ij  =NA −(b min Можно добавлять регуляризационные члены. Например: min i,j:,ρ ij  =NA ∑ (ρ ij −(b ∣∣b ∣∣c min Мы можем не игнорировать неизвестные нам элементы матрицы X X, а присвоить им нулевые значения и ставить более низкие веса соответствующим слагаемым функции потерь: min i,j ∑ w(ρ ij )(ρ ij −(b min где w(ρ ij ) маленькое, если =NA, и большое в противном случае. Это имеет",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 11,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "оценивать. Это можно промоделировать, добавив к скалярному произведению члены, зависящие только от пользователя и только от товара соответственно: +(b i ,c j ) Тогда наша задача оптимизации примет вид: min i,j:ρ ij  =NA −(b min Можно добавлять регуляризационные члены. Например: min i,j:,ρ ij  =NA ∑ (ρ ij −(b ∣∣b ∣∣c min Мы можем не игнорировать неизвестные нам элементы матрицы X X, а присвоить им нулевые значения и ставить более низкие веса соответствующим слагаемым функции потерь: min i,j ∑ w(ρ ij )(ρ ij −(b min где w(ρ ij ) маленькое, если =NA, и большое в противном случае. Это имеет смысл, например, если отсутствие данных в самом деле может быть логично интерпретировать, как отсутствие интереса. Можно ввести требования неотрицательности: ⩾0, ⩾0. Подробнее об этом в параграфе про неотрицательное матричное разложение. Или даже всё это вместе 😄 Вероятностное обличье модели латентных факторов Вы могли заметить, что задача min i,j:,ρ ij  =NA ∑ (ρ ij −(b min подозрительно напоминает задачу наименьших квадратов, и неспроста. В базовой формулировке мы предполагаем, что где - нормальный шум ρ ij =(b i ,c j )+ξ ij ,где ξ ij ∼N(0,σ 2 ) - нормальный шум Иными словами, ∼N((b i ,c j ),σ 2 ) По крайней мере, те из них, которые нам известны. Нахождение методом максимального правдоподобия как раз и приводит к описанной выше оптимизационной задаче. Как обычно, мы можем добавить априорную информацию о распределении латентных векторов . Например, такую: ∼N(0,σ b E),c j ∼N(0,σ c E) Расписывая логарифм правдоподобия p(ρ;b,c)=p(ρ∣b,c)p(b)p(c) и убирая константные члены, которые содержат только сигмы, приводим задачу максимизации логарифма правдоподобия к виду max i,j:,ρ ij  =NA ∑ (ρ ij −(b ∣∣b ∣∣c max вполне объясняющему, почему в предыдущем пункте у нас могла появляться L2-регуляризация. Анализ независимых компонент (ICA) ICA изначально был придуман для задачи разделения сигналов («blind source separation»). Рассмотрим пример из sklearn sklearn Изначально были три сигнала (красный, рыжий и синий на второй сверху картинке), их смешали, получив три линейных комбинации (на верхней картинке). Теперь попробуем их разделить. Первая мысль, которая нам приходит в голову: воспользуемся SVD (проинтерпретировав моменты времени как объекты, а сигналы из смеси как признаки — то есть взяв матрицу 2000 × 3 2000×3)! Но на нижней картинке мы видим результат, который не радует, но не радует ожидаемо, и вот почему: В первый латентный признак SVD старается собрать максимально возможную дисперсию — мы видим, что красный график на нижней картинке действительно ловит самые значительные колебания сигналов из смеси; при этом в третий (рыжий) сигнал уже попадает более или менее случайный шум. Если посмотреть на значения исходных сигналов, то они распределены не нормально (распределения значений синего и красного имеют две моды, а у рыжего близко к равномерному), а мы помним, что SVD плохо приспособлено к работе с не гауссовскими данными. Анализ независимых компонент (ICA) состоит в аппроксимации наблюдаемых признаков линейной смесью латентных, которые являются независимыми как случайные величины. Замечание. Оригинальная формулировка несколько другая: изначально ICA — это аппроксимация наблюдаемых сигналов линейной смесью некоторого числа независимых сигналов, то есть речь шла о смеси объектов. Описываемые далее методы можно",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 12,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "из смеси; при этом в третий (рыжий) сигнал уже попадает более или менее случайный шум. Если посмотреть на значения исходных сигналов, то они распределены не нормально (распределения значений синего и красного имеют две моды, а у рыжего близко к равномерному), а мы помним, что SVD плохо приспособлено к работе с не гауссовскими данными. Анализ независимых компонент (ICA) состоит в аппроксимации наблюдаемых признаков линейной смесью латентных, которые являются независимыми как случайные величины. Замечание. Оригинальная формулировка несколько другая: изначально ICA — это аппроксимация наблюдаемых сигналов линейной смесью некоторого числа независимых сигналов, то есть речь шла о смеси объектов. Описываемые далее методы можно точно также использовать и для разделения смеси объектов, конечно. Важно, что в данном случае предъявляется требование независимости, а не просто некоррелированности — более сильное, впрочем, труднодостижимое и столь же трудно проверяемое. Как построить ICA? Путешествие в мир удивительных эвристик Мы будем излагать алгоритм FastICA по статье его создателей, она же реализована в библиотеке sklearn; в статье вас ждёт гораздо больше подробностей и тонкостей реализации. Алгоритм базируется на следующем эвристическом соображении: линейная комбинация нескольких независимых негауссовских величин в большей степени гауссовская, чем сами эти величины — довольно смелый вывод из Центральной предельной теоремы. Таким образом, мы будем искать линейную комбинацию исходных признаков, которая была бы в наименьшей степени гауссовской — это и будет первая из независимых компонент. Но как померить близость к нормальности? Пусть z z — некоторая (одномерная) случайная величина с плотностью p ( z ) p(z). Рассмотрим её энтропию log H(z)=−∫p(t)logp(t)dt Имеет место теорема: гауссовская случайная величина имеет максимальную энтропию среди всех случайных величин с заданной дисперсией. Рассмотрим теперь J(z)=H(z gauss )−H(z) где gauss — гауссовская случайная величина с той же дисперсией, что и у z z. Величина J ( z ) J(z) всегда неотрицательна и равна нулю в том случае, если z z гауссовская. Решая задачу max ⁡ w J(Xw)⟶ w max мы могли бы найти самую негауссовскую линейную комбинацию наших признаков. Проблема в том, что J ( z ) J(z) трудно посчитать. Авторы статьи предлагают использовать приближение J(z)∼(EG(z)−EG(w)) 2 , где w∼N(0,1), а G G неквадратичная функция (в статье предлагаются конкретные варианты). Последующие независимые компоненты можно искать в ортогональном подпространстве (всё-таки они должны быть и некоррелированными). Подготовка данных для ICA Перед тем, как строить разложение нужно центрировать данные (вычесть из признаков их средние) и убедиться, что ковариационная матрица признаков является единичной. Неотрицательное матричное разложение (NMF) Мотивация: тематическое моделирование Допустим, что у нас есть датасет, в котором объекты — тексты, признаки — токены (например, слова), а на ( i , j ) (i,j)-м месте написана частота встречаемости j j-го токена в i i-м тексте (то есть , где n i j n ij — сколько раз i i-й токен встретился в j j-м документе, а n j n j — общее число токенов в этом документе). matrix Приблизим нашу матрицу произведением X⁡N×D ∼ B⁡N×R ⋅ C⁡R×D N×D X ∼ N×R B ⋅ R×D C Одна из возможных интерпретаций такова. Есть D D тем: matrix За этим стоит вполне ясная вероятностная модель: p(word∣document)∼ theme ∑ p(word∣theme)⋅p(theme∣document) Вопрос в том,",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 13,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— тексты, признаки — токены (например, слова), а на ( i , j ) (i,j)-м месте написана частота встречаемости j j-го токена в i i-м тексте (то есть , где n i j n ij — сколько раз i i-й токен встретился в j j-м документе, а n j n j — общее число токенов в этом документе). matrix Приблизим нашу матрицу произведением X⁡N×D ∼ B⁡N×R ⋅ C⁡R×D N×D X ∼ N×R B ⋅ R×D C Одна из возможных интерпретаций такова. Есть D D тем: matrix За этим стоит вполне ясная вероятностная модель: p(word∣document)∼ theme ∑ p(word∣theme)⋅p(theme∣document) Вопрос в том, как получить такое разложение. Конечно, чисто технически можно использовать SVD. Но тогда элементы матриц разложения вряд ли будут иметь вероятностный смысл: они же даже не обязаны быть неотрицательными. С другой стороны, если потребовать, чтобы все элементы B B и C C были неотрицательными, ситуация исправится. Определение NMF Неотрицательное матричное разложение неотрицательной матрицы X X — это произведение B C BC матриц с неотрицательным элементами, наилучшим образом приближающее X X по норме Фробениуса min ∣∣X−BC∣∣ fro 2 ⟶ B,C b ij ,c kl ⩾0∀i,j,k,l min Alternating Least Squares (ALS) ALS — один из популярных методов для решения факторизационных задач. Несмотря на то, что оптимизационная задача в целом не является выпуклой, по отдельности задача поиска каждого из сомножителей является выпуклой и может решаться с помощью привычных нам методов. Таким образом, мы можем чередовать поиск B B при фиксированном C C и поиск C C при фиксированном B B, итеративно сходясь к итоговому решению: als Заметим, что из-за насильного обнуления элементов будут получаться разреженные матрицы. Разумеется, можно рассматривать и более сложные функционалы, прибавляя к ∥X−BC∥ 2 различные регуляризационные члены, скажем, поощряющие большую разреженность матриц B B и C C. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.1. Матричное дифференцирование Как дифференцировать матрицы и дифференцировать по матрицам: всё, что вам не рассказали про дифференцирование на матанализе Следующий параграф 16.3. Вероятностные распределения",
    "metadata": {
      "title": "Матричная факторизация",
      "url": "https://education.yandex.ru/handbook/ml/article/matrichnaya-faktorizaciya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.2",
      "part": 14,
      "total_parts": 14,
      "source_file": "16.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Принимая то или иное решение в условиях недостаточной информации, нам часто приходится взвешивать шансы, просчитывать риски, а то и вовсе уповать на удачу. Теория вероятностей предоставляет математические инструменты для проведения корректных рассуждений в условиях неопределённости, количественного измерения характеристик случайных событий и оценки правдоподобия их реализации. Этот и последующий параграфы следует рассматривать как расширенный справочник, позволяющий освежить знания по вероятности и статистике, сделав при этом упор на приложении к машинному обучению. За более систематическим курсом по теории вероятностей читателю следует обратиться к серьёзным учебникам вроде Ширяева или Феллера. Для погружения в статистику смотри, например, книгу Лагутина. А особо нетерпеливым рекомендуем взглянуть на короткий и ёмкий Probability and Statistics Cookbook. Вероятностное пространство В учебниках вероятность традиционно поставляется в комплекте с вероятностным пространством. Не увлекаясь чрезмерным формализмом, можно сказать, что для задания вероятностного пространства нужны: непустое множество Ω Ω, называемое пространством элементарных событий (исходов); алгебра множеств F ⊂ 2 Ω F⊂2 Ω — набор подмножеств Ω Ω, замкнутый относительно дополнений, объединений и пересечений; каждый элемент A ∈ F A∈F называется событием; вероятностная мера P:F→[0,1], приписывающая каждому событию A ∈ F A∈F некоторую вероятность P(A)∈[0,1]. К вероятностному пространству (Ω,F,P) предъявляются следующие требования: ∅ ∈ F ∅∈F (невозможное событие), Ω ∈ F Ω∈F (достоверное событие); P(Ω)=1; P(A∪B)=P(A)+P(B), если A , B ∈ F A,B∈F и A ∩ B = ∅ A∩B=∅ (аддитивность). Упражнение. Докажите, что P(∅)=0 и )=1−P(A), если A ∈ F A∈F. Аддитивность вероятности легко обобщается по индукции до свойства конечной аддитивности: если события ,…,A n попарно несовместны, то k=1 ⋃ n A k )= k=1 ∑ n P(A k ). Множество Ω Ω часто называют носителем; говорят также, что вероятностная мера (масса) P P сосредоточена, или распределена, на носителе Ω Ω. В зависимости от типа носителя Ω Ω распределения делятся на два типа: дискретные и непрерывные. Дискретные распределения Вероятность на не более чем счётном пространстве элементарных событий Ω={ω 1 ,ω 2 ,…} задаётся просто приписыванием неотрицательного числа p k p k каждому элементарному исходу ω k ω k с условием =1. Для произвольного события A ⊂ Ω A⊂Ω полагают P(A)= . Набор чисел } называют также распределением вероятностей на множестве Ω Ω. В англоязычной литературе распространён термин probability mass function (сокращённо pmf). Зачастую в результате эксперимента нас интересуют не вероятности событий сами по себе, а значения некоторой связанной с ними случайной величины, принимающей числовые значения. Например: сумма очков, выпавших при броске двух кубиков; число метеоритов диаметром более одного метра, падающих на Землю в течение года; ежедневный доход от показа рекламных объявлений в интернете. На каждом элементарном исходе ω k ω k случайная величина ξ ξ принимает некоторое числовое значение =ξ(ω k ). Иными словами, случайная величина — это функция ξ:Ω→R, принимающая значение ξ k ξ k с вероятностью p k p k ; её математическое ожидание (среднее) и дисперсия (среднеквадратичное отклонение) вычисляются по формулам Eξ= и Vξ=E(ξ−Eξ) 2 =Eξ 2 −(Eξ) 2 соответственно. Корень из дисперсии V ξ Vξ назвают стандартным отклонением случайной величины ξ ξ. Стандартное отклонение и дисперсия показывают, насколько далеко значения случайной величины могут отклоняться от среднего значения. Стандартное отклонение",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 1,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "течение года; ежедневный доход от показа рекламных объявлений в интернете. На каждом элементарном исходе ω k ω k случайная величина ξ ξ принимает некоторое числовое значение =ξ(ω k ). Иными словами, случайная величина — это функция ξ:Ω→R, принимающая значение ξ k ξ k с вероятностью p k p k ; её математическое ожидание (среднее) и дисперсия (среднеквадратичное отклонение) вычисляются по формулам Eξ= и Vξ=E(ξ−Eξ) 2 =Eξ 2 −(Eξ) 2 соответственно. Корень из дисперсии V ξ Vξ назвают стандартным отклонением случайной величины ξ ξ. Стандартное отклонение и дисперсия показывают, насколько далеко значения случайной величины могут отклоняться от среднего значения. Стандартное отклонение хорошо тем, что, в отличие от дисперсии, измеряется в тех же единицах, что и сама случайная величина. Равномерное распределение Так называется распределение вероятностней на множестве Ω={ω 1 ,…,ω n }, у которого =P(ω 1⩽k⩽n. Тогда P(A)= ∣Ω∣ ∣A∣ , и мы получили формулу из классического подхода к вероятности, при котором вероятность события равна отношению числа благоприятных исходов к общему их количеству. Равномерным распределением моделируются различные игровые ситуации: подбрасывание симметричной монеты ( ω 1 = «орёл» ω 1 =«орёл», ω 2 = «решка» ω 2 =«решка»); подбрасывание кубика ( =k, 1 ⩽ k ⩽ 6 1⩽k⩽6); вращение рулетки в казино ( n = 37 n=37 для европейской, n = 38 n=38 для американской). Упражнение. У европейской рулетки по 18 18 чёрных и красных секторов и один сектор «зеро». Игрок ставит €10 на чёрное. В случае успеха казино выплачивает ему ещё €10, в противном случае забирает ставку. Чему равно математическое ожидание, дисперсия и стандартное отклонение выигрыша? Вопрос на подумать. Бывают ли равномерные распределения в пространствах со счётным носителем? Равномерные распределения преимущественно встречаются в разного сорта играх. В более жизненных ситациях случайность обычно распределена отнюдь не равномерно. Распределение Бернулли Так называется очень простое распределение всего лишь с двумя исходами: P ( «успех» «неудача» P(«успех»)=p,P(«неудача»)=1−p,0⩽p⩽1. Бернуллиевская случайная величина ξ∼Bern(p) — это просто индикаторная функция успешного события: ξ = 1 ξ=1, если случился «успех», ξ = 0 ξ=0, если нас постигла «неудача». Несложные вычисления показывают, что Eξ=1⋅p+0⋅(1−p)=p,Vξ=p−p 2 =p(1−p). Если , то снова получается равномерное распределение с двумя исходами. При бернуллиевская случайная величина моделирует подбрасывание несимметричной монеты. В машинном обучении часто встречается задача бинарной классификации, и разбиение на классы обычно кодируется с помощью Bern(p), например: диагностика болезни (болен — 1 1, здоров — 0 0); оценка кредитоспособности клиента (одобрить кредит — 0 0, отказать 1 1); предсказание поведения пользователя (кликнет на рекламу — 1 1, пропустит — 0 0). В этих примерах вероятности классов явно не равны, поэтому несимметричное распределение Бернулли — типичная ситуация в реальных задачах. Биномиальное распределение Биномиальное распределение Bin(n,p) имеет сумма независимых бернуллиевских случайных величин ∼Bern(p): η∼Bin(n,p), если η=ξ 1 +…+ξ n . Другими словами, случайная величина η η равна количеству успехов в n n независимых испытаниях Бернулли с вероятностью успеха p p. Случайная величина η η принимает значения от 0 0 до n n, и =P(η=k)=( k n )p k (1−p) n−k ,0⩽k⩽n. Отметим, что согласно биному Ньютона k=0 ∑ n p k =(p+(1−p)) n =1, поэтому числа } действительно представляют",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 2,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "пропустит — 0 0). В этих примерах вероятности классов явно не равны, поэтому несимметричное распределение Бернулли — типичная ситуация в реальных задачах. Биномиальное распределение Биномиальное распределение Bin(n,p) имеет сумма независимых бернуллиевских случайных величин ∼Bern(p): η∼Bin(n,p), если η=ξ 1 +…+ξ n . Другими словами, случайная величина η η равна количеству успехов в n n независимых испытаниях Бернулли с вероятностью успеха p p. Случайная величина η η принимает значения от 0 0 до n n, и =P(η=k)=( k n )p k (1−p) n−k ,0⩽k⩽n. Отметим, что согласно биному Ньютона k=0 ∑ n p k =(p+(1−p)) n =1, поэтому числа } действительно представляют собой распределение вероятностей, называемое также биномиальным. Если ξ∼Bin(n,p), то Eξ=np,Vξ=np(1−p). Пример. Каждый день рекламу компании А поисковой выдаче Яндекса видят ровно 1000 1000 человек. Вчера 50 50 из них кликнули на рекламу. Для прогнозирования объемов продаж компании А хочется знать, с какой вероятностью не менее 50 людей кликнут на ее рекламу сегодня. Если моделировать наличие или отсутствие клика бернуллиевской случайной величиной, то общее количество кликов за день моделируется случайной величиной ξ∼Bin(n,p) с параметрами n = 1000 n=1000 и p = 50 1000 = 0.05 p= 1000 50 =0.05. Тогда с помощью вычислительной техники получаем, что 1000 k ) 0.0 5 k 0.9 5 1000 − k ≈ 0.52. P(ξ⩾50)= k=50 (1−p) n−k =1− k=0 ∑ 49 ( k 1000 )0.05 k 0.95 1000−k ≈0.52. Отметим, что параметр p p в предыдущем примере нам, строго говоря, не был известен, и вместо него мы использовали частотную оценку. Распределение Пуассона Это распределение имеет счётный носитель Ω=N∪{0}. Случайная величина ξ ξ имеет пуассоновское распределение с параметром ξ∼Pois(λ), если P(ξ=k)=e −λ k! λ k ,k∈N∪{0}. Известное разложение экспоненты в ряд Тейлора k=0 ∑ ∞ k! λ k позволяет заключить, что вероятности распределения Пуассона действительно суммируются в единицу. Этот же ряд позволяет вычислить, что Eξ=Vξ=λ. Пуассоновская случайная величина моделирует число редких событий, происходящих в течение фиксированного промежутка времени: если события наступают со средней скоростью r r, то P ( k событий на промежутке P(k событий на промежутке t)=e −rt k! (rt) k ,k∈N∪{0}. Иногда приходится рассматривать биномиальное распределение Bin(n,p) с большим числом попыток n n и вероятностью успеха p p с условием np≈λ>0. Оказывается, что вне зависимости от n n такое распределение быстро стабилизируется, сходясь к пуассоновскому распределению с параметром λ λ. Точнее говоря, справедлива следующая теорема. Теорема (Пуассон). Пусть ξ∼Bin(n,p n ) и lim n→∞ lim np n =λ>0. Тогда lim n→∞ lim P(ξ=k)=e −λ k! λ k ,k∈N∪{0}. Пример. Известно, что на поисковой выдаче яндекса на рекламу компании А кликает в среднем примерно 50 пользоваталей в день. Количество показов достаточно большое и может меняться изо дня в день. Требуется оценить вероятность того, что сегодня будет совершено не менее 50 кликов по рекламным объявлениям. Распределение количества кликов снова будем моделировать биномиальным распределением Bin(n,p). На этот раз число n n нам неизвестно, но сказано, что оно велико и n p ≈ 50 np≈50 (вспомним, что E ξ = n p Eξ=np, если ξ∼Bin(n,p)). Поэтому можно воспользоваться теоремой Пуассона и заменить биномиальное распределение пуассоновским с параметром λ",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 3,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "k! λ k ,k∈N∪{0}. Пример. Известно, что на поисковой выдаче яндекса на рекламу компании А кликает в среднем примерно 50 пользоваталей в день. Количество показов достаточно большое и может меняться изо дня в день. Требуется оценить вероятность того, что сегодня будет совершено не менее 50 кликов по рекламным объявлениям. Распределение количества кликов снова будем моделировать биномиальным распределением Bin(n,p). На этот раз число n n нам неизвестно, но сказано, что оно велико и n p ≈ 50 np≈50 (вспомним, что E ξ = n p Eξ=np, если ξ∼Bin(n,p)). Поэтому можно воспользоваться теоремой Пуассона и заменить биномиальное распределение пуассоновским с параметром λ = 50 λ=50. Тогда искомая вероятность равна 0.518 , 1− k=0 ∑ 49 e −50 k! 50 k ≈0.518, что практически совпадает ответом, полученным с помощью биномиального распределения при n = 1000 n=1000. Геометрическое распределение Пусть монетка с вероятностью «успеха» p p подбрасывается до тех пор, пока впервые не случится «успех». Случайная величина ξ ξ, равная общему количеству попыток на этом пути, имеет геометрическое распределение, т.е. P(ξ=k)=q k−1 p,q=1−p,k∈N. По формуле геометрической прогрессии находим, что k=1 ∑ ∞ P(ξ=k)= k=0 ∑ ∞ q k p= 1−q p =1, поэтому с нормировкой тут всё в порядке. Чем меньше p p, тем больше геометрическое распределение похоже на равномерное, что подтверждают и формулы для среднего и дисперсии: Eξ= p 1 ,Vξ= p 2 1−p . Пример. По оценкам за предыдущие дни пользователь нажимает на рекламу с вероятностью p = 0.05 p=0.05. Сегодня компания B планирует показать очень важное рекламное объявление и требует от Яндекса, чтобы с вероятностью не менее 99 % 99% на него кликнули хотя бы раз. Скольким различным людям следует показать это объявление? Здесь мы имеем дело с геометрическим распределением с вероятностью «успеха» (клика) p p: именно так распределена случайная величина ξ ξ, равная количеству показов объявления до первого клика по нему. Следовательно, P(ξ⩽n)= k=1 ∑ n P(ξ=k)= k=1 ∑ n q k−1 p=p⋅ 1−q 1−q n =1−q n . Эта вероятность должна быть не меньше 99 % 99%, т. е. 0.9 5 n ⩾ 0.01 0.95 n ⩾0.01. Отсюда находим, что n ⩾ log ⁡ 0.01 log ⁡ 0.95 ≈ 89.78 n⩾ log0.95 log0.01 ≈89.78. Таким образом, рекламу надо показать как минимум 90 90 раз. Гипергеометрическое распределение Пример. Известно, что партия из N N деталей содержит K K бракованных. Какова вероятность того, что среди выбранных наугад n n деталей окажется ровно k k бракованных? Всего есть ) способов выбора n n деталей из партии. Число вариантов выбрать k k деталей из K K бракованных и n − k n−k из N − K N−K деталей без дефектов равно n−k N−K ). По классическому определению вероятности получаем, что искомая вероятность равна min n−k N−K ) ,0⩽k⩽min{K,n}. Такое распределение называется гипергеометрическим. Равенство ∑ k = 0 min k=0 ∑ min{K,n} p k =1 следует из тождества Вандермонда. Если случайная величина ξ ξ имеет гипергеометрическое распределение с параметрами N N, K K, n n, то Eξ= N nK ,Vξ=n N 2 (N−1) K(N−K)(N−n) . Гипергеометрическое распределение является аналогом биномиального, при котором моделируется выбор",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 4,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n деталей из партии. Число вариантов выбрать k k деталей из K K бракованных и n − k n−k из N − K N−K деталей без дефектов равно n−k N−K ). По классическому определению вероятности получаем, что искомая вероятность равна min n−k N−K ) ,0⩽k⩽min{K,n}. Такое распределение называется гипергеометрическим. Равенство ∑ k = 0 min k=0 ∑ min{K,n} p k =1 следует из тождества Вандермонда. Если случайная величина ξ ξ имеет гипергеометрическое распределение с параметрами N N, K K, n n, то Eξ= N nK ,Vξ=n N 2 (N−1) K(N−K)(N−n) . Гипергеометрическое распределение является аналогом биномиального, при котором моделируется выбор без возвращения с вероятностью успеха Непрерывные распределения Вероятностная модель с конечным или счётным носителем не подходит в тех случаях, когда результатом эксперимента удобно считать произвольное действительное число, например, распределение людей по росту или по весу. Для этого требуется пересмотреть подход к построению пространства элементарных событий Ω Ω: ведь множество действительных чисел R R континуально, и поэтому вероятность события не получится определить как сумму вероятностей всех составляющих исходов, коих тоже может оказаться континуум. Приходится искать другие способы задания вероятности. Наиболее часто встречающийся на практике класс непрерывных распределений на числовой прямой задаётся с помощью неотрицательной интегрируемой функции плотности (probability density function, pdf) p ( x ) p(x) со свойством p(x)dx=1. Вероятность события A A определяется как P(A)= A ∫ p(x)dx при условии, что этот интеграл имеет смысл. В частности, P([a,b))= a ∫ b p(x)dx. Замечание. Связь между вероятностью и плотностью распределения весьма напоминает связь между массой и физической плотностью. Когда плотность объекта всюду одинакова, то масса равна плотности, умноженной на объём. Если же объект неоднороден, то плотность становится функцией, сопоставляющей каждой точке некое число (что-то вроде предела отношения массы малого шарика вокруг этой точки к объёму шарика). Тогда масса любого куска объекта может быть вычислена, как интеграл функции плотности по объёму этого куска. С плотностью вероятности p ( x ) p(x) автоматически связана случайная величина ξ:R→R, для которой P(a⩽ξ<b)= a ∫ b p(x)dx. Функция p ( x ) p(x) называется плотностью случайной величины ξ ξ, и обозначается также как (x). Иногда используется запись ξ∼p(x). Среднее и дисперсия случайной величины ξ∼p(x) вычисляются по формулам Eξ= −∞ ∫ ∞ xp(x)dx,Vξ= −∞ ∫ ∞ x 2 p(x)dx−(Eξ) 2 . Равномерное распределение Равномерное распределение на отрезке [ a ; b ] [a;b], которое часто обозначают U[a,b], имеет постоянную плотность на этом отрезке: p(x)= b−a I(a⩽x⩽b) ={ b−a 1 , 0, x∈[a,b], x∈ / [a,b]. Если ξ∼U[a,b], то Eξ= 2 a+b ,Vξ= 12 (b−a) 2 . Вопрос на подумать. Можно ли задать равномерное распределения на неограниченном промежутке, например, на R R или на [0,+∞)? Аналогичным образом вводится равномерное распределение в многомерном пространстве: если множество V ⊂ R n V⊂R n имеет объём ∣ V ∣ ∣V∣, то плотность равномерно распределённой на V V случайной величины ξ ξ задаётся как (x)= ∣V∣ I(x∈V) . Если A ⊂ V A⊂V, то P(A)= ∣V∣ 1 A ∫ dx= ∣V∣ ∣A∣ , и мы получили формулу геометрической вероятности. Нормальное распределение Случайная величина ξ ξ имеет нормальное (гауссовское) распределение N(μ,σ 2 ),",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 5,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "то Eξ= 2 a+b ,Vξ= 12 (b−a) 2 . Вопрос на подумать. Можно ли задать равномерное распределения на неограниченном промежутке, например, на R R или на [0,+∞)? Аналогичным образом вводится равномерное распределение в многомерном пространстве: если множество V ⊂ R n V⊂R n имеет объём ∣ V ∣ ∣V∣, то плотность равномерно распределённой на V V случайной величины ξ ξ задаётся как (x)= ∣V∣ I(x∈V) . Если A ⊂ V A⊂V, то P(A)= ∣V∣ 1 A ∫ dx= ∣V∣ ∣A∣ , и мы получили формулу геометрической вероятности. Нормальное распределение Случайная величина ξ ξ имеет нормальное (гауссовское) распределение N(μ,σ 2 ), если её плотность равна (x)= (x−μ) 2 . Параметры нормального распределения N(μ,σ 2 ) представляют собой его среднее и дисперсию: Eξ=μ,Vξ=σ 2 . Параметр σ σ отвечает за выраженность «колокола» плотности нормального распределения: при σ → 0 σ→0 «колокол» приобретает очертания резко выраженного пика, то есть практически вся вероятностная масса сосретдоточена в малой окрестности точки x = μ x=μ; при σ → + ∞ σ→+∞ «колокол», наоборот, размывается, и распределение становится больше похоже на равномерное. Гауссиана, у которой μ = 0 μ=0 и σ = 1 σ=1, называется стандартным нормальным распределением. Иногда бывает полезно тесно связанное с гауссовским логнормальное распределение. Случайная величина ξ:(0,+∞)→R имеет логнормальное распределение, ξ∼LogN(μ,σ 2 ), если log logξ∼N(μ,σ 2 ). Плотность логнормальной случайной величины равна log (x)= (logx−μ) 2 ,x>0, а её среднее и дисперсию можно вычислить по формулам Eξ=e μ+ 2 σ 2 ,Vξ=(e σ 2 −1)e 2μ+σ 2 . Показательное распределение Плотность показательного (экспоненциального) распределения Exp(λ) сосредоточена на луче [0,+∞) и имеет параметр λ > 0 λ>0: p(x)=λe −λx , x ⩾ 0 x⩾0. Если ξ∼Exp(λ), то Eξ= λ 1 ,Vξ= λ 2 1 . Плотность показательного распределения является убывающей функцией на [0,+∞), а параметр λ λ отвечает за скорость этого убывания: при λ → 0 λ→0 убывание очень медленное, и распределение больше похоже на равномерное; при λ → + ∞ λ→+∞, наоборот, вся вероятностная масса сосредоточена около точки 0 0. Показательное распределение моделирует временные интервалы между случайными событиями, наступающими с постоянной скоростью, например: время ожидания автобуса на остановке; время между телефонными звонками в колл-центре; время до выхода из строя вычислительного узла в дата-центре. Гамма-распределение с положительными параметрами α α и β β имеет плотность p(x)= Γ(α)β α 1 x α−1 e − β x ,x⩾0, где Γ ( α ) Γ(α) — гамма-функция Эйлера. При α = 1 α=1 гамма-распределение превращается в показательное с параметром . Среднее и дисперсия случайной величины ξ ξ, имеющей гамма-распределение с параметрами α α и β β, равны Eξ=αβ,Vξ=αβ 2 . Бета-распределение Плотность бета-распределения с параметрами α , β > 0 α,β>0 равна p(x)= B(α,β) 1 x α−1 (1−x) β−1 ,0<x<1, где B(α,β) — бета-функция Эйлера. Бета-распределение имеет следующее статистическое приложение. Выберем случайным образом точки ,…,x n ∈[0,1], и упорядочим их по возрастанию. Получим набор значений 0⩽x (1) ⩽x (2) ⩽…⩽x (k) ⩽…⩽x (n) ⩽1. Оказывается, что случайная величина ξ=x (k) , называемая k k-й порядковой статистикой, имеет бета распределение с параметрами k k и n + 1",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 6,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "с параметром . Среднее и дисперсия случайной величины ξ ξ, имеющей гамма-распределение с параметрами α α и β β, равны Eξ=αβ,Vξ=αβ 2 . Бета-распределение Плотность бета-распределения с параметрами α , β > 0 α,β>0 равна p(x)= B(α,β) 1 x α−1 (1−x) β−1 ,0<x<1, где B(α,β) — бета-функция Эйлера. Бета-распределение имеет следующее статистическое приложение. Выберем случайным образом точки ,…,x n ∈[0,1], и упорядочим их по возрастанию. Получим набор значений 0⩽x (1) ⩽x (2) ⩽…⩽x (k) ⩽…⩽x (n) ⩽1. Оказывается, что случайная величина ξ=x (k) , называемая k k-й порядковой статистикой, имеет бета распределение с параметрами k k и n + 1 − k n+1−k: (x)= (k−1)!(n−k)! n! x k−1 (1−x) n−k =k( k n )x k−1 (1−x) n−k . Распределение Стьюдента При проверке статистических гипотез бывает полезно распределение Стьюдента (t-distribution) с ν ν степенями свободы, плотность которого равна p(x)= ν+1 ) (1+ ν x 2 ) −(ν+1)/2 ,ν>0, где Γ ( α ) Γ(α) — гамма-функция Эйлера. Распределение Стьюдента похоже на стандартное нормальное распределение; более того, при ν → + ∞ ν→+∞ оно превращается в N(0,1). Однако при малых значениях ν ν распределение Стьюдента имеет гораздо более тяжёлые «хвосты»: например, при ν ⩽ 2 ν⩽2 его дисперсия бесконечна, а при ν ⩽ 1 ν⩽1 та же участь постигает и математическое ожидание (всё из-за расходимости соответствующих интегралов). В остальных случаях Eξ=0,Vξ= ν−2 ν , если ξ ξ имеет распределение Стьюдента с ν ν степенями свободы. Распределение Лапласа Плотность распределения Лапласа с параметрами μ , b μ,b равна p(x)= 2b 1 e − b ∣x−μ∣ . Такое распределение иногда обозначают Laplace(μ,b). Если ξ∼Laplace(μ,b), то Eξ=μ,Vξ=2b 2 . При μ = 0 μ=0 распределение Лапласа представляет собой экспоненциальное распределение, плотность которого симметрично отражена на отрицательную полуось: если ξ∼Laplace(0,b), то ∣ξ∣∼Exp( b 1 ). Распределение Лапласа похоже на нормальное и отличается от него немного более тяжёлыми «хвостами» и тем, что его плотность теряет гладкость в нуле. Характеристики случайных величин Моменты Если n ∈ N n∈N, то n n-й момент μ n μ n случайной величины ξ ξ равен E ξ n Eξ n . В зависимости от типа случайной величины моменты вычисляются по-разному: P(ξ=x k ), если ξ ξ принимает дискретные значения ,…,x k ,…; (x)dx, если ξ ξ имеет плотность (x). Первый момент μ 1 μ 1 — это в точности математическое ожидание (среднее) случайной величины ξ ξ. Дисперсию тоже можно выразить через моменты: Vξ=Eξ 2 −(Eξ) Не у всех случайных величин есть конечные среднее и дисперсия. Например, распределение Коши (оно же распределение Стьюдента с одной степенью свободы) имеет плотность p(x)= π 1 1+x 2 1 , и если мы попытаемся вычислить первые два момента, то получим расходящиеся интегралы 1+x 2 x dx и π 1 −∞ ∫ +∞ 1+x 2 x 2 dx. Упражнение. Приведите пример дискретной случайной величины с бесконечным средним. Свойства математического ожидания Если ξ = C ξ=C, то E ξ = C Eξ=C. E(aξ+bη)=aEξ+bEη (линейность). Если ξ ⩽ η ξ⩽η, то E ξ ⩽ E η Eξ⩽Eη (монотонность). EI(A)=P(A). Если случайные величины ξ ξ и η η независимы, то Eξη=EξEη.",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 7,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "среднее и дисперсия. Например, распределение Коши (оно же распределение Стьюдента с одной степенью свободы) имеет плотность p(x)= π 1 1+x 2 1 , и если мы попытаемся вычислить первые два момента, то получим расходящиеся интегралы 1+x 2 x dx и π 1 −∞ ∫ +∞ 1+x 2 x 2 dx. Упражнение. Приведите пример дискретной случайной величины с бесконечным средним. Свойства математического ожидания Если ξ = C ξ=C, то E ξ = C Eξ=C. E(aξ+bη)=aEξ+bEη (линейность). Если ξ ⩽ η ξ⩽η, то E ξ ⩽ E η Eξ⩽Eη (монотонность). EI(A)=P(A). Если случайные величины ξ ξ и η η независимы, то Eξη=EξEη. Если ξ ⩾ 0 ξ⩾0, то P(ξ⩾a)⩽ a Eξ (неравенство Маркова). Если функция f f выпукла вниз, то f(Eξ)⩽E(f(ξ)) (неравенство Йенсена). Law of the unconscious statistician (LOTUS) Если случайная величина η η получена применением некоторой детерминированной функцией из случайной величины η=g(ξ), то Eη= k ∑ g(x k )P(ξ=x k ), если ξ ξ дискретна; Eη= −∞ ∫ +∞ g(x)p ξ (x)dx, если ξ ξ непрерывна. Дисперсия и ковариация Ковариация случайных величин ξ ξ и η η определяется по формуле cov(ξ,η)=E((ξ−Eξ)⋅(η−Eη))=E(ξ⋅η)−Eξ⋅Eη В частности, cov(ξ,ξ)=Vξ. На практике часто применяют коэффициент корреляции, который получается нормированием ковариации: corr(ξ,η)= Vξ Vη cov(ξ,η) . Коэффициент корреляции всегда принимает значения из отрезка [−1;1]. Если corr(ξ,η)=0, то случайные величины ξ ξ и η η называют некоррелированными. Свойства дисперсии и ковариации V ξ ⩾ 0 Vξ⩾0, причём Vξ=0⟺∃a∈R:P(ξ=a)=1. V(aξ)=a 2 Vξ, V(ξ+a)=Vξ. cov(ξ,η)=cov(η,ξ), cov(aξ,bη)=abcov(ξ,η). V(ξ+η)=Vξ+Vη+2cov(ξ,η). Если случайные величины ξ ξ и η η независимы, то cov(ξ,η)=0 и V(ξ+η)=Vξ+Vη. P(∣ξ−Eξ∣⩾a)⩽ a 2 Vξ (неравенство Чебышева). Функции распределения и плотности Случайная величина ξ:Ω→R является числовой функцией, заданной на пространстве элементарных событий; однако, больший интерес обычно представляет порождаемое ею распределение вероятностей. В дискретном случае достаточно задать вероятности отдельных значений P(ξ=x i ); для непрерывных же случайных величин на помощь приходят функция распределения и функция плотности. Функцией распределения (cumulative distribution function, cdf) случайной величины ξ ξ называется функция (x)=P(ξ⩽x). Свойства функции распределения (−∞)=0, (+∞)=1; функция F ξ F ξ неубывающая; функция F ξ F ξ непрерывна справа: lim h→0+ lim F ξ (x+h)=F ξ (x); P(a<ξ⩽b)=F ξ (b)−F ξ (a). Любая дискретная случайная величина имеет ступенчатую функцию распределения. К примеру, вот как выглядит график функции F ξ F ξ для 0.5 ) ξ∼Bin(10,0.5): Если непрерывная случайная величина ξ ξ имеет непрерывную плотность (x), то (x)−F ξ (a)= a ∫ x p ξ (t)dt, откуда следует, что (x)=p ξ (x). В типичных случаях непрерывная случайная величина имеет гладкую возрастающую функцию распределения с двумя горизонтальными асимптотами. Вот примеры графиков функций распределения гауссовских случайных величин: Медиана и мода Математическое ожидание — не единственная числовая метрика, с помощью которой можно пытаться охарактеризовать, чему равно в среднем значение случайной величины. Медиана разбивает вероятностную массу распределения на две равные части. Если случайная величина ξ ξ имеет плотность (x), то её медиана m=medξ определяется из условия P(ξ⩽m)= −∞ ∫ m p ξ (x)dx= m ∫ +∞ p ξ (x)dx=P(ξ⩾m)= 2 1 . В терминах функции распределения это означает, что (m)=1−F ξ (m), или (m)= 2 1 . В непрерывном случае",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 8,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "случайная величина имеет гладкую возрастающую функцию распределения с двумя горизонтальными асимптотами. Вот примеры графиков функций распределения гауссовских случайных величин: Медиана и мода Математическое ожидание — не единственная числовая метрика, с помощью которой можно пытаться охарактеризовать, чему равно в среднем значение случайной величины. Медиана разбивает вероятностную массу распределения на две равные части. Если случайная величина ξ ξ имеет плотность (x), то её медиана m=medξ определяется из условия P(ξ⩽m)= −∞ ∫ m p ξ (x)dx= m ∫ +∞ p ξ (x)dx=P(ξ⩾m)= 2 1 . В терминах функции распределения это означает, что (m)=1−F ξ (m), или (m)= 2 1 . В непрерывном случае функция распределения (x) строго возрастает, поэтому уравнение (m)= 2 1 имеет единственное решение. Для дискретных случайных величин это может быть не так, и поэтому в общем случае медиану определяют как число m m, удовлетворяющее условиям P(ξ⩽m)⩾ 2 1 ,P(ξ⩾m)⩾ 2 1 . Например, если ξ∼Bern( 2 1 ), то P(ξ=0)=P(ξ=1)= 2 1 , и поэтому любое число m∈(0,1) является медианой симметричного бернуллиевского распределения. Бесконечное количество медиан будет у всякой дискретной случайной величины ξ ξ, для которой (x)= 2 1 на целом промежутке. Мода распределения максимизирует его pmf или pdf: max или max mode(ξ)=arg k max P(ξ=k) или mode(ξ)=arg x max p ξ (x). Мод у распределения может быть больше одной; самое вырожденное в этом смысле распределение — равномерное, каждая точка носителя является его модой. Если плотность случайной величины имеет единственную точку максимума, то она и является модой. Например: mode(ξ)=μ, если ξ∼N(μ,σ mode(ξ)=0, если ξ∼Exp(λ); мода t-распределения Стьюдента также равна нулю. Все такие распределения унимодальны. Если плотность (x) имеет два или более максимума, то случайная величина ξ ξ называется бимодальной или мультимодальной. image1 Для симметричных распределений вроде нормального математическое ожидание, медиана и мода совпадают, однако, в общем случае это три различные меры типичного среднего значения случайной величины. Смысл каждой из этой мер наглядно демострирует следующая иллюстрация: mmm Упражнение. Найдите среднее, медиану и моду экспоненциального распределения с параметром λ λ и сравните их между собой. Классификация случайных величин У внимательного читателя (отягощённого математическим образованием впридачу) может возникнуть вопрос: а все ли случайные величины относятся к дискретным или непрерывным? В буквально такой постановке ответ, конечно, отрицательный, поскольку можно получить гибридную случайную величину, сложив дискретную и непрерывную. Но, может быть, всякая случайная величина равна сумме непрерывной и дискретной компонент? В терминах функций распределения этот вопрос можно переформулировать так: верно ли, что всякая монотонная функция F:R→[0,1] может быть представлена в виде F=F jump +F smooth , где jump — неубывающая ступенчатая функция (функция скачков), а smooth (x)= −∞ ∫ x p(t)dt — гладкая возрастающая функция, полученная интегрированием плотности? Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.2. Матричная факторизация Следующий параграф 16.4. Многомерные распределения",
    "metadata": {
      "title": "Вероятностные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.3",
      "part": 9,
      "total_parts": 9,
      "source_file": "16.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "До этого мы рассматривали только одномерные распределения вероятностей на числовой прямой. Однако ничто не мешает в качестве носителя Ω Ω выбрать пространство более высокой размерности. И снова все представляющие практический интерес распределения делятся на два класса: дискретные и непрерывные. Дискретные многомерные распределения Пусть, например, эксперимент состоит из двух фаз: сначала подбрасывается монетка, а затем кубик. Тогда вероятностная масса сосредоточена в точках ( i , j ) (i,j), i = 0 , 1 i=0,1, 1 ⩽ j ⩽ 6 1⩽j⩽6. Вероятность каждого исхода можно записать в виде таблицы «Неудача» «Успех» Результат подбрасывания монеты моделирует бернуллиевская случайная величина ξ ξ, а результат броска кубика — равномерно распределённая на множестве {1,2,3,4,5,6} случайная величина η η. Содержимое таблицы вероятностей каждого исхода можно также представить матрицей которая задаёт совместное распределение случайных величин ξ ξ и P(ξ=i,η=j)=P ij . Пару случайных величин ( ξ , η ) (ξ,η) в таком контексте называют также случайным вектором. Элементы матрицы P P не обязаны совпадать; например, монета может быть несимметричной с вероятностью «успеха» p p, и тогда таблица вероятностей примет вид «Неудача» «Успех» 1 − p 6 6 1−p 1−p 1−p 1−p 1−p 1−p p 6 6 p Контрольный вопрос. Какая таблица вероятностей соответствует эксперименту, в котором результат подбрасывания монеты «портит» кубик следующим образом: на нём могут равновероятно выпасть только значения 1 1 или 2 2 в случае «неудачи» и 4 4, 5 5 или 6 6 в случае «успеха»? В общем случае дискретное n n-мерное распределение задаётся многомерным тензором из неотрицательных чисел , суммирующихся в единицу. Такие тензоры используются для задания совместного распределения вероятностей случайного вектора ,…,ξ n ) из дискретных случайных величин: P(ξ ,…,ξ n =i n )=p Непрерывные многомерные распределения Непрерывное распределение на плоскости задаётся плотностью p(x,y)⩾0; при этом вероятность события A ⊂ R 2 A⊂R 2 равна P(A)= A ∬ p(x,y)dxdy при условии, что этот интеграл имеет смысл. Простейший пример — равномерное распределение на единичном квадрате [0,1] 2 : его плотность равна [0,1] 2 (x,y), и для P(A)= A ∬ dxdy=∣A∣ для A⊂[0,1] 2 . Именно так на единичном квадрате формально определяется геометрическая вероятность. Плотность непрерывного распределения в R n R n является неотрицательной функцией вида p(x 1 ,…,x n ) со свойством p(x 1 ,…,x n )dx 1 …dx n =1. Говорят, что случайный вектор ξ=(ξ 1 ,…,ξ n ) имеет совместную плотность ,…,x n ), если P(ξ∈A)= A ∫ p(x 1 ,…,x n )dx 1 …dx n для всех достаточно «хороших» (измеримых по Лебегу) множеств A ⊂ R n A⊂R n . Маргинальные распределения Из совместного распределения можно получить распределение в пространстве меньшей размерности путём суммирования или интегрирования по части переменных. Например, если матрица P i j P ij задаёт совместное распределение случайных величин ξ ξ и =P(ξ=i,η=j), то каждый из наборов чисел неотрицателен и суммируется в единицу: i,j ∑ P ij =1. Таким образом, числа } и } задают некоторые распределения вероятностей, называемые маргинальными. Упражнение. Найдите маргинальные распределения, если совместное распределение задано матрицей а) ( 6 1−p 6 p 6 1−p 6 p 6 1−p 6 p 6 1−p 6 p 6",
    "metadata": {
      "title": "Многомерные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.4",
      "part": 1,
      "total_parts": 5,
      "source_file": "16.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Лебегу) множеств A ⊂ R n A⊂R n . Маргинальные распределения Из совместного распределения можно получить распределение в пространстве меньшей размерности путём суммирования или интегрирования по части переменных. Например, если матрица P i j P ij задаёт совместное распределение случайных величин ξ ξ и =P(ξ=i,η=j), то каждый из наборов чисел неотрицателен и суммируется в единицу: i,j ∑ P ij =1. Таким образом, числа } и } задают некоторые распределения вероятностей, называемые маргинальными. Упражнение. Найдите маргинальные распределения, если совместное распределение задано матрицей а) ( 6 1−p 6 p 6 1−p 6 p 6 1−p 6 p 6 1−p 6 p 6 1−p 6 p 6 1−p б) ( В непрерывном случае ситуация похожая: если случайный вектор имеет совместную плотность p(x,y), то функции q(x)= −∞ ∫ ∞ p(x,y)dy,r(y)= −∞ ∫ ∞ p(x,y)dx являются плотностями маргинальных распределений. Для n n-мерных распределений можно находить маргинальные распределения, суммируя или интегрируя по любым наборам переменных с индексами 1⩽i 1 <i 2 <…<i k ⩽n; в результате получится маргинальное распределение по оставшимся n − k n−k переменным. Независимость случайных величин Случайные величины ξ ξ и η η называются независимыми, если совместное распределение случайного вектора ( ξ , η ) (ξ,η) распадается на произведение одномерных. Точнее говоря, дискретные случайные величины ξ ξ и η η независимы, если P(ξ=x i ,η=y j )=P(ξ=x i )P(η=y j ) для всех возможных непрерывные случайные величины ξ ξ и η η независимы, если их совместная плотность p(x,y)=p ξ (x)p η (y). Если случайные величины ξ ξ и η η независимы, то распределение каждой из них является маргинальным распределением их совместного распределения, поскольку P(ξ=x i )P(η=y j )=P(η=y P(ξ=x i )P(η=y j )=P(ξ=x (x)p η (y)dx=p η (y), (x)p η (y)dy=p ξ (x). Случайные величины ,…,ξ n ) независимы в совокупности, если их совместное распределение (совместная плотность) распадается в произведение одномерных распределений (плотностей). Пример. Рассмотрим n n гауссовских случайных величин ∼N(μ k ,σ k 2 ) с плотностями Совместную плотность случайного вектора ξ=(ξ 1 ,…,ξ n ) определим как произведение плотностей его компонент: ,…,x n )=p ξ 1 (x 1 )…p ξ n (x n )= (2π) n/2 k=1 Случайный вектор ξ ξ с такой плотностью имеет многомерное нормальное (гауссовское) распределение c независимыми в совокупности компонентами. Любое маргинальное распределение случайного вектора ξ ξ обладает плотностью того же вида, и поэтому также является гауссовским. Характеристики случайных векторов Математическое ожидание случайного вектора ξ=(ξ 1 ,…,ξ n ) является вектором той же размерности и вычисляется покомпонентно: Eξ=(Eξ 1 ,…,Eξ n ). Каждая компонента случайного вектора — это обычная случайная величина, и её среднее можно вычислить стандартными методами: ,…,i в дискретном случае; p(x 1 ,…,x n ),dx 1 …dx n в непрерывном случае. Математическое ожидание перестановочно с линейным преобразованием случайного вектора: E(Cξ)=CEξ, где C C — фиксированная матрица. Вместо дисперсии у случайного вектора ξ=(ξ 1 ,…,ξ n ) есть матрица ковариаций: Vξ=cov(ξ,ξ)=E(ξ−Eξ)(ξ−Eξ) T . Матрица ковариаций симметрична и состоит из попарных ковариаций компонент случайного вектора cov(ξ,ξ) ij =cov(ξ i ,ξ j ). Упражнение. Докажите, что ковариационная матрица любого случайного вектора неотрицательно определена. Если случайные величины ,…,ξ n",
    "metadata": {
      "title": "Многомерные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.4",
      "part": 2,
      "total_parts": 5,
      "source_file": "16.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Eξ=(Eξ 1 ,…,Eξ n ). Каждая компонента случайного вектора — это обычная случайная величина, и её среднее можно вычислить стандартными методами: ,…,i в дискретном случае; p(x 1 ,…,x n ),dx 1 …dx n в непрерывном случае. Математическое ожидание перестановочно с линейным преобразованием случайного вектора: E(Cξ)=CEξ, где C C — фиксированная матрица. Вместо дисперсии у случайного вектора ξ=(ξ 1 ,…,ξ n ) есть матрица ковариаций: Vξ=cov(ξ,ξ)=E(ξ−Eξ)(ξ−Eξ) T . Матрица ковариаций симметрична и состоит из попарных ковариаций компонент случайного вектора cov(ξ,ξ) ij =cov(ξ i ,ξ j ). Упражнение. Докажите, что ковариационная матрица любого случайного вектора неотрицательно определена. Если случайные величины ,…,ξ n независимы в совокупности, то cov(ξ i ,ξ j )=0, и ковариационая матрица случайного вектора ξ=(ξ 1 ,…,ξ n ) диагональна: cov(ξ,ξ)=diag{Vξ 1 ,…,Vξ n }. Например, матрица ковариации гауссовского случайного вектора ξ ξ с плотностью ,…,x n )= (2π) n/2 k=1 k=1 равна diag{σ 1 2 ,…,σ n 2 }, поскольку компоненты вектора ξ ξ независимы в совокупности и имеют нормальное распределение N(μ k ,σ k 2 ). Аналогом ковариации в многомерном случае служит матрица ковариаций между случайными векторами ξ=(ξ 1 ,…,ξ n ) и η=(η 1 ,…,η cov(ξ,η)=E(ξ−Eξ)(η−Eη) T . Матрицу ковариаций можно также вычислить по формуле cov(ξ,η)=Eξη T −Eξ(Eη) T . Упражнение. Пусть случайный вектор η η получен из случайного вектора ξ ξ линейным преобразованием: η = Cξ η=Cξ. Как связаны между собой их ковариационные матрицы? Преобразования плотностей случайных векторов Нередко приходится иметь дело не с самими случайными векторами, а с функциями от них. Но как найти плотность случайного вектора η=g(ξ), зная плотность (x)? Предположим, что g:R n →R n — гладкая обратимая функция. Тогда для измеримого A ⊂ R n A⊂R n имеем P(η∈A)=P(g(ξ)∈A)=P(ξ∈g −1 (A))= g −1 (A) ∫ p ξ (x)dx Чтобы перейти к интегралу по A A, сделаем замену переменной x=g −1 (z). По формуле замены координат в кратном интеграле получаем det (A) ∫ p ξ (x)dx= (z))∣detJ(z)∣dz, где det ⁡ J ( z ) detJ(z) – якобиан преобразования (z), т.е. определитель матрицы Якоби J(z)= ∂z ∂g −1 (z) . Таким образом, det (z)=p ξ (g −1 (z))∣detJ(z)∣. Упражнение. Пусть ξ ξ – случайный вектор с плотностью (x). Какова плотность случайного вектора η = μ + Cξ η=μ+Cξ, где μ μ – постоянный вектор, а C C – постоянная обратимая матрица? Распределение суммы независимых случайных величин В дискретном случае найти распределение суммы двух независимых случайных величин несложно. В самом деле, P(ξ+η=k)= i ∑ P(ξ+η=k,η=i)= i ∑ P(ξ=k−i,η=i). В силу независимости случайных величин ξ ξ и η η последняя сумма равна P(ξ=k−i)P(η=i). Полученная формула называется формулой свёртки. Пусть теперь – независимые непрерывные случайные величины с плотностями (x) и (x) соответственно. Сам собой напрашивается аналог формулы свёртки с плотностями вместо вероятностей, но чтобы достаточно строго вывести его и не запутаться, мы немного схитрим. А именно, мы рассмотрим случайный вектор ξ=(ξ 1 ,ξ 2 ) T и его (обратимое!) преобразование g(ξ)=( )=( 1 0 1 1 )ξ=:η=( η 1 η 2 ). Обратное к нему будет иметь вид h(η)=( 1 0 −1 1 )η=( Тогда по правилу",
    "metadata": {
      "title": "Многомерные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.4",
      "part": 3,
      "total_parts": 5,
      "source_file": "16.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i ∑ P(ξ=k−i,η=i). В силу независимости случайных величин ξ ξ и η η последняя сумма равна P(ξ=k−i)P(η=i). Полученная формула называется формулой свёртки. Пусть теперь – независимые непрерывные случайные величины с плотностями (x) и (x) соответственно. Сам собой напрашивается аналог формулы свёртки с плотностями вместо вероятностей, но чтобы достаточно строго вывести его и не запутаться, мы немного схитрим. А именно, мы рассмотрим случайный вектор ξ=(ξ 1 ,ξ 2 ) T и его (обратимое!) преобразование g(ξ)=( )=( 1 0 1 1 )ξ=:η=( η 1 η 2 ). Обратное к нему будет иметь вид h(η)=( 1 0 −1 1 )η=( Тогда по правилу преобразования плотности det (z)= =1 det( )=p где в последнем равенстве мы воспользовались независимостью . Распределение случайной величины – это маргинальное распределение, которое вычисляется следующим образом: (y)= (y−x)p ξ 2 (x)dx. Эта формула также называется формулой свёртки. Примеры многомерных распределений Рассмотрим несколько популярных распределений случайных векторов. Мультиномиальное распределение Биномиальное распределение Bin(n,p) моделирует n n-кратное подбрасывание монеты с вероятностями «успеха» p p и «неудачи» q = 1 − p q=1−p. Мультиномиальное распределение обобщает этот эксперимент: теперь подбрасывается кубик с k ⩾ 2 k⩾2 гранями, и вероятность выпадения i i-й грани равна i=1 ∑ k p i =1. Обозначим через ξ i ξ i количество выпадений i i-й грани в серии из n n бросков. Тогда случайный вектор ξ=(ξ 1 ,…,ξ k ) имеет мультиномиальное распределение, при котором P(ξ 1 =m 1 ,…,ξ !⋅…⋅m ⋅…⋅p i=1 ∑ k m i =n. При n = 1 n=1 мультиномиальное распределение превращается в категориальное, известное также под названием multinoulli. Категориальное распределение моделирует случайный выбор одного из k k классов с заданными вероятностями ,…,p k ). Многомерное нормальное распределение Многомерное нормальное (гауссовское) распределение задаётся функцией плотности det ⁡ Σ exp p(x)= (2π) n/2 detΣ 1 exp(− 2 1 (x−μ) T Σ −1 (x−μ)), где x,μ∈R n , Σ Σ — невырожденная симметричная матрица размера n × n n×n. Такое распределение обозначается N(μ,Σ). Если случайный вектор ξ∼N(μ,Σ), то E ξ = μ Eξ=μ, cov(ξ,ξ)=Σ; таким образом, параметры гауссовского распределения — это его среднее и матрица ковариаций. Упражнение. Пусть ξ∼N(μ,Σ) и η = Aξ + b η=Aξ+b. Докажите, что AΣA T ) η∼N(Aμ+b,AΣA T ). Важный частный случай случайного гауссовского вектора с независимыми компонентами был рассмотрен в примере из секции про независимость случайных величин. Такое распределение получается, если матрица Σ Σ диагональна, Σ=diag{σ 1 2 ,…,σ n 2 }. Тогда det detΣ =diag{ σ 1 2 1 ,…, σ n 2 1 }, и поэтому (x−μ) T Σ −1 (x−μ)=− 2 1 k=1 Отсюда снова получаем формулу совместной плотности (x)= (2π) n/2 k=1 которую можно переписать в виде k=1 k=1 ),ξ k ∼N(ξ k ,σ k 2 ), откуда следует независимость в совокупности компонент вектора ξ ξ. Если ковариационная матрица Σ Σ не является диагональной, то отдельные компоненты случайного вектора ξ∼N(μ,Σ) зависимы. Тем не менее, всегда найдётся линейное (и даже ортогональное) преобразование, которое превратит вектор ξ ξ в гауссовский вектор с независимыми компонентами. Для этого достаточно найти ортогональную матрицу Q Q со свойством QΣQ T =diag{σ 1 2",
    "metadata": {
      "title": "Многомерные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.4",
      "part": 4,
      "total_parts": 5,
      "source_file": "16.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "σ n 2 1 }, и поэтому (x−μ) T Σ −1 (x−μ)=− 2 1 k=1 Отсюда снова получаем формулу совместной плотности (x)= (2π) n/2 k=1 которую можно переписать в виде k=1 k=1 ),ξ k ∼N(ξ k ,σ k 2 ), откуда следует независимость в совокупности компонент вектора ξ ξ. Если ковариационная матрица Σ Σ не является диагональной, то отдельные компоненты случайного вектора ξ∼N(μ,Σ) зависимы. Тем не менее, всегда найдётся линейное (и даже ортогональное) преобразование, которое превратит вектор ξ ξ в гауссовский вектор с независимыми компонентами. Для этого достаточно найти ортогональную матрицу Q Q со свойством QΣQ T =diag{σ 1 2 ,…,σ n 2 }, и далее воспользоваться формулой плотности линейного преобразования гауссовского вектора. По тем же соображениям облако точек, сгенерированных из распределения N(μ,Σ), будет напоминать эллипсоид с полуосями, пропорциональными вектору ,…,σ n 2 ). Линии уровня плотности p ( x ) p(x) задаются уравнениями вида p(x)=C, а такое равенство эквивалентно квадратичной форме (x−μ) T Σ −1 (x−μ)=C 1 , где C C и C 1 C 1 – некоторые константы. С помощью описанной выше ортогональной замены эта квадратичная форма может быть приведена к главным осям: z=C 2 ,Λ=diag{σ 1 2 ,…,σ n 2 }; в координатах это выглядит как l=1 Мы получили практически каноническое уравнение n n-мерного эллипсоида. В R 2 R 2 это будут эллипсы, сплюснутые тем сильнее, чем дальше от единицы отношение собственных значений матрицы Σ Σ. Нормальным будет и всякое маргинальное распределение многомерного гауссовского вектора. Упражнение. Пусть случайный вектор ξ=(ξ 1 ,ξ 2 ) имеет гауссовское распределение с параметрами μ=( μ 1 μ 2 ),Σ=( где n−k ∈Mat k×k ∈Mat k×(n−k) ∈Mat (n−k)×(n−k) . Докажите, что случайный вектор ξ 1 ξ 1 , полученный маргинализацией по компонентам вектора ξ 2 ξ 2 , является гауссовским с параметрами Распределение Дирихле Распределение Дирихле сосредоточено на K K-мерном симплексе {(x 1 ,…,x K ):x 1 +…+x K =1,x i ⩾0}. Плотность распределения Дирихле Dir(α) равна p(x 1 ,…,x K )= B(α) 1 i=1 где α=(α 1 ,…,α K ) – вектор положительных параметров, а B(α)= Γ(∑ Γ(α i ) – многомерная бета-функция. Если ξ∼Dir(α), Eξ= α 0 α ,cov(ξ +1) k=1 ∑ K α k . Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.3. Вероятностные распределения Следующий параграф 16.5. Независимость и условные распределения вероятностей",
    "metadata": {
      "title": "Многомерные распределения",
      "url": "https://education.yandex.ru/handbook/ml/article/mnogomernye-raspredeleniya",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.4",
      "part": 5,
      "total_parts": 5,
      "source_file": "16.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе описываются, пожалуй, главные фичи теории вероятностей: независимые события и условные вероятности. Эти концепции имеют большое прикладное значение, да и с теоретической точки зрения главным образом благодаря им теория вероятностей выделяется в отдельную ветвь математики. Условная вероятность Условная вероятность возникает при ответе на вопрос о том, каковы шансы события A A при условии,что случилось событие B B, и обозначается P(A∣B). Пример. Согласно исследованиям, в среднем 5 % 5% пациентов испытывают приступы кашля в течение дня, однако среди курильщиков доля кашляющих составляет 40 % 40%. То есть (безусловная) вероятность P ( кашляет ) = 0.05 P(кашляет)=0.05 при добавлении обусловливания может существенно измениться: P ( кашляет ∣ курит ) = 0.4 P(кашляет∣курит)=0.4. Упражнение. Известно, что в семье два ребёнка, причём один из них мальчик. Какова вероятность, что другой ребёнок тоже мальчик? В общем случае условная вероятность P(B∣A) при P(A)  =0 полагается равной P(B∣A)= P(A) P(A∩B) . В зависимости от соотношения событий A A и B B условная вероятность P(B∣A) может принимать разные значения, например: если A ∩ B = ∅ A∩B=∅, то событие A A исключает реализацию события B B, и P(B∣A)=0; если A ⊂ B A⊂B, то событие A A гарантирует осуществление события B B, и P(B∣A)=1. Разумеется, чаще всего события A A и B B соотносятся между собой более хитрым образом, и значение условной вероятности P(B∣A) находится строго между 0 0 и 1 1. Формула полной вероятности Пусть пространство Ω Ω разбивается на попарно несовместные события ,…,B при i ≠ j . Ω=B 1 ∪…∪B n ,B i ∩B j =∅ при i  =j. Тогда A=A∩Ω=(A∩B 1 )∪…∪(A∩B n ); отсюда по свойству конечной аддитивности находим, что P(A)=P(A∩B 1 )+…+P(A∩B n ). Переходя к условным вероятностям, получаем формулу полной вероятности: P(A)= k=1 ∑ n P(A∣B k )P(B k ). Пример. Среди населения 33.7 % 33.7% имеют первую группу крови, 37.5 % 37.5% — вторую, 20.9 % 20.9% — третью, 7.9 % 7.9% — четвёртую. При переливании крови надо учитывать группы крови донора и рецепиента: реципиенту с четвёртой группой крови можно перелить кровь любой группы; реципиентам со второй и третьей группами можно перелить кровь той же группы или первой; реципиентам с первой группой крови можно перелить только кровь первой группы. С какой вероятностью допустимо переливание в случайно взятой паре донор—реципиент? Решение. Пусть событие A A состоит в том, что переливание возможно, а событие B k B k — в том, что донор имеет группу k k. По формуле полной вероятности P(A)=P(A∣B 1 )P(B 1 )+P(A∣B 2 )P(B 2 )+P(A∣B 3 )P(B 3 )+P(A∣B 4 )P(B 4 ). Вероятности P ( B k ) P(B k ) даны в условии, оттуда же находим, что P(A∣B 1 )=1, P(A∣B 2 )=P(B 2 )+P(B P(A∣B 3 )=P(B 3 )+P(B P(A∣B 4 )=P(B 4 ). Подставляя численные значения, получаем P ( A ) = 0.337 + ( 0.375 + 0.079 ) ⋅ 0.375 + ( 0.209 + 0.079 ) ⋅ 0.209 + 0.07 9 2 = 0.573683. P(A)=0.337+(0.375+0.079)⋅0.375+(0.209+0.079)⋅0.209+0.079 2 =0.573683. Упражнение. Решите предыдущий пример, выбирая в качестве разбиения набор событий C",
    "metadata": {
      "title": "Независимость и условные распределения вероятностей",
      "url": "https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.5",
      "part": 1,
      "total_parts": 5,
      "source_file": "16.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "формуле полной вероятности P(A)=P(A∣B 1 )P(B 1 )+P(A∣B 2 )P(B 2 )+P(A∣B 3 )P(B 3 )+P(A∣B 4 )P(B 4 ). Вероятности P ( B k ) P(B k ) даны в условии, оттуда же находим, что P(A∣B 1 )=1, P(A∣B 2 )=P(B 2 )+P(B P(A∣B 3 )=P(B 3 )+P(B P(A∣B 4 )=P(B 4 ). Подставляя численные значения, получаем P ( A ) = 0.337 + ( 0.375 + 0.079 ) ⋅ 0.375 + ( 0.209 + 0.079 ) ⋅ 0.209 + 0.07 9 2 = 0.573683. P(A)=0.337+(0.375+0.079)⋅0.375+(0.209+0.079)⋅0.209+0.079 2 =0.573683. Упражнение. Решите предыдущий пример, выбирая в качестве разбиения набор событий C k C k , каждое из которых заключается в том, что реципиент имеет группу k k. Формула полной вероятности легко обобщается на случай счётного числа попарно несовместных событий B k B k , а также на случай обусловливания по некоторому событию C C, например: P(A∣C)= n ∑ P(A∣B n ,C)P(B n ∣C). Формула Байеса Заметим, что вероятность P(A∩B) можно записать двумя способами P(B∣A)P(A)=P(A∩B)=P(A∣B)P(B). Оставим P(B∣A) в левой части и получим формулу Байеса. Формула Байеса. Для любых событий A A, B B c положительной вероятностью P(B∣A)= P(A) P(A∣B)P(B) . Для вычисления знаменателя в формуле Байеса часто используется формула полной вероятности. Упражнение. Среди определенной группы людей вероятность некоторой болезни 0.02. Тест, позволяющий выявить болезнь, несовершенен. На больном он дает позитивный результат в 98 случаях из 100, и, кроме того, он дает позитивный результат в 4 случаях из 100 на здоровом. Найдите вероятность того, что человек, на котором тест дал положительный результат, действительно болен. Для непрерывного случая тоже есть своя формула полной вероятности, см. раздел про условную вероятность. Независимые события События A A и B B называются независимыми, если P(A∣B)=P(A), то есть информация о реализации события B B никак не влияет на вероятность события A A. По определению условной вероятности независимость событий A A и B B эквивалентна тому, что P(A∩B)=P(A)P(B). Последнее равенство годится для определения независмости событий A A и B B даже в том случае, если P(A)=0 или P(B)=0. Пример. В полной колоде карт находится 52 52 карты: 4 4 масти от двойки до туза. Вероятность вытащить туза равна P(Ace)= 52 4 = 13 1 , карту пиковой масти — P(♠)= 52 13 = 4 1 . Эти события независимы, поскольку в пересечении этих событий лежит ровно одна карта — туз пик, вероятность появления которого равна =P(Ace)P(♠). Пусть теперь вытаскивается сразу две карты. Зависимы ли события «вытащены две карты пиковой масти» и «вытащены туз и король»? Посчитаем: P(♠♠)= 52⋅51 13⋅12 663 . P(AK)= 52⋅51 32 = 663 8 . Вероятность вытащить туза и короля пик равна 1326 ≈ 0.00075 1326 1 ≈0.00075, что отличается от 11271 ≈ 0.00071 P(♠♠)P(AK)= 11271 8 ≈0.00071. Таким образом, эти события зависимы. События ,…,A n попарно независимы, если P(A i ∩A j )=P(A i )P(A j ) при i ≠ j i  =j. Эти же события независимы в совокупности, если P(A i 1 ∩…∩A i m )= k=1 ∏ m P(A i k ) для любого набора индексов для любого набора индексов 1⩽i 1 <…<i",
    "metadata": {
      "title": "Независимость и условные распределения вероятностей",
      "url": "https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.5",
      "part": 2,
      "total_parts": 5,
      "source_file": "16.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и «вытащены туз и король»? Посчитаем: P(♠♠)= 52⋅51 13⋅12 663 . P(AK)= 52⋅51 32 = 663 8 . Вероятность вытащить туза и короля пик равна 1326 ≈ 0.00075 1326 1 ≈0.00075, что отличается от 11271 ≈ 0.00071 P(♠♠)P(AK)= 11271 8 ≈0.00071. Таким образом, эти события зависимы. События ,…,A n попарно независимы, если P(A i ∩A j )=P(A i )P(A j ) при i ≠ j i  =j. Эти же события независимы в совокупности, если P(A i 1 ∩…∩A i m )= k=1 ∏ m P(A i k ) для любого набора индексов для любого набора индексов 1⩽i 1 <…<i m ⩽n. Упражнение. Приведите пример попарно независимых событий , не являющихся независимыми в совокупности. Определение независимости случайных величин из предыдущего параграфа полностью согласуется с только что введённым определением независимых событий. Например, для случая дискретных случайных величин ξ ξ и η η обозначим =P(ξ=x i ),B j =P(η=y j ); тогда P(ξ=x i ,η=y j )=P(A i ∩B j ), и поэтому независимость случайных величин ξ ξ и η η эквивалентна независимости событий для всевозможных значений i i и j j. Условная независимость Бывает так, что зависимые события A A и B B становятся независимыми при выполнении некоторого третьего события C C. Более формально, события A A и B B условно независимы по отношению к событию C C, если P(C)>0 и P(A∣B,C)=P(A∣C). Поскольку P(A∣B,C)= P(B∩C) P(A∩B∩C) ,P(A∣C)= P(C) P(A∩C) , то условная независимость событий A A и B B эквивалетна равенству P(C) P(A∩B∩C) = P(C) P(A∩C) ⋅ P(C) P(B∩C) , а это, в свою очередь, означает, что P(A∩B∣C)=P(A∣C)P(B∣C). Таким образом, вероятность произведения условно независимых событий равна произведению условных вероятностей. Эта формула полностью аналогична формуле P(A∩B)=P(A)P(B) для (безусловно) независимых событий. Пример (цепь Маркова). Последовательность событий ,…,S t ,… называется марковской цепью, если выполняется марковское свойство P(S t+1 ∣S t ,S t−1 ,…,S 0 )=P(S t+1 ∣S t ),t∈N∪{0}. В марковском свойстве заложен следующий смысл: в каждый момент времени t t «будущее» S t + 1 S t+1 зависит только от «настоящего» S t S t , но не зависит от «прошлого» t−1 ∩…∩S 0 . Итак, цепь Маркова характеризуется равенством P(S t+1 ∣P t ,S t )=P(S t+1 ∣S t ), которое означает, что события S t + 1 S t+1 и P t P t условно независимы по отношению к событию S t S t . Условные распределения Пусть ξ ξ и η η — дискретные случайные величины и P(η=y)>0. По аналогии с условными вероятностями условное распределение случайной величины ξ ξ при условии, что значение случайной величины η η равно y y, определяется по формуле P(ξ=x i ∣η=y)= P(η=y) P(ξ=x i ,η=y) . Это действительно распределение вероятностей, поскольку P(ξ=x i ∣η=y)⩾0 и P(ξ=x i ∣η=y)= P(η=y) 1 i ∑ P(ξ=x i ,η=y)=1. В непрерывном случае условное распределение задаётся условной плотностью ξ∣η (x∣y)= p η (y) p(x,y) , где p(x,y) — совместная плотность случайных величин ξ ξ и η η. И снова проведением маргинализации по x x убеждаемся в том, что с нормировкой всё в порядке: ξ∣η (x∣y)dx= p η (y) 1",
    "metadata": {
      "title": "Независимость и условные распределения вероятностей",
      "url": "https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.5",
      "part": 3,
      "total_parts": 5,
      "source_file": "16.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "аналогии с условными вероятностями условное распределение случайной величины ξ ξ при условии, что значение случайной величины η η равно y y, определяется по формуле P(ξ=x i ∣η=y)= P(η=y) P(ξ=x i ,η=y) . Это действительно распределение вероятностей, поскольку P(ξ=x i ∣η=y)⩾0 и P(ξ=x i ∣η=y)= P(η=y) 1 i ∑ P(ξ=x i ,η=y)=1. В непрерывном случае условное распределение задаётся условной плотностью ξ∣η (x∣y)= p η (y) p(x,y) , где p(x,y) — совместная плотность случайных величин ξ ξ и η η. И снова проведением маргинализации по x x убеждаемся в том, что с нормировкой всё в порядке: ξ∣η (x∣y)dx= p η (y) 1 −∞ ∫ +∞ p(x,y)dx= p η (y) p η (y) =1. Поскольку p(x,y)dy=p ξ (x), из формулы условной плотности получаем непрерывный аналог формулы полной вероятности: (x)= R ∫ p ξ∣η (x∣y)p η (y)dy. Пример. Выберем случайное число x∈[ 2 1 ,1], а затем — случайное число y∈[0,x]. Как распределена случайная величина y y? Переформулируем задачу: известно, что ξ∼U[ 2 1 ,1] и η∣ξ∼U[0,x]. Требуется найти плотность случайной величины η η. Имеем (x)=2I [ 2 1 ,1] (x),p η∣ξ (y∣x)= x 1 I [0,x] (y). Применяя формулу полной вероятности, находим (y)= 1/2 ∫ 1 x 2 I[y⩽x]dx={ 2ln2, −2lny, 0⩽y< 2 1 , 2 1 ⩽y⩽1. Упражнение. Пусть случайные величины ∼Exp(λ k=1,…,n, независимы в совокупности. Чему равна вероятность P ( ξ k = min P(ξ k =min{ξ 1 ,…,ξ n })? Условные математические ожидания Условное математическое ожидание E(ξ∣η=y) отвечает на вопрос «чему равно среднее значение случайной величины ξ ξ при условии, что η = y η=y?». Имея в распоряжении матрицу условного дискретного распределения P(ξ=x i ∣η=y j ) или условную плотность ξ∣η (x∣y), условное математическое ожидание можно вычислить следующим образом: E(ξ∣η)≡E(ξ∣η=y)= i ∑ x i P(ξ=x i ∣η=y) в дискретном случае; E(ξ∣η)≡E(ξ∣η=y)= R ∫ xp ξ∣η (x∣y)dx для непрерывных ξ ξ и η η. Важно отметить, что после суммирования или интегрирования по переменной x x в формуле условного математического ожидания остаются зависимость от y y. Таким образом, в отличие от обычного среднего, которое является просто числом, условное ожидание представляет собой случайную величину ζ=E(ξ∣η=y), поскольку его значение зависит от случайного значения η = y η=y. Свойства условного математического ожидания E(aξ 1 +bξ 2 ∣η)=aE(ξ 1 ∣η)+bE(ξ 2 ∣η) (линейность). Если , то E(ξ 1 ∣η)⩽E(ξ 2 ∣η) (монотонность). Если случайные величины ξ ξ и η η независимы, то E(ξ∣η)=Eξ. E(g(η)ξ∣η)=g(η)E(ξ∣η). E(E(ξ∣η))=Eξ (law of total expectation). Упражнение. Prove the law of total expectation. Условная дисперсия определяется по формуле V(ξ∣η)=E((ξ−E(ξ∣η)) 2 ∣η)=E(ξ 2 ∣η)−(E(ξ∣η)) 2 . Справедливо равенство Vξ=E(V(ξ∣η))+V(E(ξ∣η)) (law of total variance). Регрессия В машинном обучении часто встречается задача регрессии, в которой требуется восстановить зависимость Y=f(X) при наличии выборки ),…,(X n ,Y n ) из некоторого неизвестного распределения с совместной плотностью p(x,y). Стандартный способ решения задачи регресии — минимизация среднего значения функции потерь L(Y,f(X)): min ⁡ . E[L(Y,f(X))]= R 2 ∬ L(y,f(x))p(x,y)dxdy→min. В качестве функции потерь на одном объекте ( x , y ) (x,y) в задаче регрессии обычно выбирают квадратичную функцию: L(y,f(x))=(y−f(x)) 2 . Тогда E[L(Y,f(X))]= R 2 ∬ (y−f(x)) 2 p(x,y)dxdy;",
    "metadata": {
      "title": "Независимость и условные распределения вероятностей",
      "url": "https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.5",
      "part": 4,
      "total_parts": 5,
      "source_file": "16.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Условная дисперсия определяется по формуле V(ξ∣η)=E((ξ−E(ξ∣η)) 2 ∣η)=E(ξ 2 ∣η)−(E(ξ∣η)) 2 . Справедливо равенство Vξ=E(V(ξ∣η))+V(E(ξ∣η)) (law of total variance). Регрессия В машинном обучении часто встречается задача регрессии, в которой требуется восстановить зависимость Y=f(X) при наличии выборки ),…,(X n ,Y n ) из некоторого неизвестного распределения с совместной плотностью p(x,y). Стандартный способ решения задачи регресии — минимизация среднего значения функции потерь L(Y,f(X)): min ⁡ . E[L(Y,f(X))]= R 2 ∬ L(y,f(x))p(x,y)dxdy→min. В качестве функции потерь на одном объекте ( x , y ) (x,y) в задаче регрессии обычно выбирают квадратичную функцию: L(y,f(x))=(y−f(x)) 2 . Тогда E[L(Y,f(X))]= R 2 ∬ (y−f(x)) 2 p(x,y)dxdy; для минимизации этого функционала применим немножко вариационного исчисления и продифференцируем по функции f ( x ) f(x). Получим (f(x)−y)p(x,y)dxdy=0, откуда f(x)= p(x) 1 −∞ ∫ +∞ yp(x,y)dy= −∞ ∫ +∞ yp Y∣X (y∣x)dy=E(Y∣X=x). Полученное условное математическое ожидание, называемое функцией регрессии, показывает, чему в среднем равно значение зависимой переменной Y Y при условии, что X = x X=x. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.4. Многомерные распределения Следующий параграф 16.6. Параметрические оценки",
    "metadata": {
      "title": "Независимость и условные распределения вероятностей",
      "url": "https://education.yandex.ru/handbook/ml/article/nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.5",
      "part": 5,
      "total_parts": 5,
      "source_file": "16.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Различные типы распределений, описанные в предыдущих параграфах, применяются в качестве теоретических моделей в задачах, связанных со случайностью и неопределённостью. Однако на практике далеко не всегда ясно, какое именно распределение моделирует имеющиеся в наличии данные. А если из каких-либо соображений тип распределения всё же установлен, то следующая задача — оценить параметры этого распределения, например, среднее и/или дисперсию в случае гауссовского распределения N(μ,σ 2 ). Подобными обратными по отношению к теории вероятностей задачами занимается математическая статистика. Типичный пример статистической задачи: по числовой выборке ,…,X n оценить параметры распределения, из которого они были получены. Обычно предполагается, что выборка i.i.d. (independent and identically distributed), то есть представляет собой независимые реализации случайной величины с одним и тем же распределением. Параметр этого определения θ θ может быть числом или вектором; оценку этого параметра по выборке ,…,X n обычно обозначают ,…,X n ) или просто θ ^ θ . Предельные теоремы Как правило, чем больше размер выборки, тем более информативны параметрические оценки вида ,…,X n ). Теоретические свойства таких оценок при n → ∞ n→∞ устанавливаются с помощью предельных теорем теории вероятностей. Закон больших чисел Внимательный читатель мог обратить внимание, что в ряде примеров из предыдущих параграфов параметры некоторых распределений почему-то молчаливо подменялись средними значениями. Так мы поступили в задаче о показе рекламы, взяв в качестве параметра пуассоновского распределение среднее количество кликов пользователей. Фактически мы оценили неизвестный параметра λ λ средним по выборке: k=1 ∑ n X k . В общем-то это кажется логичным, поскольку λ = E ξ λ=Eξ, если ξ∼Pois(λ). Однако у такой оценки есть также мощное теоретическое обоснование. Теорема (Закон больших чисел, ЗБЧ). Пусть ,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ μ. Тогда для любого ε > 0 ε>0 lim где n→∞ lim P(∣ X n −μ∣>ε)=0, где X n = n 1 k=1 ∑ n X k . Таким образом, чем больше размер выборки n n, тем менее вероятно отклонение выборочного среднего X ‾ n X n от истинного среднего μ μ на любое число ε > 0 ε>0. Закон больших чисел особенно легко обосновать для случая конечных дисперсий: <+∞. Имеем k=1 ∑ n EX k =μ,V k=1 Отсюда видно, что lim n→∞ lim V X n =0, поэтому при больших n n распределение случайной величины всё больше похоже на распределение, сосредоточенное в одной лишь точке μ μ. Формально же утверждение ЗБЧ получается с помощью неравенства Чебышева: P(∣ X n −μ∣>ε)⩽ →0,n→∞. Закон больших чисел допускает следующее усиление. Теорема (Усиленный закон больших чисел, УЗБЧ). Пусть ,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ μ. Тогда выборочное среднее X ‾ n X n почти наверное сходится к μ μ, т.е. P ( lim n→∞ lim X n =μ)=1. Теорема Муавра-Лапласа Доска Гальтона иллюстрирует биномиальное распределение. До поворота на ее дне лежит множество маленьких шариков. Сразу после переворота шарики проходят через 10 рядов гладких круглых препятствий. Преодоление каждого препятствия можно рассматривать как испытание Бернулли: с равными вероятностями шарик может пойти как налево, так и направо. Поэтому финальное положение шарика в одной из",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 1,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "следующее усиление. Теорема (Усиленный закон больших чисел, УЗБЧ). Пусть ,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ μ. Тогда выборочное среднее X ‾ n X n почти наверное сходится к μ μ, т.е. P ( lim n→∞ lim X n =μ)=1. Теорема Муавра-Лапласа Доска Гальтона иллюстрирует биномиальное распределение. До поворота на ее дне лежит множество маленьких шариков. Сразу после переворота шарики проходят через 10 рядов гладких круглых препятствий. Преодоление каждого препятствия можно рассматривать как испытание Бернулли: с равными вероятностями шарик может пойти как налево, так и направо. Поэтому финальное положение шарика в одной из 10 корзин является приблизительной реализацией биномиального распределения 0.5 ) Bin(10,0.5). Уже при n = 10 n=10 биномиальное распределение напоминает нормальное. И действительно, чем больше n n, тем лучше дискретная случайная величина ξ∼Bin(n,p) аппроксимируется непрерывной гауссианой N(np,np(1−p)). Теорема Муавра-Лапласа. Пусть ξ∼Bin(n,p), q = 1 − p q=1−p, тогда lim n→∞ lim P(a< npq ξ−np ⩽b)= dx. Из теоремы Муавра-Лапласа вытекает, что при больших n n вероятность попадания биномиальной случайной величины ξ∼Bin(n,p) в заданный интервал можно оценить как P(A<ξ⩽B)≈Φ( npq B−np )−Φ( npq A−np ). где Φ ( z ) Φ(z) — функция распределения стандартного нормального распределения. Центральная предельная теорема При выводе закона больших чисел мы видели, что выборочное среднее X ‾ n X n имеет среднее μ μ и дисперсию . Но как именно выглядит распределение случайной величины X ‾ n X n при увеличении n n? Оказывается, что оно становится всё больше похоже на N(μ, n σ 2 ). Вот как, например, выглядят нормализованные гистограммы 5000 5000 выборочных средних, построенных по i.i.d. выборкам 0.3 ) X 1 ,…,X n ∼Bin(30,0.3) для разных значений n n: Эти гистограммы и впрямь очень напоминают гауссианы, и это прямое следствие следующей теоремы. Центральная предельная теорема, ЦПТ. Пусть ,… – последовательность попарно независимых одинаково распределенных случайных величин с конечным математическим ожиданием μ μ и дисперсией σ 2 σ 2 . Тогда при −μ) ≈N(0,1) при n≫1. Точнее говоря, lim n→∞ lim P(Z n ⩽z)=Φ(z). Таким образом, случайная величина Z n Z n сходится по распределению к N(0,1): N(0,1). Если применить центральную предельную теорему к бернуллиевским случайным величинам с вероятностью успеха p p, то вновь получим теорему Муавра-Лапласа. Свойства параметрических оценок Оценивать параметры можно по-разному, хочется делать это хорошо. Ценные свойства оценок, которые обычно желательны – это несмещенность и состоятельность. Несмещённость Каждый элемент i.i.d выборки ,…,X n можно рассматривать как значение случайной величины из некоторого распределения с неизвестным параметром θ θ. А раз так, то всякую оценку этого параметра ,…,X n ) также можно считать случайной величиной, у которой можно пытаться вычислять математическое ожидание, например. Оценка ,…,X n ) параметра θ θ называется несмещенной, если =θ. Несмещённость оценки означает, что она в среднем будет равна истинному значению параметра. Интуитивно можно представлять себе несмещённость следующим образом: если мы нагенерим большое количество выборок (i) ,X 2 (i) ,…,X n (i) 1⩽i⩽N, и для каждой посчитаем оценку (i) , то в среднем получится более или менее истинное значение параметра i=1 ∑ N θ (i) ≈θ. Простейший пример несмещённой оценки",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 2,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "с неизвестным параметром θ θ. А раз так, то всякую оценку этого параметра ,…,X n ) также можно считать случайной величиной, у которой можно пытаться вычислять математическое ожидание, например. Оценка ,…,X n ) параметра θ θ называется несмещенной, если =θ. Несмещённость оценки означает, что она в среднем будет равна истинному значению параметра. Интуитивно можно представлять себе несмещённость следующим образом: если мы нагенерим большое количество выборок (i) ,X 2 (i) ,…,X n (i) 1⩽i⩽N, и для каждой посчитаем оценку (i) , то в среднем получится более или менее истинное значение параметра i=1 ∑ N θ (i) ≈θ. Простейший пример несмещённой оценки среднего значения θ θ даёт выборочное среднее , поскольку k=1 ⋅nθ=θ. Медианой выборки ,…,X n называется средний член вариационного ряда, состоящего из отсортированных по возрастанию элементов выборки: (1) ⩽X (2) ⩽…⩽X (n) . Если n n нечётно, n=2m+1, то есть ровно один элемент в середине вариационного ряда, именно он называется медианой: med(X 1 ,…,X n )=X (m) =X ( 2 n+1 ) . При чётном n = 2 m n=2m в качестве медианы берут среднее двух центральных элементов вариационного ряда: med(X 1 ,…,X n )= 2 1 (X (m) +X (m+1) +1) ). Упражнение. Дана i.i.d. выборка ,…,X n из равномерного распределения U[0,2θ]. Докажите, что выборочная медиана даёт несмещённую оценку медианы распределения U[0,2θ]. В некоторых случаях оценка ,…,X n ) смещена, но с ростом n n это смещение нивелируется. Если lim n→∞ lim E θ n =θ, то оценка θ ^ n θ n называется асимптотически несмещённой. Упражнение. Пусть ,…,X n ∼U[0,θ] — i.i.d. выборка. Оценим параметр θ θ как максимальное значение выборки: max (n) =max{X 1 ,…,X n }. Является ли эта оценка несмещённой? Асимптотически несмещённой? Состоятельность Оценка ,…,X n ) называется состоятельной, если она сходится по вероятности к θ, то есть lim для любого ε > 0. n→∞ lim P(∣ θ n −θ∣>ε)=0 для любого ε>0. Cостоятельность означает, что с ростом размера выборки всё менее вероятны хоть сколько нибудь значимые отклонения оценки от истинного значения параметра. Если i.i.d. выборка ,…,X n получена из распределения с конечным математическим ожиданием θ θ, то в силу закона больших чисел выборочное среднее X ‾ n X n является состоятельной оценкой для θ θ. Состоятельность оценки – независимое от несмещенности свойство: оценки могут быть состоятельными, но не несмещенными и наоборот. Например, оценка (n) из предыдущего упражнения оказалась смещённой, однако, она состоятельна: P(∣X (n) −θ∣>ε)=P(X (n) <θ+ε)= θ−ε ) n =(1− θ ε ) n →0,n→∞. Упражнение. Приведите пример несмещённой оценки, не являющейся состоятельной. Имея i.i.d. выборку ,…,X n из невырожденного распределения с конечным средним θ θ, оценим это среднее как . Эта оценка, очевидно, несмещённая: =EX 1 =θ. Состоятельной, однако, она не является, ведь выражение P(∣ θ −θ∣>ε)=P(∣X 1 −θ∣>ε) никоим образом не зависит от n n. Следовательно, состоятельность оценки θ ^ θ означала бы, что P(∣X 1 −θ∣>ε)=0 для любого ε > 0 ε>0. Такое возможно только для вырожденного распределения, сосредоточенного в одной лишь точке P(X 1 =θ)=1. Bias-variance decomposition Смещение (bias) оценки ,…,X n ) определяется как bias( θ )=E θ",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 3,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n →0,n→∞. Упражнение. Приведите пример несмещённой оценки, не являющейся состоятельной. Имея i.i.d. выборку ,…,X n из невырожденного распределения с конечным средним θ θ, оценим это среднее как . Эта оценка, очевидно, несмещённая: =EX 1 =θ. Состоятельной, однако, она не является, ведь выражение P(∣ θ −θ∣>ε)=P(∣X 1 −θ∣>ε) никоим образом не зависит от n n. Следовательно, состоятельность оценки θ ^ θ означала бы, что P(∣X 1 −θ∣>ε)=0 для любого ε > 0 ε>0. Такое возможно только для вырожденного распределения, сосредоточенного в одной лишь точке P(X 1 =θ)=1. Bias-variance decomposition Смещение (bias) оценки ,…,X n ) определяется как bias( θ )=E θ −θ. Смещение показывает, насколько оценка в среднем отклоняется от истинного значения. Оценка θ ^ n θ n несмещённая, если bias( θ n )=0; асимптотически несмещённая, если lim n→∞ lim bias( θ n )=0. Среднеквадратичной ошибкой (mean squared error, MSE) оценки называется величина MSE( θ )=E( θ −θ) 2 . Смещение, дисперсия и среднеквадратичная ошибка связаны между собой следующим соотношением (bias-variance decomposition): bias MSE( θ )=bias 2 ( θ )+V( θ ). Упражнение. Докажите, что оценка θ ^ n θ n состоятельная, если она асимптотически несмещённая и lim n→∞ lim V( θ n )=0. Таким образом, если lim n→∞ lim MSE( θ n )=0, то оценка θ ^ n θ n параметра θ θ асимптотически несмещённая и состоятельная. Асимптотическая нормальность Стандартным отклонением оценки θ ^ n θ n параметра θ θ называется корень из дисперсии: se( Оценка θ ^ n θ n асимптотически нормальна, если se( N(0,1), т.е. lim n→∞ lim P( se( ⩽z)=Φ(z). Согласно центральной предельной теореме выборочное среднее i.i.d. выборки из распределения с конечными средним μ μ и дисперсией σ 2 σ 2 является асимптотически нормальной оценкой параметра μ μ. Эффективность Пусть — несмещённые оценки параметра θ θ. Оценка θ ^ θ эффективнее оценки θ ~ θ ~ , если . Такое определение эффективности вполне логично, ведь чем меньше дисперсия несмещённой оценки, тем меньше у неё шансов удалиться куда-то далеко от истинного значения параметра. Пример. Пусть ,…,X n — i.i.d. выборка из распределения U[0,2θ]. Какая оценка параметра θ θ эффективнее: выборочное среднее или медиана? Несмещённость оценок =med(X 1 ,…,X n ) уже была показана выше. Найдём дисперсию наших оценок. Диспресия случайной величины ξ∼U[0,2θ] равна Vξ= 3 θ 2 , следовательно, Найти дисперсию медианы несколько сложнее. Ограничимся случаем n=2m+1. Тогда (m+1) , и =EX (m+1) = 2θ 1 (m!) 2 (2m+1)! (1− 2θ x ) m dx. С помощью замены отсюда находим, что (m!) 2 (2m+1)! m+2 (1−t) dt=4θ 2 (m!) 2 (2m+1)! B(m+3,m+1)= =4θ 2 (m!) 2 (2m+1)! (2m+3)! (m+2)!m! =2θ 2 2m+3 m+2 =θ 2 + n+3 θ 2 . Следовательно, n+3 θ 2 , что при n > 1 n>1 больше, чем , так что выборочное среднее эффективнее медианы (примерно в 3 3 раз при больших n n, если считать по отношению стандартных отклонений). Несмотря на то что в плане эффективности среднее оказалось предпочтительнее в этом примере, в статистике медиану любят за бОльшую устойчивость к выбросам. Ниже приведён scatter-plot, по которому можно наглядно оценить меру разброса среднего",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 4,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "отсюда находим, что (m!) 2 (2m+1)! m+2 (1−t) dt=4θ 2 (m!) 2 (2m+1)! B(m+3,m+1)= =4θ 2 (m!) 2 (2m+1)! (2m+3)! (m+2)!m! =2θ 2 2m+3 m+2 =θ 2 + n+3 θ 2 . Следовательно, n+3 θ 2 , что при n > 1 n>1 больше, чем , так что выборочное среднее эффективнее медианы (примерно в 3 3 раз при больших n n, если считать по отношению стандартных отклонений). Несмотря на то что в плане эффективности среднее оказалось предпочтительнее в этом примере, в статистике медиану любят за бОльшую устойчивость к выбросам. Ниже приведён scatter-plot, по которому можно наглядно оценить меру разброса среднего и медианы выборки из равномерного распределения на отрезке [0,2θ] для θ = 5 θ=5. Для построения этого графика были взяты 200 200 i.i.d. выборок из U[0,10] размера n = 10 , 100 , 1000 , 10000 n=10,100,1000,10000, и для каждого n n посчитаны выборочное среднее и медиана. Эти статистики и задают координаты точки на графике. Разумеется, чем больше значение n n, тем кучнее локализованы точки вокруг среднего значения θ = 5 θ=5, совпадающего в данном случае с медианой. Как видно, облако точек сосредоточено вдоль прямой y=θ+ 3 (x−θ). Выборочная дисперсия Как мы уже убедились, выборочное среднее k=1 ∑ n X k представляет собой несмещённую и состоятельную оценку для математического ожидания. Можно ли то же самое сказать про выборочную дисперсию k=1 в предположении, что i.i.d. выборка ,…,X n состоит из реализаций случайной величины ξ ξ с конечными моментами E ξ = θ 1 Eξ=θ Прежде всего раскроем скобки и перепишем S ‾ n S n в виде k=1 ∑ n (X k 2 −2X k=1 ∑ n X k 2 −2( где k=1 ∑ n X k 2 — выборочное среднее, построенное по выборке ,…,X n 2 . Оно несмещённое, поэтому . Заметим также, что k=1 k=1 1⩽i<j⩽n откуда в силу независимости при i ≠ j i  =j получаем 1⩽i<j⩽n n−1 θ 1 2 . Итак, n−1 θ 1 2 = n n−1 Vξ. Таким образом, оценка дисперсии S ‾ n S n смещённая (хотя и асимптотически несмещённая). По этой причине для оценки дисперсии часто используют аналогичную несмещённую оценку n−1 n S n = n−1 1 k=1 которую также называют выборочной дисперсией. Обоснуем теперь состоятельность оценки . Согласно закону больших чисел . Здесь нам потребуется пара свойств сходимости по вероятности. Упражнение. Пусть η. Докажите, что ξ+η. Упражнение. Пусть ξ. Докажите, что Пользуясь результатами этих упражнений, заключаем, что =Vξ, и, стало быть, оценка S ‾ n S n состоятельна. Методы оценки параметров До этого мы обсуждали разные приятные свойства оценок, а теперь рассмотрим некоторые методы, позволяющие систематически получать по выборке оценки параметров с нужными свойствами. Метод моментов Пусть выборка ,…,X n получена сэмплированием из некоторого семейства распределений (x) с параметрами θ=(θ 1 ,…,θ m ). Метод моментов для оценки этих параметров заключается в приравнивании выборочных моментов j=1 ∑ n X j k к теоретическим (θ)= (x). Решая полученную систему уравнений (θ)= 1⩽k⩽m, находим оценки параметров Пример. Оценим параметры нормального распределения N(μ,σ 2 ) с помощью метода моментов. Упражнение. Оцените по методу",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 5,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "быть, оценка S ‾ n S n состоятельна. Методы оценки параметров До этого мы обсуждали разные приятные свойства оценок, а теперь рассмотрим некоторые методы, позволяющие систематически получать по выборке оценки параметров с нужными свойствами. Метод моментов Пусть выборка ,…,X n получена сэмплированием из некоторого семейства распределений (x) с параметрами θ=(θ 1 ,…,θ m ). Метод моментов для оценки этих параметров заключается в приравнивании выборочных моментов j=1 ∑ n X j k к теоретическим (θ)= (x). Решая полученную систему уравнений (θ)= 1⩽k⩽m, находим оценки параметров Пример. Оценим параметры нормального распределения N(μ,σ 2 ) с помощью метода моментов. Упражнение. Оцените по методу моментов параметры a a и b b для выборки ,…,X U[a,b]. При некоторых условиях на регулярность семейства распределений (x) оценка по методу моментов получается состоятельной и асимптотически нормальной. Метод максимального правдоподбия Пусть, как обычно, выборка ,…,X n ∼F θ (x). Правдоподобие (функция правдоподобия, likelihood) выборки ,…,…X n — это просто её совместная pmf или pdf. Вне зависимости от типа распределения будем обозначать правдоподобие как L(θ)≡L(X 1 ,…,X n ∣θ)=p(X 1 ,…,X n ∣θ). Если выборка i.i.d., то функция правдоподобия распадается в произведение одномерных функций: L(X 1 ,…,X n ∣θ)= k=1 ∏ n p(X k ∣θ). Оценка максимального правдоподобия (maximum likelihood estimation, MLE) максимизирует правдоподобие: θ ^ M L = arg ⁡ max =arg θ max L(θ) Поскольку максимизировать сумму проще, чем произведение, обычно переходят к логарифму правдоподобия (log-likelihood). Это особенно удобно в случае i.i.d. выборки, тогда θ ^ M L = arg ⁡ max ⁡ θ log arg ⁡ max log =arg θ max logL(θ)=arg θ max k=1 ∑ n logp(X k ∣θ). Пример. В результате n n подбрасываний монеты выпало k k «орлов» и n − k n−k «решек». Оценим вероятность выпадения «орла» методом максимального правдоподобия. Пусть p p — вероятность выпадения «орла», тогда правдоподобие равно L(p)=p k (1−p) n−k . Дифференцируя логарифм правдоподобия log log log logL(p)=klogp+(n−k)log(1−p) и приравнивая к нулю производную, находим 1−p n−k ⟺k(1−p)=(n−k)p⟺p= n k . Нетрудно убедиться, что это точка максимума. Итак, оценка максимального правдоподобия вероятности «успеха» в схеме Бернулли вполне ожидаемо оказалась равна доле «успехов» в серии из n n испытаний. Упражнение. Пусть i.i.d. выборка ,…,X n взята из пуассоновского распределения с параметром λ λ. Найдите его оценку максимального правдоподобия. Методом максимального правдоподобия можно оценить сразу несколько параметров. Пример. Найдём MLE-оценки параметров распределения N(μ,τ) по i.i.d. выборке ,…,X n . Запишем правдоподобие: exp L(μ,τ)= k=1 ∏ n 2πτ 1 exp 2τ −(X k −μ) 2 . Перейдём к log-likelihood: log log logL(μ,τ)=− 2 n (logτ+ln2π)− 2τ 1 k=1 ∑ n (X k −μ) 2 . Приравняем частные производные по μ μ и τ τ к нулю: ∂ log ∂logL = τ 1 k=1 ∑ N (X k −μ)=0, ∂ log ∂logL k=1 ∑ n (X k −μ) 2 =0, откуда – выборочное среднее, k=1 – выборочная дисперсия. Упражнение. Пусть i.i.d. выборка ,…,X n ∼U[a,b]. Найдите оценки максимального правдоподобия для параметров a a и b b. Свойства оценки максимального правдоподобия состоятельность: инвариантность относительно параметризации: если — MLE-оценка для θ θ, то ) — MLE-оценка",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 6,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "−μ) 2 . Перейдём к log-likelihood: log log logL(μ,τ)=− 2 n (logτ+ln2π)− 2τ 1 k=1 ∑ n (X k −μ) 2 . Приравняем частные производные по μ μ и τ τ к нулю: ∂ log ∂logL = τ 1 k=1 ∑ N (X k −μ)=0, ∂ log ∂logL k=1 ∑ n (X k −μ) 2 =0, откуда – выборочное среднее, k=1 – выборочная дисперсия. Упражнение. Пусть i.i.d. выборка ,…,X n ∼U[a,b]. Найдите оценки максимального правдоподобия для параметров a a и b b. Свойства оценки максимального правдоподобия состоятельность: инвариантность относительно параметризации: если — MLE-оценка для θ θ, то ) — MLE-оценка для φ ( θ ) φ(θ); асимптотическая нормальность: N(0,1); асимптотическая оптимальность: при достаточно больших n n оценка имеет минимальную дисперсию. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.5. Независимость и условные распределения вероятностей Следующий параграф 16.7. Энтропия и семейство экспоненциальных распределений",
    "metadata": {
      "title": "Параметрические оценки",
      "url": "https://education.yandex.ru/handbook/ml/article/parametricheskie-ocenki",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.6",
      "part": 7,
      "total_parts": 7,
      "source_file": "16.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "h(x), которое вы при этом получили? Следующие соображения кажутся в этом плане вполне естественными: чем выше вероятность P(ξ=x), тем более ожидаемо появление значения x x и, соответственно, менее информативно; и наоборот, наблюдение маловероятного значения x x обычно даёт обильную пищу для размышлений и повышает h ( x ) h(x); при наблюдении двух независимых реализаций x x и y y случайной величины ξ ξ логично складывать полученную информацию: h((x,y))=h(x)+h(y). Указанные соображения наводят на мысль, что информацию следует считать убывающей функцией от вероятности: h(x)=g(P(ξ=x)). Кроме того, функция g g должна превращать произведение в сумму, поскольку для независимых случайных величин ξ ξ и η η равенство h((x,y))=h(x)+h(y) влечёт g(P(ξ=x,η=y))=g(P(ξ=x)P(η=y))=g(P(ξ=x))+g(P(η=y)). На самом деле выбор тут небогат. Единственная непрерывная функция, обладающая такими свойствами, — это логарифм: log ⁡ p g(p)=−logp. Основание логарифма может быть любым числом больше единицы. Поскольку информацию измеряют в битах и байтах, в теории информации обычно предпочитают двоичные логарифмы. Однако для вычислений удобнее использовать натуральный логарифм, и по умолчанию мы будем подразумевать именно его (кстати, соответствующую единицу информации называют «нат»). Энтропия Шеннона Среднее количество информации, которое несёт в себе значение дискретной случайной ξ ξ с распределением вероятностей =P(ξ=k), вычисляется по формуле log log ⁡ p k . Hξ=E(g(p(ξ)))=−E(logp(ξ))=− k ∑ p k logp k . Это так называемая энтропия (Шэннона). Пример. Рассмотрим схему Бернулли с вероятностью «успеха» p p. Энтропия её результата равна log log Hξ=−(1−p)log(1−p)−plogp,ξ∼Bern(p). Давайте посмотрим на график этой функции: entropy Минимальное значение (нулевое) энтропия принимает при p = 0 p=0 или p = 1 p=1. Исход такого вырожденного эксперимента заранее известен, и чтобы сообщить кому-то о его результате, достаточно 0 0 бит информации. Иначе говоря, можно вообще ничего не передавать, и так всё предельно ясно. Максимальное значение энтропии достигается в точке 1 2 2 1 , что вполне соответствует тому, что при предсказать исход эксперимента сложнее всего. Упражнение. Найдите энтропию геометрического распределения с вероятностью «успеха» ξ∼Geom(p), P(ξ=k)=p(1−p) k−1 ,k∈N,0<p⩽1. Следующие свойства энтропии дискретной случайной величины ξ ξ вытекают прямо из определения: неотрицательность: H ξ ⩾ 0 Hξ⩾0; Hξ=0⟺P(ξ=a)=1 при некотором a ∈ R a∈R (нулевую энтропию имеют вырожденные распределения и только они); H ξ ⩽ log ⁡ n Hξ⩽logn, если случайная величина имеет конечный носитель мощности n n. Последнее свойство выводится из неравенства Йенсена. Применяя его к выпуклой вверх логарифмической функции, с учётом нормировки условия k=1 ∑ n p k =1 получаем log log ⁡ 1 p k ⩽ log log ⁡ n . − k=1 ∑ n p k logp k = k=1 ∑ n p k log p 1 k ⩽log( k=1 )=logn. Вопрос на подумать. Итак, всякое распределение с носителем {1,2,…,n} имеет энтропию не больше log ⁡ n logn. А у какого распределения она в точности равна log ⁡ n logn? Дифференциальная энтропия Чтобы вычислить энтропию непрерывной случайной величины ξ ξ, надо, как водится, сумму заменить на интеграл, и получится формула дифференциальной энтропии: log Hξ=−∫p ξ (x)logp ξ (x)dx. Замечание. В дальнейшем мы будем использовать одинаковый термин энтропия как для дискретных, так и для непрерывных случайных величин, для краткости опуская слово дифференциальная в последнем случае. Кроме",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 1,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "k logp k = k=1 ∑ n p k log p 1 k ⩽log( k=1 )=logn. Вопрос на подумать. Итак, всякое распределение с носителем {1,2,…,n} имеет энтропию не больше log ⁡ n logn. А у какого распределения она в точности равна log ⁡ n logn? Дифференциальная энтропия Чтобы вычислить энтропию непрерывной случайной величины ξ ξ, надо, как водится, сумму заменить на интеграл, и получится формула дифференциальной энтропии: log Hξ=−∫p ξ (x)logp ξ (x)dx. Замечание. В дальнейшем мы будем использовать одинаковый термин энтропия как для дискретных, так и для непрерывных случайных величин, для краткости опуская слово дифференциальная в последнем случае. Кроме того, энтропию распределения p p, заданного через pmf или pdf, будем обозначать H [ p ] H[p]. Такое обозначение позволяет избежать привязки к случайной величине там, где это излишне. Если ξ∼p(x), то обозначения H ξ Hξ и H [ p ] H[p] эквивалентны. Также отметим, что энтропию можно записать в виде математического ожидания: log H[p]=E ξ∼p(x) log p(ξ) 1 . Пример. Найдём энтропию нормального распределения N(μ,σ 2 ). Его плотность равна p(x)= (x−μ) 2 , следовательно, H[p]= −∞ ∫ +∞ p(x)( 2 1 ln(2πσ 2 )+ 2σ 2 (x−μ) 2 )dx= ln(2πσ (x−μ) 2 e − 2σ 2 (x−μ) 2 dx. Делая в последнем интеграле замену t=− 2σ 2 (x−μ) 2 , получаем, что H[p]= 2 1 ln(2πσ dt= 2 1 ln(2πσ По свойству гамма-функции . Таким образом, H[p]= 2 1 ln(2πσ 2 )+ 2 1 . Как видно, энтропия гауссовского распределения N(μ,σ 2 ) не ограничена ни сверху, ни снизу: lim lim σ→+∞ lim 2 1 ln(2πσ 2 )=+∞, σ→0+ lim 2 1 ln(2πσ 2 )=−∞. И да, в отличие от энтропии дискретного распределения дифференциальная энтропия может быть отрицательной. Это связано с тем, что плотность может принимать значения больше единицы, и поэтому математическое ожидание её логарифма с обратным знаком может оказаться меньше нуля. В частности, с нормальным распределением так происходит, если 2πe 1 . Упражнение. Найдите энтропию показательного распределения Exp(λ). KL-дивергенция В задачах машинного обучения истинное распределение p ( x ) p(x), из которого приходят наблюдения, обычно неизвестно, и его пытаются приблизить распределением q ( x ) q(x) из некоторого класса модельных распределений. Дивергенция Кульбака-Лейблера (KL-дивергенция, относительная энтропия) позволяет оценить расстояние между распределениями p p и log ⁡ p k q k KL(p∣∣q)= k ∑ p k log q k p k в дискретном случае и log KL(p∣∣q)=∫p(x)log q(x) p(x) dx в непрерывном. KL-дивергенцию можно представить в виде разности: log log KL(p∣∣q)=∫p(x)log q(x) 1 dx−∫p(x)log p(x) 1 dx= log кросс-энтропия log энтропия . = кросс-энтропия E ξ∼p(x) logq(ξ) 1 − энтропия E ξ∼p(x) logp(ξ) 1 . Здесь вычитаемое – это уже знакомая нам энтропия распределения p ( x ) p(x), которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины ξ∼p(x). Уменьшаемое носит название кросс-энтропии распределений p ( x ) p(x) и q ( x ) q(x). Кросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величины ξ∼p(x) алгоритмом, оптимизированным для кодирования случайной величины η∼q(x). Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 2,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log log KL(p∣∣q)=∫p(x)log q(x) 1 dx−∫p(x)log p(x) 1 dx= log кросс-энтропия log энтропия . = кросс-энтропия E ξ∼p(x) logq(ξ) 1 − энтропия E ξ∼p(x) logp(ξ) 1 . Здесь вычитаемое – это уже знакомая нам энтропия распределения p ( x ) p(x), которая показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины ξ∼p(x). Уменьшаемое носит название кросс-энтропии распределений p ( x ) p(x) и q ( x ) q(x). Кросс-энтропию можно интерпретировать как среднее число бит для кодирования значения случайной величины ξ∼p(x) алгоритмом, оптимизированным для кодирования случайной величины η∼q(x). Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений p p, если при настройке алгоритма кодирования вместо p p использовать q q. Подробнее об этом вы можете почитать, например, в данном посте. Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду (для дискретных и непрерывных распределений это означает, что они просто тождественны). Но при этом она не является симметричной: вообще говоря, KL(p∣∣q)  =KL(q∣∣p). Упражнение. Пользуясь неравенством ln(1+t)⩽t, t > − 1 t>−1, докажите неотрицательность KL-дивергенции. Пример. С помощью KL-дивергенции измерим расстояние между двумя гауссианами p(x)=N(x∣μ 1 ,σ 1 2 ) и q(x)=N(x∣μ 2 ,σ 2 2 ). Подставляя явные выражения для плотностей p(x)= (x−μ 1 ) 2 и q(x)= (x−μ 2 ) 2 , находим q(x) p(x) =ln (x−μ (x−μ =ln )(x−μ (x−μ Из свойств нормального распределения вытекает, что p(x)(x−μ 1 )dx=0, −∞ ∫ +∞ p(x)(x−μ 1 ) 2 dx=σ 1 2 . Таким образом, KL(p∣∣q)=E ξ∼p(x) ln q(ξ) p(ξ) =ln +(μ Как и должно быть, полученное выражение равно нулю, если гауссианы совпадают. При равных дисперсиях получаем, что KL(p∣∣q)= . Это выражение остаётся прежним, если поменять местами , поэтому в этом случае KL(p∣∣q)=KL(q∣∣p). Если же , то выражение для KL(q∣∣p) явно отличается от KL(p∣∣q), что лишний раз показывает несимметричность KL-дивергенции. Упражнение. Найдите дивергенцию Кульбака-Лейблера двух показательных распределений p(x)=Exp(x∣λ) и q(x)=Exp(x∣μ). Кросс-энтропия При определении KL-дивергенции мы уже встречались с кросс-энтропией log ⁡ q ( ξ ) H[p,q]=−E ξ∼p(x) logq(ξ) В зависимости от типа распределений кросс-энтропия вычисляется по формуле log ⁡ q k или log H[p,q]=− k ∑ p k logq k или H[p,q]=− −∞ ∫ +∞ p(x)logq(x)dx. Поскольку KL(p∣∣q)=H[p,q]−H[p], задача минимизации KL-дивергенции между неизвестным распределением данных p ( x ) p(x) и модельным распределением q ( x ) q(x) эквивалентна задаче минимизации кросс-энтропии. Разница между ними равна энтропии распределения p ( x ) p(x), которая, очевидно, не зависит от q ( x ) q(x). В машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации на K > 1 K>1 классов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным: y=(y 1 ,…,y K ),y k ∈{0,1}, k=1 ∑ K y k =1. Классификатор обычно выдаёт вероятности принадлежности каждому из классов, класс ,…, y K ), y k =P(класс k). Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями: log ⁡ y ^ k . L(y, y )=− k=1",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 3,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "x ) p(x), которая, очевидно, не зависит от q ( x ) q(x). В машинном обучении кросс-энтропию часто используют в качестве функции потерь в задаче классификации на K > 1 K>1 классов. Истинное распределение на каждом обучающем объекте задаётся с помощью one hot encoding и является вырожденным: y=(y 1 ,…,y K ),y k ∈{0,1}, k=1 ∑ K y k =1. Классификатор обычно выдаёт вероятности принадлежности каждому из классов, класс ,…, y K ), y k =P(класс k). Функция потерь на одном объекте полагается равной кросс-энтропии между истинным и предсказанным распределениями: log ⁡ y ^ k . L(y, y )=− k=1 ∑ K y k log y k . И это вполне логично: чем ближе модельное распределение к истинному, тем меньше наши потери. В идеале L(y, y )=0, если Чтобы вычислить функцию потерь по обучающей выборке из N N объектов с метками y ( i ) y (i) , обычно берут усреднённную кросс-энтропию log i=1 ∑ N L(y (i) , y (i) )=− N 1 i=1 ∑ N k=1 ∑ K y k (i) log y k (i) . Принцип максимальной энтропии В параграфе про оценки параметров были описаны различные свойства параметрических оценок и методы их получения, например, метод моментов или метод максимального правдоподобия. В принципе, если мы уже выбрали для наших данных ,…,X n некоторое параметрическое семейство (x), моделирующее их распределение, восстановить его параметры чаще всего можно по выборочному среднему k=1 ∑ n X k и/или выборочной дисперсии k=1 А теперь представим, что мы посчитали эти (или какие-то другие) статистики, а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию? Three Почему-то хочется сказать, что в первом. Почему? Второе не симметрично – но что нас может заставить подозревать, что интересующее нас распределение не симметрично? С третьим проблема в том, что, выбирая его, мы добавляем дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет. Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Таким образом, искомое распределение должно обладать максимальной неопределённостью при заданных ограничениях, или, говоря более научно, иметь максимально возможную энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше – тем более «‎произвольное» распределение, по крайней мере, в теории. В этом и заключается принцип максимальной энтропии для выбора модели машинного обучения. Как мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно. Пример. Покажем, что среди распределений на множестве натуральных чисел N N и математическим ожиданием",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 4,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "– тем более «‎произвольное» распределение, по крайней мере, в теории. В этом и заключается принцип максимальной энтропии для выбора модели машинного обучения. Как мы уже видели выше, среди распределений с конечным носителем максимальную энтропию имеет равномерное распределение. Примеры геометрического и нормального распределения показывают, что энтропия распределений с бесконечным носителем (счётным или континуальным) может быть сколь угодно большой, и среди них нет какого-то одного распределения с максимальной энтропией. Однако в более узком классе распределений с фиксированным средним и/или дисперсией найти распределение с максимальной энтропией, как правило, можно. Пример. Покажем, что среди распределений на множестве натуральных чисел N N и математическим ожиданием μ > 1 μ>1 максимальную энтропию имеет геометрическое распределение. Для минимизации энтропии log ⁡ p n H[p]=− n=1 ∑ ∞ p n logp n с учётом ограничений n=1 ∑ ∞ p n =1, n=1 ∑ ∞ np n =μ воспользуемся методом множителей Лагранжа, согласно которому требуется минимизировать функцию Лагранжа log L(p,a,b)= n=1 ∑ ∞ p n logp n −a( n=1 ∑ ∞ p n −1)−b( n=1 ∑ ∞ np n −μ). Приравняем к нулю частные производные по log =1+logp n −a−bn=0. Отсюда следует, что a−1+bn =αβ n , так что распределение действительно получается геометрическое. Параметры α α и β β найдём из уравнений n=1 ∑ ∞ p n = n=1 ∑ ∞ αβ n = 1−β n=1 ∑ ∞ np n =αβ n=1 ∑ ∞ nβ n−1 = (1−β) 2 αβ . Деля первое уравнение на второе, получаем =1−β, или β=1− μ 1 . Далее из первого уравнения находим 1−β = μ−1 1 . Итак, μ−1 1 (1− (1− μ 1 ) n−1 , а это и есть геометрическое распределение с параметром 1 μ μ 1 . У непрерывных распределений возможны более интересные комбинации из ограничений на носитель и параметры. И конечно же, первую скрипку среди распределений с максимальной энтропией играет гауссовское распределение. Пример. Докажем, что среди распределений на R R c математическим ожиданием μ μ и дисперсией σ 2 σ 2 наибольшую энтропию имеет нормальное распределение N(μ,σ 2 ). Пусть p ( x ) p(x) – некоторое распределение со средним μ μ и дисперсией q(x)∼N(μ,σ 2 ). Как было показано выше, log H[q]= 2 1 log(2πσ 2 )+ 2 1 . Запишем дивергенцию Кульбака-Лейблера: log log KL(p∣∣q)= −∞ ∫ +∞ p(x)logp(x)dx− −∞ ∫ +∞ p(x)logq(x)dx= log =−H[p]− −∞ ∫ +∞ p(x)(− 2 1 log(2πσ 2 )− 2σ 2 1 (x−μ) 2 )dx= log =−H[p]+ 2 1 log(2πσ 2 ) −∞ ∫ +∞ p(x)dx+ 2σ 2 1 =V[p]=σ 2 −∞ ∫ +∞ (x−μ) 2 p(x)dx log =−H[p]+ 2 1 log(2πσ 2 )+ 2 1 =H[q]−H[p]. Так как KL-дивергенция всегда неотрицательна, получаем, что H[p]⩽H[q] при любом распределении p p, удовлетворяющем заданным ограничениям. Можно показать, что максимальную энтропию среди многомерных распределений с вектором средних μ μ и матрицей ковариаций Σ Σ имеет также гауссовское распределение N(μ,Σ). Упражнение. Докажите, что среди распределений на отрезке [ a , b ] [a,b] максимальную энтропию имеет равномерное распределение U[a,b]. Упражнение. Докажите, что среди распределений на промежутке [0,+∞) с математическим ожиданием λ > 0 λ>0 максимальную энтропию",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 5,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "−∞ ∫ +∞ p(x)dx+ 2σ 2 1 =V[p]=σ 2 −∞ ∫ +∞ (x−μ) 2 p(x)dx log =−H[p]+ 2 1 log(2πσ 2 )+ 2 1 =H[q]−H[p]. Так как KL-дивергенция всегда неотрицательна, получаем, что H[p]⩽H[q] при любом распределении p p, удовлетворяющем заданным ограничениям. Можно показать, что максимальную энтропию среди многомерных распределений с вектором средних μ μ и матрицей ковариаций Σ Σ имеет также гауссовское распределение N(μ,Σ). Упражнение. Докажите, что среди распределений на отрезке [ a , b ] [a,b] максимальную энтропию имеет равномерное распределение U[a,b]. Упражнение. Докажите, что среди распределений на промежутке [0,+∞) с математическим ожиданием λ > 0 λ>0 максимальную энтропию имеет показательное распределение Exp( λ 1 ). Как выяснилось, многие классические распределения имеют максимальную энтропию при весьма естественных ограничениях. Но как быть, если даны не эти конкретные, а какие-то другие ограничения? Есть ли какой-нибудь надёжный алгоритм вывода распределения с максимальной энтропией, позволяющий избежать случайных озарений и гаданий на кофейной гуще? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Экспоненциальное семейство распределений Говорят, что параметрическое семейство распределений относится к экспоненциальному классу, если его pdf (или pmf) может быть представлена в виде exp exp p(x∣θ)= h(θ) g(x) exp(θ T u(x))=g(x)exp(θ T u(x)−A(θ)), где θ ∈ R m θ∈R m – вектор натуральных параметров распределения; g ( x ) g(x) — неотрицательная функция (base measure), часто равная единице; h(θ)>0 — нормализатор (partition), обеспечивающий суммируемость pmf или интегрируемость pdf в единицу: exp h(θ)=∫g(x)exp(θ T u(x))dx; A(θ)=lnh(θ) — log-partition; u(x)∈R m — вектор достаточных статистик распределения. Пример. Покажем, что нормальное распределение N(x∣μ,σ 2 ) принадлежит экспоненциальному классу. Оно имеет два параметра, поэтому такую же размерность имеют θ θ и вектор-функция u u. Распишем плотность: 1 2 π σ exp exp exp exp exp(− 2σ 2 (x−μ) 2 )= 2π σ 1 exp(− σexp(− 2σ 2 μ 2 ) exp(− Положим g(x)= u(x)=(x,x θ=(θ 1 ,θ 2 ),θ Остаётся выразить функцию exp h(θ)=σexp(− 2σ 2 μ 2 ) через Упражнение. Выразите partition h ( θ ) h(θ) и log-partition A ( θ ) A(θ) через θ θ и запишите плотность нормального распределения в экспоненциальном виде. Пример. Покажем, что распределение Бернулли Bern(p) принадлежит экспоненциальному классу. Его pmf P(ξ=x∣p) можно записать как exp ⁡ ( x log log exp ⁡ ( x log (1−p) 1−x =exp(xlogp+(1−x)log(1−p))=(1−p)exp(xlog 1−p p ). Параметр здесь один, поэтому натуральный параметр θ θ тоже один: θ = log ⁡ p 1 − p θ=log 1−p p . Такая функция от p p называется функцией логитов и активно участвует в построении модели логистической регрессии. Остальные функции положим равными u(x)=x, g(x)=1, h(θ)= 1−p 1 . Остаётся выразить partition через θ θ: log log log 1−p p =log(−1+ 1−p 1 )=θ⟺ 1−p 1 =1+e θ . Итак, h(θ)=1+e θ , и экспоненциальный вид распределения Бернулли записывается как 1 1 + e θ exp exp ⁡ ( θ x − log 1+e θ 1 exp(θx)=exp(θx−log(1+e θ )). Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках U[a,b]? Казалось бы, да, ведь exp ⁡ ( 0 )",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 6,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": ". Такая функция от p p называется функцией логитов и активно участвует в построении модели логистической регрессии. Остальные функции положим равными u(x)=x, g(x)=1, h(θ)= 1−p 1 . Остаётся выразить partition через θ θ: log log log 1−p p =log(−1+ 1−p 1 )=θ⟺ 1−p 1 =1+e θ . Итак, h(θ)=1+e θ , и экспоненциальный вид распределения Бернулли записывается как 1 1 + e θ exp exp ⁡ ( θ x − log 1+e θ 1 exp(θx)=exp(θx−log(1+e θ )). Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках U[a,b]? Казалось бы, да, ведь exp ⁡ ( 0 ) . p(x)= b−a 1 I [a,b] (x)exp(0). В чём может быть подвох? К экспоненциальным семействам относятся многие непрерывные и дискретные распределения из часто встречающихся в теории и на практике, в том числе нормальное N(μ,σ 2 ); распределение Пуассона Pois(λ); экспоненциальное Exp(λ); биномиальное Bin(n,p); геометрическое Geom(p); бета-распределение; гамма-распределение; распределение Дирихле. Как выглядят натуральные параметры, достаточные статистики и нормализаторы этих и других распределений из экспоненциального класса, можно посмотреть на википедии. К экспоненциальным семействам не относятся, например, равномерное распределение U[a,b], t t-распределение Стьюдента, распределение Коши, смесь нормальных распределений. Дифференцирование log-partition Если распределение p(x∣θ) принадлежит экспоненциальному классу, exp exp p(x∣θ)= h(θ) g(x) exp(θ T u(x))=g(x)exp(θ T u(x)−A(θ)), то моменты его достаточных статистик u ( x ) u(x) могут быть получены дифференцированием функции A ( θ ) = log ⁡ h ( θ ) A(θ)=logh(θ). Утверждение. ∇ θ log logh(θ)=E ξ∼p(x∣θ) u(ξ). Доказательство. По правилу дифференцирования сложной функции имеем ∇ θ log logh(θ)= h(θ) ∇ θ h(θ) . Нормализатор h ( θ ) h(θ) записывается в виде интеграла exp h(θ)=∫g(x)exp(θ T u(x))dx, который мы продифференцируем внесением градиента внутрь под знак интеграла: exp h(θ)=∇ θ ∫g(x)exp(θ T u(x))dx= exp exp =∫g(x)∇ θ exp(θ T u(x))dx=∫g(x)u(x)exp(θ T u(x))dx. Таким образом, exp h(θ) ∇ θ h(θ) =∫u(x) p(x∣θ) h(θ) g(x) exp(θ T u(x)) dx=E ξ∼p(x∣θ) u(ξ). Если (x)=x i , то в соответствии с только что доказанным частная производная ∂A(θ) даёт i i-й момент распределения p(x∣θ). Упражнение. Вычислите производные по натуральным параметрам от log-partition для распределения Бернулли Bern(x∣p) и нормального распределения N(μ,σ 2 ) и проверьте, что они совпадают со значениями соответствующих моментов. Кстати, можно продифференцировать ещё раз и доказать, что ∇ θ 2 log logh(θ)=cov(u(ξ),u(ξ)). MLE для семейства из экспоненциального класса Возможно, вас удивил странный и на первый взгляд не очень естественный вид p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе. Запишем функцию правдоподобия i.i.d. выборки ,…,x exp p(x 1 ,…,x n ∣θ)=h(θ) −n ⋅( i=1 ∏ n g(x i ))⋅exp(θ T i=1 ∑ n u(x i )). Её логарифм равен log log log logp(x 1 ,…,x n ∣θ)=−nlogh(θ)+ i=1 ∑ n logg(x i )+θ T i=1 ∑ n u(x i ). Дифференцируя по θ θ, получаем ∇ θ log log logp(x 1 ,…,x n ∣θ)=−n∇ θ logh(θ)+ i=1 ∑ n u(x i ). Приравнивая ∇ θ log logp(x 1 ,…,x n ∣θ) к нулю и пользуясь равенством ∇ θ log logh(θ)=E ξ∼p(x∣θ) u(ξ), находим ξ∼p(x∣θ)",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 7,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "экспоненциального класса устроена очень интригующе. Запишем функцию правдоподобия i.i.d. выборки ,…,x exp p(x 1 ,…,x n ∣θ)=h(θ) −n ⋅( i=1 ∏ n g(x i ))⋅exp(θ T i=1 ∑ n u(x i )). Её логарифм равен log log log logp(x 1 ,…,x n ∣θ)=−nlogh(θ)+ i=1 ∑ n logg(x i )+θ T i=1 ∑ n u(x i ). Дифференцируя по θ θ, получаем ∇ θ log log logp(x 1 ,…,x n ∣θ)=−n∇ θ logh(θ)+ i=1 ∑ n u(x i ). Приравнивая ∇ θ log logp(x 1 ,…,x n ∣θ) к нулю и пользуясь равенством ∇ θ log logh(θ)=E ξ∼p(x∣θ) u(ξ), находим ξ∼p(x∣θ) u(ξ)= n 1 i=1 ∑ n u(x i ). Таким образом, теоретические матожидания всех компонент (ξ) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для (ξ) в качестве моментов. И в следующем пункте выяснится, что распределения из экспоненциальных семейств обладают максимальной энтропией среди тех, что имеют заданные моменты (ξ). Теорема Купмана—Питмана—Дармуа Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса. В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так. Теорема. Пусть параметр θ ∈ R m θ∈R m распределения exp (x)= h(θ) 1 exp(θ T u(x)) выбран так, что ξ∼p θ (x) u(ξ)=α для некоторого фиксированного α ∈ R m α∈R m . Тогда распределение (x) обладает наибольшей энтропией среди распределений q q с тем же носителем, для которых ξ∼q(x) u(ξ)=α. Выше мы уже находили обладающее максимальной энтропией распределение на множестве натуральных чисел с заданным математическим ожиданием μ > 1 μ>1. Таковым оказалось геометрическое распределение Geom( μ 1 ). Теорема Купмана—Питмана—Дармуа позволяет сделать это гораздо быстрее. В данном случае у нас лишь одна функция (x)=x, которая соответствует фиксации математического ожидания E ξ Eξ. Искомое дискретное распределение имеет вид exp =P(ξ=k)= h(θ) 1 exp(θk)= h(θ) Это уже похоже на геометрическое распределение с параметром p=1−e θ . Его математическое ожидание равно 1 p p 1 , что по условию должно равняться μ μ. Итак, наше распределение с максимальной этропией выглядит так: (1− μ 1 ) k−1 ,k∈N. Пример. Среди распределений на всей вещественной прямой с заданным математическим ожиданием μ μ найдём распределение с максимальной энтропией. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 16.5. Независимость и условные распределения вероятностей Предыдущий параграф 16.6. Параметрические оценки",
    "metadata": {
      "title": "Энтропия и семейство экспоненциальных распределений",
      "url": "https://education.yandex.ru/handbook/ml/article/entropiya-i-semejstvo-eksponencialnyh-raspredelenij",
      "course": "ml",
      "chapter": "16. Теормин",
      "chapter_id": "16.7",
      "part": 8,
      "total_parts": 8,
      "source_file": "16.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая классификация Мы начнем с самых простых и понятных моделей машинного обучения: линейных. В этом параграфе мы разберёмся, что это такое, почему они работают и в каких случаях их стоит использовать. Так как это первый класс моделей, с которым вы столкнётесь, мы постараемся подробно проговорить все важные моменты. Заодно объясним, как работает машинное обучение, на сравнительно простых примерах. Почему модели линейные? Представьте, что у вас есть множество объектов X X, а вы хотели бы каждому объекту сопоставить какое-то значение. К примеру, у вас есть набор операций по банковской карте, а вы бы хотели, понять, какие из этих операций сделали мошенники. Если вы разделите все операции на два класса и нулём обозначите законные действия, а единицей мошеннические, то у вас получится простейшая задача классификации. Представьте другую ситуацию: у вас есть данные геологоразведки, по которым вы хотели бы оценить перспективы разных месторождений. В данном случае по набору геологических данных ваша модель будет, к примеру, оценивать потенциальную годовую доходность шахты. Это пример задачи регрессии. Числа, которым мы хотим сопоставить объекты из нашего множества иногда называют таргетами (от английского target). Таким образом, задачи классификации и регрессии можно сформулировать как поиск отображения из множества объектов X X в множество возможных таргетов. Математически задачи можно описать так: классификация: X→{0,1,…,K}, где 0 , … , K 0,…,K – номера классов, регрессия: X → R X→R. Очевидно, что просто сопоставить какие-то объекты каким-то числам — дело довольно бессмысленное. Мы же хотим быстро обнаруживать мошенников или принимать решение, где строить шахту. Значит нам нужен какой-то критерий качества. Мы бы хотели найти такое отображение, которое лучше всего приближает истинное соответствие между объектами и таргетами. Что значит «лучше всего» – вопрос сложный. Мы к нему будем много раз возвращаться. Однако, есть более простой вопрос: среди каких отображений мы будем искать самое лучшее? Возможных отображений может быть много, но мы можем упростить себе задачу и договориться, что хотим искать решение только в каком-то заранее заданном параметризированном семействе функций. Весь этот параграф будет посвящен самому простому такому семейству — линейным функциям вида y=w 1 x 1 +…+w где y y – целевая переменная (таргет), ,…,x D ) – вектор, соответствующий объекту выборки (вектор признаков), а ,…,w D ,w 0 – параметры модели. Признаки ещё называют фичами (от английского features). Вектор w=(w 1 ,…,w D ) часто называют вектором весов, так как на предсказание модели можно смотреть как на взвешенную сумму признаков объекта, а число w 0 w 0 – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде y=⟨x,w⟩+w 0 Теперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор ,…,w D )∈R D+1 . Замечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков ,…,x D . Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам. Разберёмся,",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 1,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объекта, а число w 0 w 0 – свободным коэффициентом, или сдвигом (bias). Более компактно линейную модель можно записать в виде y=⟨x,w⟩+w 0 Теперь, когда мы выбрали семейство функций, в котором будем искать решение, задача стала существенно проще. Мы теперь ищем не какое-то абстрактное отображение, а конкретный вектор ,…,w D )∈R D+1 . Замечание. Чтобы применять линейную модель, нужно, чтобы каждый объект уже был представлен вектором численных признаков ,…,x D . Конечно, просто текст или граф в линейную модель не положить, придётся сначала придумать для него численные фичи. Модель называют линейной, если она является линейной по этим численным признакам. Разберёмся, как будет работать такая модель в случае, если D = 1 D=1. То есть у наших объектов есть ровно один численный признак, по которому они отличаются. Теперь наша линейная модель будет выглядеть совсем просто: y=w 1 x 1 +w 0 . Для задачи регрессии мы теперь пытаемся приблизить значение игрек какой-то линейной функцией от переменной икс. А что будет значить линейность для задачи классификации? Давайте вспомним про пример с поиском мошеннических транзакций по картам. Допустим, нам известна ровно одна численная переменная — объём транзакции. Для бинарной классификации транзакций на законные и потенциально мошеннические мы будем искать так называемое разделяющее правило: там, где значение функции положительно, мы будем предсказывать один класс, где отрицательно – другой. В нашем примере простейшим правилом будет какое-то пороговое значение объёма транзакций, после которого есть смысл пометить транзакцию как подозрительную. 1 В случае более высоких размерностей вместо прямой будет гиперплоскость с аналогичным смыслом. Вопрос на подумать. Если вы посмотрите содержание учебника, то не найдёте в нём ни «полиномиальных» моделей, ни каких-нибудь «логарифмических», хотя, казалось бы, зависимости бывают довольно сложными. Почему так? Вопрос на подумать. А как быть, если одна из фичей является категориальной, то есть принимает значения из (обычно конечного числа) значений, не являющихся числами? Например, это может быть время года, уровень образования, марка машины и так далее. Как правило, с такими значениями невозможно производить арифметические операции или же результаты их применения не имеют смысла. Помимо простоты, у линейных моделей есть несколько других достоинств. К примеру, мы можем достаточно легко судить, как влияют на результат те или иные признаки. Скажем, если вес w i w i положителен, то с ростом i i-го признака таргет в случае регрессии будет увеличиваться, а в случае классификации наш выбор будет сдвигаться в пользу одного из классов. Значение весов тоже имеет прозрачную интерпретацию: чем вес w i w i больше, тем «важнее» i i-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно. В то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин:",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 2,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i i-й признак для итогового предсказания. То есть, если вы построили линейную модель, вы неплохо можете объяснить заказчику те или иные её результаты. Это качество моделей называют интерпретируемостью. Оно особенно ценится в индустриальных задачах, цена ошибки в которых высока. Если от работы вашей модели может зависеть жизнь человека, то очень важно понимать, как модель принимает те или иные решения и какими принципами руководствуется. При этом не все методы машинного обучения хорошо интерпретируемы, к примеру, поведение искусственных нейронных сетей или градиентного бустинга интерпретировать довольно сложно. В то же время слепо доверять весам линейных моделей тоже не стоит по целому ряду причин: Линейные модели всё-таки довольно узкий класс функций, они неплохо работают для небольших датасетов и простых задач. Однако, если вы решаете линейной моделью более сложную задачу, то вам, скорее всего, придётся выдумывать дополнительные признаки, являющиеся сложными функциями от исходных. Поиск таких дополнительных признаков называется feature engineering, технически он устроен примерно так, как мы описали в вопросе про \"полиномиальные модели\". Вот только поиском таких искусственных фичей можно сильно увлечься, так что осмысленность интерпретации будет сильно зависеть от здравого смысла эксперта, строившего модель. Если между признаками есть приближённая линейная зависимость, коэффициенты в линейной модели могут совершенно потерять физический смысл (об этой проблеме и о том, как с ней бороться, мы поговорим дальше, когда будем обсуждать регуляризацию). Особенно осторожно стоит верить в утверждения вида «этот коэффициент маленький, значит, этот признак не важен». Во-первых, всё зависит от масштаба признака: вдруг коэффициент мал, чтобы скомпенсировать его. Во-вторых, зависимость действительно может быть слабой, но кто знает, в какой ситуации она окажется важна. Такие решения принимаются на основе данных, например, путём проверки статистического критерия (об этом мы коротко упомянем в разделе про вероятностные модели). Конкретные значения весов могут меняться в зависимости от обучающей выборки, хотя с ростом её размера они будут потихоньку сходиться к весам «наилучшей» линейной модели, которую можно было бы построить по всем-всем-всем данным на свете. Обсудив немного общие свойства линейных моделей, перейдём к тому, как их всё-таки обучать. Сначала разберёмся с регрессией, а затем настанет черёд классификации. Линейная регрессия и метод наименьших квадратов (МНК) Мы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является метод наименьших квадратов (Ordinary least squares). Пусть у нас задан датасет ( X , y ) (X,y), где y=(y i ) i=1 N ∈R N – вектор значений целевой переменной, а X=(x i ) i=1 N ∈R N×D ,x i ∈R D – матрица объекты-признаки, в которой i i-я строка – это вектор признаков i i-го объекта выборки. Мы хотим моделировать зависимость как линейную функцию со свободным членом. Общий вид такой функции из R выглядит следующим образом: )=⟨w,x i ⟩+w 0 Свободный член w 0 w 0 часто опускают, потому что такого же результата можно добиться, добавив ко всем x i x i признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес: Поскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто )=⟨w,x i ⟩. Сведение к задаче оптимизации Мы хотим,",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 3,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объекты-признаки, в которой i i-я строка – это вектор признаков i i-го объекта выборки. Мы хотим моделировать зависимость как линейную функцию со свободным членом. Общий вид такой функции из R выглядит следующим образом: )=⟨w,x i ⟩+w 0 Свободный член w 0 w 0 часто опускают, потому что такого же результата можно добиться, добавив ко всем x i x i признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес: Поскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто )=⟨w,x i ⟩. Сведение к задаче оптимизации Мы хотим, чтобы на нашем датасете (то есть на парах ) из обучающей выборки) функция f w f w как можно лучше приближала нашу зависимость. 1 Для того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание «приблизить (x) к y y». Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса w w. Функция, оценивающая то, как часто модель ошибается, традиционно называется функцией потерь, функционалом качества или просто лоссом (loss function). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь – это хорошо, а кусочно постоянная – просто ужасно. Функции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор y y и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора «живут» в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже. Сейчас давайте в качестве лосса возьмём квадрат L 2 L 2 -нормы вектора разницы предсказаний модели и y y. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств: L 2 L 2 -норма разницы – это евклидово расстояние ∥y−f w (x)∥ 2 между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного «расстояния». Как мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного «сигнала» и нормально распределенного «шума». Так вот, наша функция потерь выглядит так: L(f,X,y)=∥y−f(X)∥ =∥y−Xw∥ 2 2 = i=1 ∑ N (y i −⟨x i ,w⟩) 2 Такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из 2500 2500 объектов хуже, чем на обучающей из 5000 5000 объектов. Вы измерили L 2 L 2 -норму ошибки и получили в одном случае 300 300, а в другом 500 500. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 4,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "гипотезе о том, что наши данные состоят из линейного «сигнала» и нормально распределенного «шума». Так вот, наша функция потерь выглядит так: L(f,X,y)=∥y−f(X)∥ =∥y−Xw∥ 2 2 = i=1 ∑ N (y i −⟨x i ,w⟩) 2 Такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из 2500 2500 объектов хуже, чем на обучающей из 5000 5000 объектов. Вы измерили L 2 L 2 -норму ошибки и получили в одном случае 300 300, а в другом 500 500. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение L(f,X,y)= N 1 i=1 ∑ N (y i −⟨x i ,w⟩) 2 По этой метрике на тестовой выборке получаем 0 , 12 0,12, а на обучающей 0 , 1 0,1. Функция потерь i=1 N (y i −⟨x i ,w⟩) 2 называется Mean Squared Error, MSE или среднеквадратическим отклонением. Разница с L 2 L 2 -нормой чисто косметическая, на алгоритм решения задачи она не влияет: MSE MSE(f,X,y)= N 1 ∥y−Xw∥ 2 2 В самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют функционалом. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса w i w i , мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения y y. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по min ⁡ w ∥y−Xw∥ 2 2 ⟶ w min Эту задачу можно решать разными способами. В этом параграфе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ \"на пальцах\" показать суть машинного обучения. МНК: точный аналитический метод Точку минимума можно найти разными способами. Если вам интересно аналитическое решение, вы можете найти его в параграфе про матричные дифференцирования (раздел «Примеры вычисления производных сложных функций»). Здесь же мы воспользуемся геометрическим подходом. Пусть (1) ,…,x (D) – столбцы матрицы X X, то есть столбцы признаков. Тогда Xw=w 1 x (1) +…+w D x (D) , и задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов (1) ,…,x (D) , которая наилучшим способом приближает столбец y y по евклидовой норме – то есть найти проекцию вектора y y на подпространство, образованное векторами (1) ,…,x (D) . Разложим y=y ∥ +y ⊥ , где =Xw – та самая проекция, а y ⊥ y ⊥ – ортогональная составляющая, то есть =y−Xw⊥x (1) ,…,x (D) . Как это можно выразить в матричном виде? Оказывается, очень просто: (y−Xw)=0 В самом деле, каждый элемент столбца (y−Xw) – это скалярное произведение строки X T X T (=столбца X X = одного из x ( i ) x",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 5,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "образом: найти линейную комбинацию столбцов (1) ,…,x (D) , которая наилучшим способом приближает столбец y y по евклидовой норме – то есть найти проекцию вектора y y на подпространство, образованное векторами (1) ,…,x (D) . Разложим y=y ∥ +y ⊥ , где =Xw – та самая проекция, а y ⊥ y ⊥ – ортогональная составляющая, то есть =y−Xw⊥x (1) ,…,x (D) . Как это можно выразить в матричном виде? Оказывается, очень просто: (y−Xw)=0 В самом деле, каждый элемент столбца (y−Xw) – это скалярное произведение строки X T X T (=столбца X X = одного из x ( i ) x (i) ) на y − X w y−Xw. Из уравнения (y−Xw)=0 уже очень легко выразить w=(X Вопрос на подумать Для вычисления w ∗ w ∗ нам приходится обращать (квадратную) матрицу X T X X T X, что возможно, только если она невырождена. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях? Вычислительная сложность аналитического решения — O(D 2 N+D 3 ), где N N — длина выборки, D D — число признаков у одного объекта. Слагаемое N D 2 ND 2 отвечает за сложность перемножения матриц X, а слагаемое D 3 D 3 — за сложность обращения их произведения. Перемножать матрицы не стоит. Гораздо лучше сначала умножить y y на X T X T , а затем полученный вектор на : так будет быстрее и, кроме того, не нужно будет хранить матрицу Вычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц или итерационные методы поиска обратной матрицы. Проблемы «точного» решения Заметим, что для получения ответа нам нужно обратить матрицу X T X X T X. Это создает множество проблем: Основная проблема в обращении матрицы — это то, что вычислительно обращать большие матрицы дело сложное, а мы бы хотели работать с датасетами, в которых у нас могут быть миллионы точек, Матрица X T X X T X, хотя почти всегда обратима в разумных задачах машинного обучения, зачастую плохо обусловлена. Особенно если признаков много, между ними может появляться приближённая линейная зависимость, которую мы можем упустить на этапе формулировки задачи. В подобных случаях погрешность нахождения w w будет зависеть от квадрата числа обусловленности матрицы X X, что очень плохо. Это делает полученное таким образом решение численно неустойчивым: малые возмущения y y могут приводить к катастрофическим изменениям w w. Полностью вылечить проблемы мы не сможем, но никто и не обязывает нас останавливаться на «точном» решении (которое всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом. МНК: приближенный численный метод Минимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе. Как известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра w w, мы можем его улучшить, посчитав",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 6,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "всё равно никогда не будет вполне точным). Поэтому ниже мы познакомим вас с совершенно другим методом. МНК: приближенный численный метод Минимизируемый функционал является гладким и выпуклым, а это значит, что можно эффективно искать точку его минимума с помощью итеративных градиентных методов. Более подробно вы можете прочитать о них в разделе про методы оптимизации, а здесь мы лишь коротко расскажем об одном самом базовом подходе. Как известно, градиент функции в точке направлен в сторону её наискорейшего роста, а антиградиент (противоположный градиенту вектор) в сторону наискорейшего убывания. То есть имея какое-то приближение оптимального значения параметра w w, мы можем его улучшить, посчитав градиент функции потерь в точке и немного сдвинув вектор весов в направлении антиградиента: L(f w ,X,y) где α α – это параметр алгоритма («темп обучения»), который контролирует величину шага в направлении антиградиента. Описанный алгоритм называется градиентным спуском. Посмотрим, как будет выглядеть градиентный спуск для функции потерь L(f w ,X,y)= N 1 ∣∣Xw−y∣∣ 2 . Градиент квадрата евклидовой нормы мы уже считали; соответственно, (Xw−y) Следовательно, стартовав из какого-то начального приближения, мы можем итеративно уменьшать значение функции, пока не сойдёмся (по крайней мере в теории) к минимуму (вообще говоря, локальному, но в данном случае глобальному). Алгоритм градиентного спуска w = random_normal() # можно пробовать и другие виды инициализации repeat S times: # другой вариант: while abs(err) > tolerance f = X.dot(w) # посчитать предсказание err = f - y # посчитать ошибку grad = 2 * X.T.dot(err) / N # посчитать градиент w -= alpha * grad # обновить веса С теоретическими результатами о скорости и гарантиях сходимости градиентного спуска вы можете познакомиться в параграфе про методы оптимизации. Мы позволим себе лишь несколько общих замечаний: Поскольку задача выпуклая, выбор начальной точки влияет на скорость сходимости, но не настолько сильно, чтобы на практике нельзя было стартовать всегда из нуля или из любой другой приятной вам точки; Число обусловленности матрицы X X существенно влияет на скорость сходимости градиентного спуска: чем более вытянуты эллипсоиды уровня функции потерь, тем хуже; Темп обучения α α тоже сильно влияет на поведение градиентного спуска; вообще говоря, он является гиперпараметром алгоритма, и его, возможно, придётся подбирать отдельно. Другими гиперпараметрами являются максимальное число итераций S S и/или порог tolerance. Вычислительная сложность градиентного спуска – O(NDS), где, как и выше, N N – длина выборки, D D – число признаков у одного объекта. Сравните с оценкой O(D 2 N+D 3 ) для «наивного» вычисления аналитического решения. Сложность по памяти – O ( N D ) O(ND) на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка. Стохастический градиентный спуск На каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность O ( N D ) O(ND)). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч). А именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет L(w,X,y)= N 1 i=1 ∑ N L(w,x i ,y i ),",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 7,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "N D ) O(ND) на хранение выборки. В памяти мы держим и выборку, и градиент, но в большинстве реалистичных сценариев доминирует выборка. Стохастический градиентный спуск На каждом шаге градиентного спуска нам требуется выполнить потенциально дорогую операцию вычисления градиента по всей выборке (сложность O ( N D ) O(ND)). Возникает идея заменить градиент его оценкой на подвыборке (в английской литературе такую подвыборку обычно именуют batch или mini-batch; в русской разговорной терминологии тоже часто встречается слово батч или мини-батч). А именно, если функция потерь имеет вид суммы по отдельным парам объект-таргет L(w,X,y)= N 1 i=1 ∑ N L(w,x i ,y i ), а градиент, соответственно, записывается в виде L(w,X,y)= N 1 i=1 ∑ N ∇ w L(w,x i ,y i ), то предлагается брать оценку L(w,X,y)≈ B 1 t=1 ∑ B ∇ w L(w,x для некоторого подмножества этих пар t=1 B . Обратите внимание на множители перед суммами. Почему они нужны? Полный градиент L(w,X,y) можно воспринимать как среднее градиентов по всем объектам, то есть как оценку матожидания L(w,x,y); тогда, конечно, оценка матожидания по меньшей подвыборке тоже будет иметь вид среднего градиентов по объектам этой подвыборки. Как делить выборку на батчи? Ясно, что можно было бы случайным образом сэмплировать их из полного датасета, но даже если использовать быстрый алгоритм вроде резервуарного сэмплирования, сложность этой операции не самая оптимальная. Поэтому используют линейный проход по выборке (которую перед этим лучше всё-таки случайным образом перемешать). Давайте введём ещё один параметр нашего алгоритма: размер батча, который мы обозначим B B. Теперь на B B очередных примерах вычислим градиент и обновим веса модели. При этом вместо количества шагов алгоритма обычно задают количество эпох E E. Это ещё один гиперпараметр. Одна эпоха – это один полный проход нашего сэмплера по выборке. Заметим, что если выборка очень большая, а модель компактная, то даже первый проход бывает можно не заканчивать. Алгоритм: w = normal(0, 1) repeat E times: for i = B, i <= n, i += B X_batch = X[i-B : i] y_batch = y[i-B : i] f = X_batch.dot(w) # посчитать предсказание err = f - y_batch # посчитать ошибку grad = 2 * X_batch.T.dot(err) / B # посчитать градиент w -= alpha * grad Сложность по времени – O(NDE). На первый взгляд, она такая же, как и у обычного градиентного спуска, но заметим, что мы сделали в N / B N/B раз больше шагов, то есть веса модели претерпели намного больше обновлений. Сложность по памяти можно довести до O ( B D ) O(BD): ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы B B взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти. В целом, разницу между алгоритмами можно представлять как-то так: 1 Шаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 8,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "O ( B D ) O(BD): ведь теперь всю выборку не надо держать в памяти, а достаточно загружать лишь текущий батч (а остальная выборка может лежать на диске, что удобно, так как в реальности задачи, в которых выборка целиком не влезает в оперативную память, встречаются сплошь и рядом). Заметим, впрочем, что при этом лучше бы B B взять побольше: ведь чтение с диска – намного более затратная по времени операция, чем чтение из оперативной памяти. В целом, разницу между алгоритмами можно представлять как-то так: 1 Шаги стохастического градиентного спуска заметно более шумные, но считать их получается значительно быстрее. В итоге они тоже сходятся к оптимальному значению из-за того, что матожидание оценки градиента на батче равно самому градиенту. По крайней мере, сходимость можно получить при хорошо подобранных коэффициентах темпа обучения в случае выпуклого функционала качества. Подробнее мы об этом поговорим в параграфе про оптимизацию. Для сложных моделей и лоссов стохастический градиентный спуск может сходиться плохо или застревать в локальных минимумах, поэтому придумано множество его улучшений. О некоторых из них также рассказано в параграфе про оптимизацию. Существует определённая терминологическая путаница, иногда стохастическим градиентным спуском называют версию алгоритма, в которой размер батча равен единице (то есть максимально шумная и быстрая версия алгоритма), а версии с бОльшим размером батча называют batch gradient descent. В книгах, которые, возможно, старше вас, такая процедура иногда ещё называется incremental gradient descent. Это не очень принципиально, но вы будьте готовы, если что. Вопрос на подумать. Вообще говоря, если объём данных не слишком велик и позволяет это сделать, объекты лучше случайным образом перемешивать перед тем, как подавать их в алгоритм стохастического градиентного спуска. Как вам кажется, почему? Также можно использовать различные стратегии отбора объектов. Например, чаще брать объекты, на которых ошибка больше. Какие ещё стратегии вы могли бы придумать? Неградиентные методы После прочтения этой главы у вас может сложиться ощущение, что приближённые способы решения ML задач и градиентные методы – это одно и тоже, но вы будете правы в этом только на 98%. В принципе, существуют и другие способы численно решать эти задачи, но в общем случае они работают гораздо хуже, чем градиентный спуск, и не обладают таким хорошим теоретическим обоснованием. Мы не будем рассказывать про них подробно, но можете на досуге почитать, скажем, про Stepwise regression, Orthogonal matching pursuit или LARS. У LARS, кстати, есть довольно интересное свойство: он может эффективно работать на выборках, в которых число признаков больше числа примеров. С алгоритмом LARS вы можете познакомиться в параграфе про оптимизацию. Регуляризация Всегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше D D), то гарантировано найдётся такой вектор весов ν ν что ⟨ν,x i ⟩=0 ∀x i . В этом случае, если какой-то w w является решением оптимизационной задачи, то и w + α ν w+αν тоже является решением для любого α α. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 9,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "про оптимизацию. Регуляризация Всегда ли решение задачи регрессии единственно? Вообще говоря, нет. Так, если в выборке два признака будут линейно зависимы (и следовательно, ранг матрицы будет меньше D D), то гарантировано найдётся такой вектор весов ν ν что ⟨ν,x i ⟩=0 ∀x i . В этом случае, если какой-то w w является решением оптимизационной задачи, то и w + α ν w+αν тоже является решением для любого α α. То есть решение не только не обязано быть уникальным, так ещё может быть сколь угодно большим по модулю. Это создаёт вычислительные трудности. Малые погрешности признаков сильно возрастают при предсказании ответа, а в градиентном спуске накапливается погрешность из-за операций со слишком большими числами. Конечно, в жизни редко бывает так, что признаки строго линейно зависимы, а вот быть приближённо линейно зависимыми они вполне могут быть. Такая ситуация называется мультиколлинеарностью. В этом случае у нас, всё равно, возникают проблемы, близкие к описанным выше. Дело в том, что X ν ∼ 0 Xν∼0 для вектора ν ν, состоящего из коэффициентов приближённой линейной зависимости, и, соответственно, Xν≈0, то есть матрица X T X X T X снова будет близка к вырожденной. Как и любая симметричная матрица, она диагонализуется в некотором ортонормированном базисе, и некоторые из собственных значений λ i λ i близки к нулю. Если вектор X T y X T y в выражении y будет близким к соответствующему собственному вектору, то он будет умножаться на 1 / λ i 1/λ i , что опять же приведёт к появлению у w w очень больших по модулю компонент (при этом w w ещё и будет вычислен с большой погрешностью из-за деления на маленькое число). И, конечно же, все ошибки и весь шум, которые имелись в матрице X X, при вычислении y ∼ X w y∼Xw будут умножаться на эти большие и неточные числа и возрастать во много-много раз, что приведёт к проблемам, от которых нас не спасёт никакое сингулярное разложение. Важно ещё отметить, что в случае, когда несколько признаков линейно зависимы, веса w i w i при них теряют физический смысл. Может даже оказаться, что вес признака, с ростом которого таргет, казалось бы, должен увеличиваться, станет отрицательным. Это делает модель не только неточной, но и принципиально не интерпретируемой. Вообще, неадекватность знаков или величины весов – хорошее указание на мультиколлинеарность. Для того, чтобы справиться с этой проблемой, задачу обычно регуляризуют, то есть добавляют к ней дополнительное ограничение на вектор весов. Это ограничение можно, как и исходный лосс, задавать по-разному, но, как правило, ничего сложнее, чем L 1 L 1 - и L 2 L 2 -нормы, не требуется. Вместо исходной задачи теперь предлагается решить такую: min min min L(f,X,y)= w min (∥Xw−y∥ 2 2 +λ∥w∥ k k ) λ λ – это очередной параметр, а ∥w∥ k ˆ k – это один из двух вариантов: ∥w∥ 2 2 =w 1 2 +…+w D 2 или ∥w∥ 1 1 =∣w 1 ∣+…+∣w D ∣ Добавка λ∥w∥ k k называется регуляризационным членом или регуляризатором, а число λ λ – коэффициентом регуляризации. Коэффициент λ λ является гиперпараметром модели и достаточно сильно",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 10,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "по-разному, но, как правило, ничего сложнее, чем L 1 L 1 - и L 2 L 2 -нормы, не требуется. Вместо исходной задачи теперь предлагается решить такую: min min min L(f,X,y)= w min (∥Xw−y∥ 2 2 +λ∥w∥ k k ) λ λ – это очередной параметр, а ∥w∥ k ˆ k – это один из двух вариантов: ∥w∥ 2 2 =w 1 2 +…+w D 2 или ∥w∥ 1 1 =∣w 1 ∣+…+∣w D ∣ Добавка λ∥w∥ k k называется регуляризационным членом или регуляризатором, а число λ λ – коэффициентом регуляризации. Коэффициент λ λ является гиперпараметром модели и достаточно сильно влияет на качество итогового решения. Его подбирают по логарифмической шкале (скажем, от 1e-2 до 1e+2), используя для сравнения моделей с разными значениями λ λ дополнительную валидационную выборку. При этом качество модели с подобранным коэффициентом регуляризации уже проверяют на тестовой выборке, чтобы исключить переобучение. Более подробно о том, как нужно подбирать гиперпараметры, вы можете почитать в соответствующем параграфе. Отдельно надо договориться о том, что вес w 0 w 0 , соответствующий отступу от начала координат (то есть признаку из всех единичек), мы регуляризовать не будем, потому что это не имеет смысла: если даже все значения y y равномерно велики, это не должно портить качество обучения. Обычно это не отображают в формулах, но если придираться к деталям, то стоило бы написать сумму по всем весам, кроме ∥w∥ 2 2 = j=1 ∥w∥ 1 = j=1 ∑ D ∣w j ∣ В случае L 2 L 2 -регуляризации решение задачи изменяется не очень сильно. Например, продифференцировав новый лосс по w w, легко получить, что «точное» решение имеет вид: w=(X T X+λI) −1 X T y Отметим, что за этой формулой стоит и понятная численная интуиция: раз матрица X T X X T X близка к вырожденной, то обращать её сродни самоубийству. Мы лучше слегка исказим её добавкой λ I λI, которая увеличит все собственные значения на λ λ, отодвинув их от нуля. Да, аналитическое решение перестаёт быть «точным», но за счёт снижения численных проблем мы получим более качественное решение, чем при использовании «точной» формулы. В свою очередь, градиент функции потерь L(f w ,X,y)=∥Xw−y∥ 2 +λ∥w∥ 2 по весам теперь выглядит так: L(f w ,X,y)=2X T (Xw−y)+2λw Подставив этот градиент в алгоритм стохастического градиентного спуска, мы получаем обновлённую версию приближенного алгоритма, отличающуюся от старой только наличием дополнительного слагаемого. Вопрос на подумать. Рассмотрим стохастический градиентный спуск для L 2 L 2 -регуляризованной линейной регрессии с батчами размера 1 1. Выберите правильный вариант шага SGD: (а) −2α(⟨w,x j ⟩−y j )x ji − N 2αλ w i ,i=1,…,D; (б) −2α(⟨w,x j ⟩−y j )x ji −2αλw i ,i=1,…,D; (в) −2α(⟨w,x j ⟩−y j )x ji −2λNw i ,i=1,…D. Вопрос на подумать. Распишите процедуру стохастического градиентного спуска для L 1 L 1 -регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема? Отметим, что L 1 L 1 - и L 2 L 2 -регуляризацию можно определять для любой функции потерь L(w,X,y) (и не только в задаче регрессии, а и,",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 11,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "батчами размера 1 1. Выберите правильный вариант шага SGD: (а) −2α(⟨w,x j ⟩−y j )x ji − N 2αλ w i ,i=1,…,D; (б) −2α(⟨w,x j ⟩−y j )x ji −2αλw i ,i=1,…,D; (в) −2α(⟨w,x j ⟩−y j )x ji −2λNw i ,i=1,…D. Вопрос на подумать. Распишите процедуру стохастического градиентного спуска для L 1 L 1 -регуляризованной линейной регрессии. Как вам кажется, почему никого не волнует, что функция потерь, строго говоря, не дифференцируема? Отметим, что L 1 L 1 - и L 2 L 2 -регуляризацию можно определять для любой функции потерь L(w,X,y) (и не только в задаче регрессии, а и, например, в задаче классификации тоже). Новая функция потерь будет соответственно равна (w,X,y)=L(w,X,y)+λ∥w∥ 1 или (w,X,y)=L(w,X,y)+λ∥w∥ 2 2 Разреживание весов в L 1 L 1 -регуляризации L 2 L 2 -регуляризация работает прекрасно и используется в большинстве случаев, но есть одна полезная особенность L 1 L 1 -регуляризации: её применение приводит к тому, что у признаков, которые не оказывают большого влияния на ответ, вес в результате оптимизации получается равным 0 0. Это позволяет удобным образом удалять признаки, слабо влияющие на таргет. Кроме того, это даёт возможность автоматически избавляться от признаков, которые участвуют в соотношениях приближённой линейной зависимости, соответственно, спасает от проблем, связанных с мультиколлинеарностью, о которых мы писали выше. Не очень строгим, но довольно интуитивным образом это можно объяснить так: В точке оптимума линии уровня регуляризационного члена касаются линий уровня основного лосса, потому что, во-первых, и те, и другие выпуклые, а во-вторых, если они пересекаются трансверсально, то существует более оптимальная точка: 1 Линии уровня L 1 L 1 -нормы – это N N-мерные октаэдры. Точки их касания с линиями уровня лосса, скорее всего, лежат на грани размерности, меньшей N − 1 N−1, то есть как раз в области, где часть координат равна нулю: 1 Заметим, что данное построение говорит о том, как выглядит оптимальное решение задачи, но ничего не говорит о способе, которым это решение можно найти. На самом деле, найти такой оптимум непросто: у L 1 L 1 меры довольно плохая производная. Однако, способы есть. Можете на досуге прочитать, например, вот эту статью о том, как работало предсказание CTR в google в 2012 году. Там этой теме посвящается довольно много места. Кроме того, рекомендуем посмотреть про проксимальные методы в разделе этой книги про оптимизацию в ML. Заметим также, что вообще-то оптимизация любой нормы , 0<x≤1, приведёт к появлению разреженных векторов весов, просто если c L 1 L 1 ещё хоть как-то можно работать, то с остальными всё будет ещё сложнее. Другие лоссы Стохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов. MAE Mean absolute error, абсолютная ошибка, появляется при замене L 2 L 2 нормы в MSE на MAE(y, y )= N 1 i=1 Можно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры,",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 12,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "как-то можно работать, то с остальными всё будет ещё сложнее. Другие лоссы Стохастический градиентный спуск можно очевидным образом обобщить для решения задачи линейной регрессии с любой другой функцией потерь, не только квадратичной: ведь всё, что нам нужно от неё, – это чтобы у функции потерь был градиент. На практике это делают редко, но тем не менее рассмотрим ещё пару вариантов. MAE Mean absolute error, абсолютная ошибка, появляется при замене L 2 L 2 нормы в MSE на MAE(y, y )= N 1 i=1 Можно заметить, что в MAE по сравнению с MSE существенно меньший вклад в ошибку будут вносить примеры, сильно удалённые от ответов модели. Дело тут в том, что в MAE мы считаем модуль расстояния, а не квадрат, соответственно, вклад больших ошибок в MSE получается существенно больше. Такая функция потерь уместна в случаях, когда вы пытаетесь обучить регрессию на данных с большим количеством выбросов в таргете. Иначе на эту разницу можно посмотреть так: MSE приближает матожидание условного распределения y ∣ x y∣x, а MAE – медиану. MAPE Mean absolute percentage error, относительная ошибка. MAPE(y, y )= N 1 i=1 Часто используется в задачах прогнозирования (например, погоды, загруженности дорог, кассовых сборов фильмов, цен), когда ответы могут быть различными по порядку величины, и при этом мы бы хотели верно угадать порядок, то есть мы не хотим штрафовать модель за предсказание 2000 вместо 1000 в разы сильней, чем за предсказание 2 вместо 1. Вопрос на подумать. Кроме описанных выше в задаче линейной регрессии можно использовать и другие функции потерь, например, Huber loss: где L(f,X,y)= i=1 −⟨w i ,x⟩), где h δ (z)={ 2 1 z 2 ,∣z∣⩽δ δ(∣z∣− 2 1 δ),∣z∣>δ Число δ δ является гиперпараметром. Сложная формула при ∣ z ∣ > δ ∣z∣>δ нужна, чтобы функция (z) была непрерывной. Попробуйте объяснить, зачем может быть нужна такая функция потерь. Линейная классификация Теперь давайте поговорим про задачу классификации. Для начала будем говорить про бинарную классификацию на два класса. Обобщить эту задачу до задачи классификации на K K классов не составит большого труда. Пусть теперь наши таргеты y y кодируют принадлежность к положительному или отрицательному классу, то есть принадлежность множеству {−1,1} (в этом параграфе договоримся именно так обозначать классы, хотя в жизни вам будут нередко встречаться и метки { 0 , 1 } {0,1}), а x x – по-прежнему векторы из R D R D . Мы хотим обучить линейную модель так, чтобы плоскость, которую она задаёт, как можно лучше отделяла объекты одного класса от другого. 1 В идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко. Как обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле y = sign y=sign⟨w,x i ⟩ Сконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть min ⁡ w i ∑ I[y i  =sign⟨w,x i ⟩]⟶ w min Домножим обе части",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 13,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "класса от другого. 1 В идеальной ситуации найдётся плоскость, которая разделит классы: положительный окажется с одной стороны от неё, а отрицательный с другой. Выборка, для которой это возможно, называется линейно разделимой. Увы, в реальной жизни такое встречается крайне редко. Как обучить линейную модель классификации, нам ещё предстоит понять, но уже ясно, что итоговое предсказание можно будет вычислить по формуле y = sign y=sign⟨w,x i ⟩ Сконструируем теперь функционал ошибки так, чтобы он вышеперечисленными проблемами не обладал. Мы хотим минимизировать число ошибок классификатора, то есть min ⁡ w i ∑ I[y i  =sign⟨w,x i ⟩]⟶ w min Домножим обе части на y i y i и немного упростим min ⁡ w i ∑ I[y i ⟨w,x i ⟩<0]⟶ w min Величина M=y i ⟨w,x i ⟩ называется отступом (margin) классификатора. Такая фунция потерь называется misclassification loss. Легко видеть, что отступ положителен, когда sign(y i )=sign(⟨w,x i ⟩), то есть класс угадан верно; при этом чем больше отступ, тем больше расстояние от x i x i до разделяющей гиперплоскости, то есть «уверенность классификатора»; отступ отрицателен, когда sign(y i )  =sign(⟨w,x i ⟩), то есть класс угадан неверно; при этом чем больше по модулю отступ, тем более сокрушительно ошибается классификатор. От каждого из отступов мы вычисляем функцию F(M)=I[M<0]={ 1, M<0, 0, M⩾0 Она кусочно-постоянная, и из-за этого всю сумму невозможно оптимизировать градиентными методами: ведь её производная равна нулю во всех точках, где она существует. Но мы можем мажорировать её какой-нибудь более гладкой функцией, и тогда задачу можно будет решить. Функции можно использовать разные, у них свои достоинства и недостатки, давайте рассмотрим несколько примеров: 1 Вопрос на подумать. Допустим, мы как-то обучили классификатор, и подавляющее большинство отступов оказались отрицательными. Правда ли нас постигла катастрофа? Вопрос на подумать. Предположим, что у нас есть два классификатора с примерно одинаковыми и достаточно приемлемыми значениями интересующей нас метрики. При этом одна почти всегда выдаёт предсказания с большими по модулю отступами, а вторая – с относительно маленькими. Верно ли, что первая модель лучше, чем вторая? Ошибка перцептрона Реализуем простейшую идею: давайте считать отступы только на неправильно классифицированных объектах и учитывать их не бинарно, а линейно, пропорционально их размеру. Получается такая функция: F ( M ) = max F(M)=max(0,−M) Давайте запишем такой лосс с L 2 L 2 -регуляризацией: max L(w,x,y)=λ∣∣w∣∣ 2 2 + i ∑ max(0,−y i ⟨w,x i ⟩) Найдём градиент: L(w,x,y)=2λw+ ⟨w,x i ⟩>0 y i ⟨w,x i ⟩≤0 Имея аналитическую формулу для градиента, мы теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена. Данная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения. Она решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс: 1 Hinge loss, SVM Для таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 14,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "теперь можем так же, как и раньше, применить стохастический градиентный спуск, и задача будет решена. Данная функция потерь впервые была предложена для перцептрона Розенблатта, первой вычислительной модели нейросети, которая в итоге привела к появлению глубокого обучения. Она решает задачу линейной классификации, но у неё есть одна особенность: её решение не единственно и сильно зависит от начальных параметров. Например, все изображённые ниже классификаторы имеют одинаковый нулевой лосс: 1 Hinge loss, SVM Для таких случаев, как на картинке выше, возникает логичное желание не только найти разделяющую прямую, но и постараться провести её на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ: 1 Это можно сделать, слегка поменяв функцию ошибки, а именно положив её равной: F ( M ) = max F(M)=max(0,1−M) max L(w,x,y)=λ∥∣w∥∣ 2 2 + i ∑ max(0,1−y i ⟨w,x L(w,x,y)=2λw+ 1−y i ⟨w,x i ⟩≤0 1−y i ⟨w,x i ⟩>0 Почему же добавленная единичка приводит к желаемому результату? Интуитивно это можно объяснить так: объекты, которые проклассифицированы правильно, но не очень \"уверенно\" (то есть 0≤y i ⟨w,x i ⟩<1), продолжают вносить свой вклад в градиент и пытаются \"отодвинуть\" от себя разделяющую плоскость как можно дальше. К данному выводу можно прийти и чуть более строго; для этого надо совершенно по-другому взглянуть на выражение, которое мы минимизируем. Поможет вот эта картинка: 1 Если мы максимизируем минимальный отступ, то надо максимизировать 2 ∥ w ∥ 2 ∥w∥ 2 2 , то есть ширину полосы при условии того, что большинство объектов лежат с правильной стороны, что эквивалентно решению нашей исходной задачи: max min ⁡ w λ∥w∥ 2 2 + i ∑ max(0,1−y i ⟨w,x i ⟩)⟶ w min Отметим, что первое слагаемое у нас обратно пропорционально ширине полосы, но мы и максимизацию заменили на минимизацию, так что тут всё в порядке. Второе слагаемое – это штраф за то, что некоторые объекты неправильно расположены относительно разделительной полосы. В конце концов, никто нам не обещал, что классы наши линейно разделимы и можно провести оптимальную плоскость вообще без ошибок. Итоговое положение плоскости задаётся всего несколькими обучающими примерами. Это ближайшие к плоскости правильно классифицированные объекты, которые называют опорными векторами или support vectors. Весь метод, соответственно, зовётся методом опорных векторов, или support vector machine, или сокращённо SVM. Начиная с шестидесятых годов это был сильнейший из известных методов машинного обучения. В девяностые его сменили методы, основанные на деревьях решений, которые, в свою очередь, недавно передали «пальму первенства» нейросетям. Почему же SVM был столь популярен? Из-за небольшого количества параметров и доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM. Другие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности. Строгий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова. Логистическая регрессия В этом",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 15,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "доказуемой оптимальности. Сейчас для нас нормально выбирать специальный алгоритм под задачу и подбирать оптимальные гиперпараметры для этого алгоритма перебором, а когда-то трава была зеленее, а компьютеры медленнее, и такой роскоши у людей не было. Поэтому им нужны были модели, которые гарантированно неплохо работали бы в любой ситуации. Такой моделью и был SVM. Другие замечательные свойства SVM: существование уникального решения и доказуемо минимальная склонность к переобучению среди всех популярных классов линейных классификаторов. Кроме того, несложная модификация алгоритма, ядровый SVM, позволяет проводить нелинейные разделяющие поверхности. Строгий вывод постановки задачи SVM можно прочитать тут или в лекции К.В. Воронцова. Логистическая регрессия В этом параграфе мы будем обозначать классы нулём и единицей. Ещё один интересный метод появляется из желания посмотреть на классификацию как на задачу предсказания вероятностей. Хороший пример – предсказание кликов в интернете (например, в рекламе и поиске). Наличие клика в обучающем логе не означает, что, если повторить полностью условия эксперимента, пользователь обязательно кликнет по объекту опять. Скорее у объектов есть какая-то \"кликабельность\", то есть истинная вероятность клика по данному объекту. Клик на каждом обучающем примере является реализацией этой случайной величины, и мы считаем, что в пределе в каждой точке отношение положительных и отрицательных примеров должно сходиться к этой вероятности. Проблема состоит в том, что вероятность, по определению, величина от 0 до 1, а простого способа обучить линейную модель так, чтобы это ограничение соблюдалось, нет. Из этой ситуации можно выйти так: научить линейную модель правильно предсказывать какой-то объект, связанный с вероятностью, но с диапазоном значений (−∞,∞), и преобразовать ответы модели в вероятность. Таким объектом является logit или log odds – логарифм отношения вероятности положительного события к отрицательному log log( 1−p p ). Если ответом нашей модели является log log( 1−p p ), то искомую вероятность посчитать не трудно: log ⟨w,x i ⟩=log( 1−p ⟨w,x i ⟩ = 1−p 1+e −⟨w,x i ⟩ 1 Функция в правой части называется сигмоидой и обозначается σ(z)= 1+e −z 1 Таким образом, p=σ(⟨w,x i ⟩) Как теперь научиться оптимизировать w w так, чтобы модель как можно лучше предсказывала логиты? Нужно применить метод максимума правдоподобия для распределения Бернулли. Это самое простое распределение, которое возникает, к примеру, при бросках монетки, которая орлом выпадает с вероятностью p p. У нас только событием будет не орёл, а то, что пользователь кликнул на объект с такой вероятностью. Если хотите больше подробностей, почитайте про распределение Бернулли в теоретическом минимуме. Правдоподобие позволяет понять, насколько вероятно получить данные значения таргета y y при данных X X и весах w w. Оно имеет вид p(y∣X,w)= i ∏ p(y i ∣x i ,w) и для распределения Бернулли его можно выписать следующим образом: p(y∣X,w)= (1−p i ) 1−y i где p i p i – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше: log log ℓ(w,X,y)= i ∑ (y i log(p i )+(1−y i )log(1−p i ))= log log log(σ(⟨w,x i ⟩))+(1−y i )log(1−σ(⟨w,x i ⟩))) Если заметить, что σ(−z)= 1+e =1−σ(z), то выражение можно переписать проще:",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 16,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и весах w w. Оно имеет вид p(y∣X,w)= i ∏ p(y i ∣x i ,w) и для распределения Бернулли его можно выписать следующим образом: p(y∣X,w)= (1−p i ) 1−y i где p i p i – это вероятность, посчитанная из ответов модели. Оптимизировать произведение неудобно, хочется иметь дело с суммой, так что мы перейдём к логарифмическому правдоподобию и подставим формулу для вероятности, которую мы получили выше: log log ℓ(w,X,y)= i ∑ (y i log(p i )+(1−y i )log(1−p i ))= log log log(σ(⟨w,x i ⟩))+(1−y i )log(1−σ(⟨w,x i ⟩))) Если заметить, что σ(−z)= 1+e =1−σ(z), то выражение можно переписать проще: log log ℓ(w,X,y)= i ∑ (y i log(σ(⟨w,x i ⟩))+(1−y i )log(σ(−⟨w,x i ⟩))) Нас интересует w w, для которого правдоподобие максимально. Чтобы получить функцию потерь, которую мы будем минимизировать, умножим его на минус один: log log L(w,X,y)=− i ∑ (y i log(σ(⟨w,x i ⟩))+(1−y i )log(σ(−⟨w,x i ⟩))) В отличие от линейной регрессии, для логистической нет явной формулы решения. Деваться некуда, будем использовать градиентный спуск. К счастью, градиент устроен очень просто: L(y,X,w)=− −σ(⟨w,x i ⟩)) Предсказание модели будет вычисляться, как мы договаривались, следующим образом: p=σ(⟨w,x i ⟩) Это вероятность положительного класса, а как от неё перейти к предсказанию самого класса? В других методах нам достаточно было посчитать знак предсказания, но теперь все наши предсказания положительные и находятся в диапазоне от 0 до 1. Что же делать? Интуитивным и не совсем (и даже совсем не) правильным является ответ «взять порог 0.5». Более корректным будет подобрать этот порог отдельно, для уже построенной регрессии минимизируя нужную вам метрику на отложенной тестовой выборке. Например, сделать так, чтобы доля положительных и отрицательных классов примерно совпадала с реальной. Отдельно заметим, что метод называется логистической регрессией, а не логистической классификацией именно потому, что предсказываем мы не классы, а вещественные числа – логиты. Вопрос на подумать. Проверьте, что, если метки классов – это ± 1 ±1, а не 0 0 и 1 1, то функцию потерь для логистической регрессии можно записать в более компактном виде: log L(w,X,y)= i=1 ∑ N log(1+e −y i ⟨w,x i ⟩ ) Вопрос на подумать. Правда ли разделяющая поверхность модели логистической регрессии является гиперплоскостью? Вопрос на подумать. Допустим, что матрица объекты-признаки X X имеет полный ранг по столбцам (то есть все её столбцы линейно независимы). Верно ли, что решение задачи восстановления логистической регрессии единственно? Вопрос на подумать. На картинке ниже представлены результаты работы на одном и том же датасете трёх моделей логистической регрессии с разными коэффициентами L 2 L 2 -регуляризации: 1 Наверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности. Как вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему? Многоклассовая классификация В этом разделе мы будем следовать изложению из лекций Евгения Соколова. Пусть каждый объект нашей выборки относится к одному из K K классов: Y={1,…,K}. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 17,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "регрессии с разными коэффициентами L 2 L 2 -регуляризации: 1 Наверху показаны предсказанные вероятности положительного класса, внизу – вид разделяющей поверхности. Как вам кажется, какие картинки соответствуют самому большому коэффициенту регуляризации, а какие – самому маленькому? Почему? Многоклассовая классификация В этом разделе мы будем следовать изложению из лекций Евгения Соколова. Пусть каждый объект нашей выборки относится к одному из K K классов: Y={1,…,K}. Чтобы предсказывать эти классы с помощью линейных моделей, нам придётся свести задачу многоклассовой классификации к набору бинарных, которые мы уже хорошо умеем решать. Мы разберём два самых популярных способа это сделать – one-vs-all и all-vs-all, а проиллюстрировать их нам поможет вот такой игрушечный датасет 1 Один против всех (one-versus-all) Обучим K K линейных классификаторов (x),…,b K (x), выдающих оценки принадлежности классам 1 , … , K 1,…,K соответственно. В случае с линейными моделями эти классификаторы будут иметь вид sgn (x)=sgn(⟨w k ,x⟩+w 0k ) Классификатор с номером k k будем обучать по выборке ,2I[y i =k]−1) i=1 N ; иными словами, мы учим классификатор отличать k k-й класс от всех остальных. Логично, чтобы итоговый классификатор выдавал класс, соответствующий самому уверенному из бинарных алгоритмов. Уверенность можно в каком-то смысле измерить с помощью значений линейных функций: a ( x ) = argmax a(x)=argmax k (⟨w k ,x⟩+w 0k ) Давайте посмотрим, что даст этот подход применительно к нашему датасету. Обучим три линейных модели, отличающих один класс от остальных: 1 Теперь сравним значения линейных функций 1 и для каждой точки выберем тот класс, которому соответствует большее значение, то есть самый «уверенный» классификатор: 1 Хочется сказать, что самый маленький класс «обидели». Проблема данного подхода заключается в том, что каждый из классификаторов (x),…,b K (x) обучается на своей выборке, и значения линейных функций ,x⟩+w 0k или, проще говоря, \"выходы\" классификаторов могут иметь разные масштабы. Из-за этого сравнивать их будет неправильно. Нормировать вектора весов, чтобы они выдавали ответы в одной и той же шкале, не всегда может быть разумным решением: так, в случае с SVM веса перестанут являться решением задачи, поскольку нормировка изменит норму весов. Все против всех (all-versus-all) Обучим классификаторов (x), i,j=1,…,K, i ≠ j i  =j. Например, в случае с линейными моделями эти модели будут иметь вид sgn (x)=sgn(⟨w ij ,x⟩+w 0,ij ) Классификатор (x) будем настраивать по подвыборке ⊂X, содержащей только объекты классов i i и j j. Соответственно, классификатор (x) будет выдавать для любого объекта либо класс i i, либо класс j j. Проиллюстрируем это для нашей выборки: 1 Чтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за свой класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов: a ( x ) = argmax a(x)=argmax k i=1 I[a ij (x)=k] Для нашего датасета получается следующая картинка: 1 Обратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание. Многоклассовая логистическая регрессия Некоторые методы бинарной классификации",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 18,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "1 Чтобы классифицировать новый объект, подадим его на вход каждого из построенных бинарных классификаторов. Каждый из них проголосует за свой класс; в качестве ответа выберем тот класс, за который наберется больше всего голосов: a ( x ) = argmax a(x)=argmax k i=1 I[a ij (x)=k] Для нашего датасета получается следующая картинка: 1 Обратите внимание на серый треугольник на стыке областей. Это точки, для которых голоса разделились (в данном случае каждый классификатор выдал какой-то свой класс, то есть у каждого класса было по одному голосу). Для этих точек нет явного способа выдать обоснованное предсказание. Многоклассовая логистическая регрессия Некоторые методы бинарной классификации можно напрямую обобщить на случай многих классов. Выясним, как это можно проделать с логистической регрессией. В логистической регрессии для двух классов мы строили линейную модель b(x)=⟨w,x⟩+w 0 , а затем переводили её прогноз в вероятность с помощью сигмоидной функции exp ⁡ ( − z ) σ(z)= 1+exp(−z) 1 . Допустим, что мы теперь решаем многоклассовую задачу и построили K K линейных моделей (x)=⟨w k ,x⟩+w 0k , каждая из которых даёт оценку принадлежности объекта одному из классов. Как преобразовать вектор оценок (x),…,b K (x)) в вероятности? Для этого можно воспользоваться оператором softmax softmax(z 1 ,…,z K ), который производит «нормировку» вектора: softmax exp exp exp exp softmax(z 1 ,…,z K )=( ∑ k=1 K exp(z k ) exp(z 1 ) ,…, ∑ k=1 K exp(z k ) exp(z K ) ). В этом случае вероятность k k-го класса будет выражаться как exp exp P(y=k∣x,w)= ∑ j=1 K exp(⟨w j ,x⟩+w 0j ) exp(⟨w k ,x⟩+w 0k ) . Обучать эти веса предлагается с помощью метода максимального правдоподобия: так же, как и в случае с двухклассовой логистической регрессией: ∑ i = 1 N log max i=1 ∑ N logP(y=y i ∣x i ,w)→ w 1 ,…,w K max Масштабируемость линейных моделей Мы уже обсуждали, что SGD позволяет обучению хорошо масштабироваться по числу объектов, так как мы можем не загружать их целиком в оперативную память. А что делать, если признаков очень много, или мы не знаем заранее, сколько их будет? Такое может быть актуально, например, в следующих ситуациях: Классификация текстов: мы можем представить текст в формате «мешка слов», то есть неупорядоченного набора слов, встретившихся в данном тексте, и обучить на нём, например, определение тональности отзыва в интернете. Наличие каждого слова из языка в тексте у нас будет кодироваться отдельной фичой. Тогда размерность каждого элемента обучающей выборки будет порядка нескольких сотен тысяч. В задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка ⋅10 7 =10 16 . Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы. Есть несколько хаков, которые позволяют бороться с такими проблемами: Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора. Даже",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 19,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "задаче предсказания кликов по рекламе можно получить выборку любой размерности, например, так: в качестве фичи закодируем индикатор того, что пользователь X побывал на веб-странице Y. Суммарная размерность тогда будет порядка ⋅10 7 =10 16 . Кроме того, всё время появляются новые пользователи и веб-страницы, так что на этапе применения нас ждут сюрпризы. Есть несколько хаков, которые позволяют бороться с такими проблемами: Несмотря на то, что полная размерность объекта в выборке огромна, количество ненулевых элементов в нём невелико. Значит, можно использовать разреженное кодирование, то есть вместо плотного вектора хранить словарь, в котором будут перечислены индексы и значения ненулевых элементов вектора. Даже хранить все веса не обязательно! Можно хранить их в хэш-таблице и вычислять индекс по формуле hash(feature) % tablesize. Хэш может вычисляться прямо от слова или id пользователя. Таким образом, несколько фичей будут иметь общий вес, который тем не менее обучится оптимальным образом. Такой подход называется hashing trick. Ясно, что сжатие вектора весов приводит к потерям в качестве, но, как правило, ценой совсем небольших потерь можно сжать этот вектор на много порядков. Примером открытой библиотеки, в которой реализованы эти возможности, является vowpal wabbit. Parameter server Если при решении задачи ставки столь высоки, что мы не можем разменивать качество на сжатие вектора весов, а признаков всё-таки очень много, то задачу можно решать распределённо, храня все признаки в шардированной хеш-таблице 1 Кружки здесь означают отдельные сервера. Жёлтые загружают данные, а серые хранят части модели. Для обучения жёлтый кружок запрашивает у серого нужные ему для предсказания веса, считает градиент и отправляет его обратно, где тот потом применяется. Схема обладает бесконечной масштабируемостью, но задач, где это оправдано, не очень много. Подытожим На линейную модель можно смотреть как на однослойную нейросеть, поэтому многие методы, которые были изначально разработаны для них, сейчас переиспользуются в задачах глубокого обучения, а базовые подходы к регрессии, классификации и оптимизации вообще выглядят абсолютно так же. Так что несмотря на то, что в целом линейные модели на сегодня применяются редко, то, из чего они состоят и как строятся, знать очень и очень полезно. Надеемся также, что главным итогом прочтения этого параграфа для вас будет осознание того, что решение любой ML-задачи состоит из выбора функции потерь, параметризованного класса моделей и способа оптимизации. В следующих параграфах мы познакомимся с другими моделями и оптимизаторами, но эти базовые принципы не изменятся. Теперь предлагаем вам потренировать изученный материал на практике. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике! Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Выполните задачи урока 0 / 7 выполнено Сообщить об ошибке Предыдущий параграф 1.3. Машинное обучение Что такое машинное обучение и каким оно бывает. Основные понятия машинного обучения: признаки, таргеты, метрики, переобучение Следующий параграф 2.2. Метрические методы Алгоритмы KNN. Быстрый поиск ближайших соседей",
    "metadata": {
      "title": "Линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/linear-models",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.1",
      "part": 20,
      "total_parts": 20,
      "source_file": "2.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Смысл метрических методов очень хорошо раскрывает фраза «Скажи мне, кто твой друг, и я скажу, кто ты». Алгоритмы этого класса почти не имеют фазы обучения. Вместо этого они просто запоминают всю обучающую выборку, а на этапе предсказания просто ищут объекты, похожие на целевой. Такой процесс называют lazy learning, потому что никакого обучения, по сути, не происходит. Также метрические модели являются непараметрическими, потому что они не делают явных допущений о глобальных законах, которым подчиняются данные. Так, линейная регрессия основывается на предположении о том, что изучаемая закономерность линейная (с неизвестными коэффициентами, которые восстанавливаются по выборке), а линейная бинарная классификация — что существует гиперплоскость, неплохо разделяющая классы. Метрические методы же локальны: они исходят из допущения, что свойства объекта можно узнать, имея представление о его соседях. Указанные выше свойства могут быть полезными, особенно в случае сложно устроенных данных, для которых мы не можем придумать глобальную модель. Однако с другой стороны, из-за lazy learning алгоритм становится абсолютно неприменимым при большом количестве данных. Несмотря на то, что эти алгоритмы очень просты для понимания, они довольно точны и хорошо интерпретируемы — и часто используются как минимум в качестве бейзлайнов в разных задачах. В первой части параграфа мы расскажем об одном из самых известных метрических алгоритмов — методе k-ближайших соседей (k-nearest neighbors, KNN). Этот подход в основном чисто инженерный из-за отсутствия фазы обучения — в настоящее время уже почти нигде не применяется. Однако многие техники, на которых основан алгоритм, используются и в других методах. Например, у алгоритмов поиска ближайших соседей, — неотъемлемой часть метода, — намного более широкая область применения. Плюс ко всему KNN — очень простой и легко интерпретируемый алгоритм, поэтому изучить его всё равно полезно. Мы обсудим подробнее его преимущества, недостатки, область его применения, а также возможные обобщения. Для метрических методов очень важно уметь эффективно находить ближайшие объекты, поэтому задача их поиска неизбежно возникает при применении любого такого алгоритма. Поэтому во второй части параграфа мы рассмотрим возможные подходы к быстрому поиску ближайших соседей. Метод k-ближайших соседей (KNN) Представим, что мы проводим классификацию объектов на два класса — красный или жёлтый. Нам дана некоторая обучающая выборка и целевой объект (серый): 2 Мы хотим определить, к какому классу относится серый объект. Интуитивно очевидно, что он должен быть жёлтым, потому что все его соседи жёлтые. Эта интуиция и отражает суть метода KNN — классифицировать целевой объект, исходя из того, какие классы у объектов, которые максимально похожи на него. Перейдём теперь к более формальному описанию алгоритма. Рассмотрим сначала задачу многоклассовой классификации, а регрессией займёмся позже. Пусть дана обучающая выборка X=(x i ,y i ) i=1 N , где ∈X, y i ∈Y= 1,…,C. Пусть также задана некоторая симметричная по своим аргументам функция расстояния ρ:X×X→[0,+∞). Предположим, что требуется классифицировать новый объект u u. Для этого найдём k k наиболее близких к u u в смысле расстояния ρ ρ объектов обучающей выборки (u)= x u (1) ,…,x u (k) (1) ∀x in ∈X k (u) ∀x out ∈X∖X k (u)ρ(u,x in )⩽ρ(u,x out ).(1) Метку класса объекта (i) будем обозначать (i) . Класс нового объекта тогда естественным образом определим как наиболее часто",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 1,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "классификации, а регрессией займёмся позже. Пусть дана обучающая выборка X=(x i ,y i ) i=1 N , где ∈X, y i ∈Y= 1,…,C. Пусть также задана некоторая симметричная по своим аргументам функция расстояния ρ:X×X→[0,+∞). Предположим, что требуется классифицировать новый объект u u. Для этого найдём k k наиболее близких к u u в смысле расстояния ρ ρ объектов обучающей выборки (u)= x u (1) ,…,x u (k) (1) ∀x in ∈X k (u) ∀x out ∈X∖X k (u)ρ(u,x in )⩽ρ(u,x out ).(1) Метку класса объекта (i) будем обозначать (i) . Класс нового объекта тогда естественным образом определим как наиболее часто встречающийся класс среди объектов из (u): a ( u ) = argmax⁡y∈Y (2) a(u)= y∈Y argmax i=1 ∑ k I[y u (i) =y](2) Формула может показаться страшной, но на самом деле всё довольно просто: для каждой метки класса y ∈ Y y∈Y количество соседей u u с такой меткой можно посчитать, просто просуммировав по всем соседям индикаторы событий, соответствующих тому, что метка соседа равна y y. Легко заметить, что этот алгоритм позволяет также оценивать вероятности классов. Для этого достаточно просто посчитать частоты классов соседей: P(u∼y)= k ∑ i=1 k I[y u (i) =y] Стоит, однако, понимать, что, хоть такая функция и удовлетворяет свойствам вероятности (она неотрицательна, аддитивна и ограничена единицей), это не более чем эвристика. Несмотря на то что формально фаза обучения отсутствует, алгоритм может легко переобучиться. Вы можете убедиться в этом сами, использовав маленькое количество соседей (например, одного или двух), — границы классов оказываются довольно сложными. Происходит это из-за того, что параметрами алгоритма можно считать всю обучающую выборку, довольно большую по размеру. Из-за этого алгоритму легко подстроиться под конкретные данные. По ссылке вы можете увидеть интерактивный пример работы алгоритма. Автор примера - Анастасия Чирикова. Выбор метрики Может возникнуть закономерный вопрос, как же правильно выбрать функцию расстояния ρ ρ. В подавляющем большинстве случаев обычное евклидово расстояние ρ(x,y)= будет хорошим выбором. Однако в некоторых случаях другие функции будут подходить лучше, поэтому давайте разберём ещё несколько функций, наиболее используемых на практике. 2 Манхэттенская метрика ρ(x,y)= Часто используется в высокоразмерных пространствах из-за лучшей устойчивости к выбросам. Представим, что два объекта в 1000-размерном пространстве почти идентичны, но сильно отличаются по одному из признаков. Это почти наверняка свидетельствует о выбросе в этом признаке, и объекты, скорее всего, очень близки. Однако евклидово расстояние усилит различие в единственном признаке и сделает их более далёкими друг от друга. Этого недостатка лишена манхэттенская метрика — в ней вместо квадрата используется модуль. Метрика Минковского ρ(x,y)=( 1/p Является обобщением евклидовой ( p = 2 p=2) и манхэттенской ( p = 1 p=1) метрик. Косинусное расстояние cos ρ(x,y)=1−cosθ=1− ∥x∥∥y∥ x⋅y Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 2,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "используется модуль. Метрика Минковского ρ(x,y)=( 1/p Является обобщением евклидовой ( p = 2 p=2) и манхэттенской ( p = 1 p=1) метрик. Косинусное расстояние cos ρ(x,y)=1−cosθ=1− ∥x∥∥y∥ x⋅y Эта метрика хороша тем, что не зависит от норм векторов. Такое поведение бывает полезно в некоторых задачах, например при поиске похожих документов. В качестве признаков там часто используются количества слов. При этом интуитивно кажется, что если в тексте использовать каждое слово в два раза больше, то тема этого текста поменяться не должна. Поэтому как раз в этом случае нам не важна норма вектор-признака, и в задачах, связанных с текстами, часто применяется именно косинусное расстояние. Расстояние Жаккара ρ(A,B)=1− ∣A∪B∣ ∣A∩B∣ Его стоит использовать, если исследуемые объекты — это некоторые множества. Это полезно тем, что нет нужды придумывать векторные представления для этих множеств, чтобы использовать традиционные метрики. Вообще говоря, несмотря на некоторые эвристические соображения по выбору метрики, её можно считать гиперпараметром и подбирать соответствующими способами. Часто качество модели сильно зависит от выбора метрики, а иногда выбрать правильную метрику очень тяжело. Например, в случае когда данные имеют сильно разный масштаб, выбрать подходящую метрику почти невозможно, и нужно сперва проводить нормализацию. Замечание. Упомянутые в этом параграфе функции мы называем «метриками», но, конечно же, они не обязаны быть метриками в строгом математическом смысле. Они неотрицательны и симметричны, но могут не удовлетворять неравенству треугольника. Обобщения алгоритма Взвешенный KNN У оригинального алгоритма есть один большой недостаток: он никак не учитывает расстояния до соседних объектов, хотя эта информация может быть полезной. Давайте попробуем придумать, как исправить этот недостаток. Нам нужно каким-то образом увеличивать вклад близких объектов и уменьшать вклад далёких. Можно заметить, что все индикаторы в формуле ( 2 ) (2) учитываются в сумме с одинаковыми коэффициентами. Возникает идея — назначить этим индикаторам веса, которые тем больше, чем ближе объект к целевому. Таким образом, получаем следующую формулу: a ( u ) = argmax⁡y∈Y (3) a(u)= y∈Y argmax i=1 ∑ k w i I[y u (i) =y].(3) Такой алгоритм называется взвешенным KNN (weighted KNN). Есть множество вариантов выбора весов для объектов, которые можно поделить на две большие группы. В первой группе веса зависят лишь от порядкового номера объекта в отсортированном по близости к u u массиве (u). Чаще всего затухающие веса берутся линейно k+1−i ) или экспоненциально , 0<q<1) . Однако здесь мы также не используем всю информацию, которая нам доступна. Зачем использовать порядок соседей, порождаемый расстояниями, если можно использовать сами расстояния? Во второй группе методов вес — это некоторая функция от расстояния. Давайте подумаем, какие должны быть свойства у этой функции. Очевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями. Также необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далёких. Таким образом вводится так называемая ядерная функция (kernel function) K : R → R K:R→R, обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа: a ( u ) = argmax⁡y∈Y (4) a(u)= y∈Y argmax i=1 ∑ k K( h ρ(u,x u (i) ) )I[y u (i) =y],(4) где h h — некое положительное",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 3,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "от расстояния. Давайте подумаем, какие должны быть свойства у этой функции. Очевидно, она должна быть положительной на своей области определения, иначе модель будет поощрять несовпадение с некоторыми ближайшими соседями. Также необходимо, чтобы функция монотонно не возрастала, чтобы вес близких соседей был больше, чем далёких. Таким образом вводится так называемая ядерная функция (kernel function) K : R → R K:R→R, обладающая перечисленными выше свойствами, с помощью которой и высчитывается вес каждого соседа: a ( u ) = argmax⁡y∈Y (4) a(u)= y∈Y argmax i=1 ∑ k K( h ρ(u,x u (i) ) )I[y u (i) =y],(4) где h h — некое положительное число, которое называется шириной окна. От выбора ядра зависит гладкость аппроксимации, но на её качество этот выбор почти не влияет. Примеры ядерных функций в порядке увеличения их гладкости: K(x)= 2 1 I[∣x∣⩽1] — прямоугольное ядро; K(x)=(1−∣x∣)I[∣x∣⩽1] — треугольное ядро (непрерывное); K(x)= 4 3 (1−x 2 )I[∣x∣⩽1] — ядро Епанечникова (гладкое везде, кроме –1 и 1); K(x)= 16 15 (1−x 2 ) 2 I[∣x∣⩽1] — биквадратное ядро (гладкое везде); K(x)= 2π 1 e −2x 2 — гауссовское ядро (бесконечно гладкое везде). На практике чаще всего используют либо прямоугольное для простоты, либо гауссовское, в случае когда важна гладкость модели (немного забегая вперёд — это особенно важно в регрессии). Ширина окна, в свою очередь, сильно влияет как раз на качество модели. При слишком маленькой ширине модель сильно подстраивается под обучающую выборку и теряет свою обобщающую способность. При слишком большой ширине, напротив, модель становится слишком простой. Универсальной ширины окна не существует, поэтому для каждой задачи её приходится подбирать отдельно. Kernel regression Алгоритм KNN можно довольно легко обобщить и на задачу регрессии. Самые очевидные способы — брать для некоторого ядра K K либо обычное среднее: (5) a(u)= k 1 i=1 ∑ k y u (i) ,(5) либо взвешенный вариант: (6) a(u)= ∑ i=1 k K( h ρ(u,x u (i) ) ) ∑ i=1 k K( h ρ(u,x u (i) ) )y u (i) (6) Последняя формула называется формулой Надарая — Ватсона. Она — один из непараметрических методов восстановления регрессии, объединённых названием ядерная регрессия (kernel regression). Выписать ответ, конечно, просто, но возникает интересный вопрос: можно ли использовать оптимизационные формулы из задачи классификации? Сначала давайте подумаем, что выдаст алгоритм, если формулу ( 4 ) (4) применить без изменений. В задаче регрессии почти наверняка все значения (i) будут различными. Поэтому для любого y y сумма в формуле ( 4 ) (4) будет состоять из не более чем одного слагаемого, а значит, максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм. Давайте сперва подумаем, а для чего вообще в формуле ( 4 ) (4) используется индикатор. В задаче классификации индикатор — естественная мера близости двух объектов: если объекты совпадают, то значение 1 1, если различаются, то 0 0. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдаёт отличное от нуля значение лишь в одной точке y =",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 4,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "слагаемого, а значит, максимум будет достигаться на соседе с наибольшим весом, то есть на ближайшем соседе. Это означает, что метод всегда вырождается в 1-NN. Это не совсем то, чего мы добиваемся, поэтому давайте немного модифицируем алгоритм. Давайте сперва подумаем, а для чего вообще в формуле ( 4 ) (4) используется индикатор. В задаче классификации индикатор — естественная мера близости двух объектов: если объекты совпадают, то значение 1 1, если различаются, то 0 0. Проблема в том, что в задаче регрессии объекты являются действительными числами, и для них функция, которая выдаёт отличное от нуля значение лишь в одной точке y = y i y=y i , — плохая мера близости. В случае непрерывных значений y y естественно использовать более гладкие функции для выражения близости. Таким образом, для обобщения формулы ( 4 ) (4) на задачу регрессии нам необходимо всего лишь заменить индикатор на некоторую более гладкую функцию. При этом для действительных чисел чаще всего рассматривают не близость, а расстояние между ними, то есть некоторую метрику. Например, в качестве такой метрики можно взять квадрат евклидова расстояния (y−y u (i) ) 2 . Отметим, что максимизация близости эквивалентна минимизации расстояния, и получим следующую формулу: a ( u ) = argmin⁡y∈R (7) a(u)= y∈R argmin i=1 ∑ k K( h ρ(u,x u (i) ) )(y−y u (i) ) 2 .(7) Выбор именно этой функции хорош тем, что у этой оптимизационной задачи есть точное решение, и оно записывается как раз формулой ( 6 ) (6). Для ядерной регрессии справедливы те же рассуждения про выбор ядра и ширины окна, которые были приведены в прошлом разделе про классификацию. Влияние ширины окна и вида ядра на вид функции: 2 2 2 Преимущества и недостатки Сперва поговорим о преимуществах алгоритма. Непараметрический, то есть не делает явных предположений о распределении данных. Очень простой в объяснении и интерпретации. Достаточно точный, хоть и чаще всего уступает градиентному бустингу и случайному лесу в accuracy. Может быть использован как для классификации, так и для регрессии. Несмотря на большие преимущества, алгоритм не лишён и минусов. Неэффективный по памяти, поскольку нужно хранить всю обучающую выборку. Вычислительно дорогой по той же причине. Чувствителен к масштабу данных, а также к неинформативным признакам. Для применения алгоритма необходимо, чтобы метрическая близость объектов совпадала с их семантической близостью, чего не всегда просто добиться. Представим, например, что мы решаем задачу нахождения похожих изображений. Мы хотим, чтобы картинки с лесом находились близко друг к другу, однако, если взять любую попиксельную метрику, такие картинки могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления. Применение Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них: Рекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна. Поиск семантически похожих документов. Если векторные представления близки друг к другу, то",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 5,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "могут быть очень далеки друг от друга. Зачастую для решения этой проблемы вначале обучают представления. Применение Из-за своих недостатков алгоритм очень неэффективен в задачах с большим количеством данных. Однако у него всё равно есть много применений в реальном мире. Приведём лишь некоторые из них: Рекомендательные системы. Если посмотреть на саму формулировку задачи «предложить пользователю что-то похожее на то, что он любит», то KNN прямо напрашивается в качестве решения. Несмотря на то что сейчас часто используются более совершенные алгоритмы, метод ближайших соседей всё равно применяется в качестве хорошего бейзлайна. Поиск семантически похожих документов. Если векторные представления близки друг к другу, то темы документов схожи. Поиск аномалий и выбросов. Из-за того что алгоритм запоминает обучающую выборку полностью, ему легко посмотреть, насколько целевой объект похож на все данные, которые он видел. Задача кредитного скоринга. Рейтинги двух людей, у которых примерно одинаковая зарплата, схожие должности и кредитные истории, не должны сильно отличаться, поэтому KNN отлично подходит для решения такой задачи. Вопрос сложности алгоритма неочевиден и требует детального анализа, который будет частично проведён в следующем разделе. Поиск ближайших соседей Для того чтобы применять метод ближайших соседей, нужно уметь как-то находить этих самых соседей. С первого взгляда может показаться, что никакой проблемы нет: действительно, можно ведь просто перебрать все объекты из обучающей выборки X=(x i ,y i ) i=1 N , посчитать для каждого из них расстояние до тестового объекта и затем найти минимум. Однако несмотря на то что сложность такого поиска линейная по N N, она также зависит и от размерности пространства признаков. Если x ∈ R D x∈R D , то сложность такого алгоритма поиска O ( N D ) O(ND). Если вспомнить, что в типичной задаче машинного обучения количество признаков D D может быть порядка 100 100, а размер выборки и вовсе может исчисляться десятками и сотнями тысяч объектов, то становится ясно, что такая сложность никуда не годится. Проблема осложняется ещё и тем, что данный поиск необходимо выполнять на этапе применения модели, который должен быть быстрым. Всё это означает, что возникает необходимость в более быстрых методах поиска ближайших соседей, чем простой перебор. Все такие методы можно поделить на две основные группы: точные и приближённые. Последние, как следует из их названия, находят соседей лишь приближённо, то есть найденные объекты хоть и будут действительно близки, но не обязательно будут самыми близкими. В этом разделе мы подробнее рассмотрим методы из каждой группы. Перед началом обзора стоит сказать, что хоть мы и рассматриваем алгоритмы поиска соседей именно в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в «Гугле» или «Яндексе» до всем известных алгоритмов «Ютьюба». Поиск ближайших соседей: точные методы Точных методов существует довольно мало. Можно сказать, что их, по сути, два. Первый — полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 6,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "в контексте их использования в KNN, область их применения значительно шире, и она не ограничивается исключительно машинным обучением. Например, на их основе работает любая информационно-поисковая система, от поиска в «Гугле» или «Яндексе» до всем известных алгоритмов «Ютьюба». Поиск ближайших соседей: точные методы Точных методов существует довольно мало. Можно сказать, что их, по сути, два. Первый — полный перебор с различными эвристиками. Например, можно выбрать подмножество признаков и считать расстояние только по ним. Оно будет оценкой снизу на реальное расстояние, поэтому если оно уже больше, чем до текущего ближайшего объекта, то можно сразу отбросить этот объект и переходить к следующему. Такие эвристики хоть и могут давать некоторый выигрыш по времени, но не улучшат асимптотическую сложность. Второй — k-d-деревья, о которых стоит поговорить подробнее. K-d-деревья Представим на секунду, что у нас есть всего лишь один признак, то есть объекты выражаются вещественными числами, а не векторами. В этом случае для поиска ближайшего соседа напрашивается всем вам известное бинарное дерево поиска, которое позволяет находить элементы за логарифмическое время. Оказывается, существует аналог данной структуры в многомерном пространстве, который называется k-d-дерево (k-d tree, сокращение от k-dimensional tree). Как и в обычном дереве поиска, в k-d-дереве каждый узел является объектом обучающей выборки, который особым образом делит пространство на два полупространства. Таким образом, всё пространство оказывается поделено на множество малых областей, и такое деление оказывается очень полезным при поиске ближайших соседей. Рассмотрим подробнее, как строится такое дерево. Трудность в применении обычного дерева поиска состоит в том, что мы не можем напрямую сравнить два вектора так же, как два вещественных числа. Чтобы эту проблему преодолеть, узлы дерева будут делить пространство лишь по одной оси. При движении вниз по дереву оси, по которым точки делят пространство, циклически сменяют друг друга. Например, в двумерном пространстве корень будет отвечать за деление по x-координате, его сыновья — за деление по y-координате, а внуки — снова за x-координату, и т. д. Посмотрим, как это работает на примере: Источник На картинке выше корень ( 30 , 40 ) (30,40) делит все точки по оси х: слева оказываются точки, у которых x < 30 x<30, а справа — те, у которых x ⩾ 30 x⩾30. Аналогично левый сын корня ( 5 , 25 ) (5,25) делит своё поддерево по оси y: слева оказываются точки, у которых y < 25 y<25, а справа — те, у которых y ⩾ 25 y⩾25. Остаётся вопрос — как выбирать точки, которые будут делить пространство пополам? Чтобы дерево было сбалансированным, нужно находить точку с медианой, соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближённому поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность. Добавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d-деревьев, которые сохраняют сбалансированность при добавлении / удалении точек. Поговорим теперь про",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 7,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "находить точку с медианой, соответствующей уровню поддерева координаты. На практике часто ограничиваются выбором случайной точки или любой эвристикой по приближённому поиску медианы (например, медиана некоторого подмножества точек). Это позволяет ускорить построение дерева, но убирает все гарантии на его сбалансированность. Добавлять новые точки можно так же, как и в одномерном дереве поиска. Спускаясь по дереву, можно однозначно определить лист, к которому нужно подвесить новую точку, чтобы не нарушить все свойства дерева. При добавлении большого количества точек, однако, дерево может перестать быть сбалансированным, и нужно проводить ребалансировку. Также существуют варианты k-d-деревьев, которые сохраняют сбалансированность при добавлении / удалении точек. Поговорим теперь про то, как же находить ближайших соседей с помощью такого дерева. Будем производить обход дерева в глубину с двумя модификациями. Во-первых, будем запоминать наиболее близкую точку. Это позволит не заходить в поддеревья, задающие области, которые заведомо дальше, чем текущая наиболее близкая точка, поэтому не имеет смысла искать в них ближайших соседей. Во-вторых, будем прежде всего обходить те поддеревья, которые задают наиболее близкие области, а значит, с большей вероятностью содержат ближайшего соседа. Источник Сложность метода по размеру обучающей выборки в среднем равна O ( log ⁡ N ) O(logN) при равномерном распределении точек. При большой размерности пространства, однако, алгоритму приходится посещать больше ветвей дерева, чтобы найти ближайших соседей. Например, если N ≈ D N≈D, то сложность становится примерно такой же, как и в случае полного перебора. В общем случае считается, что для того чтобы асимптотика действительно была логарифмической, нужно, чтобы N ≳ 2 D N≳2 D . Поэтому уже при количестве признаков порядка сотни алгоритм не даёт существенных преимуществ перед полным перебором. Почитать по теме: Хорошая презентация, объясняющая структуру и поиск соседей. Балансировка деревьев. Поиск ближайших соседей: приближённые методы Почти всегда находить именно самых близких соседей необязательно. Например, в задаче подбора рекомендаций фильмов пользователю чаще всего не нужны наиболее похожие картины, достаточно, к примеру, 10 из 15 наиболее близких. Поэтому, чтобы ускорить процесс поиска соседей, используют приближённые методы. Разберём основные идеи, которые применяются в таких методах. Random projection trees Алгоритмы, основанные на деревьях, очень часто применяются в задачах поиска соседей. Идея всех таких методов заключается в итеративном разделении пространства случайными гиперплоскостями и построении на базе этого разделения дерева, в листах которого содержится малое число объектов. Одним из наиболее ярких представителей этого семейства является Annoy — алгоритм, который используется Spotify для рекомендаций музыки. Задача подобных рекомендательных систем довольно простая — нужно посоветовать пользователю композиции, которые он ещё не слушал, но которые при этом с высокой вероятностью ему понравятся. Простая и рабочая идея — предлагать композиции, похожие на те, которые он уже слушает. Здесь на помощь как раз и приходят методы поиска ближайших соседей. Annoy в какой-то степени похож на k-d-деревья. Сначала выбираются два случайных объекта обучающей выборки и проводится гиперплоскость, симметрично их разделяющая. Затем для каждого полученного полупространства итеративно запускается такая же процедура, которая продолжается до тех пор, пока в каждой области будет не более M M объектов ( M M — гиперпараметр). Источник Таким образом задаётся бинарное дерево с глубиной порядка O ( log ⁡ N ) O(logN) в среднем. Источник Спускаясь",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 8,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "с высокой вероятностью ему понравятся. Простая и рабочая идея — предлагать композиции, похожие на те, которые он уже слушает. Здесь на помощь как раз и приходят методы поиска ближайших соседей. Annoy в какой-то степени похож на k-d-деревья. Сначала выбираются два случайных объекта обучающей выборки и проводится гиперплоскость, симметрично их разделяющая. Затем для каждого полученного полупространства итеративно запускается такая же процедура, которая продолжается до тех пор, пока в каждой области будет не более M M объектов ( M M — гиперпараметр). Источник Таким образом задаётся бинарное дерево с глубиной порядка O ( log ⁡ N ) O(logN) в среднем. Источник Спускаясь по этому дереву, можно найти область, в которой лежит целевой объект и некоторое количество близких к нему элементов обучающей выборки. Проблема в том, что это не обязательно будут самые близкие объекты, поэтому для увеличения точности составляется лес из таких деревьев и берётся объединение соответствующих целевому объекту областей. Источник Чем больше таких деревьев берётся, тем более точным будет результат, но придётся тратить большее время на его поиск. Преимущество алгоритма — простота нахождения компромисса между скоростью работы и точностью с помощью тюнинга гиперпараметров. К минусам можно отнести то, что алгоритм плохо параллелится и переносится на GPU, не работает эффективно с батчами, а также то, что для добавления новой точки в обучающую выборку придётся перезапускать процедуру с самого начала. Почитать по теме: Отличная статья с иллюстрациями и подробным описанием алгоритма. Locality-sensitive hashing (LSH) Предположим, что мы можем построить такую хеш-функцию, которая переводит близкие объекты в один бакет. Тогда близких соседей целевого объекта можно найти, посчитав его хеш и посмотрев на коллизии. Оказывается, такие хеш-функции существуют, и на этой идее основано несколько алгоритмов, которые объединяются названием Locality-sensitive hashing (LSH). К этому классу алгоритмов относится, например, FAISS, используемый Facebook. Источник Определим формально семейство хеш-функций, которое мы хотим использовать. Нам нужно, чтобы вероятность коллизии на близких объектах была высокая, а на далёких — низкая. Назовём семейство хеш-функций (R,cR,p 1 ,p 2 )-чувствительным, если для любой h(x)∈H: для ρ(x,y)<R вероятность коллизии Pr[h(x)=h(y)]>p 1 ; для ρ(x,y)>cR вероятность коллизии Pr[h(x)=h(y)]<p 2 . Формулы могут выглядеть сложными, но это всего лишь формализация нашей интуиции. Картинка ниже поясняет определение: для близких красных объектов в шаре радиусом R R вероятность коллизии больше p 1 p 1 , для далёких синих объектов на расстоянии больше c R cR вероятность коллизии меньше p 2 p 2 , а про серые объекты в слое между R R и c R cR мы ничего не знаем. Источник Для каждой функции расстояния, используемой в задаче, существует своё подходящее семейство хеш-функций. Например, для евклидовой и манхэттенской метрик используются случайные проекции, где хеш-функция имеет следующий вид: w,b (x)=⌊ r w T x+b ⌋, где w w и b b — случайные параметры, а r r выбирается пользователем. b b выбирается равномерно из отрезка [ 0 , r ] [0,r], а w w генерируется либо из нормального распределения, что соответствует евклидовой метрике, либо из распределения Коши — для манхэттенской метрики. По сути, такая функция разбивает всё пространство на слои в направлении вектора w w. Параметр r r при этом",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 9,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "знаем. Источник Для каждой функции расстояния, используемой в задаче, существует своё подходящее семейство хеш-функций. Например, для евклидовой и манхэттенской метрик используются случайные проекции, где хеш-функция имеет следующий вид: w,b (x)=⌊ r w T x+b ⌋, где w w и b b — случайные параметры, а r r выбирается пользователем. b b выбирается равномерно из отрезка [ 0 , r ] [0,r], а w w генерируется либо из нормального распределения, что соответствует евклидовой метрике, либо из распределения Коши — для манхэттенской метрики. По сути, такая функция разбивает всё пространство на слои в направлении вектора w w. Параметр r r при этом задаёт ширину слоя. Источник На практике при использовании лишь одной хеш-функции разница между оказывается очень маленькой, поэтому применяют различные методы для её увеличения. Первый способ — уменьшать размер бакетов в хеш-таблице путём использования композиции разных хеш-функций из одного семейства g(x)=(h 1 (x),…,h m (x)). Преимущество этого способа как раз хорошо видно на примере случайных проекций. При использовании лишь одной хеш-функции бакетами являются слои бесконечного объёма. Однако при использовании композиции размером, как минимум равным количеству признаков D D, из-за случайности выбора вектора w w бакеты почти наверное станут замкнутыми фигурами с конечным объёмом. Второй способ повышения эффективности алгоритма — использовать несколько хеш-таблиц и искать соседей среди коллизий в каждой из них. На практике используют оба метода сразу, подбирая m m и L L — количество хеш-таблиц как гиперпараметры. К плюсам алгоритма можно отнести хорошие теоретические гарантии на сублинейное время и, как и в Annoy, простой поиск компромисса между точностью и скоростью работы. Минусами можно назвать высокую потребность в памяти, плохую адаптируемость под GPU, а также тот факт, что, несмотря на теоретические гарантии в среднем, на практике алгоритм может работать даже чуть дольше полного перебора из-за того, что, помимо самого поиска, требуется искать хеши объектов. Почитать по теме: Отличная статья с объяснением в иллюстрациях и примерами хеш-функций для других метрик. Ещё одна статья, в которой шаг за шагом выводится алгоритм на примере расстояния Жаккара. Proximity graphs & Hierarchical navigable small world (HNSW) Следующий класс алгоритмов основан на построении специального графа близости (proximity graph) на объектах выборки и дальнейшем жадном поиске по этому графу. Алгоритмы этого семейства сейчас считаются state-of-the-art (SotA) для многих задач. Рассмотрим подробнее этот класс алгоритмов на примере одного из наиболее популярных из них под названием Navigable small world (NSW). Идея его в следующем: на данных строится граф (он также называется NSW), который удовлетворяет двум следующим свойствам: Между любыми двумя точками существует короткий путь, или, более формально, матожидание числа кратчайшего пути между двумя случайно выбранными вершинами растёт как O ( log ⁡ N ) O(logN). Средняя степень вершины мала. На первый взгляд может показаться, что тяжело выполнить одновременно оба свойства, но на самом деле большая часть графов в реальном мире являются NSW-графами. Самый простой пример — это известное правило шести рукопожатий: любые два случайных человека соединены короткой последовательностью личных контактов длиной не более шести, несмотря на то, что количество знакомых у среднего человека ( 100 100– 1000 1000) мало по сравнению с населением Земли. В таких графах существует очень простой",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 10,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "точками существует короткий путь, или, более формально, матожидание числа кратчайшего пути между двумя случайно выбранными вершинами растёт как O ( log ⁡ N ) O(logN). Средняя степень вершины мала. На первый взгляд может показаться, что тяжело выполнить одновременно оба свойства, но на самом деле большая часть графов в реальном мире являются NSW-графами. Самый простой пример — это известное правило шести рукопожатий: любые два случайных человека соединены короткой последовательностью личных контактов длиной не более шести, несмотря на то, что количество знакомых у среднего человека ( 100 100– 1000 1000) мало по сравнению с населением Земли. В таких графах существует очень простой метод поиска соседей. Нужно выбрать случайную точку, среди её соседей выбрать того, который ближе всего к целевому объекту, и повторить процедуру уже для него. Показано, что такой жадный поиск имеет полилогарифмическую асимптотику. Источник Проблема такого подхода в том, что можно попасть в плотный кластер и очень долго оттуда выбираться. Для решения этой проблемы используется иерархия NSW, или Hierarchical navigable small world (HNSW). Исходный граф является нулевым слоем. Каждый следующий слой строится в два шага: Каждая вершина текущего слоя попадает в следующий с некоторой вероятностью p p. На всех вершинах, попавших в новый слой, строится NSW. По построению количество слоёв будет O ( log ⁡ N ) O(logN). Источник Поиск начинается в самом верхнем слое. После нахождения ближайшей к целевому объекту вершины спускаемся на слой ниже и начинаем поиск из этой вершины. Повторяем процедуру, пока не спустимся до нулевого слоя. Таким образом, на каждом слое мы всё больше уточняем наш ответ. Стоит отметить, что для ускорения работы иногда поиск останавливают не при нахождении ближайшей вершины, а раньше, используя критерии остановки. Интуитивно легко понять, почему такая иерархическая структура решает проблему плотных кластеров: в верхних слоях вершин мало, а расстояния между ними в среднем большие, а значит, таких кластеров там почти нет. Поэтому, попадая в нижний слой, мы чаще всего оказываемся уже в нужном кластере и просто уточняем результат работы алгоритма. HNSW, так же как и рассмотренные ранее приближённые методы, позволяет искать трейд-офф между точностью и скоростью работы. Плюс ко всему на реальных данных он часто работает лучше других методов и сейчас считается SotA. Однако этот способ поиска не лишён и недостатков. Главный заключается в том, что нельзя добавлять точки в обучающую выборку без перестройки структуры. Помимо этого, он довольно требователен по памяти из-за того, что для каждого слоя приходится хранить как вершины, которые в него входят, так и связи между этими вершинами. Подробнее — в оригинальной статье. В завершение стоит сказать, что не существует универсального метода поиска соседей — каждый из описанных методов может быть лучше других в определённой задаче. К тому же, несмотря на то что приближённые методы имеют лучшую асимптотику, многие из них плохо переносятся на GPU. Из-за этого на практике полный перебор бывает быстрее любого из таких приближённых методов. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.1. Линейные модели Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая классификация Следующий параграф 2.3. Решающие деревья Обучение древесных моделей",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 11,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Подробнее — в оригинальной статье. В завершение стоит сказать, что не существует универсального метода поиска соседей — каждый из описанных методов может быть лучше других в определённой задаче. К тому же, несмотря на то что приближённые методы имеют лучшую асимптотику, многие из них плохо переносятся на GPU. Из-за этого на практике полный перебор бывает быстрее любого из таких приближённых методов. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.1. Линейные модели Линейные модели от линейной до логистической регрессии. Регуляризация, работа с категориальными признаками, многоклассовая классификация Следующий параграф 2.3. Решающие деревья Обучение древесных моделей для классификации и регрессии. Эффективное построение решающих деревьев",
    "metadata": {
      "title": "Метрические методы",
      "url": "https://education.yandex.ru/handbook/ml/article/metricheskiye-metody",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.2",
      "part": 12,
      "total_parts": 12,
      "source_file": "2.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Обучение древесных моделей для классификации и регрессии. Эффективное построение решающих деревьев В этом параграфе мы рассмотрим ещё одно семейство моделей машинного обучения — решающие деревья (decision trees). Решающее дерево предсказывает значение целевой переменной с помощью применения последовательности простых решающих правил (которые называются предикатами). Этот процесс в некотором смысле согласуется с естественным для человека процессом принятия решений. Хотя обобщающая способность решающих деревьев невысока, их предсказания вычисляются довольно просто, из-за чего решающие деревья часто используют как кирпичики для построения ансамблей — моделей, делающих предсказания на основе агрегации предсказаний других моделей. О них мы поговорим в следующем параграфе. Пример решающего дерева Начнём с небольшого примера. На картинке ниже изображено дерево, построенное для задачи классификации на пять классов: 3 Объекты в этом примере имеют два признака с вещественными значениями: X X и Y Y. Решение о том, к какому классу будет отнесён текущий объект выборки, будет приниматься с помощью прохода от корня дерева к некоторому листу. В каждом узле этого дерева находится предикат. Если предикат верен для текущего примера из выборки, мы переходим в правого потомка, если нет — в левого. В данном примере все предикаты — это просто взятие порога по значению какого-то признака: B(x,j,t)=[x j ≤t] В листьях записаны предсказания (например, метки классов). Как только мы дошли до листа, мы присваиваем объекту ответ, записанный в вершине. На картинке ниже визуализирован процесс построения решающих поверхностей, порождаемых деревом (правая часть картинки): 3 Каждый предикат порождает разделение текущего подмножества пространства признаков на две части. На первом этапе, когда происходило деление по [X≤X 1 ], вся плоскость была поделена на две соответствующие части. На следующем уровне часть плоскости, для которой выполняется X ≤ X 1 X≤X 1 , была поделена на две части по значению второго признака Y ≤ Y 1 Y≤Y 1 — так образовались области 1 и 2. То же самое повторяется для правой части дерева — и так далее до листовых вершин: получится пять областей на плоскости. Теперь любому объекту выборки будет присваиваться один из пяти классов в зависимости от того, в какую из образовавшихся областей он попадает. Этот пример хорошо демонстрирует, в частности, то, что дерево осуществляет кусочно-постоянную аппроксимацию целевой зависимости. Ниже приведён пример визуализации решающей поверхности, которая соответствует дереву глубины 4, построенному для объектов данных из Ames Housing Dataset, где из всех признаков, описывающих объекты недвижимости, были выбраны ширина фасада (Lot_Frontage) и площадь (Lot_Area), а предсказать нужно стоимость. Для более понятной визуализации перед построением дерева из датасета были выкинуты объекты с Lot_Frontage > 150 и с Lot_Area > 20000. Вот что получилось — в каждой из прямоугольных областей предсказывается одна и та же стоимость: 3 Определение решающего дерева Разобравшись с приведёнными выше примерами, мы можем дать определение решающего дерева. Пусть задано бинарное дерево, в котором: каждой внутренней вершине v v приписан предикат :X→{0,1}; каждой листовой вершине v v приписан прогноз ∈Y, где Y Y — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов). В ходе предсказания осуществляется проход по этому дереву к некоторому листу. Для каждого объекта выборки x x движение начинается из корня.",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 1,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Lot_Frontage > 150 и с Lot_Area > 20000. Вот что получилось — в каждой из прямоугольных областей предсказывается одна и та же стоимость: 3 Определение решающего дерева Разобравшись с приведёнными выше примерами, мы можем дать определение решающего дерева. Пусть задано бинарное дерево, в котором: каждой внутренней вершине v v приписан предикат :X→{0,1}; каждой листовой вершине v v приписан прогноз ∈Y, где Y Y — область значений целевой переменной (в случае классификации листу может быть также приписан вектор вероятностей классов). В ходе предсказания осуществляется проход по этому дереву к некоторому листу. Для каждого объекта выборки x x движение начинается из корня. В очередной внутренней вершине v v проход продолжится вправо, если (x)=1, и влево, если (x)=0. Проход продолжается до момента, пока не будет достигнут некоторый лист, и ответом алгоритма на объекте x x считается прогноз c v c v , приписанный этому листу. Вообще, предикат B v B v может иметь, произвольную структуру, но на практике чаще используют просто сравнение с порогом t ∈ R t∈R по какому-то j j-му признаку: (x,j,t)=[x j ≤t] При проходе через узел дерева с данным предикатом объекты будут отправлены в правое поддерево, если значение j j-го признака у них меньше либо равно t t, и в левое — если больше. В дальнейшем рассказе мы будем по умолчанию использовать именно такие предикаты. Из структуры дерева решений следует несколько интересных свойств: выученная функция — кусочно-постоянная, из-за чего производная равна нулю везде, где задана. Следовательно, о градиентных методах при поиске оптимального решения можно забыть; дерево решений (в отличие от, например, линейной модели) не сможет экстраполировать зависимости за границы области значений обучающей выборки; дерево решений способно идеально приблизить обучающую выборку и ничего не выучить (то есть такой классификатор будет обладать низкой обобщающей способностью): для этого достаточно построить такое дерево, в каждый лист которого будет попадать только один объект. Следовательно, при обучении нам надо не просто приближать обучающую выборку как можно лучше, но и стремиться оставлять дерево как можно более простым, чтобы результат обладал хорошей обобщающей способностью. Почему построение оптимального решающего дерева — сложная задача? Пусть, как обычно, у нас задан датасет ( X , y ) (X,y), где y={y i } i=1 N ⊂R N — вектор таргетов, а X={x i } i=1 N ∈R N×D ,x i ∈R D — матрица признаков, в которой i i-я строка — это вектор признаков i i-го объекта выборки. Пусть у нас также задана функция потерь L(f,X,y), которую мы бы хотели минимизировать. Наша задача — построить решающее дерево, наилучшим образом предсказывающее целевую зависимость. Однако, как уже было замечено выше, оптимизировать структуру дерева с помощью градиентного спуска не представляется возможным. Как ещё можно было бы решить эту задачу? Давайте начнём с простого — научимся строить решающие пни, то есть решающие деревья глубины 1. Как и раньше, мы будем рассматривать только самые простые предикаты: j,t (x i )=[x ij ≤t] Ясно, что задачу можно решить полным перебором: существует не более (N−1)D предикатов такого вида. Действительно, индекс j j (номер признака) пробегает значения от 1 1 до D D, а всего значений порога t t,",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 2,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Наша задача — построить решающее дерево, наилучшим образом предсказывающее целевую зависимость. Однако, как уже было замечено выше, оптимизировать структуру дерева с помощью градиентного спуска не представляется возможным. Как ещё можно было бы решить эту задачу? Давайте начнём с простого — научимся строить решающие пни, то есть решающие деревья глубины 1. Как и раньше, мы будем рассматривать только самые простые предикаты: j,t (x i )=[x ij ≤t] Ясно, что задачу можно решить полным перебором: существует не более (N−1)D предикатов такого вида. Действительно, индекс j j (номер признака) пробегает значения от 1 1 до D D, а всего значений порога t t, при которых меняется значение предиката, может быть не более N − 1 N−1: 3 Решение, которое мы ищем, будет иметь вид: arg ⁡ min opt ,t opt )=arg j,t min L(B j,t ,X,y) Для каждого из предикатов B j , t B j,t нам нужно посчитать значение функции потерь на всей выборке, что, в свою очередь, тоже занимает O ( N ) O(N). Следовательно, полный алгоритм выглядит так: min_loss = inf optimal_border = None for j in range(D): for t in X[:, j]: # Можно брать сами значения признаков в качестве порогов loss = calculate_loss(t, j, X, y) if loss < min_loss: min_loss, optimal_border = loss, (j, t) Сложность алгоритма — O(N 2 D). Это не заоблачная сложность, хотя, конечно, не идеальная. Но это была схема возможного алгоритма поиска оптимального дерева высоты 1. Как обобщить алгоритм для дерева произвольной глубины? Мы можем сделать наш алгоритм поиска решающего пня рекурсивным и в теле цикла вызывать исходную функцию для всех возможных разбиений. Как мы упоминали выше, так можно построить дерево, идеально запоминающее всю выборку, однако на тестовых данных такой алгоритм вряд ли покажет высокое качество. Можно поставить другую задачу: построить оптимальное с точки зрения качества на обучающей выборке дерево минимальной глубины (чтобы снизить переобучение). Проблема в том, что поиск такого дерева — NP-полная задача, то есть человечеству пока неизвестны способы решить её за полиномиальное время. Как быть? Идеального ответа на этот вопрос нет, но до некоторой степени ситуацию можно улучшить двумя не исключающими друг друга способами: Разрешить себе искать не оптимальное решение, а просто достаточно хорошее. Начать можно с того, чтобы строить дерево с помощью жадного алгоритма, то есть не искать всю структуру сразу, а строить дерево этаж за этажом. Тогда в каждой внутренней вершине дерева будет решаться задача, схожая с задачей построения решающего пня. Для того чтобы этот подход хоть как-то работал, его придётся прокачать внушительным набором эвристик. Заняться оптимизацией с точки зрения computer science — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз. Эти две идеи мы и будем обсуждать в дальнейшем. Сначала попытаемся подробно разобраться с первой — как использовать жадный алгоритм. Жадный алгоритм построения решающего дерева Пусть X X — исходное множество объектов обучающей выборки, а X m X m — множество объектов, попавших в текущий лист (в самом начале =X). Тогда жадный алгоритм можно верхнеуровнево описать следующим образом: Создаём вершину v v. Если выполнен критерий остановки Stop(X m",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 3,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "работал, его придётся прокачать внушительным набором эвристик. Заняться оптимизацией с точки зрения computer science — наивную версию алгоритма (перебор наборов возможных предикатов и порогов) можно ускорить и асимптотически, и в константу раз. Эти две идеи мы и будем обсуждать в дальнейшем. Сначала попытаемся подробно разобраться с первой — как использовать жадный алгоритм. Жадный алгоритм построения решающего дерева Пусть X X — исходное множество объектов обучающей выборки, а X m X m — множество объектов, попавших в текущий лист (в самом начале =X). Тогда жадный алгоритм можно верхнеуровнево описать следующим образом: Создаём вершину v v. Если выполнен критерий остановки Stop(X m ), то останавливаемся, объявляем эту вершину листом и ставим ей в соответствие ответ Ans(X m ), после чего возвращаем её. Иначе: находим предикат (иногда ещё говорят сплит) B j , t B j,t , который определит наилучшее разбиение текущего множества объектов X m X m на две подвыборки , максимизируя критерий ветвления Branch(X m ,j,t). Для рекурсивно повторим процедуру. Данный алгоритм содержит в себе несколько вспомогательных функций, которые надо выбрать так, чтобы итоговое дерево было способно минимизировать Ans(X m ), вычисляющая ответ для листа по попавшим в него объектам из обучающей выборки, может быть, например: в случае задачи классификации — меткой самого частого класса или оценкой дискретного распределения вероятностей классов для объектов, попавших в этот лист; в случае задачи регрессии — средним, медианой или другой статистикой; простой моделью. К примеру, листы в дереве, задающем регрессию, могут быть линейными функциями или синусоидами, обученными на данных, попавших в лист. Впрочем, везде ниже мы будем предполагать, что в каждом листе просто предсказывается константа. Критерий остановки Stop(X m ) — функция, которая решает, нужно ли продолжать ветвление или пора остановиться. Это может быть какое-то тривиальное правило: например, остановиться только в тот момент, когда объекты в листе получились достаточно однородными и/или их не слишком много. Более детально мы поговорим о критериях остановки в параграфе про регуляризацию деревьев. Критерий ветвления Branch(X m ,feature,value) — пожалуй, самая интересная компонента алгоритма. Это функция, измеряющая, насколько хорош предлагаемый сплит. Чаще всего эта функция оценивает, насколько улучшится некоторая финальная метрика качества дерева в случае, если получившиеся два листа будут терминальными, по сравнению с ситуацией, когда сама исходная вершина — это лист. Выбирается такой сплит, который даёт наиболее существенное улучшение. Впрочем, есть и другие подходы. При этом строгой теории, которая бы связывала оптимальность выбора разных вариантов этих функций и разных метрик классификации и регрессии, в общем случае не существует. Однако есть набор интуитивных и хорошо себя зарекомендовавших соображений, с которыми мы вас сейчас познакомим. Критерии ветвления: общая идея Давайте теперь по очереди посмотрим на популярные постановки задач ML и под каждую подберём свой критерий. Ответы дерева будем кодировать так: c ∈ R c∈R — для ответов регрессии и меток класса. Для случаев, когда надо ответить дискретным распределением на классах, c ∈ R K c∈R K будет вектором вероятностей: c=(c 1 ,…,c K ), i=1 ∑ K c i =1 Предположим также, что задана некоторая функция потерь L(y i ,c). О том, что это может быть за функция, мы поговорим ниже. В",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 4,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "набор интуитивных и хорошо себя зарекомендовавших соображений, с которыми мы вас сейчас познакомим. Критерии ветвления: общая идея Давайте теперь по очереди посмотрим на популярные постановки задач ML и под каждую подберём свой критерий. Ответы дерева будем кодировать так: c ∈ R c∈R — для ответов регрессии и меток класса. Для случаев, когда надо ответить дискретным распределением на классах, c ∈ R K c∈R K будет вектором вероятностей: c=(c 1 ,…,c K ), i=1 ∑ K c i =1 Предположим также, что задана некоторая функция потерь L(y i ,c). О том, что это может быть за функция, мы поговорим ниже. В момент, когда мы ищем оптимальный сплит , мы можем вычислить для объектов из X m X m тот константный таргет c c, которые предсказало бы дерево, будь текущая вершина терминальной, и связанное с ними значение исходного функционала качества L L. А именно — константа c c должна минимизировать среднее значение функции потерь: )∈X m ∑ L(y i ,c) Оптимальное значение этой величины min H(X m )= c∈Y min )∈X m ∑ L(y i ,c) обычно называют информативностью, или impurity. Чем она ниже, тем лучше объекты в листе можно приблизить константным значением. Похожим образом можно определить информативность решающего пня. Пусть, как и выше, X l X l — множество объектов, попавших в левую вершину, а X r X r — в правую; пусть также — константы, которые предсказываются в этих вершинах. Тогда функция потерь для всего пня в целом будет равна L(y L(y i ,c r )) Вопрос на подумать. Как информативность решающего пня связана с информативностью его двух листьев? Теперь для того чтобы принять решение о разделении, мы можем сравнить значение информативности для исходного листа и для получившегося после разделения решающего пня. Разность информативности исходной вершины и решающего пня равна H(X H(X H(X r ) Для симметрии её принято умножить на ∣; тогда получится следующий критерий ветвления: Branch(X m ,j,t)=∣X m ∣⋅H(X m )−∣X l ∣⋅H(X l )−∣X r ∣⋅H(X r ) Получившаяся величина неотрицательна: ведь, разделив объекты на две кучки и подобрав ответ для каждой, мы точно не сделаем хуже. Кроме того, она тем больше, чем лучше предлагаемый сплит. Теперь посмотрим, какими будут критерии ветвления для типичных задач. Информативность в задаче регрессии: MSE Посмотрим на простой пример — регрессию с минимизацией среднеквадратичной ошибки: L(y i ,c)=(y i −c) 2 Информативность листа будет выглядеть следующим образом: min H(X c∈Y min (x i ,y i )∈X m ∑ (y i −c) 2 Как мы уже знаем, оптимальным предсказанием константного классификатора для задачи минимизации MSE является среднее значение, то есть Подставив в формулу информативности сплита, получаем: где H(X )∈X , где То есть при жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше. Информативность в задаче регрессии: MAE L(y i ,c)=∣y i −c∣ Случай средней абсолютной ошибки так же прост: в листе надо предсказывать медиану, ведь именно медиана таргетов для обучающих",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 5,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "2 Как мы уже знаем, оптимальным предсказанием константного классификатора для задачи минимизации MSE является среднее значение, то есть Подставив в формулу информативности сплита, получаем: где H(X )∈X , где То есть при жадной минимизации MSE информативность — это оценка дисперсии таргетов для объектов, попавших в лист. Получается очень стройная картинка: оценка значения в каждом листе — это среднее, а выбирать сплиты надо так, чтобы сумма дисперсий в листьях была как можно меньше. Информативность в задаче регрессии: MAE L(y i ,c)=∣y i −c∣ Случай средней абсолютной ошибки так же прост: в листе надо предсказывать медиану, ведь именно медиана таргетов для обучающих примеров минимизирует MAE констатного предсказателя (мы это обсуждали в параграфе про линейные модели). В качестве информативности выступает абсолютное отклонение от медианы: H(X )∈X −MEDIAN(Y)∣ Критерий информативности в задаче классификации: misclassification error Пусть в нашей задаче K K классов, а p k p k — доля объектов класса k k в текущей вершине )∈X m ∑ I[y i =k] Допустим, мы заботимся о доле верно угаданных классов, то есть функция потерь — это индикатор ошибки: L(y i ,c)=I[y i  =c] Пусть также предсказание модели в листе — один какой-то класс. Информативность для такой функции потерь выглядит так: min H(X m )= c∈Y min )∈X m ∑ I[y i  =c] Ясно, что оптимальным предсказанием в листе будет наиболее частотный класс k ∗ k ∗ , а выражение для информативности упростится следующим образом: H(X )∈X m ∑ I[y i  =k ∗ ]=1−p k ∗ Информативность в задаче классификации: энтропия Если же мы собрались предсказывать вероятностное распределение классов ,…,c K ), то к этому вопросу можно подойти так же, как мы поступали при выводе логистической регрессии: через максимизацию логарифма правдоподобия (= минимизацию минус логарифма) распределения Бернулли. А именно, пусть в вершине дерева предсказывается фиксированное распределение c c (не зависящее от x i x i ), тогда правдоподобие имеет вид P(y∣x,c)=P(y∣c)= (x i ,y i )∈X m ∏ P(y i ∣c)= (x i ,y i )∈X m ∏ k=1 ∏ K c k I[y i =k] , откуда min log ⁡ c k ) H(X min )∈X m ∑ k=1 ∑ K I[y i =k]logc k То, что оценка вероятностей в листе c k c k , минимизирующая H ( X m ) H(X m ), должна быть равна p k p k , то есть доле попавших в лист объектов этого класса, до некоторой степени очевидно, но это можно вывести и строго. Подставляя вектор c=(p 1 ,…,p K ) в выражение выше, мы в качестве информативности получим энтропию распределения классов: log ⁡ p k H(X m )=− k=1 ∑ K p k logp k Информативность в задаче классификации: критерий Джини Пусть предсказание модели — это распределение вероятностей классов ,…,c k ). Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, метрику Бриера (за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной min H(X min )∈X m ∑ k=1 ∑ K (c k −I[y i =k]) 2 Можно показать, что оптимальное значение этой метрики, как и в",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 6,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "строго. Подставляя вектор c=(p 1 ,…,p K ) в выражение выше, мы в качестве информативности получим энтропию распределения классов: log ⁡ p k H(X m )=− k=1 ∑ K p k logp k Информативность в задаче классификации: критерий Джини Пусть предсказание модели — это распределение вероятностей классов ,…,c k ). Вместо логарифма правдоподобия в качестве критерия можно выбрать, например, метрику Бриера (за которой стоит всего лишь идея посчитать MSE от вероятностей). Тогда информативность получится равной min H(X min )∈X m ∑ k=1 ∑ K (c k −I[y i =k]) 2 Можно показать, что оптимальное значение этой метрики, как и в случае энтропии, достигается на векторе c c, состоящем из выборочных оценок частот классов ,…,p I[y i =k]. Если подставить ,…,p k ) в выражение выше и упростить его, получится критерий Джини: H(X m )= k=1 ∑ K p k (1−p k ) Критерий Джини допускает и следующую интерпретацию: H ( X m ) H(X m ) равно математическому ожиданию числа неправильно классифицированных объектов в случае, если мы будем приписывать им случайные метки из дискретного распределения, заданного вероятностями ,…,p k ). Неоптимальность полученных критериев Казалось бы, мы вывели критерии информативности для всех популярных задач, и они довольно логично следуют из их постановок, но получилось ли у нас обмануть NP-полноту и научиться строить оптимальные деревья легко и быстро? Конечно, нет. Простейший пример — решение задачи XOR с помощью жадного алгоритма и любого критерия, который мы построили выше: Источник Вне зависимости от того, что вы оптимизируете, жадный алгоритм не даст оптимального решения задачи XOR. Но этим примером проблемы не исчерпываются. Скажем, бывают ситуации, когда оптимальное с точки зрения выбранной метрики дерево вы получите с критерием ветвления, построенным по другой метрике (например, MSE-критерий для MAE-задачи или Джини для misclassification error). Особенности данных Категориальные признаки На первый взгляд, деревья прекрасно могут работать с категориальными переменными. А именно, если признак x i x i принимает значения из множества C=c 1 ,…,c M , то при очередном разбиении мы можем рассматривать по этому признаку произвольные сплиты вида C=C l ⊔C r (предикат будет иметь вид ]). Это очень логично и естественно, но проблема в том, что при больших M M у нас будет M−1 −1 сплитов, и перебирать их будет слишком долго. Было бы здорово уметь каким-то образом упорядочивать значения c m c m , чтобы работать с ними так же, как с обычными числами: разделяя на значения, «не превосходящие» и «большие» определённого порога. Оказывается, что для некоторых задач такое упорядочение можно построить вполне естественным образом. Так, для задачи бинарной классификации значения c m c m можно упорядочить по неубыванию доли объектов класса 1 с , после чего работать с ними, как со значениями вещественного признака. Показано, что в случае, если мы выбираем таким образом сплит, оптимальный с точки зрения энтропийного критерия или критерия Джини, то он будет оптимальным среди всех M−1 −1 сплитов. Для задачи регрессии с функцией потерь MSE значения c m c m можно упорядочивать по среднему значению таргета на подмножестве X∣x i =c m . Полученный таким образом сплит тоже будет оптимальным. Работа",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 7,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "некоторых задач такое упорядочение можно построить вполне естественным образом. Так, для задачи бинарной классификации значения c m c m можно упорядочить по неубыванию доли объектов класса 1 с , после чего работать с ними, как со значениями вещественного признака. Показано, что в случае, если мы выбираем таким образом сплит, оптимальный с точки зрения энтропийного критерия или критерия Джини, то он будет оптимальным среди всех M−1 −1 сплитов. Для задачи регрессии с функцией потерь MSE значения c m c m можно упорядочивать по среднему значению таргета на подмножестве X∣x i =c m . Полученный таким образом сплит тоже будет оптимальным. Работа с пропусками Одна из приятных особенностей деревьев — это способность обрабатывать пропуски в данных. Разберёмся, что при этом происходит на этапе обучения и на этапе применения дерева. Пусть у нас есть некоторый признак x i x i , значение которого пропущено у некоторых объектов. Как обычно, обозначим через X m X m множество объектов, пришедших в рассматриваемую вершину, а через V m V m — подмножество X m X m , состоящее из объектов с пропущенным значением x i x i . В момент выбора сплитов по этому признаку мы будем просто игнорировать объекты из V m V m , а когда сплит выбран, мы отправим их в оба поддерева. При этом логично присвоить им веса: для левого поддерева и для правого. Веса будут учитываться как коэффициенты при L(y i ,c) в формуле информативности. Вопрос на подумать. Во всех критериях ветвления участвуют мощности множеств . Нужно ли уменьшение размера выборки учитывать в формулах для информативности? Если нужно, то как? Теперь рассмотрим этап применения дерева. Допустим, в вершину, где сплит идёт по i i-му признаку, пришёл объект x 0 x 0 с пропущенным значением этого признака. Предлагается отправить его в каждую из дальнейших веток и получить по ним предсказания . Эти предсказания мы усредним с весами (которые мы запомнили в ходе обучения): Для задачи регрессии это сразу даст нам таргет, а в задаче бинарной классификации — оценку вероятности класса 1. Замечание. Если речь идёт о категориальном признаке, может оказаться хорошей идеей ввести дополнительное значение «пропущено» для категориального признака и дальше работать с пропусками, как с обычным значением. Особенно это актуально в ситуациях, когда пропуски имеют системный характер и их наличие несёт в себе определённую информацию. Методы регуляризации решающих деревьев Мы уже упоминали выше, что деревья легко переобучаются и процесс ветвления надо в какой-то момент останавливать. Для этого есть разные критерии, обычно используются все сразу: ограничение по максимальной глубине дерева; ограничение на минимальное количество объектов в листе; ограничение на максимальное количество листьев в дереве; требование, чтобы функционал качества Branch при делении текущей подвыборки на две улучшался не менее чем на s s процентов. Делать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия: можно проверять критерии прямо во время построения дерева, такой способ называется pre-pruning или early stopping; а можно построить дерево жадно без ограничений, а затем провести стрижку (pruning), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 8,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "используются все сразу: ограничение по максимальной глубине дерева; ограничение на минимальное количество объектов в листе; ограничение на максимальное количество листьев в дереве; требование, чтобы функционал качества Branch при делении текущей подвыборки на две улучшался не менее чем на s s процентов. Делать это можно на разных этапах работы алгоритма, что не меняет сути, но имеет разные устоявшиеся названия: можно проверять критерии прямо во время построения дерева, такой способ называется pre-pruning или early stopping; а можно построить дерево жадно без ограничений, а затем провести стрижку (pruning), то есть удалить некоторые вершины из дерева так, чтобы итоговое качество упало не сильно, но дерево начало подходить под условия регуляризации. При этом качество стоит измерять на отдельной, отложенной выборке. Алгоритмические трюки Теперь временно снимем шапочку ML-аналитика, наденем шапочку разработчика и специалиста по computer science и посмотрим, как можно сделать полученный алгоритм более вычислительно эффективным. В базовом алгоритме мы в каждой вершине дерева для всех возможных значений сплитов вычисляем информативность. Если в вершину пришло q q объектов, то мы рассматриваем q D qD сплитов и для каждого тратим O ( q ) O(q) операций на подсчёт информативности. Отметим, что в разных вершинах, находящихся в нашем дереве на одном уровне, оказываются разные объекты, то есть сумма этих q q по всем вершинам заданного уровня не превосходит N N, а значит, выбор сплитов во всех вершинах уровня потребует O(N 2 D) операций. Таким образом, общая сложность построения дерева — O(hN 2 D) (где h h — высота дерева), и доминирует в ней перебор всех возможных предикатов на каждом уровне построения дерева. Посмотрим, что с этим можно сделать. Динамическое программирование Постараемся оптимизировать процесс выбора сплита в одной конкретной вершине. Вместо того чтобы рассматривать все O ( N D ) O(ND) возможных сплитов, для каждого тратя O ( N ) O(N) на вычисление информативности, можно использовать одномерную динамику. Для этого заметим, что если отсортировать объекты по какому-то признаку, то, проходя по отсортированному массиву, можно одновременно и перебирать все значения предикатов, и поддерживать все необходимые статистики для пересчёта значений информативности за O ( 1 ) O(1) для каждого следующего варианта сплита (против изначальных O ( N ) O(N)). Давайте разберём, как это работает, на примере построения дерева для MSE. Чтобы оценить информативность для листа, нам нужно знать несколько вещей: дисперсию и среднее значение таргета в текущем листе; дисперсию и среднее значение таргета в обоих потомках для каждого потенциального значения сплита. Дисперсию и среднее текущего листа легко посчитать за O ( n ) O(n). С дисперсией и средним для всех значений сплитов чуть сложнее, но помогут следующие оценки математического ожидания и дисперсии: (Y)= (∑y i ) 2 Следовательно, нам достаточно для каждого потенциального значения сплита знать количество элементов в правом и левом поддеревьях, их сумму и сумму их квадратов. Впрочем, всё это необходимо знать только для одной из половинок сплита, а для второй это можно получить, вычитая значения для первой из полных сумм. Это можно сделать за один проход по массиву, просто накапливая значения частичных сумм. Если в вершину дерева пришло q q объектов, сложность построения одного сплита складывается из D",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 9,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n ) O(n). С дисперсией и средним для всех значений сплитов чуть сложнее, но помогут следующие оценки математического ожидания и дисперсии: (Y)= (∑y i ) 2 Следовательно, нам достаточно для каждого потенциального значения сплита знать количество элементов в правом и левом поддеревьях, их сумму и сумму их квадратов. Впрочем, всё это необходимо знать только для одной из половинок сплита, а для второй это можно получить, вычитая значения для первой из полных сумм. Это можно сделать за один проход по массиву, просто накапливая значения частичных сумм. Если в вершину дерева пришло q q объектов, сложность построения одного сплита складывается из D D сортировок каждая по O ( q log ⁡ q ) O(qlogq) и одного линейного прохода с динамикой, всего O ( q D log log ⁡ q ) O(qDlogq+qD)=O(qDlogq), что лучше исходного O(q 2 D). Итоговая сложность алгоритма построения дерева — O ( h N D log ⁡ N ) O(hNDlogN) (где h h – высота дерева) против D в наивной его версии. Какие именно статистики накапливать (средние, медианы, частоты), зависит от критерия, который вы используете. Гистограммный метод Если бы мощность множества значений признаков была ограничена какой-то разумной константой b ≪ N b≪N, то сортировку в предыдущем способе можно было бы заменить сортировкой подсчётом и за счёт этого существенно ускорить алгоритм: ведь сложность такой сортировки — O ( N ) O(N). Чтобы провернуть это с любой выборкой, мы можем искусственно дискретизировать значения всех признаков. Это приведёт к локально менее оптимальным значениям сплитов, но, учитывая, что наш алгоритм и без этого был весьма приблизительным, это не ухудшит ничего драматически, а вот ускорение получается очень неплохое. Самый популярный и простой способ дискретизации основан на частотах значений признаков: отрезок между максимальным и минимальным значением признака разбивается на b b подотрезков, длины которых выбираются так, чтобы в каждый попадало примерно равное число обучающих примеров. После чего значения признака заменяются на номера отрезков, на которые они попали. 3 Аналогичная процедура проводится для всех признаков выборки. Полная сложность предобработки — O ( D N log ⁡ N ) O(DNlogN) — сортировка за O ( N log ⁡ N ) O(NlogN) для каждого из D D признаков. Теперь в процедуре динамического алгоритма поиска оптимального сплита нам надо перебирать не все N N объектов выборки, а всего лишь b b подготовленных заранее границ подотрезков. Частичные суммы статистик тоже придётся поддерживать не для исходного массива данных, а для списка из b b возможных сплитов. А для того чтобы делать это эффективно, необходим объект, называемый гистограммой: упорядоченный словарь, сопоставляющий каждому значению дискретизированного признака сумму необходимой статистики от таргета на отрезке [B[i-1], B[i]]. Финальный вид алгоритма таков: Дискретизируем каждый из признаков на b b значений. Сложность O ( D N log ⁡ N ) O(DNlogN). Создаём корневую вершину root. Вызываем build_tree_recursive(root, data). Функция build_tree_recursive выглядит следующим образом: Проверяем, не пора ли остановиться. Если пора — считаем значение в листе. Теперь мы снова используем динамический алгоритм, но объекты будем сортировать не по исходным значениям признаков, а по их дискретизированным версиям, упорядочивая их с помощью сортировки подсчётом (для вершины, в которую попало q",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 10,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "это эффективно, необходим объект, называемый гистограммой: упорядоченный словарь, сопоставляющий каждому значению дискретизированного признака сумму необходимой статистики от таргета на отрезке [B[i-1], B[i]]. Финальный вид алгоритма таков: Дискретизируем каждый из признаков на b b значений. Сложность O ( D N log ⁡ N ) O(DNlogN). Создаём корневую вершину root. Вызываем build_tree_recursive(root, data). Функция build_tree_recursive выглядит следующим образом: Проверяем, не пора ли остановиться. Если пора — считаем значение в листе. Теперь мы снова используем динамический алгоритм, но объекты будем сортировать не по исходным значениям признаков, а по их дискретизированным версиям, упорядочивая их с помощью сортировки подсчётом (для вершины, в которую попало q q объектов, сложность будет равна O ( q D ) O(qD) против O ( q log ⁡ q ⋅ D ) O(qlogq⋅D) в стандартной динамике). Находим оптимальный сплит за O ( q D ) O(qD). Делим данные, запускаем процедуру рекурсивно для обоих поддеревьев. Общая сложность: O ( D N log O(DNlogN+hND) Mixed integer optimization Если вам действительно хочется построить оптимальное (или хотя бы очень близкое к оптимальному) дерево, то на сегодня для решения этой проблемы не нужно придумывать кучу эвристик самостоятельно, а можно воспользоваться специальными солверами, которые решают NP-полные задачи приближённо, но всё-таки почти точно. Так что единственной (и вполне решаемой) проблемой будет представить исходную задачу в понятном для солвера виде. По ссылке — пример построения оптимального дерева с помощью решения задачи целочисленного программирования. Историческая справка Как вы, может быть, уже заметили, решающие деревья — это одна большая эвристика для решения NP-полной задачи, практически лишённая какой-либо стройной теоретической подоплёки. В 1970–1990-e годы интерес к ним был весьма велик как в индустрии, где был полезен хорошо интерпретируемый классификатор, так и в науке, где учёные интересовались способами приближённого решения NP-полных задач. В связи с этим сложилось много хорошо работающих наборов эвристик, у которых даже были имена: например, ID3 был первой реализацией дерева, минимизирующего энтропию, а CART — первым деревом для регрессии. Некоторые из них были запатентованы и распространялись коммерчески. На сегодня это всё потеряло актуальность в связи с тем, что существуют хорошо написанные библиотеки (например, sklearn, в которой реализована оптимизированная версия CART). Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.2. Метрические методы Алгоритмы KNN. Быстрый поиск ближайших соседей Следующий параграф 2.4. Ансамбли в машинном обучении Как смешать несколько моделей в одну. Стэкинг, бэггинг, случайные леса",
    "metadata": {
      "title": "Решающие деревья",
      "url": "https://education.yandex.ru/handbook/ml/article/reshayushchiye-derevya",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.3",
      "part": 11,
      "total_parts": 11,
      "source_file": "2.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как смешать несколько моделей в одну. Стэкинг, бэггинг, случайные леса Представим, что у вас есть несколько моделей, обученных на ваших данных. Можно ли придумать процедуру, которая позволит использовать все имеющиеся модели и при этом получить на тестовых данных качество выше, чем могла показать каждая из этих моделей в отдельности? Да. И в этом параграфе мы расскажем, как именно. Смещение и разброс Предположим, мы решаем задачу регрессии с квадратичной функцией потерь. При использовании квадратичной функции потерь для оценки качества работы алгоритма a a можно воспользоваться следующим функционалом: Q(a)=E x E X,ϵ [y(x,ϵ)−a(x,X)] 2 , где X X — обучающая выборка x x — точка из тестового множества y=f(x)+ϵ — целевая зависимость, которую мы можем измерить с точностью до случайного шума a(x,X) — значение алгоритма, обученного на выборке X X, в точке — среднее по всем тестовым точкам и E X , ϵ E X,ϵ — среднее по всем обучающим выборкам X X и случайному шуму ϵ ϵ Для Q ( a ) Q(a) существует разложение на три компоненты — шум, смещение и разброс. Это разложение называется bias-variance decomposition, оно — одно из мощных средств для анализа работы ансамблей. О том, как его вывести, вы узнаете в соответствующем параграфе, а здесь мы приведём его формулировку. Существует представление Q ( a ) Q(a) в виде трёх компонент: bias Q(a)=E x bias X 2 a(x,X)+E x V X [a(x,X)]+σ 2 , где bias bias X a(x,X)=f(x)−E X [a(x,X)] это смещение предсказания алгоритма в точке x x, усреднённого по всем возможным обучающим выборкам, относительно истинной зависимости [a(x,X)]=E X [a(x,X)−E X [a(x,X)]] 2 это дисперсия (разброс) предсказаний алгоритма в зависимости от обучающей выборки [y(x,ϵ)−f(x)] 2 это неустранимый шум в данных. Раз нам известно, что ошибка алгоритма раскладывается на шум, смещение и разброс, можно подумать над способом сократить ошибку. Будет разумно попытаться сначала уменьшить одну из составляющих. Понятно, что с шумом уже ничего не сделать — это минимально возможная ошибка. Какую можно придумать процедуру, чтобы, например, сократить разброс, не увеличивая смещение? Пример приходит из жизни древних греков: если много человек проголосуют независимо друг от друга, то вместе они придут к разумному решению несмотря на то, что опыт каждого из них субъективен. Аналогом голосования в мире машинного обучения является бэггинг. Бэггинг Идея бэггинга (bagging, bootstrap aggregation) заключается в следующем. Пусть обучающая выборка состояла из n n объектов. Выберем из неё n n примеров равновероятно, с возвращением. Получим новую выборку X 1 X 1 , в которой некоторых элементов исходной выборки не будет, а какие-то могут войти несколько раз. С помощью некоторого алгоритма b b обучим на этой выборке модель (x)=b(x,X 1 ). Повторим процедуру: сформируем вторую выборку n элементов с возвращением и с помощью того же алгоритма обучим на ней модель (x)=b(x,X 2 ). Повторив процедуру k k раз, получим k k моделей, обученных на k k выборках. Чтобы получить одно предсказание, усредним предсказания всех моделей: a(x)= k 1 (b 1 (x)+⋯+b k (x)). Процесс генерации подвыборок с помощью семплирования с возвращением называется бутстрепом (bootstrap), а модели (x),…,b k (x) часто называют базовыми алгоритмами (хотя, наверное, лучше было",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 1,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "исходной выборки не будет, а какие-то могут войти несколько раз. С помощью некоторого алгоритма b b обучим на этой выборке модель (x)=b(x,X 1 ). Повторим процедуру: сформируем вторую выборку n элементов с возвращением и с помощью того же алгоритма обучим на ней модель (x)=b(x,X 2 ). Повторив процедуру k k раз, получим k k моделей, обученных на k k выборках. Чтобы получить одно предсказание, усредним предсказания всех моделей: a(x)= k 1 (b 1 (x)+⋯+b k (x)). Процесс генерации подвыборок с помощью семплирования с возвращением называется бутстрепом (bootstrap), а модели (x),…,b k (x) часто называют базовыми алгоритмами (хотя, наверное, лучше было бы назвать их базовыми моделями). Модель a ( x ) a(x) называется ансамблем этих моделей. Посмотрим, что происходит с качеством предсказания при переходе от одной модели к ансамблю. Сначала убедимся, что смещение ансамбля не изменилось по сравнению со средним смещением отдельных моделей. Будем считать, что когда мы берём матожидание по всем обучающим выборкам X X, то в эти выборки включены также все подвыборки, полученные бутстрепом. bias bias X a(x,X)=f(x)−E X [a(x,X)]=f(x)−E X [ k 1 i=1 ∑ k b(x,X i )]= =f(x)− k 1 i=1 ∑ k E X [b(x,X i )]=f(x)− k 1 i=1 ∑ k E X [b(x,X)]=f(x)−E X b(x,X) bias =f(x)−E X b(x,X)=bias X b(x,X) Получили, что смещение композиции равно смещению одного алгоритма. Теперь посмотрим, что происходит с разбросом. [a(x,X)]=E X [a(x,X)−E X [a(x,X)]] i=1 ∑ k b(x,X i )−E X [ k 1 i=1 ∑ k b(x,X i )]] i=1 ∑ k (b(x,X i )−E X b(x,X i ))] i=1 ∑ k E X (b(x,X i )−E X b(x,X [(b(x,X k 1 )−E X b(x,X k 1 ))(b(x,X k 2 )−E X b(x,X k 2 ))]= cov i=1 ∑ k V X b(x,X cov(b(x,X k 1 ),b(x,X k 2 )) Если предположить, что базовые алгоритмы некоррелированы, то: [a(x,X)]= k 2 1 i=1 ∑ k V X b(x,X i=1 ∑ k V X b(x,X)= k 1 V X b(x,X) Получилось, что в этом случае дисперсия композиции в k k раз меньше дисперсии отдельного алгоритма. Пример: бэггинг над решающими деревьями Пусть наша целевая зависимость f ( x ) f(x) задаётся как sin ⁡ x , f(x)=xsinx, и к ней добавляется нормальный шум ϵ∼N(0,9). Пример семпла из таких данных: 4 Попробуем посмотреть, как выглядят предсказания решающих деревьев глубины 7 и бэггинга над такими деревьями в зависимости от обучающей выборки. Обучим решающие деревья 100 раз на различных случайных семплах размера 20. Возьмём также бэггинг над 10 решающими деревьями глубины 7 в качестве базовых классификаторов и тоже 100 раз обучим его на случайных выборках размера 20. Если изобразить предсказания обученных моделей на каждой из 100 итераций, то можно увидеть примерно такую картину: 4 По этому рисунку видно, что общая дисперсия предсказаний в зависимости от обучающего множества у бэггинга значительно ниже, чем у отдельных деревьев, а в среднем предсказания деревьев и бэггинга не отличаются. Чтобы подтвердить это наблюдение, мы можем изобразить смещение и разброс случайных деревьев и бэггинга в зависимости от максимальной глубины: 4 На графике видно, как значительно бэггинг сократил дисперсию.",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 2,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "20. Возьмём также бэггинг над 10 решающими деревьями глубины 7 в качестве базовых классификаторов и тоже 100 раз обучим его на случайных выборках размера 20. Если изобразить предсказания обученных моделей на каждой из 100 итераций, то можно увидеть примерно такую картину: 4 По этому рисунку видно, что общая дисперсия предсказаний в зависимости от обучающего множества у бэггинга значительно ниже, чем у отдельных деревьев, а в среднем предсказания деревьев и бэггинга не отличаются. Чтобы подтвердить это наблюдение, мы можем изобразить смещение и разброс случайных деревьев и бэггинга в зависимости от максимальной глубины: 4 На графике видно, как значительно бэггинг сократил дисперсию. На самом деле, дисперсия уменьшилась практически в 10 раз, что равняется числу базовых алгоритмов ( k k), которые бэггинг использовал для предсказания: 4 Код для отрисовки картинок и подсчёта смещения и разброса можно найти тут. Random Forest В предыдущем разделе мы сделали предположение, что базовые алгоритмы некоррелированы, и за счёт этого получили очень сильное уменьшение дисперсии у ансамбля относительно входящих в него базовых алгоритмов. Однако в реальной жизни добиться этого сложно: ведь базовые алгоритмы учили одну и ту же зависимость на пересекающихся выборках. Поэтому будет странно, если корреляция на самом деле нулевая. Но на практике оказывается, что строгое выполнение этого предположения не обязательно. Достаточно, чтобы алгоритмы были в некоторой степени не похожи друг на друга. На этом строится развитие идеи бэггинга для решающих деревьев — случайный лес. Построим ансамбль алгоритмов, где базовый алгоритм — это решающее дерево. Будем строить по следующей схеме: Для построения i i-го дерева: Сначала, как в обычном бэггинге, из обучающей выборки X X выбирается с возвращением случайная подвыборка X i X i того же размера, что и X X. В процессе обучения каждого дерева в каждой вершине случайно выбираются n < N n<N признаков, где N N — полное число признаков (метод случайных подпространств), и среди них ищется оптимальный сплит. Такой приём как раз позволяет управлять степенью скоррелированности базовых алгоритмов. Чтобы получить предсказание ансамбля на тестовом объекте, усредняем отдельные ответы деревьев (для регрессии) или берём самый популярный класс (для классификации). Profit. Мы построили Random Forest (случайный лес) — комбинацию бэггинга и метода случайных подпространств над решающими деревьями. Внимательный читатель мог заметить, что при построении случайного леса у специалиста по машинному обучению есть несколько степеней свободы. Давайте обсудим их подробнее. Какая должна быть глубина деревьев в случайном лесу? Ошибка модели (на которую мы можем повлиять) состоит из смещения и разброса. Разброс мы уменьшаем с помощью процедуры бэггинга. На смещение бэггинг не влияет, а хочется, чтобы у леса оно было небольшим. Поэтому смещение должно быть небольшим у самих деревьев, из которых строится ансамбль. У неглубоких деревьев малое число параметров, то есть дерево способно запомнить только верхнеуровневые статистики обучающей подвыборки. Они во всех подвыборках будут похожи, но будут не очень подробно описывать целевую зависимость. Поэтому при изменении обучающей подвыборки предсказание на тестовом объекте будет стабильным, но не точным (низкая дисперсия, высокое смещение). Наоборот, у глубоких деревьев нет проблем запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 3,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "На смещение бэггинг не влияет, а хочется, чтобы у леса оно было небольшим. Поэтому смещение должно быть небольшим у самих деревьев, из которых строится ансамбль. У неглубоких деревьев малое число параметров, то есть дерево способно запомнить только верхнеуровневые статистики обучающей подвыборки. Они во всех подвыборках будут похожи, но будут не очень подробно описывать целевую зависимость. Поэтому при изменении обучающей подвыборки предсказание на тестовом объекте будет стабильным, но не точным (низкая дисперсия, высокое смещение). Наоборот, у глубоких деревьев нет проблем запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко к истине (высокая дисперсия, низкое смещение). Вывод: используем глубокие деревья. Сколько признаков надо подавать дереву для обучения? Ограничивая число признаков, которые используются в обучении одного дерева, мы также управляем качеством случайного леса. Чем больше признаков, тем больше корреляция между деревьями и тем меньше чувствуется эффект от ансамблирования. Чем меньше признаков, тем слабее сами деревья. Практическая рекомендация — брать корень из числа всех признаков для классификации и треть признаков для регрессии. Сколько должно быть деревьев в случайном лесе? Выше было показано, что увеличение числа элементарных алгоритмов в ансамбле не меняет смещения и уменьшает разброс. Так как число признаков и варианты подвыборок, на которых строятся деревья в случайном лесе, ограничены, уменьшать разброс до бесконечности не получится. Поэтому имеет смысл построить график ошибки от числа деревьев и ограничить размер леса в тот момент, когда ошибка перестанет значимо уменьшаться. Вторым практическим ограничением на количество деревьев может быть время работы ансамбля. Однако есть положительное свойство случайного леса: случайный лес можно строить и применять параллельно, что сокращает время работы, если у нас есть несколько процессоров. Но процессоров, скорее всего, всё же сильно меньше числа деревьев, а сами деревья обычно глубокие. Поэтому на большом числе деревьев Random Forest может работать дольше желаемого и количество деревьев можно сократить, немного пожертвовав качеством. Бустинг Бустинг (boosting) — это ансамблевый метод, в котором так же, как и в методах выше, строится множество базовых алгоритмов из одного семейства, объединяющихся затем в более сильную модель. Отличие состоит в том, что в бэггинге и случайном лесе базовые алгоритмы учатся независимо и параллельно, а в бустинге — последовательно. Автор изображения – Joseph Rocca. Каждый следующий базовый алгоритм в бустинге обучается так, чтобы уменьшить общую ошибку всех своих предшественников. Как следствие, итоговая композиция будет иметь меньшее смещение, чем каждый отдельный базовый алгоритм (хотя уменьшение разброса также может происходить). Поскольку основная цель бустинга — уменьшение смещения, в качестве базовых алгоритмов часто выбирают алгоритмы с высоким смещением и небольшим разбросом. Например, если в качестве базовых классификаторов выступают деревья, то их глубина должна быть небольшой — обычно не больше 2-3 уровней. Ещё одной важной причиной для выбора моделей с высоким смещением в качестве базовых является то, что такие модели, как правило, быстрее учатся. Это важно для их последовательного обучения, которое может стать очень дорогим по времени, если на каждой итерации будет учиться сложная модель. На текущий момент основным видом бустинга с точки зрения применения на практике является градиентный бустинг, о котором подробно рассказывается в соответствующем параграфе. Хотя случайный лес",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 4,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "качестве базовых алгоритмов часто выбирают алгоритмы с высоким смещением и небольшим разбросом. Например, если в качестве базовых классификаторов выступают деревья, то их глубина должна быть небольшой — обычно не больше 2-3 уровней. Ещё одной важной причиной для выбора моделей с высоким смещением в качестве базовых является то, что такие модели, как правило, быстрее учатся. Это важно для их последовательного обучения, которое может стать очень дорогим по времени, если на каждой итерации будет учиться сложная модель. На текущий момент основным видом бустинга с точки зрения применения на практике является градиентный бустинг, о котором подробно рассказывается в соответствующем параграфе. Хотя случайный лес — мощный и достаточно простой для понимания и реализации алгоритм, на практике он чаще всего уступает градиентному бустингу. Поэтому градиентный бустинг сейчас — основное продакшн-решение, если работа происходит с табличными данными (в работе с однородными данными — картинками, текстами — доминируют нейросети). Стекинг Стекинг (stacking) — алгоритм ансамблирования, основные отличия которого от предыдущих состоят в следующем: он может использовать алгоритмы разного типа, а не только из какого-то фиксированного семейства. Например, в качестве базовых алгоритмов могут выступать метод ближайших соседей и линейная регрессия результаты базовых алгоритмов объединяются в один с помощью обучаемой мета-модели, а не с помощью какого-либо обычного способа агрегации (суммирования или усреднения) Обучение стекинга проходит в несколько этапов: общая выборка разделяется на тренировочную и тестовую тренировочная выборка делится на n n фолдов. Затем эти фолды перебираются тем же способом, что используется при кросс-валидации: на каждом шаге фиксируются ( n − 1 ) (n−1) фолдов для обучения базовых алгоритмов и один — для их предсказаний (вычисления мета-факторов). Такой подход нужен для того, чтобы можно было использовать всё тренировочное множество, и при этом базовые алгоритмы не переобучались Автор изображения — Steven Yu. на полученных мета-факторах обучается мета-модель. Кроме мета-факторов, она может принимать на вход и фичи из исходного датасета. Выбор зависит от решаемой задачи Автор изображения — Steven Yu. Для получения мета-факторов на тестовом множестве базовые алгоритмы можно обучить на всём тренировочном множестве — переобучения в данном случае возникнуть не должно. Если данных достаточно много, то можно просто разделить обучающие данные на две непересекающиеся части: ту, на которой учатся базовые алгоритмы, и ту, на которой они делают свои предсказания и обучается мета-модель. Использование такого простого разбиения вместо кросс-валидации на тренировочных данных иногда называют блендингом (blending). Если данных совсем много, то тестовое множество тоже можно разделить на две части: тестовую и валидационную, и использовать последнюю для подбора гиперпараметров моделей-участников. С точки зрения смещения и разброса стекинг не имеет прямой интерпретации, так как не минимизирует напрямую ни ту, ни другую компоненту ошибки. Удачно работающий стекинг просто уменьшает ошибку, и, как следствие, её компоненты тоже будут убывать. Почитать по теме Лекция Евгения Соколова про bias-variance decomposition и бэггинг Блог-пост про ансамбли от Joseph Rocca Блог-пост про стекинг и блендинг от Steven Yu Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.3. Решающие деревья Обучение древесных моделей для классификации и регрессии. Эффективное построение решающих деревьев Следующий параграф 2.5. Градиентный бустинг Как устроено самое мощное семейство не-нейросетевых моделей: градиентный",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 5,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "смещения и разброса стекинг не имеет прямой интерпретации, так как не минимизирует напрямую ни ту, ни другую компоненту ошибки. Удачно работающий стекинг просто уменьшает ошибку, и, как следствие, её компоненты тоже будут убывать. Почитать по теме Лекция Евгения Соколова про bias-variance decomposition и бэггинг Блог-пост про ансамбли от Joseph Rocca Блог-пост про стекинг и блендинг от Steven Yu Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.3. Решающие деревья Обучение древесных моделей для классификации и регрессии. Эффективное построение решающих деревьев Следующий параграф 2.5. Градиентный бустинг Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями",
    "metadata": {
      "title": "Ансамбли в машинном обучении",
      "url": "https://education.yandex.ru/handbook/ml/article/ansambli-v-mashinnom-obuchenii",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.4",
      "part": 6,
      "total_parts": 6,
      "source_file": "2.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями В прошлых разделах мы научились соединять базовые алгоритмы в ансамбль с помощью бэггинга (и, в частности, строить из решающих деревьев случайные леса). Теперь мы рассмотрим другой способ объединять базовые алгоритмы в композицию — градиентный бустинг. В ходе обучения случайного леса каждый базовый алгоритм строится независимо от остальных. Бустинг, в свою очередь, воплощает идею последовательного построения линейной комбинации алгоритмов. Каждый следующий алгоритм старается уменьшить ошибку текущего ансамбля. Бустинг, использующий деревья решений в качестве базовых алгоритмов, называется градиентным бустингом над решающими деревьями, (Gradient Boosting on Decision Trees, GBDT). Он отлично работает на выборках с «табличными», неоднородными данными. Пример таких данных — описание пользователя Яндекса через его возраст, пол, среднее число поисковых запросов в день, число заказов такси и так далее. Такой бустинг способен эффективно находить нелинейные зависимости в данных различной природы. Этим свойством обладают все алгоритмы, которые используют деревья решений, однако именно GBDT обычно выигрывает в подавляющем большинстве задач. Благодаря этому он широко применяется во многих конкурсах по машинному обучению и задачах из индустрии: поисковом ранжировании; рекомендательных системах; таргетировании рекламы; предсказании погоды; выбора пункта назначения такси и многих других. Не так хорошо бустинг проявляет себя на однородных данных: текстах, изображениях, звуке, видео. В таких задачах нейросетевые подходы почти всегда демонстрируют лучшее качество. И хотя деревья решений — традиционный выбор для объединения в ансамбли, никто не запрещает использовать и другие алгоритмы (например, линейные модели) в качестве базовых. Эта возможность реализована в пакете XGBoost. Стоит только понимать, что построенная композиция окажется линейной комбинацией линейных моделей, то есть опять-таки линейной моделью - или нейросетью с одним полносвязным слоем. Это уменьшает возможности ансамбля эффективно определять нелинейные зависимости в данных. Поэтому в этом параграфе мы рассмотрим только бустинг над решающими деревьями. Интуиция Рассмотрим задачу регрессии с квадратичной функцией потерь: min ⁡ L(y,x)= 2 1 i=1 ∑ N (y i −a(x i )) 2 →min Для решения будем строить композицию из K K базовых алгоритмов: a(x)=a K (x)=b 1 (x)+b 2 (x)+⋯+b K (x) Если мы обучим единственное решающее дерево, то качество такой модели, скорее всего, будет низким. Однако мы знаем, на каких объектах построенное дерево давало точные предсказания, а на каких ошибалось. Попробуем использовать эту информацию и обучим ещё одну модель. Допустим, что предсказание первой модели на объекте x l x l на 10 больше, чем необходимо (т.е. )=y l +10). Если бы мы могли обучить новую модель, которая на x l x l будет выдавать ответ − 10 −10, то сумма ответов этих двух моделей на объекте x l x l в точности совпала бы с истинным значением: )+b 2 (x l )=(y l +10)+(−10)=y l Другими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции. В реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет «компенсировать» неточности первых двух. Будем продолжать так, пока не построим композицию из K K алгоритмов. Для объяснения метода градиентного бустинга полезно воспользоваться следующей аналогией. Бустинг можно представить как гольфиста, цель которого — загнать мяч",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 1,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "выдавать ответ − 10 −10, то сумма ответов этих двух моделей на объекте x l x l в точности совпала бы с истинным значением: )+b 2 (x l )=(y l +10)+(−10)=y l Другими словами, если вторая модель научится предсказывать разницу между реальным значением и ответом первой, то это позволит уменьшить ошибку композиции. В реальности вторая модель тоже не сможет обучиться идеально, поэтому обучим третью, которая будет «компенсировать» неточности первых двух. Будем продолжать так, пока не построим композицию из K K алгоритмов. Для объяснения метода градиентного бустинга полезно воспользоваться следующей аналогией. Бустинг можно представить как гольфиста, цель которого — загнать мяч в лунку с координатой y ball y ball . Положение мяча здесь – ответ композиции a ( x ball ) a(x ball ). Гольфист мог бы один раз ударить по мячу, не попасть в лунку и пойти домой, но настырность заставляет его продолжить. Источник По счастью, ему не нужно начинать каждый раз с начальной позиции. Следующий удар гольфиста переводит мяч из текущего положения a k ( x ball ) a k (x ball ) в положение ball ) a k+1 (x ball ). Каждый следующий удар — это та поправка, которую вносит очередной базовый алгоритм в композицию. Если гольфист все делает правильно, то функция потерь будет уменьшаться: L(y,a k+1 (x))<L(y,a k (x)), то есть мяч постепенно будет приближаться к лунке. Удары при этом делаются не хаотично. Гольфист оценивает текущее положение мяча относительно лунки и следующим ударом старается нивелировать те проблемы, которые он создал себе всеми предыдущими. Подбираясь к лунке, он будет бить всё аккуратнее и, возможно, даже возьмет другую клюшку, но точно не будет лупить так же, как из первоначальной позиции. В итоге комбинация всех ударов рано или поздно перенесет мяч в лунку. Подобно тому, как гольфист постепенно подводит мяч к цели, бустинг с каждым новым базовым алгоритмом всё больше приближает предсказание к истинному значению метки объекта. Рассмотрим теперь другую аналогию — разложение функции в ряд Тейлора. Из курса математического анализа известно, что (достаточно хорошую) бесконечно дифференцируемую функцию f ( x ) f(x) на интервале x∈(a−R,a+R) можно представить в виде бесконечной суммы степенных функций: f(x)= n=0 ∑ ∞ n! f (n) (a) (x−a) n . Одна, самая первая степенная функция в разложении, очень грубо приближает f ( x ) f(x). Прибавляя к ней следующую, мы получим более точное приближение. Каждая следующая элементарная функция увеличивает точность приближения, но менее заметна в общей сумме. Если нам не требуется абсолютно точное разложение, вместо бесконечного ряда Тейлора мы можем ограничиться суммой его первых k k элементов. Таким образом, интересующую нас функцию мы с некоторой точностью представили в виде суммы «простых» функций. Перенесём эту идею на задачи машинного обучения. В машинном обучении мы пытаемся по выборке ) восстановить неизвестную истинную зависимость. Прежде всего, мы выбираем подходящий алгоритм. Мы можем выбрать «сложный» алгоритм, который сразу хорошо выучит истинную зависимость. А можем обучить «простой», который выучит истинную зависимость посредственно. Затем мы добавим к нему ещё один такой простой алгоритм, чтобы уточнить предсказание первого алгоритма. Продолжая этот процесс, мы получим сумму простых алгоритмов, где первый алгоритм грубо приближает",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 2,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "бесконечного ряда Тейлора мы можем ограничиться суммой его первых k k элементов. Таким образом, интересующую нас функцию мы с некоторой точностью представили в виде суммы «простых» функций. Перенесём эту идею на задачи машинного обучения. В машинном обучении мы пытаемся по выборке ) восстановить неизвестную истинную зависимость. Прежде всего, мы выбираем подходящий алгоритм. Мы можем выбрать «сложный» алгоритм, который сразу хорошо выучит истинную зависимость. А можем обучить «простой», который выучит истинную зависимость посредственно. Затем мы добавим к нему ещё один такой простой алгоритм, чтобы уточнить предсказание первого алгоритма. Продолжая этот процесс, мы получим сумму простых алгоритмов, где первый алгоритм грубо приближает истинную зависимость, а каждый следующий делает приближение всё точнее. Пример с задачей регрессии: формальное описание Рассмотрим тот же пример с задачей регрессии и квадратичной функцией потерь: min ⁡ L(y,x)= 2 1 i=1 ∑ N (y i −a(x i )) 2 →min Для решения также будем строить композицию из K K базовых алгоритмов семейства a(x)=a K (x)=b 1 (x)+b 2 (x)+⋯+b K (x) В качестве базовых алгоритмов выберем, как и условились в начале параграфа, семейство B B решающих деревьев некоторой фиксированной глубины. Используя известные нам методы построения решающих деревьев, обучим алгоритм (x)∈B, который наилучшим образом приближает целевую переменную: argminb∈B (x)= b∈B argmin L(y,b(x)) Построенный алгоритм (x), скорее всего, работает не идеально. Более того, если базовый алгоритм работает слишком хорошо на обучающей выборке, то высока вероятность переобучения: низкий уровень смещения, но высокий уровень разброса. Далее вычислим, насколько сильно отличаются предсказания этого дерева от истинных значений: Теперь мы хотим скорректировать (x) с помощью (x). В идеале так, чтобы (x) идеально предсказывал величины , ведь в этом случае )=b 1 (x i )+b )+s )+(y i −b 1 (x i ))=y i Найти совершенный алгоритм, скорее всего, не получится, но по крайней мере мы можем выбрать из семейства наилучшего представителя для такой задачи. Итак, второе решающее дерево будет обучаться предсказывать разности argminb∈B (x)= b∈B argmin L(s 1 ,b(x)) Ожидается, что композиция из двух таких моделей (x)=b 1 (x)+b 2 (x) станет более качественно предсказывать целевую переменную y y. Далее рассуждения повторяются до построения всей композиции. На k k-ом шаге вычисляется разность между правильным ответом и текущим предсказанием композиции из k − 1 k−1 алгоритмов: k−1 =y i − j=1 ∑ k−1 b j (x i )=y i −a k−1 (x i ) Затем k k-й алгоритм учится предсказывать эту разность: argminb∈B (x)= b∈B argmin L(s k−1 ,b(x)), а композиция в целом обновляется по формуле (x)=a k−1 (x)+b k (x) Обучение K K базовых алгоритмов завершает построение композиции. Обобщение на другие функции потерь Интуиция Отметим теперь важное свойство функции потерь в рассмотренном выше примере с регрессией. Для этого посчитаем производную функции потерь по предсказанию z=a k (x i ) модели для i i-го объекта: ∂L(y i ,z) z=a −z) 2 z=a )−y i Видим, что разность, на которую обучается k k-й алгоритм, выражается через производную: )=− ∂z ∂L(y i ,z) z=a k (x i ) Таким образом, для каждого объекта x i x i очередной алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 3,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(x)=a k−1 (x)+b k (x) Обучение K K базовых алгоритмов завершает построение композиции. Обобщение на другие функции потерь Интуиция Отметим теперь важное свойство функции потерь в рассмотренном выше примере с регрессией. Для этого посчитаем производную функции потерь по предсказанию z=a k (x i ) модели для i i-го объекта: ∂L(y i ,z) z=a −z) 2 z=a )−y i Видим, что разность, на которую обучается k k-й алгоритм, выражается через производную: )=− ∂z ∂L(y i ,z) z=a k (x i ) Таким образом, для каждого объекта x i x i очередной алгоритм в бустинге обучается предсказывать антиградиент функции потерь по предсказанию модели ∂L(y i ,z) в точке ) предсказания текущей части композиции на объекте x i x i . Почему же это важно? Дело в том, что это наблюдение позволяет обобщить подход построения бустинга на произвольную дифференцируемую функцию потерь. Для этого мы заменяем обучение на разность обучением на антиградиент функции потерь (−g i k ), где ∂L(y i ,z) z=a k (x i ) Вспомните аналогию с гольфистом: обучение композиции можно представить как перемещение предсказания из точки ),a k (x 2 ),…,a k (x N )) в точку k+1 (x 1 ),a k+1 (x 2 ),…,a k+1 (x N )). В конечном итоге мы ожидаем, что точка ),a K (x 2 ),…,a K (x N )) будет располагаться как можно ближе к точке с истинными значениями ,…,y N ). 5 В случае квадратичной функции потерь интуиция вполне подкрепляется математикой. Изменится ли что-либо в наших действиях, если мы поменяем квадратичную функцию потерь на любую другую? С одной стороны, мы, как и прежде, можем двигаться в направлении уменьшения разности предсказания и истинного значения: любая функция потерь поощряет такие шаги для каждого отдельного объекта, ведь для любой адекватной функции потерь выполнено L(y,y)=0. Но мы можем посмотреть на задачу и с другой стороны: не с точки зрения уменьшения расстояния между вектором предсказаний и вектором истинных значений, а с точки зрения уменьшения значения функции потерь. Для наискорейшего уменьшения функции потерь нам необходимо шагнуть в сторону её антиградиента по вектору предсказаний текущей композиции, то есть как раз таки в сторону вектора (−g 1 k ,…,−g N k ). Это направление не обязано совпадать с шагом по направлению уменьшения разности предсказания и истинного значения. Например, может возникнуть гипотетическая ситуация, как на рисунке ниже: 5 В изображённом примере рассматриваются два объекта . Текущее предсказание для них — ),a k (x 2 )), а окружность определяет варианты следующего шага: первый вариант — пойти в направлении ), как делалось ранее; второй — пойти в направлении антиградиента. Также показаны линии уровня значений функции потерь. Функция потерь в этом примере устроена таким образом, что , из-за чего шаг по антиградиенту оказывается более выгодным. Движение в сторону антиградиента более выгодно с точки зрения минимизации функции потерь — плюс оно также позволяет справляться с ситуациями, когда явно посчитать остаток (разницу между целевым значением и предсказанием) не представляется возможным. Один из таких примеров — задача ранжирования. В задаче ранжирования объекты в датасете разбиты на группы и требуется построить модель, по предсказаниям которой можно было бы «правильно»",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 4,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вариант — пойти в направлении ), как делалось ранее; второй — пойти в направлении антиградиента. Также показаны линии уровня значений функции потерь. Функция потерь в этом примере устроена таким образом, что , из-за чего шаг по антиградиенту оказывается более выгодным. Движение в сторону антиградиента более выгодно с точки зрения минимизации функции потерь — плюс оно также позволяет справляться с ситуациями, когда явно посчитать остаток (разницу между целевым значением и предсказанием) не представляется возможным. Один из таких примеров — задача ранжирования. В задаче ранжирования объекты в датасете разбиты на группы и требуется построить модель, по предсказаниям которой можно было бы «правильно» упорядочить документы в каждой группе (обычно по убыванию предсказания модели). Что значит упорядочить «правильно»? Это значит, что полученная по предсказаниям модели перестановка объектов в группе должна быть близка к идеальной по некоторой метрике. Как задается идеальная перестановка? Есть два способа: Первый способ — проставить каждому объекту число y y, по которому можно отсортировать объекты для получения идеальной перестановки. Это число можно рассматривать как таргет и обучать модель регрессии — в некоторых случаях это даже будет работать хорошо. Второй способ — задать набор пар объектов, которые обозначают их порядок относительно друг друга в идеальной перестановке. То есть пара ( i , j ) (i,j) означает, что объект с номером i i должен стоять раньше в перестановке, чем объект с номером j j. Во втором способе таргетов у объектов нет, но дифференцируемая функция потерь есть — в библиотеке CatBoost она называется PairLogit и вычисляется по формуле: PairLogit= ∣Pairs∣ − p,n∈Pairs ∑ (log( 1+e −(a где — это предсказания модели на объектах p p и n n соответственно. Градиент такой функции потерь посчитать можно, а разницу между предсказанием и истинным значением — нет. Математическое обоснование Попробуем записать наши интуитивные соображения более формально. Пусть L L – дифференцируемая функция потерь, а наш алгоритм a ( x ) a(x) представляет собой композицию базовых алгоритмов: a(x)=a k (x)=b 1 (x)+…+b k (x) Мы строим нашу композицию «жадно»: (x)=a k−1 (x)+b k (x), где вновь добавляемый базовый алгоритм b k b k обучается так, чтобы улучшить предсказания текущей композиции: b k = argminb∈B b∈B argmin i=1 ∑ N L(y i ,a k−1 (x i )+b(x i )) Модель b 0 b 0 выбирается так, чтобы минимизировать потери на обучающей выборке: b 0 = argminb∈B b∈B argmin i=1 ∑ N L(y i ,b(x i )) Для построения базовых алгоритмов на следующих шагах рассмотрим разложение Тейлора функции потерь L L до первого члена в окрестности точки k−1 (x i )): L(y i ,a k−1 (x i )+b(x i ))≈L(y i ,a k−1 (x i ))+b(x i ) ∂z ∂L(y i ,z) z=a k−1 (x i ) =L(y i ,a k−1 (x i ))+b(x i )g i k−1 Избавившись от постоянных членов, мы получим следующую оптимизационную задачу: b k ≈ argminb∈B b∈B argmin i=1 ∑ N b(x i )g i k−1 Поскольку суммируемое выражение — это скалярное произведение двух векторов, его значение минимизируют b ( x i ) b(x i ), пропорциональные значениям k−1 . Поэтому на каждой итерации базовые",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 5,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "L до первого члена в окрестности точки k−1 (x i )): L(y i ,a k−1 (x i )+b(x i ))≈L(y i ,a k−1 (x i ))+b(x i ) ∂z ∂L(y i ,z) z=a k−1 (x i ) =L(y i ,a k−1 (x i ))+b(x i )g i k−1 Избавившись от постоянных членов, мы получим следующую оптимизационную задачу: b k ≈ argminb∈B b∈B argmin i=1 ∑ N b(x i )g i k−1 Поскольку суммируемое выражение — это скалярное произведение двух векторов, его значение минимизируют b ( x i ) b(x i ), пропорциональные значениям k−1 . Поэтому на каждой итерации базовые алгоритмы b k b k обучаются предсказывать значения антиградиента функции потерь по текущим предсказаниям композиции. Итак, использованная нами интуиция шага в сторону «уменьшения остатка» удивительным образом привела к оптимальным смещениям в случае квадратичной функции потерь, но для других функций потерь это не так: для них смещение происходит в сторону антиградиента. Получается, что в общем случае на каждой итерации базовые алгоритмы должны приближать значения антиградиента функции потерь. Однако есть частный случай, в котором в качестве таргета для базового алгоритма выгоднее использовать именно «остатки» — это касается функции потерь MAE. Её производная равна -1, 0 или +1. Приближая базовым алгоритмом антиградиент MAE, количество итераций до сходимости будет расти пропорционально масштабу таргета. То есть, если домножить целевое значение на 10, то потребуется в 10 раз больше итераций градиентного бустинга. Использование остатков в качестве таргета для базового алгоритма не имеет такой проблемы. Аналогичные рассуждения верны также для функции MAPE, в которой проблема с масштабом таргета может проявляться еще сильнее. Обучение базового алгоритма При построении очередного базового алгоритма b k + 1 b k+1 мы решаем задачу регрессии с таргетом, равным антиградиенту функции потерь исходной задачи на предсказании +…+b k . Теоретически можно воспользоваться любым методом построения регрессионного дерева. Важно выбрать оценочную функцию S S, которая будет показывать, насколько текущая структура дерева хорошо приближает антиградиент. Её нужно будет использовать для построения критерия ветвления: max ⁡ , ∣R∣⋅S(R)−∣R right ∣⋅S(R right )−∣R left ∣⋅S(R left )→max, где S ( R ) S(R) — значение функции S S в вершине S(R left ),S(R right ) — значения в левом и правом сыновьях R R после добавления предиката, ∣ ⋅ ∣ ∣⋅∣ — количество элементов, пришедших в вершину. Например, можно использовать следующие оценочные функции: (g,p)= i=1 Cosine(g,p)=− i=1 i=1 ∑ N g i 2 i=1 где p i p i — предсказание дерева на объекте — антиградиент, на который учится дерево, p=p i i=1 g=g i N . Функция L 2 L 2 представляет собой среднеквадратичную ошибку, а функция Cosine определяет близость через косинусное расстояние между векторами предсказаний и антиградиентов. В итоге обучение базового алгоритма проходит в два шага: по функции потерь вычисляется целевая переменная для обучения следующего базового алгоритма: ∂L(y i ,z) z=a k (x i ) строится регрессионное дерево на обучающей выборке ,−g i k ), минимизирующее выбранную оценочную функцию. На практике Поскольку для построения градиентного бустинга достаточно уметь считать градиент функции потерь по предсказаниям, с его помощью можно решать широкий спектр задач. В библиотеках градиентного",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 6,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "который учится дерево, p=p i i=1 g=g i N . Функция L 2 L 2 представляет собой среднеквадратичную ошибку, а функция Cosine определяет близость через косинусное расстояние между векторами предсказаний и антиградиентов. В итоге обучение базового алгоритма проходит в два шага: по функции потерь вычисляется целевая переменная для обучения следующего базового алгоритма: ∂L(y i ,z) z=a k (x i ) строится регрессионное дерево на обучающей выборке ,−g i k ), минимизирующее выбранную оценочную функцию. На практике Поскольку для построения градиентного бустинга достаточно уметь считать градиент функции потерь по предсказаниям, с его помощью можно решать широкий спектр задач. В библиотеках градиентного бустинга даже реализована возможность создавать свои функции потерь: для этого достаточно уметь вычислять ее градиент, зная истинные значения и текущие предсказания для элементов обучающей выборки. Типичный градиентный бустинг имеет в составе несколько тысяч деревьев решений, которые необходимо строить последовательно. Построение решающего дерева на выборках типичного размера и современном железе, даже с учетом всех оптимизаций, требует небольшого, но всё-таки заметного времени (0.1-1c), которое для всего ансамбля превратится в десятки минут. Это не так быстро, как обучение линейных моделей, но всё-таки значительно быстрее, чем обучение типичных нейросетей. Темп обучения (learning rate) Обучение композиции с помощью градиентного бустинга может привести к переобучению, если базовые алгоритмы слишком сложные. Например, если сделать решающие деревья слишком глубокими (более 10 уровней), то при обучении бустинга ошибка на обучающей выборке даже при довольно скромном K K может приблизиться к нулю, то есть предсказание будет почти идеальным, но на тестовой выборке всё будет плохо. Существует два решения этой проблемы. Во-первых, необходимо упростить базовую модель, уменьшив глубину дерева (либо примерив какие-либо ещё техники регуляризации). Во-вторых, мы можем ввести параметр, называемый темпом обучения (learning rate) η∈(0,1]: k+1 (x)=a k (x)+ηb k+1 (x) Присутствие этого параметра означает, что каждый базовый алгоритм вносит относительно небольшой вклад во всю композицию: если расписать сумму целиком, она будет иметь вид k+1 (x)=b 1 (x)+ηb 2 (x)+ηb 3 (x)+…+ηb k+1 (x) Значение параметра обычно определяется эмпирически по входным данным. В библиотеке CatBoost темп обучения может быть выбран автоматически по набору данных. Для этого используется заранее обученная линейная модель, предсказывающая темп обучения по мета-параметрам выборки данных: числу объектов, числу признаков и другим. Темп обучения связан с количеством итераций градиентного бустинга. Чем меньше learning rate, тем больше итераций потребуется сделать для достижения того же качества на обучающей выборке. Feature importance Отдельные деревья решений можно легко интерпретировать, просто визуализируя их структуру. Но в модели градиентного бустинга содержатся сотни деревьев, и поэтому её нелегко интерпретировать с помощью визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции. Можно сделать следующее наблюдение: признаки из верхней части дерева влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни. Таким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 7,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "поэтому её нелегко интерпретировать с помощью визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции. Можно сделать следующее наблюдение: признаки из верхней части дерева влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни. Таким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков. Этот метод известен как MDI (mean decrease in impurity). Существуют и другие методы оценки важности признаков для ансамблей: например, Permutation feature importance (см. описание в sklearn) и множество разных подходов, предлагаемых в библиотеке CatBoost. Все эти техники отбора признаков применимы также и для случайных лесов. Реализации Для общего развития имеет смысл посмотреть реализацию в sklearn, но на практике она весьма медленная и не такая уж умная. Хороших реализаций GBDT есть, как минимум, три: LightGBM, XGBoost и CatBoost. Исторически они отличались довольно сильно, но за последние годы успели скопировать друг у друга все хорошие идеи. Форма деревьев Одно из основных отличий LightGBM, XGBoost и CatBoost — форма решающих деревьев. LightGBM строит деревья по принципу: «На каждом шаге делим вершину с наилучшим скором», а основным критерием остановки выступает максимально допустимое количество вершин в дереве. Это приводит к тому, что деревья получаются несимметричными, то есть поддеревья могут иметь разную глубину — например, левое поддерево может иметь глубину 2 2, а правое может разрастись до глубины 15 15. С одной стороны, это позволяет быстро подогнаться под обучающие данные. С другой — бесконтрольный рост дерева в глубину неизбежно ведет к переобучению, поэтому LightGBM позволяет помимо количества вершин ограничивать и максимальную глубину. Впрочем, это ограничение обычно все равно выше, чем для XGBoost и CatBoost. tree XGBoost строит деревья по принципу: «Строим дерево последовательно по уровням до достижения максимальной глубины». Отдельного ограничения на количество вершин нет, так как оно естественным образом получается из ограничения на глубину дерева. В XGBoost деревья «стремятся» быть симметричными по глубине, и в идеале получается полное бинарное дерево, если это не противоречит другим ограничениям (например, ограничению на минимальное количество объектов в листе). Такие деревья обычно являются более устойчивыми к переобучению. tree CatBoost строит деревья по принципу: «Все вершины одного уровня имеют одинаковый предикат». Одинаковые сплиты во всех вершинах одного уровня позволяют избавиться от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах. Кроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основной критерий остановки, как и в случае XGBoost, — ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки. tree Где используется градиентный бустинг Если коротко — везде. Сегодня",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 8,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "от ветвлений (конструкций if-else) в коде инференса модели с помощью битовых операций и получить более эффективный код, который в разы ускоряет применение модели, в особенности в случае применения на батчах. Кроме этого, такое ограничение на форму дерева выступает в качестве сильной регуляризации, что делает модель более устойчивой к переобучению. Основной критерий остановки, как и в случае XGBoost, — ограничение на глубину дерева. Однако, в отличие от XGBoost, в CatBoost всегда создаются полные бинарные деревья, несмотря на то, что в некоторые поддеревья может не попасть ни одного объекта из обучающей выборки. tree Где используется градиентный бустинг Если коротко — везде. Сегодня это один из двух главных подходов, которые используются на практике (второй — это нейронные сети, конечно). Формально градиентный бустинг слабее и менее гибок, чем сети, но выигрывает в простоте настройки темпа обучения и применения, размере и интерпретируемости модели. Во многих компаниях, так или иначе связанных с ML, он используется для всех задач, которые не связаны с однородными данными (картинками, текстами, и так далее). Типичный поисковый запрос в Яндексе, выбор отеля на Booking.com или сериала на вечер в Netflix задействует несколько десятков моделей GBDT. Впрочем, в будущем можно ожидать плавного исчезновения этого подхода, так как улучшение архитектур глубинного обучения и дальнейшее развитие железа нивелирует его преимущество по сравнению с нейросетями. Почитать по теме Серия блог-постов о градиентном бустинге от Terence Parr and Jeremy Howard Раздел документации sklearn с теоретическими выкладками для градиентного бустинга Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.4. Ансамбли в машинном обучении Как смешать несколько моделей в одну. Стэкинг, бэггинг, случайные леса Следующий параграф 3.1. Метрики классификации и регрессии Как оценить качество модели для классификации или регрессии и почему для разных задач нужны разные метрики",
    "metadata": {
      "title": "Градиентный бустинг",
      "url": "https://education.yandex.ru/handbook/ml/article/gradientnyj-busting",
      "course": "ml",
      "chapter": "2. Классическое обучение с учителем",
      "chapter_id": "2.5",
      "part": 9,
      "total_parts": 9,
      "source_file": "2.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как оценить качество модели для классификации или регрессии и почему для разных задач нужны разные метрики Гораздо легче что-то измерить, чем понять, что именно вы измеряете Джон Уильям Салливан Задачи машинного обучения с учителем, как правило, состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация? Почти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению — подобрать подходящий критерий, который позволит сравнивать различные модели. Важно: качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки — например, кросс-валидации (о ней мы поговорим в следующем параграфе). Выбор метрик в реальных задачах Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее. Например, мы хотим: решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин — чтобы предложение соответствовало спросу, и не пришлось выбрасывать излишки; увеличить счастье пользователя от работы с сервисом, чтобы он стал лояльным и приносил стабильный прогнозируемый доход; решить, нужно ли направить человека на дополнительное обследование. В каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками — не защищёнными авторским правом, а потому совершенно бесплатными. Иерархия метрик могла бы иметь такой вид: Самый верхний уровень: будущий доход сервиса — невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий; Медианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку — её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество; Доля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей; Функция потерь, на которую мы будем обучать генеративную сеть. На этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают offline и online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 1,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠  = метрика качества Как мы узнали ранее, методы обучения реализуют разные подходы к обучению: обучение на основе прироста информации (как в деревьях решений); обучение на основе сходства (как в методах ближайших соседей); обучение на основе вероятностной модели данных (например, максимизацией правдоподобия); обучение на основе ошибок (минимизация эмпирического риска). И в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте. Во время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборки. Важно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом: Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью). Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток. В некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играют роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики — число верно угаданных меток (accuracy). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy — предсказанные метки (то есть по сути argmax логитов). Бинарная классификация: метки классов Перейдём к обзору метрик и начнём с самой простой разновидности классификации — бинарной, а затем постепенно будем наращивать сложность. Напомним постановку задачи бинарной классификации: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈{0,1} построить модель, которая по объекту x x предсказывает метку класса f(x)∈{0,1}. Первый критерий качества, который приходит в голову, — accuracy, то есть доля объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 2,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия. Confusion matrix (матрица ошибок) Исторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей). Поэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся — «отрицательным». Заметим, что для каждого объекта в выборке возможно 4 ситуации: мы предсказали положительную метку и угадали. Будет относить такие объекты к true positive (TP) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку; мы предсказали положительную метку, но ошиблись в своём предсказании — false positive (FP). False, потому что предсказание было неправильным; мы предсказали отрицательную метку и угадали — true negative (TN); и наконец, мы предсказали отрицательную метку, но ошиблись — false negative (FN). Для удобства все эти 4 числа изображают в виде таблицы, которую называют confusion matrix (матрицей ошибок): 6 Не волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая — какой класс мы предсказали. 6 Пример Попробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на Breast cancer wisconsin (diagnostic) dataset. Объекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные. Модель 1. Константное предсказание Решение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание — самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 3,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли). Модель 2. Случайный лес. Настало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса. from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train, y_train) y_true = y_test y_pred = rfc.predict(X_test) rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 52 FN = 1 Истинный класс - FP = 4 TN = 86 Можно сказать, что этот классификатор чему-то научился, так как главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 + 1 = 5 объектов (сравните с 0 + 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными). Отметим, что вычисляя долю недиагональных элементов, мы приходим к метрике error rate, о которой мы говорили в самом начале: Error rate Error rate= TP+TN+FP+FN FP+FN тогда как доля объектов, попавших на главную диагональ — это как раз таки accuracy: Accuracy Accuracy= TP+TN+FP+FN TP+TN Модель 3. Метод опорных векторов. Давайте построим еще один классификатор на основе линейного метода опорных векторов. Важно: Не забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение. from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) scaled_linsvc = LinearSVC(C=0.01,random_state=42) scaled_linsvc.fit(ss.transform(X_train), y_train) y_true = y_test y_pred = scaled_linsvc.predict(ss.transform(X_test)) tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 50 FN = 3 Истинный класс - FP = 1 TN = 89 Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 4,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, так как на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества. Что же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче: FP — доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной; FN — доля злокачественных опухолей, которые классификатор пропускает. В такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни. Рассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее. Итак, из примеров мы видим, что в текущем виде введенная нами доля ошибочных классификаций не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту. Точность и полнота Accuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой. Рассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше. Пусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 5,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision. Если же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется полнотой (recall) Recall Recall= TP+FN TP Интуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели. Например, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить. Хорошее понимание происходящего даёт следующая картинка: Источник Recall@k, Precision@k Метрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных. Например, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно. F1-мера Как мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название F1-меры (F1-measure). Recall 1 + Precision Recall+Precision Recall⋅Precision = TP+ 2 FP+FN TP Стоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться F β F β мерой: =(β 2 +1) Recall+β 2 Precision Recall⋅Precision Бинарная классификация: вероятности классов Многие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу: f(x;w,w 0 )=I[g(x,w)>w 0 ]. Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 6,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели? AUC Пусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики TPR и FPR: TPR (true positive rate) — это полнота, доля положительных объектов, правильно предсказанных положительными: TPR= P TP = TP+FN TP FPR (false positive rate) — это доля отрицательных объектов, неправильно предсказанных положительными: FPR= N FP = FP+TN FP Обе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется ROC-кривой (receiver operating characteristics curve, сокращённо ROC curve). Следующий интерактивный график поможет вам понять поведение ROC-кривой. Желтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора. Если классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1), площадь под которой равна 1. Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5. Поэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая. Чем лучше классификатор разделяет два класса, тем больше площадь (area under curve) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется AUC и она работает благодаря следующему свойству ROC-кривой: AUC равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше: AUC AUC= i=1 ∑ N j=1 ∑ N I[y i <y j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 7,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и 0 0 — иначе. Однако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях. Таким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC. Утверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными. Average Precision Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 0 0 до 1 1, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет). Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида: Источник Рассмотрим среднее значение точности (оно равно площади под кривой точность-полнота): AP =∫ 0 1 p(r)dr Получим показатель эффективности, который называется average precision. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа. Многоклассовая классификация Если классов становится больше двух, расчёт метрик усложняется. Если задача классификации на K K классов ставится как K K задач об отделении класса i i от остальных ( i=1,…,K), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из K K матриц ошибок: Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 8,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым. Рассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4. 6 Модель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы). 6 Посчитаем Precision классификации двумя способами: С помощью микроусреднения получаем Precision 0.87 Precision= 3 1 (20+0+19)+ 3 1 (5+1+0) 3 1 (20+0+19) =0.87 С помощью макроусреднения получаем Precision 0.6 Precision= 3 1 ( 20+5 20 + 0+1 0 + 19+0 19 )=0.6 Видим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует. Как оптимизировать метрики классификации? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F F на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики F F — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же F(a(X),Y). К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе. Метрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 9,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, TP+FP=N, где N N — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP. По формуле точность Precision Precision= TP+FP TP = N 1 ∑ i=1 N I[y i =1] равна среднему таргету в выборке. А полнота Recall Recall= TP+FN TP = TP+0 TP =1 равна единице. Пусть теперь порог равен единице Тогда ни один объект не будет назван положительным, TP=FP=0. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть =0, будет верна формула Recall Recall= TP+FN TP = 0+FN 0 =0. То есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, Precision Precision= TP+FP TP = 0+0 0 то есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными. Скорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом предположении F P ≈ 0 FP≈0. Тогда точность Precision Precision= TP+FP TP ≈ TP+0 TP ≈1 будет близка к единице. Изменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так: 6 Recall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный). Итого оптимизация precision и recall происходит так: Модель обучается на стандартную функцию потерь (например, LogLoss). Используя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога. Выбираем нужное сочетание точности и полноты. Пусть теперь мы хотим максимизировать метрику AUC. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 10,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия В задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений: величина температуры в определенный момент времени на метеостанции количество прочтений статьи на сайте количество проданных бананов в конкретном магазине, сети магазинов или стране дебит добывающей скважины на нефтегазовом месторождении за месяц и т.п. Мы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление. Общая рекомендация такова: оценивайте весь каскад решающих правил: и те «внутренние», которые вы получаете в результате обучения, и те «итоговые», которые вы отдаёте бизнес-заказчику. Например, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов. Итак, напомним постановку задачи регрессии: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈R построить модель f(x). Величину =f(x i )−y i называют ошибкой на объекте i или регрессионным остатком. Весь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики. MSE, RMSE, R 2 R 2 MSE — одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 11,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ y ˉ . При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке Рассматривают в качестве показателя ошибки: =1− ∑ i=1 i=1 N (y i −f(x i )) 2 . У идеального решающего правила R 2 R 2 равен 1 1, у наилучшего константного предсказания он равен 0 0 на обучающей выборке. Можно заметить, что R 2 R 2 показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью. MSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок — там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы. Пусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них — очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов. Таким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача. MAE Использовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике MAE (mean absolute error): MAE(y true ,y pred )= N 1 i=1 ∑ N ∣y i −f(x i )∣ Метрики, учитывающие относительные ошибки И MSE и MAE считаются как сумма абсолютных ошибок на объектах. Рассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 12,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на объектах. MAPE, SMAPE Когда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель? В метрике MAPE (mean absolute percentage error) в знаменатель помещают целевое значение: MAPE(y true ,y pred )= N 1 i=1 −f(x i )∣ С особым случаем, когда в знаменателе оказывается 0 0, обычно поступают «инженерным» способом: или выдают за непредсказание 0 0 на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике SMAPE (symmetric mean absolute percentage error): SMAPE(y true ,y pred )= N 1 i=1 ∑ N y i +f(x i ) 2∣y i −f(x i )∣ Если же предсказывается ноль, штраф считаем нулевым. Таким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE. WAPE Как и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример: Понедельник Вторник Среда Прогноз 55 2 50 Продажи 50 1 50 MAPE 10% 100% 0% Среднее MAPE — 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает WAPE (weighted average percentage error): WAPE(y true ,y pred )= ∑ i=1 N ∣y i ∣ ∑ i=1 N ∣y i −f(x i )∣ Если мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1. В нашем примере получим WAPE = 5.9% RMSLE Альтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика RMSLE (root mean squared logarithmic error): log log RMSLE(y true ,y pred ∣c)= N 1 i=1 ∑ N ( 2 1 log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 13,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑ N I[∣y i −f(x i )∣>d] Например, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся. Как оптимизировать метрики регрессии? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики F F — выбрать в качестве функции потерь ту же F(a(X),Y). К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль. Для оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.5. Градиентный бустинг Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями Следующий параграф 3.2. Кросс-валидация Как строить надёжные оценки качества моделей и никогда не смешивать train и test",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/metriki-klassifikacii-i-regressii",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.1",
      "part": 14,
      "total_parts": 14,
      "source_file": "3.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как оценить качество модели для классификации или регрессии и почему для разных задач нужны разные метрики Гораздо легче что-то измерить, чем понять, что именно вы измеряете Джон Уильям Салливан Задачи машинного обучения с учителем, как правило, состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация? Почти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению — подобрать подходящий критерий, который позволит сравнивать различные модели. Важно: качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки — например, кросс-валидации (о ней мы поговорим в следующем параграфе). Выбор метрик в реальных задачах Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее. Например, мы хотим: решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин — чтобы предложение соответствовало спросу, и не пришлось выбрасывать излишки; увеличить счастье пользователя от работы с сервисом, чтобы он стал лояльным и приносил стабильный прогнозируемый доход; решить, нужно ли направить человека на дополнительное обследование. В каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками — не защищёнными авторским правом, а потому совершенно бесплатными. Иерархия метрик могла бы иметь такой вид: Самый верхний уровень: будущий доход сервиса — невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий; Медианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку — её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество; Доля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей; Функция потерь, на которую мы будем обучать генеративную сеть. На этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают offline и online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 1,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠  = метрика качества Как мы узнали ранее, методы обучения реализуют разные подходы к обучению: обучение на основе прироста информации (как в деревьях решений); обучение на основе сходства (как в методах ближайших соседей); обучение на основе вероятностной модели данных (например, максимизацией правдоподобия); обучение на основе ошибок (минимизация эмпирического риска). И в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте. Во время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборки. Важно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом: Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью). Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток. В некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играют роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики — число верно угаданных меток (accuracy). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy — предсказанные метки (то есть по сути argmax логитов). Бинарная классификация: метки классов Перейдём к обзору метрик и начнём с самой простой разновидности классификации — бинарной, а затем постепенно будем наращивать сложность. Напомним постановку задачи бинарной классификации: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈{0,1} построить модель, которая по объекту x x предсказывает метку класса f(x)∈{0,1}. Первый критерий качества, который приходит в голову, — accuracy, то есть доля объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 2,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия. Confusion matrix (матрица ошибок) Исторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей). Поэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся — «отрицательным». Заметим, что для каждого объекта в выборке возможно 4 ситуации: мы предсказали положительную метку и угадали. Будет относить такие объекты к true positive (TP) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку; мы предсказали положительную метку, но ошиблись в своём предсказании — false positive (FP). False, потому что предсказание было неправильным; мы предсказали отрицательную метку и угадали — true negative (TN); и наконец, мы предсказали отрицательную метку, но ошиблись — false negative (FN). Для удобства все эти 4 числа изображают в виде таблицы, которую называют confusion matrix (матрицей ошибок): 6 Не волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая — какой класс мы предсказали. 6 Пример Попробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на Breast cancer wisconsin (diagnostic) dataset. Объекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные. Модель 1. Константное предсказание Решение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание — самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 3,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли). Модель 2. Случайный лес. Настало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса. from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train, y_train) y_true = y_test y_pred = rfc.predict(X_test) rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 52 FN = 1 Истинный класс - FP = 4 TN = 86 Можно сказать, что этот классификатор чему-то научился, так как главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 + 1 = 5 объектов (сравните с 0 + 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными). Отметим, что вычисляя долю недиагональных элементов, мы приходим к метрике error rate, о которой мы говорили в самом начале: Error rate Error rate= TP+TN+FP+FN FP+FN тогда как доля объектов, попавших на главную диагональ — это как раз таки accuracy: Accuracy Accuracy= TP+TN+FP+FN TP+TN Модель 3. Метод опорных векторов. Давайте построим еще один классификатор на основе линейного метода опорных векторов. Важно: Не забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение. from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) scaled_linsvc = LinearSVC(C=0.01,random_state=42) scaled_linsvc.fit(ss.transform(X_train), y_train) y_true = y_test y_pred = scaled_linsvc.predict(ss.transform(X_test)) tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 50 FN = 3 Истинный класс - FP = 1 TN = 89 Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 4,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, так как на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества. Что же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче: FP — доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной; FN — доля злокачественных опухолей, которые классификатор пропускает. В такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни. Рассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее. Итак, из примеров мы видим, что в текущем виде введенная нами доля ошибочных классификаций не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту. Точность и полнота Accuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой. Рассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше. Пусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 5,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision. Если же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется полнотой (recall) Recall Recall= TP+FN TP Интуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели. Например, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить. Хорошее понимание происходящего даёт следующая картинка: Источник Recall@k, Precision@k Метрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных. Например, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно. F1-мера Как мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название F1-меры (F1-measure). Recall 1 + Precision Recall+Precision Recall⋅Precision = TP+ 2 FP+FN TP Стоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться F β F β мерой: =(β 2 +1) Recall+β 2 Precision Recall⋅Precision Бинарная классификация: вероятности классов Многие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу: f(x;w,w 0 )=I[g(x,w)>w 0 ]. Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 6,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели? AUC Пусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики TPR и FPR: TPR (true positive rate) — это полнота, доля положительных объектов, правильно предсказанных положительными: TPR= P TP = TP+FN TP FPR (false positive rate) — это доля отрицательных объектов, неправильно предсказанных положительными: FPR= N FP = FP+TN FP Обе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется ROC-кривой (receiver operating characteristics curve, сокращённо ROC curve). Следующий интерактивный график поможет вам понять поведение ROC-кривой. Желтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора. Если классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1), площадь под которой равна 1. Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5. Поэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая. Чем лучше классификатор разделяет два класса, тем больше площадь (area under curve) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется AUC и она работает благодаря следующему свойству ROC-кривой: AUC равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше: AUC AUC= i=1 ∑ N j=1 ∑ N I[y i <y j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 7,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и 0 0 — иначе. Однако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях. Таким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC. Утверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными. Average Precision Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 0 0 до 1 1, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет). Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида: Источник Рассмотрим среднее значение точности (оно равно площади под кривой точность-полнота): AP =∫ 0 1 p(r)dr Получим показатель эффективности, который называется average precision. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа. Многоклассовая классификация Если классов становится больше двух, расчёт метрик усложняется. Если задача классификации на K K классов ставится как K K задач об отделении класса i i от остальных ( i=1,…,K), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из K K матриц ошибок: Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 8,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым. Рассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4. 6 Модель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы). 6 Посчитаем Precision классификации двумя способами: С помощью микроусреднения получаем Precision 0.87 Precision= 3 1 (20+0+19)+ 3 1 (5+1+0) 3 1 (20+0+19) =0.87 С помощью макроусреднения получаем Precision 0.6 Precision= 3 1 ( 20+5 20 + 0+1 0 + 19+0 19 )=0.6 Видим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует. Как оптимизировать метрики классификации? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F F на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики F F — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же F(a(X),Y). К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе. Метрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 9,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, TP+FP=N, где N N — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP. По формуле точность Precision Precision= TP+FP TP = N 1 ∑ i=1 N I[y i =1] равна среднему таргету в выборке. А полнота Recall Recall= TP+FN TP = TP+0 TP =1 равна единице. Пусть теперь порог равен единице Тогда ни один объект не будет назван положительным, TP=FP=0. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть =0, будет верна формула Recall Recall= TP+FN TP = 0+FN 0 =0. То есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, Precision Precision= TP+FP TP = 0+0 0 то есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными. Скорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом предположении F P ≈ 0 FP≈0. Тогда точность Precision Precision= TP+FP TP ≈ TP+0 TP ≈1 будет близка к единице. Изменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так: 6 Recall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный). Итого оптимизация precision и recall происходит так: Модель обучается на стандартную функцию потерь (например, LogLoss). Используя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога. Выбираем нужное сочетание точности и полноты. Пусть теперь мы хотим максимизировать метрику AUC. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 10,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия В задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений: величина температуры в определенный момент времени на метеостанции количество прочтений статьи на сайте количество проданных бананов в конкретном магазине, сети магазинов или стране дебит добывающей скважины на нефтегазовом месторождении за месяц и т.п. Мы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление. Общая рекомендация такова: оценивайте весь каскад решающих правил: и те «внутренние», которые вы получаете в результате обучения, и те «итоговые», которые вы отдаёте бизнес-заказчику. Например, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов. Итак, напомним постановку задачи регрессии: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈R построить модель f(x). Величину =f(x i )−y i называют ошибкой на объекте i или регрессионным остатком. Весь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики. MSE, RMSE, R 2 R 2 MSE — одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 11,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ y ˉ . При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке Рассматривают в качестве показателя ошибки: =1− ∑ i=1 i=1 N (y i −f(x i )) 2 . У идеального решающего правила R 2 R 2 равен 1 1, у наилучшего константного предсказания он равен 0 0 на обучающей выборке. Можно заметить, что R 2 R 2 показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью. MSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок — там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы. Пусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них — очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов. Таким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача. MAE Использовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике MAE (mean absolute error): MAE(y true ,y pred )= N 1 i=1 ∑ N ∣y i −f(x i )∣ Метрики, учитывающие относительные ошибки И MSE и MAE считаются как сумма абсолютных ошибок на объектах. Рассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 12,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на объектах. MAPE, SMAPE Когда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель? В метрике MAPE (mean absolute percentage error) в знаменатель помещают целевое значение: MAPE(y true ,y pred )= N 1 i=1 −f(x i )∣ С особым случаем, когда в знаменателе оказывается 0 0, обычно поступают «инженерным» способом: или выдают за непредсказание 0 0 на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике SMAPE (symmetric mean absolute percentage error): SMAPE(y true ,y pred )= N 1 i=1 ∑ N y i +f(x i ) 2∣y i −f(x i )∣ Если же предсказывается ноль, штраф считаем нулевым. Таким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE. WAPE Как и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример: Понедельник Вторник Среда Прогноз 55 2 50 Продажи 50 1 50 MAPE 10% 100% 0% Среднее MAPE — 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает WAPE (weighted average percentage error): WAPE(y true ,y pred )= ∑ i=1 N ∣y i ∣ ∑ i=1 N ∣y i −f(x i )∣ Если мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1. В нашем примере получим WAPE = 5.9% RMSLE Альтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика RMSLE (root mean squared logarithmic error): log log RMSLE(y true ,y pred ∣c)= N 1 i=1 ∑ N ( 2 1 log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 13,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑ N I[∣y i −f(x i )∣>d] Например, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся. Как оптимизировать метрики регрессии? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики F F — выбрать в качестве функции потерь ту же F(a(X),Y). К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль. Для оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.5. Градиентный бустинг Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями Следующий параграф 3.2. Кросс-валидация Как строить надёжные оценки качества моделей и никогда не смешивать train и test",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/kross-validaciya",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.2",
      "part": 14,
      "total_parts": 14,
      "source_file": "3.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как оценить качество модели для классификации или регрессии и почему для разных задач нужны разные метрики Гораздо легче что-то измерить, чем понять, что именно вы измеряете Джон Уильям Салливан Задачи машинного обучения с учителем, как правило, состоят в восстановлении зависимости между парами (признаковое описание, целевая переменная) по данным, доступным нам для анализа. Алгоритмы машинного обучения (learning algorithm), со многими из которых вы уже успели познакомиться, позволяют построить модель, аппроксимирующую эту зависимость. Но как понять, насколько качественной получилась аппроксимация? Почти наверняка наша модель будет ошибаться на некоторых объектах: будь она даже идеальной, шум или выбросы в тестовых данных всё испортят. При этом разные модели будут ошибаться на разных объектах и в разной степени. Задача специалиста по машинному обучению — подобрать подходящий критерий, который позволит сравнивать различные модели. Важно: качество модели нельзя оценивать на обучающей выборке. Как минимум, это стоит делать на отложенной (тестовой) выборке, но если вам это позволяют время и вычислительные ресурсы, стоит прибегнуть и к более надёжным способам проверки — например, кросс-валидации (о ней мы поговорим в следующем параграфе). Выбор метрик в реальных задачах Возможно, вы уже участвовали в соревнованиях по анализу данных. На таких соревнованиях метрику (критерий качества модели) организатор выбирает за вас, и она, как правило, довольно понятным образом связана с результатами предсказаний. Но на практике всё бывает намного сложнее. Например, мы хотим: решить, сколько коробок с бананами нужно завтра привезти в конкретный магазин — чтобы предложение соответствовало спросу, и не пришлось выбрасывать излишки; увеличить счастье пользователя от работы с сервисом, чтобы он стал лояльным и приносил стабильный прогнозируемый доход; решить, нужно ли направить человека на дополнительное обследование. В каждом конкретном случае может возникать целая иерархия метрик. Представим, например, что речь идёт о стриминговом музыкальном сервисе, пользователей которого мы решили порадовать сгенерированными самодельной нейросетью треками — не защищёнными авторским правом, а потому совершенно бесплатными. Иерархия метрик могла бы иметь такой вид: Самый верхний уровень: будущий доход сервиса — невозможно измерить в моменте, сложным образом зависит от совокупности всех наших усилий; Медианная длина сессии, возможно, служащая оценкой радости пользователей, которая, как мы надеемся, повлияет на их желание продолжать платить за подписку — её нам придётся измерять в продакшене, ведь нас интересует реакция настоящих пользователей на новшество; Доля удовлетворённых качеством сгенерированной музыки асессоров, на которых мы потестируем её до того, как выставить на суд пользователей; Функция потерь, на которую мы будем обучать генеративную сеть. На этом примере мы можем заметить сразу несколько общих закономерностей. Во-первых, метрики бывают offline и online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 1,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "online (оффлайновыми и онлайновыми). Online-метрики вычисляются по данным, собираемым с работающей системы (например, медианная длина сессии). Offline-метрики могут быть измерены до введения модели в эксплуатацию, например, по историческим данным или с привлечением специальных людей, асессоров. Последнее часто применяется, когда метрика — это реакция живого человека: скажем, так поступают поисковые компании, которые предлагают людям оценить качество ранжирования экспериментальной системы ещё до того, как рядовые пользователи увидят эти результаты в обычном порядке. На самом же нижнем этаже иерархии лежат оптимизируемые в ходе обучения функции потерь. В данном разделе нас будут интересовать offline-метрики, которые могут быть измерены без привлечения людей. Функция потерь ≠  = метрика качества Как мы узнали ранее, методы обучения реализуют разные подходы к обучению: обучение на основе прироста информации (как в деревьях решений); обучение на основе сходства (как в методах ближайших соседей); обучение на основе вероятностной модели данных (например, максимизацией правдоподобия); обучение на основе ошибок (минимизация эмпирического риска). И в рамках обучения на основе минимизации ошибок мы уже отвечали на вопрос: как можно штрафовать модель за предсказание на обучающем объекте. Во время сведения задачи о построении решающего правила к задаче численной оптимизации, мы вводили понятие функции потерь и, обычно, объявляли целевой функцией сумму потерь от предсказаний на всех объектах обучающей выборки. Важно понимать разницу между функцией потерь и метрикой качества. Её можно сформулировать следующим образом: Функция потерь возникает в тот момент, когда мы сводим задачу построения модели к задаче оптимизации. Обычно требуется, чтобы она обладала хорошими свойствами (например, дифференцируемостью). Метрика — внешний, объективный критерий качества, обычно зависящий не от параметров модели, а только от предсказанных меток. В некоторых случаях метрика может совпадать с функцией потерь. Например, в задаче регрессии MSE играют роль как функции потерь, так и метрики. Но, скажем, в задаче бинарной классификации они почти всегда различаются: в качестве функции потерь может выступать кросс-энтропия, а в качестве метрики — число верно угаданных меток (accuracy). Отметим, что в последнем примере у них различные аргументы: на вход кросс-энтропии нужно подавать логиты, а на вход accuracy — предсказанные метки (то есть по сути argmax логитов). Бинарная классификация: метки классов Перейдём к обзору метрик и начнём с самой простой разновидности классификации — бинарной, а затем постепенно будем наращивать сложность. Напомним постановку задачи бинарной классификации: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈{0,1} построить модель, которая по объекту x x предсказывает метку класса f(x)∈{0,1}. Первый критерий качества, который приходит в голову, — accuracy, то есть доля объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 2,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектов, для которых мы правильно предсказали класс: Accuracy Accuracy(y,y pred )= N 1 i=1 ∑ N I[y i =f(x i )] Или же сопряженная ей метрика — доля ошибочных классификаций (error rate): Error rate = 1 − Accuracy Error rate=1−Accuracy Познакомившись чуть внимательнее с этой метрикой, можно заметить, что у неё есть несколько недостатков: она не учитывает дисбаланс классов. Например, в задаче диагностики редких заболеваний классификатор, предсказывающий всем пациентам отсутствие болезни будет иметь достаточно высокую accuracy просто потому, что больных людей в выборке намного меньше; она также не учитывает цену ошибки на объектах разных классов. Для примера снова можно привести задачу медицинской диагностики: если ошибочный положительный диагноз для здорового больного обернётся лишь ещё одним обследованием, то ошибочно отрицательный вердикт может повлечь роковые последствия. Confusion matrix (матрица ошибок) Исторически задача бинарной классификации — это задача об обнаружении чего-то редкого в большом потоке объектов, например, поиск человека, больного туберкулёзом, по флюорографии. Или задача признания пятна на экране приёмника радиолокационной станции бомбардировщиком, представляющем угрозу охраняемому объекту (в противовес стае гусей). Поэтому класс, который представляет для нас интерес, называется «положительным», а оставшийся — «отрицательным». Заметим, что для каждого объекта в выборке возможно 4 ситуации: мы предсказали положительную метку и угадали. Будет относить такие объекты к true positive (TP) группе. True — потому что предсказали мы правильно, а positive — потому что предсказали положительную метку; мы предсказали положительную метку, но ошиблись в своём предсказании — false positive (FP). False, потому что предсказание было неправильным; мы предсказали отрицательную метку и угадали — true negative (TN); и наконец, мы предсказали отрицательную метку, но ошиблись — false negative (FN). Для удобства все эти 4 числа изображают в виде таблицы, которую называют confusion matrix (матрицей ошибок): 6 Не волнуйтесь, если первое время эти обозначения будут сводить вас с ума (будем откровенны, даже профи со стажем в них порой путаются), однако логика за ними достаточно простая: первая часть названия группы показывает угадали ли мы с классом, а вторая — какой класс мы предсказали. 6 Пример Попробуем воспользоваться введёнными метриками в боевом примере: сравним работу нескольких моделей классификации на Breast cancer wisconsin (diagnostic) dataset. Объекты выборки — фотографии биопсии грудных опухолей. С их помощью было сформировано признаковое описание, которое заключается в характеристиках ядер клеток (таких как радиус ядра, его текстура, симметричность). Положительным классом в такой постановке будут злокачественные опухоли, а отрицательным — доброкачественные. Модель 1. Константное предсказание Решение задачи начнём с самого простого классификатора, который выдаёт на каждом объекте константное предсказание — самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 3,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— самый часто встречающийся класс. from sklearn.datasets import load_breast_cancer the_data = load_breast_cancer() # 0 — «доброкачественный» # 1 — «злокачественный» relabeled_target = 1 - the_data[\"target\"] from sklearn.model_selection import train_test_split X = the_data[\"data\"] y = relabeled_target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) from sklearn.dummy import DummyClassifier dc_mf = DummyClassifier(strategy=\"most_frequent\") dc_mf.fit(X_train, y_train) from sklearn.metrics import confusion_matrix y_true = y_test y_pred = dc_mf.predict(X_test) dc_mf_tn, dc_mf_fp, dc_mf_fn, dc_mf_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 0 FN = 53 Истинный класс - FP = 0 TN = 90 Обучающие данные таковы, что наш dummy-классификатор все объекты записывает в отрицательный класс, то есть признаёт все опухоли доброкачественными. Такой наивный подход позволяет нам получить минимальный штраф за FP (действительно, нельзя ошибиться в предсказании, если положительный класс вообще не предсказывается), но и максимальный штраф за FN (в эту группу попадут все злокачественные опухоли). Модель 2. Случайный лес. Настало время воспользоваться всем арсеналом моделей машинного обучения, и начнём мы со случайного леса. from sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train, y_train) y_true = y_test y_pred = rfc.predict(X_test) rfc_tn, rfc_fp, rfc_fn, rfc_tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 52 FN = 1 Истинный класс - FP = 4 TN = 86 Можно сказать, что этот классификатор чему-то научился, так как главная диагональ матрицы стала содержать все объекты из отложенной выборки, за исключением 4 + 1 = 5 объектов (сравните с 0 + 53 объектами dummy-классификатора, все опухоли объявляющего доброкачественными). Отметим, что вычисляя долю недиагональных элементов, мы приходим к метрике error rate, о которой мы говорили в самом начале: Error rate Error rate= TP+TN+FP+FN FP+FN тогда как доля объектов, попавших на главную диагональ — это как раз таки accuracy: Accuracy Accuracy= TP+TN+FP+FN TP+TN Модель 3. Метод опорных векторов. Давайте построим еще один классификатор на основе линейного метода опорных векторов. Важно: Не забудьте привести признаки к единому масштабу, иначе численный алгоритм не сойдется к решению и мы получим гораздо более плохо работающее решающее правило. Попробуйте проделать это упражнение. from sklearn.svm import LinearSVC from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) scaled_linsvc = LinearSVC(C=0.01,random_state=42) scaled_linsvc.fit(ss.transform(X_train), y_train) y_true = y_test y_pred = scaled_linsvc.predict(ss.transform(X_test)) tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel() Прогнозируемый класс + Прогнозируемый класс - Истинный класс + TP = 50 FN = 3 Истинный класс - FP = 1 TN = 89 Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 4,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Сравним результаты Легко заметить, что каждая из двух моделей лучше классификатора-пустышки, однако давайте попробуем сравнить их между собой. С точки зрения error rate модели практически одинаковы: 5/143 для леса против 4/143 для SVM. Посмотрим на структуру ошибок чуть более внимательно: лес — (FP = 4, FN = 1), SVM — (FP = 1, FN = 3). Какая из моделей предпочтительнее? Замечание: Мы сравниваем несколько классификаторов на основании их предсказаний на отложенной выборке. Насколько ошибки данных классификаторов зависят от разбиения исходного набора данных? Иногда в процессе оценки качества мы будем получать модели, чьи показатели эффективности будут статистически неразличимыми. Пусть мы учли предыдущее замечание и эти модели действительно статистически значимо ошибаются в разную сторону. Мы встретились с очевидной вещью: на матрицах нет отношения порядка. Когда мы сравнивали dummy-классификатор и случайный лес с помощью Accuracy, мы всю сложную структуру ошибок свели к одному числу, так как на вещественных числах отношение порядка есть. Сводить оценку модели к одному числу очень удобно, однако не стоит забывать, что у вашей модели есть много аспектов качества. Что же всё-таки важнее уменьшить: FP или FN? Вернёмся к задаче: FP — доля доброкачественных опухолей, которым ошибочно присваивается метка злокачественной; FN — доля злокачественных опухолей, которые классификатор пропускает. В такой постановке становится понятно, что при сравнении выиграет модель с меньшим FN (то есть лес в нашем примере), ведь каждая не обнаруженная опухоль может стоить человеческой жизни. Рассмотрим теперь другую задачу: по данным о погоде предсказать, будет ли успешным запуск спутника. FN в такой постановке — это ошибочное предсказание неуспеха, то есть не более, чем упущенный шанс (если вас, конечно не уволят за срыв сроков). С FP всё серьёзней: если вы предскажете удачный запуск спутника, а на деле он потерпит крушение из-за погодных условий, то ваши потери будут в разы существеннее. Итак, из примеров мы видим, что в текущем виде введенная нами доля ошибочных классификаций не даст нам возможности учесть неравную важность FP и FN. Поэтому введем две новые метрики: точность и полноту. Точность и полнота Accuracy - это метрика, которая характеризует качество модели, агрегированное по всем классам. Это полезно, когда классы для нас имеют одинаковое значение. В случае, если это не так, accuracy может быть обманчивой. Рассмотрим ситуацию, когда положительный класс это событие редкое. Возьмем в качестве примера поисковую систему - в нашем хранилище хранятся миллиарды документов, а релевантных к конкретному поисковому запросу на несколько порядков меньше. Пусть мы хотим решить задачу бинарной классификации «документ d релевантен по запросу q». Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 5,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Благодаря большому дисбалансу, Accuracy dummy-классификатора, объявляющего все документы нерелевантными, будет близка к единице. Напомним, что Accuracy Accuracy= TP+TN+FP+FN TP+TN , и в нашем случае высокое значение метрики будет обеспечено членом TN, в то время для пользователей более важен высокий TP. Поэтому в случае ассиметрии классов, можно использовать метрики, которые не учитывают TN и ориентируются на TP. Если мы рассмотрим долю правильно предсказанных положительных объектов среди всех объектов, предсказанных положительным классом, то мы получим метрику, которая называется точностью (precision) Precision Precision= TP+FP TP Интуитивно метрика показывает долю релевантных документов среди всех найденных классификатором. Чем меньше ложноположительных срабатываний будет допускать модель, тем больше будет её Precision. Если же мы рассмотрим долю правильно найденных положительных объектов среди всех объектов положительного класса, то мы получим метрику, которая называется полнотой (recall) Recall Recall= TP+FN TP Интуитивно метрика показывает долю найденных документов из всех релевантных. Чем меньше ложно отрицательных срабатываний, тем выше recall модели. Например, в задаче предсказания злокачественности опухоли точность показывает, сколько из определённых нами как злокачественные опухолей действительно злокачественные, а полнота — какую долю злокачественных опухолей нам удалось выявить. Хорошее понимание происходящего даёт следующая картинка: Источник Recall@k, Precision@k Метрики Recall и Precision хорошо подходят для задачи поиска «документ d релевантен запросу q», когда из списка рекомендованных алгоритмом документов нас интересует только первый. Но не всегда алгоритм машинного обучения вынужден работать в таких жестких условиях. Может быть такое, что вполне достаточно, что релевантный документ попал в первые k рекомендованных. Например, в интерфейсе выдачи первые три подсказки видны всегда одновременно и вообще не очень понятно, какой у них порядок. Тогда более честной оценкой качества алгоритма будет «в выдаче D размера k по запросу q нашлись релевантные документы». Для расчёта метрики по всей выборке объединим все выдачи и рассчитаем precision, recall как обычно подокументно. F1-мера Как мы уже отмечали ранее, модели очень удобно сравнивать, когда их качество выражено одним числом. В случае пары Precision-Recall существует популярный способ скомпоновать их в одну метрику - взять их среднее гармоническое. Данный показатель эффективности исторически носит название F1-меры (F1-measure). Recall 1 + Precision Recall+Precision Recall⋅Precision = TP+ 2 FP+FN TP Стоит иметь в виду, что F1-мера предполагает одинаковую важность Precision и Recall, если одна из этих метрик для вас приоритетнее, то можно воспользоваться F β F β мерой: =(β 2 +1) Recall+β 2 Precision Recall⋅Precision Бинарная классификация: вероятности классов Многие модели бинарной классификации устроены так, что класс объекта получается бинаризацией выхода классификатора по некоторому фиксированному порогу: f(x;w,w 0 )=I[g(x,w)>w 0 ]. Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 6,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Например, модель логистической регрессии возвращает оценку вероятности принадлежности примера к положительному классу. Другие модели бинарной классификации обычно возвращают произвольные вещественные значения, но существуют техники, называемые калибровкой классификатора, которые позволяют преобразовать предсказания в более или менее корректную оценку вероятности принадлежности к положительному классу. Как оценить качество предсказываемых вероятностей, если именно они являются нашей конечной целью? Общепринятой мерой является логистическая функция потерь, которую мы изучали раньше, когда говорили об устройстве некоторых методов классификации (например уже упоминавшейся логистической регрессии). Если же нашей целью является построение прогноза в терминах метки класса, то нам нужно учесть, что в зависимости от порога мы будем получать разные предсказания и разное качество на отложенной выборке. Так, чем ниже порог отсечения, тем больше объектов модель будет относить к положительному классу. Как в этом случае оценить качество модели? AUC Пусть мы хотим учитывать ошибки на объектах обоих классов. При уменьшении порога отсечения мы будем находить (правильно предсказывать) всё большее число положительных объектов, но также и неправильно предсказывать положительную метку на всё большем числе отрицательных объектов. Естественным кажется ввести две метрики TPR и FPR: TPR (true positive rate) — это полнота, доля положительных объектов, правильно предсказанных положительными: TPR= P TP = TP+FN TP FPR (false positive rate) — это доля отрицательных объектов, неправильно предсказанных положительными: FPR= N FP = FP+TN FP Обе эти величины растут при уменьшении порога. Кривая в осях TPR/FPR, которая получается при варьировании порога, исторически называется ROC-кривой (receiver operating characteristics curve, сокращённо ROC curve). Следующий интерактивный график поможет вам понять поведение ROC-кривой. Желтая и синяя кривые показывают распределение предсказаний классификатора на объектах положительного и отрицательного классов соответственно. То есть значения на оси X (на графике с двумя гауссианами) мы получаем из классификатора. Если классификатор идеальный, — две кривые разделимы по оси X, — то на правом графике мы получаем ROC-кривую (0,0)->(0,1)->(1,1), площадь под которой равна 1. Если классификатор случайный (предсказывает одинаковые метки положительным и отрицательным объектам), то мы получаем ROC-кривую (0,0)->(1,1), площадь под которой равна 0.5. Поэкспериментируйте с разными вариантами распределения предсказаний по классам и посмотрите, как меняется ROC-кривая. Чем лучше классификатор разделяет два класса, тем больше площадь (area under curve) под ROC-кривой — и мы можем использовать её в качестве метрики. Эта метрика называется AUC и она работает благодаря следующему свойству ROC-кривой: AUC равен доле пар объектов вида (объект класса 1, объект класса 0), которые алгоритм верно упорядочил, то есть предсказание классификатора на первом объекте больше: AUC AUC= i=1 ∑ N j=1 ∑ N I[y i <y j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 7,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "j ] i=1 ∑ N j=1 ∑ N I[y i <y j ]I ′ [f(x i )<f(x 0.5 [f(x i )<f(x j )]= ⎩ ⎨ ⎧ 0, 0.5 1, f(x i )>f(x j ) f(x i )=f(x j ) f(x i )<f(x I[y i <y j ]={ Чтобы детальнее разобраться, почему это так, советуем вам обратиться к материалам А.Г.Дьяконова. В каких случаях лучше отдать предпочтение этой метрике? Рассмотрим следующую задачу: некоторый сотовый оператор хочет научиться предсказывать, будет ли клиент пользоваться его услугами через месяц. На первый взгляд кажется, что задача сводится к бинарной классификации с метками 1, если клиент останется с компанией и 0 0 — иначе. Однако если копнуть глубже в процессы компании, то окажется, что такие метки практически бесполезны. Компании скорее интересно упорядочить клиентов по вероятности прекращения обслуживания и в зависимости от этого применять разные варианты удержания: кому-то прислать скидочный купон от партнёра, кому-то предложить скидку на следующий месяц, а кому-то и новый тариф на особых условиях. Таким образом, в любой задаче, где нам важна не метка сама по себе, а правильный порядок на объектах, имеет смысл применять AUC. Утверждение выше может вызывать у вас желание использовать AUC в качестве метрики в задачах ранжирования, но мы призываем вас быть аккуратными. Average Precision Будем постепенно уменьшать порог бинаризации. При этом полнота будет расти от 0 0 до 1 1, так как будет увеличиваться количество объектов, которым мы приписываем положительный класс (а количество объектов, на самом деле относящихся к положительному классу, очевидно, меняться не будет). Про точность же нельзя сказать ничего определённого, но мы понимаем, что скорее всего она будет выше при более высоком пороге отсечения (мы оставим только объекты, в которых модель «уверена» больше всего). Варьируя порог и пересчитывая значения Precision и Recall на каждом пороге, мы получим некоторую кривую примерно следующего вида: Источник Рассмотрим среднее значение точности (оно равно площади под кривой точность-полнота): AP =∫ 0 1 p(r)dr Получим показатель эффективности, который называется average precision. Как в случае матрицы ошибок мы переходили к скалярным показателям эффективности, так и в случае с кривой точность-полнота мы охарактеризовали ее в виде числа. Многоклассовая классификация Если классов становится больше двух, расчёт метрик усложняется. Если задача классификации на K K классов ставится как K K задач об отделении класса i i от остальных ( i=1,…,K), то для каждой из них можно посчитать свою матрицу ошибок. Затем есть два варианта получения итогового значения метрики из K K матриц ошибок: Усредняем элементы матрицы ошибок (TP, FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 8,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "FP, TN, FN) между бинарными классификаторами, например TP= K 1 ∑ i=1 K TP i . Затем по одной усреднённой матрице ошибок считаем Precision, Recall, F-меру. Это называют микроусреднением. Считаем Precision, Recall для каждого классификатора отдельно, а потом усредняем. Это называют макроусреднением. Порядок усреднения влияет на результат в случае дисбаланса классов. Показатели TP, FP, FN — это счётчики объектов. Пусть некоторый класс обладает маленькой мощностью (обозначим её M M). Тогда значения TP и FN при классификации этого класса против остальных будут не больше M M, то есть тоже маленькие. Про FP мы ничего уверенно сказать не можем, но скорее всего при дисбалансе классов классификатор не будет предсказывать редкий класс слишком часто, потому что есть большая вероятность ошибиться. Так что FP тоже мало. Поэтому усреднение первым способом сделает вклад маленького класса в общую метрику незаметным. А при усреднении вторым способом среднее считается уже для нормированных величин, так что вклад каждого класса будет одинаковым. Рассмотрим пример. Пусть есть датасет из объектов трёх цветов: желтого, зелёного и синего. Желтого и зелёного цветов почти поровну — 21 и 20 объектов соответственно, а синих объектов всего 4. 6 Модель по очереди для каждого цвета пытается отделить объекты этого цвета от объектов оставшихся двух цветов. Результаты классификации проиллюстрированы матрицей ошибок. Модель «покрасила» в жёлтый 25 объектов, 20 из которых были действительно жёлтыми (левый столбец матрицы). В синий был «покрашен» только один объект, который на самом деле жёлтый (средний столбец матрицы). В зелёный — 19 объектов, все на самом деле зелёные (правый столбец матрицы). 6 Посчитаем Precision классификации двумя способами: С помощью микроусреднения получаем Precision 0.87 Precision= 3 1 (20+0+19)+ 3 1 (5+1+0) 3 1 (20+0+19) =0.87 С помощью макроусреднения получаем Precision 0.6 Precision= 3 1 ( 20+5 20 + 0+1 0 + 19+0 19 )=0.6 Видим, что макроусреднение лучше отражает тот факт, что синий цвет, которого в датасете было совсем мало, модель практически игнорирует. Как оптимизировать метрики классификации? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F F на валидационной выборке была минимальная/максимальная. Лучший способ добиться минимизации метрики F F — оптимизировать её напрямую, то есть выбрать в качестве функции потерь ту же F(a(X),Y). К сожалению, это не всегда возможно. Рассмотрим, как оптимизировать метрики иначе. Метрики precision и recall невозможно оптимизировать напрямую, потому что эти метрики нельзя рассчитать на одном объекте, а затем усреднить. Они зависят от того, какими были правильная метка класса и ответ алгоритма на всех объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP,",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 9,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объектах. Чтобы понять, как оптимизировать precision, recall, рассмотрим, как расчитать эти метрики на отложенной выборке. Пусть модель обучена на стандартную для классификации функцию потерь (LogLoss). Для получения меток класса специалист по машинному обучению сначала применяет на объектах модель и получает вещественные предсказания модели ( ∈(0,1)). Затем предсказания бинаризуются по порогу, выбранному специалистом: если предсказание на объекте больше порога, то метка класса 1 (или «положительная»), если меньше — 0 (или «отрицательная»). Рассмотрим, что будет с метриками precision, recall в крайних положениях порога. Пусть порог равен нулю Тогда всем объектам будет присвоена положительная метка. Следовательно, все объекты будут либо TP, либо FP, потому что отрицательных предсказаний нет, TP+FP=N, где N N — размер выборки. Также все объекты, у которых метка на самом деле 1, попадут в TP. По формуле точность Precision Precision= TP+FP TP = N 1 ∑ i=1 N I[y i =1] равна среднему таргету в выборке. А полнота Recall Recall= TP+FN TP = TP+0 TP =1 равна единице. Пусть теперь порог равен единице Тогда ни один объект не будет назван положительным, TP=FP=0. Все объекты с меткой класса 1 попадут в FN. Если есть хотя бы один такой объект, то есть =0, будет верна формула Recall Recall= TP+FN TP = 0+FN 0 =0. То есть при пороге единица, полнота равна нулю. Теперь посмотрим на точность. Формула для Precision состоит только из счётчиков положительных ответов модели (TP, FP). При единичном пороге они оба равны нулю, Precision Precision= TP+FP TP = 0+0 0 то есть при единичном пороге точность неопределена. Пусть мы отступили чуть-чуть назад по порогу, чтобы хотя бы несколько объектов были названы моделью положительными. Скорее всего это будут самые «простые» объекты, которые модель распознает хорошо, потому что её предсказание близко к единице. В этом предположении F P ≈ 0 FP≈0. Тогда точность Precision Precision= TP+FP TP ≈ TP+0 TP ≈1 будет близка к единице. Изменяя порог, между крайними положениями, получим графики Precision и Recall, которые выглядят как-то так: 6 Recall меняется от единицы до нуля, а Precision от среднего тагрета до какого-то другого значения (нет гарантий, что график монотонный). Итого оптимизация precision и recall происходит так: Модель обучается на стандартную функцию потерь (например, LogLoss). Используя вещественные предсказания на валидационной выборке, перебирая разные пороги от 0 до 1, получаем графики метрик в зависимости от порога. Выбираем нужное сочетание точности и полноты. Пусть теперь мы хотим максимизировать метрику AUC. Стандартный метод оптимизации, градиентный спуск, предполагает, что функция потерь дифференцируема. AUC этим качеством не обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 10,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "обладает, то есть мы не можем оптимизировать её напрямую. Поэтому для метрики AUC приходится изменять оптимизационную задачу. Метрика AUC считает долю верно упорядоченных пар. Значит от исходной выборки можно перейти к выборке упорядоченных пар объектов. На этой выборке ставится задача классификации: метка класса 1 соответствует правильно упорядоченной паре, 0 — неправильно. Новой метрикой становится accuracy — доля правильно классифицированных объектов, то есть доля правильно упорядоченных пар. Оптимизировать accuracy можно по той же схеме, что и precision, recall: обучаем модель на LogLoss и предсказываем вероятности положительной метки у объекта выборки, считаем accuracy для разных порогов по вероятности и выбираем понравившийся. Регрессия В задачах регрессии целевая метка у нас имеет потенциально бесконечное число значений. И природа этих значений, обычно, связана с каким-то процессом измерений: величина температуры в определенный момент времени на метеостанции количество прочтений статьи на сайте количество проданных бананов в конкретном магазине, сети магазинов или стране дебит добывающей скважины на нефтегазовом месторождении за месяц и т.п. Мы видим, что иногда метка это целое число, а иногда произвольное вещественное число. Обычно случаи целочисленных меток моделируют так, словно это просто обычное вещественное число. При таком подходе может оказаться так, что модель A лучше модели B по некоторой метрике, но при этом предсказания у модели A могут быть не целыми. Если в бизнес-задаче ожидается именно целочисленный ответ, то и оценивать нужно огрубление. Общая рекомендация такова: оценивайте весь каскад решающих правил: и те «внутренние», которые вы получаете в результате обучения, и те «итоговые», которые вы отдаёте бизнес-заказчику. Например, вы можете быть удовлетворены, что стали ошибаться не во втором, а только в третьем знаке после запятой при предсказании погоды. Но сами погодные данные измеряются с точностью до десятых долей градуса, а пользователь и вовсе может интересоваться лишь целым числом градусов. Итак, напомним постановку задачи регрессии: нам нужно по обучающей выборке {(x i ,y i )} i=1 N , где ∈R построить модель f(x). Величину =f(x i )−y i называют ошибкой на объекте i или регрессионным остатком. Весь набор ошибок на отложенной выборке может служить аналогом матрицы ошибок из задачи классификации. А именно, когда мы рассматриваем две разные модели, то, глядя на то, как и на каких объектах они ошиблись, мы можем прийти к выводу, что для решения бизнес-задачи нам выгоднее взять ту или иную модель. И, аналогично со случаем бинарной классификации, мы можем начать строить агрегаты от вектора ошибок, получая тем самым разные метрики. MSE, RMSE, R 2 R 2 MSE — одна из самых популярных метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 11,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "метрик в задаче регрессии. Она уже знакома вам, так как применяется в качестве функции потерь (или входит в ее состав) во многих ранее рассмотренных методах. MSE(y true ,y pred )= N 1 i=1 ∑ N (y i −f(x i )) 2 Иногда для того, чтобы показатель эффективности MSE имел размерность исходных данных, из него извлекают квадратный корень и получают показатель эффективности RMSE. MSE неограничен сверху, и может быть нелегко понять, насколько «хорошим» или «плохим» является то или иное его значение. Чтобы появились какие-то ориентиры, делают следующее: Берут наилучшее константное предсказание с точки зрения MSE — среднее арифметическое меток y ˉ y ˉ . При этом чтобы не было подглядывания в test, среднее нужно вычислять по обучающей выборке Рассматривают в качестве показателя ошибки: =1− ∑ i=1 i=1 N (y i −f(x i )) 2 . У идеального решающего правила R 2 R 2 равен 1 1, у наилучшего константного предсказания он равен 0 0 на обучающей выборке. Можно заметить, что R 2 R 2 показывает, какая доля дисперсии таргетов (знаменатель) объяснена моделью. MSE квадратично штрафует за большие ошибки на объектах. Мы уже видели проявление этого при обучении моделей методом минимизации квадратичных ошибок — там это проявлялось в том, что модель старалась хорошо подстроиться под выбросы. Пусть теперь мы хотим использовать MSE для оценки наших регрессионных моделей. Если большие ошибки для нас действительно неприемлемы, то квадратичный штраф за них — очень полезное свойство (и его даже можно усиливать, повышая степень, в которую мы возводим ошибку на объекте). Однако если в наших тестовых данных присутствуют выбросы, то нам будет сложно объективно сравнить модели между собой: ошибки на выбросах будет маскировать различия в ошибках на основном множестве объектов. Таким образом, если мы будем сравнивать две модели при помощи MSE, у нас будет выигрывать та модель, у которой меньше ошибка на объектах-выбросах, а это, скорее всего, не то, чего требует от нас наша бизнес-задача. MAE Использовать RMSE для сравнения моделей на выборках с большим количеством выбросов может быть неудобно. В таких случаях прибегают к также знакомой вам в качестве функции потери метрике MAE (mean absolute error): MAE(y true ,y pred )= N 1 i=1 ∑ N ∣y i −f(x i )∣ Метрики, учитывающие относительные ошибки И MSE и MAE считаются как сумма абсолютных ошибок на объектах. Рассмотрим следующую задачу: мы хотим спрогнозировать спрос товаров на следующий месяц. Пусть у нас есть два продукта: продукт A продаётся в количестве 100 штук, а продукт В в количестве 10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 12,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "10 штук. И пусть базовая модель предсказывает количество продаж продукта A как 98 штук, а продукта B как 8 штук. Ошибки на этих объектах добавляют 4 штрафных единицы в MAE. И есть 2 модели-кандидата на улучшение. Первая предсказывает товар А 99 штук, а товар B 8 штук. Вторая предсказывает товар А 98 штук, а товар B 9 штук. Обе модели улучшают MAE базовой модели на 1 единицу. Однако, с точки зрения бизнес-заказчика вторая модель может оказаться предпочтительнее, так как предсказание продажи редких товаров может быть приоритетнее. Один из способов учесть такое требование — рассматривать не абсолютную, а относительную ошибку на объектах. MAPE, SMAPE Когда речь заходит об относительных ошибках, сразу возникает вопрос: что мы будем ставить в знаменатель? В метрике MAPE (mean absolute percentage error) в знаменатель помещают целевое значение: MAPE(y true ,y pred )= N 1 i=1 −f(x i )∣ С особым случаем, когда в знаменателе оказывается 0 0, обычно поступают «инженерным» способом: или выдают за непредсказание 0 0 на таком объекте большой, но фиксированный штраф, или пытаются застраховаться от подобного на уровне формулы и переходят к метрике SMAPE (symmetric mean absolute percentage error): SMAPE(y true ,y pred )= N 1 i=1 ∑ N y i +f(x i ) 2∣y i −f(x i )∣ Если же предсказывается ноль, штраф считаем нулевым. Таким переходом от абсолютных ошибок на объекте к относительным мы сделали объекты в тестовой выборке равнозначными: даже если мы делаем абсурдно большое предсказание, на фоне которого истинная метка теряется, мы получаем штраф за этот объект порядка 1 в случае MAPE и 2 в случае SMAPE. WAPE Как и любая другая метрика, MAPE имеет свои границы применимости: например, она плохо справляется с прогнозом спроса на товары с прерывистыми продажами. Рассмотрим такой пример: Понедельник Вторник Среда Прогноз 55 2 50 Продажи 50 1 50 MAPE 10% 100% 0% Среднее MAPE — 36.7%, что не очень отражает реальную ситуацию, ведь два дня мы предсказывали с хорошей точностью. В таких ситуациях помогает WAPE (weighted average percentage error): WAPE(y true ,y pred )= ∑ i=1 N ∣y i ∣ ∑ i=1 N ∣y i −f(x i )∣ Если мы предсказываем идеально, то WAPE = 0, если все предсказания отдаём нулевыми, то WAPE = 1. В нашем примере получим WAPE = 5.9% RMSLE Альтернативный способ уйти от абсолютных ошибок к относительным предлагает метрика RMSLE (root mean squared logarithmic error): log log RMSLE(y true ,y pred ∣c)= N 1 i=1 ∑ N ( 2 1 log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 13,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log(y i +c)−log(f(x i )+c)) 2 где нормировочная константа c c вводится искусственно, чтобы не брать логарифм от нуля. Также по построению видно, что метрика пригодна лишь для неотрицательных меток. Веса в метриках Все вышеописанные метрики легко допускают введение весов для объектов. Если мы из каких-то соображений можем определить стоимость ошибки на объекте, можно брать эту величину в качестве веса. Например, в задаче предсказания спроса в качестве веса можно использовать стоимость объекта. Доля предсказаний с абсолютными ошибками больше, чем d Еще одним способом охарактеризовать качество модели в задаче регрессии является доля предсказаний с абсолютными ошибками больше заданного порога i=1 ∑ N I[∣y i −f(x i )∣>d] Например, можно считать, что прогноз погоды сбылся, если ошибка предсказания составила меньше 1/2/3 градусов. Тогда рассматриваемая метрика покажет, в какой доле случаев прогноз не сбылся. Как оптимизировать метрики регрессии? Пусть мы выбрали, что метрика качества алгоритма будет F(a(X),Y). Тогда мы хотим обучить модель так, чтобы F на валидационной выборке была минимальная/максимальная. Аналогично задачам классификации лучший способ добиться минимизации метрики F F — выбрать в качестве функции потерь ту же F(a(X),Y). К счастью, основные метрики для регрессии: MSE, RMSE, MAE можно оптимизировать напрямую. С формальной точки зрения MAE не дифференцируема, так как там присутствует модуль, чья производная не определена в нуле. На практике для этого выколотого случая в коде можно возвращать ноль. Для оптимизации MAPE придётся изменять оптимизационную задачу. Оптимизацию MAPE можно представить как оптимизацию MAE, где объектам выборки присвоен вес Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 2.5. Градиентный бустинг Как устроено самое мощное семейство не-нейросетевых моделей: градиентный бустинг над решающими деревьями Следующий параграф 3.2. Кросс-валидация Как строить надёжные оценки качества моделей и никогда не смешивать train и test",
    "metadata": {
      "title": "Метрики классификации и регрессии",
      "url": "https://education.yandex.ru/handbook/ml/article/podbor-giperparametrov",
      "course": "ml",
      "chapter": "3. Оценка качества моделей",
      "chapter_id": "3.3",
      "part": 14,
      "total_parts": 14,
      "source_file": "3.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как описать привычные модели на языке статистики. Оптимизация функции потерь vs оценка максимального правоподобия В этом разделе мы посмотрим на те же самые модели машинного обучения, но с другой стороны: будем интерпретировать их как вероятностные. В первом параграфе мы расскажем, как обращаться с вероятностными моделями, и покажем, что привычный вам подбор параметров модели с помощью минимизации функции потерь соответствует подбору параметров методом максимального правдоподобия. Это даст возможность транслировать в мир ML известные результаты о свойствах оценок максимального правдоподобия, но в то же время и обнажит их недостатки. Благодаря этому мы сможем по-новому взглянуть на логистическую регрессию и с новым пониманием сформулировать её обобщение — generalized linear model (GLM). По ходу дела мы обнаружим, что большинство классификаторов, хоть и делают вид, что предсказывают корректные вероятности, на самом деле вводят в заблуждение. В третьем параграфе мы поговорим о том, как проверить отклонение предсказанных значений от истинных вероятностей и как поправить ситуацию. Далее мы обсудим генеративный подход к классификации и разберём несколько примеров генеративных моделей, после чего перейдём к байесовскому подходу оценивания параметров, который, хоть зачастую и трудно осуществим вычислительно, однако обладает большей теоретической стройностью. Он позволяет оценивать распределение параметров и предсказаний (например, уверенность в нашей оценке), а кроме того — даёт нам возможность измерить качество модели, не прибегая к проверке на тестовой выборке. Если вы готовы — давайте приступим! Случайность как источник несовершенства модели Практически любая наша модель — несовершенна. Но объяснять это несовершенство можно по-разному. Представим, что мы решаем задачу регрессии y≃⟨x,w⟩: например, пытаемся по университетским оценкам выпускника предсказать его годовую зарплату. Ясно, что точная зависимость у нас не получится как минимум потому, что мы многого не знаем о выпускнике: куда он пошёл работать, насколько он усерден, как у него с soft skills и так далее. Как же нам быть? Первый вариант — просто признать, что мы не получим идеальную модель, но постараться выучить оптимальную, насколько это возможно. То есть приблизить таргет предсказаниями наилучшим образом с точки зрения какой-то меры близости, которую мы подберём из экспертных соображений. Так мы получаем простой инженерный подход к машинному обучению: есть формула, в которой присутствуют некоторые параметры ( w w), есть формализация того, что такое «приблизить» (функция потерь) — и мы бодро решаем задачу оптимизации по параметрам. Второй вариант — свалить вину за неточности наших предсказаний на случайность. В самом деле: если мы что-то не можем измерить, то для нас это всё равно что случайный фактор. В постановке задачи мы заменяем приближённое равенство y≃⟨x,w⟩ на точное искажённое шумом ε ) y=(⟨x,w⟩, искажённое шумом ε) Например, это может быть аддитивный шум (чаще всего так и делают): y=⟨x,w⟩+ε где ε ε — некоторая случайная величина, которая представляет этот самый случайный шум. Тогда получается, что для каждого конкретного объекта x i x i соответствующий ему истинный таргет — это сумма ,w⟩ и конкретной реализации шума ε ε. При построении такой модели мы можем выбирать различные распределения шума, кодируя тем самым, какой может быть ошибка. Чаще всего выбирают гауссовский шум: ε∼N(0,σ 2 ) с некоторой фиксированной дисперсией σ 2 σ 2 — но могут быть и другие варианты. Проиллюстрируем,",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 1,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "точное искажённое шумом ε ) y=(⟨x,w⟩, искажённое шумом ε) Например, это может быть аддитивный шум (чаще всего так и делают): y=⟨x,w⟩+ε где ε ε — некоторая случайная величина, которая представляет этот самый случайный шум. Тогда получается, что для каждого конкретного объекта x i x i соответствующий ему истинный таргет — это сумма ,w⟩ и конкретной реализации шума ε ε. При построении такой модели мы можем выбирать различные распределения шума, кодируя тем самым, какой может быть ошибка. Чаще всего выбирают гауссовский шум: ε∼N(0,σ 2 ) с некоторой фиксированной дисперсией σ 2 σ 2 — но могут быть и другие варианты. Проиллюстрируем, как ведут себя данные, подчиняющиеся закону y=ax+b+ε, ε∼N(0,σ 2 ): 9 Вопрос на подумать. Зачем человеку может прийти в голову предположить, что в модели линейной регрессии y∼Xw+ε шум ε ε имеет распределение Лапласа? А распределение Коши? Чем свойства таких моделей будут отличаться от свойств модели с нормальным шумом? Как вы могли заметить, в каждом из подходов после того, как мы зафиксировали признаки (то есть координаты x i x i ), остаётся своя степень свободы: в инженерном это выбор функции потерь, а в вероятностном — выбор распределения шума. Дальше в этом параграфе мы увидим, что на самом деле эти два подхода глубинным образом связаны между собой, причём выбор функции потерь — это в некотором смысле то же самое, что выбор распределения шума. Условное распределение на таргет, непрерывный случай Допустим, что мы исследуем вероятностную модель таргета с аддитивным шумом y=f w (x)+ε, где f w f w — некоторая функция, не обязательно линейная с (неизвестными пока) параметрами w w, а ε ε — случайный шум с плотностью распределения ε∼p ε (t). Для каждого конкретного объекта x i x i значение ) — это просто константа, но для y i y i оно превращается в случайную величину, зависящую от x i x i (и ещё от w w, на самом деле). Таким образом, можно говорить об условном распределении (y∣x,w) Для каждого конкретного w распределение соответствующего y i y i — это просто (y−f w (x i )), ведь y−f w (X)=ε. Пример. Рассмотрим вероятностную модель y=⟨x,w⟩+ε, где ε∼N(0,σ 2 ). Тогда для фиксированного x i x i имеем =⟨x i ,w⟩+ε. Поскольку ,w⟩ — константа, мы получаем ∼N(⟨x i ,w⟩,σ 2 ). Это можно записать и так: p(y i ∣x i ,w)∼N(y i ∣⟨x i ,w⟩,σ 2 ), где выражение справа — это значение функции плотности нормального распределения с параметрами ,w⟩,σ 2 в точке y i y i . В частности, ,w⟩=E(y i ∣x i ). Более сложные вероятностные модели На самом деле, мы можем для нашей задачи придумывать любую вероятностную модель (y∣x,w), не обязательно вида y=f w (X)+ε. Представьте, что мы хотим предсказывать точку в плоскости штанг, в которую попадает мячом бьющий по воротам футболист. Можно предположить, что она имеет нормальное распределение со средним (цель удара), которое определяется ситуацией на поле и состянием игрока, и некоторой дисперсией (то есть скалярной ковариационной матрицей), которая тоже зависит от состояния игрока и ещё разных сложных факторов, которые мы объявим случайными. Состояние игрока — это сложное",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 2,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "точке y i y i . В частности, ,w⟩=E(y i ∣x i ). Более сложные вероятностные модели На самом деле, мы можем для нашей задачи придумывать любую вероятностную модель (y∣x,w), не обязательно вида y=f w (X)+ε. Представьте, что мы хотим предсказывать точку в плоскости штанг, в которую попадает мячом бьющий по воротам футболист. Можно предположить, что она имеет нормальное распределение со средним (цель удара), которое определяется ситуацией на поле и состянием игрока, и некоторой дисперсией (то есть скалярной ковариационной матрицей), которая тоже зависит от состояния игрока и ещё разных сложных факторов, которые мы объявим случайными. Состояние игрока — это сложное понятие, но, вероятно, мы можем выразить его, зная пульс, давление и другие физические показатели. В свою очередь, ситуацию на поле можно описать, как функцию от позиций и движений других игроков, судьи и зрителей — но всего не перечислишь, поэтому нам снова придётся привлекать случайность. Таким образом, мы получаем то, что называется графической моделью: 9 Здесь стрелки означают статистические зависимости, а отсутствие стрелок — допущение о статистической независимости. Конечно же, это лишь допущение, принятое нами для ограничения сложности модели: ведь пульс человека и давление взаимосвязаны, равно как и поведение различных игроков на поле. Но мы уже обсуждали, что каждая модель, в том числе и вероятностная, является лишь приблизительным отражением бесконечно сложного мира. Впрочем, если у нас много вычислительных ресурсов, то никто не мешает нам попробовать учесть и все пропущенные сейчас зависимости. Расписав всё по определению условной вероятности, мы получаем следующую вероятностную модель: 9 в которой, конечно же, мы должны все вероятности расписать через какие-то понятные и логически обоснованные распределения — но пока воздержимся от этого. Оценка максимального правдоподобия = оптимизация функции потерь Мы хотим подобрать такие значения параметров w w, для которых модель (y∣x,w) была бы наиболее адекватна обучающим данным. Суть метода максимального правдоподобия (maximum likelihood estimation) состоит в том, чтобы найти такое w w, для которого вероятность (а в данном, непрерывном, случае плотность вероятности) появления выборки y={y 1 ,…,y N } была бы максимальной, то есть argmax⁡w MLE = w argmax p(y∣X,w) Величина p(y∣X,w) называется функцией правдоподобия (likelihood). Если мы считаем, что все объекты независимы, то функция правдоподобия распадается в произведение: p(y∣X,w)=p(y 1 ∣x 1 ,w)⋅…⋅p(y i ∣x i ,w) Теперь, поскольку перемножать сложно, а складывать легко (и ещё поскольку мы надеемся, что раз наши объекты всё-таки наблюдаются в природе, их правдоподобие отлично от нуля), мы переходим к логарифму функции правдоподобия: log log l(y∣X,w)=logp(y 1 ∣x 1 ,w)+…+logp(y i ∣x i ,w) эту функцию мы так или иначе максимизируем по w w, находя оценку максимального правдоподобия w ^ w ^ . Как мы уже обсуждали выше, p(y i ∣x i ,w)=p ε (y−f w (x i )), то есть log l(y∣X,w)= i=1 ∑ N logp Максимизация функции правдоподобия соответствует минимизации log i=1 ∑ N [−logp ))] а это выражение можно интерпретировать, как функцию потерь. Вот и оказывается, что подбор параметров вероятностей модели с помощью метода максимального правдоподобия — это то же самое, что «инженерная» оптимизация функции потерь. Давайте посмотрим, как это выглядит в нескольких простых случаях. Пример. Давайте предположим, что",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 3,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i ,w) эту функцию мы так или иначе максимизируем по w w, находя оценку максимального правдоподобия w ^ w ^ . Как мы уже обсуждали выше, p(y i ∣x i ,w)=p ε (y−f w (x i )), то есть log l(y∣X,w)= i=1 ∑ N logp Максимизация функции правдоподобия соответствует минимизации log i=1 ∑ N [−logp ))] а это выражение можно интерпретировать, как функцию потерь. Вот и оказывается, что подбор параметров вероятностей модели с помощью метода максимального правдоподобия — это то же самое, что «инженерная» оптимизация функции потерь. Давайте посмотрим, как это выглядит в нескольких простых случаях. Пример. Давайте предположим, что наш таргет связан с данными вот так: =⟨x i ,w⟩+ε где ε∼N(0,σ 2 ), то есть exp p(ε)= 2πσ 2 1 exp(− 2σ 2 ε 2 ) Случайная величина y i y i получается из шума ε ε сдвигом на постоянный вектор ,w⟩, так что она тоже распределена нормально с той же дисперсией σ 2 σ 2 и со средним ,w⟩ exp p(y i ∣⟨x i ,w⟩)= 2πσ 2 1 exp(− 2σ 2 (y i −⟨x i ,w⟩) 2 ) Правдоподобие выборки имеет вид exp p(y∣X,w)= i=1 ∏ N p(y i ∣x i ,w)= i=1 ∏ N 2πσ 2 1 exp(− 2σ 2 (y i −⟨w,x i ⟩) 2 ) Логарифм правдоподобия можно переписать в виде log l(y∣X,w)= i=1 ∑ N (−log( 2πσ −⟨w,x i ⟩) 2 ) Постоянными слагаемыми можно пренебречь, и тогда оказывается, что максимизация этой величины равносильна минимизации i=1 ∑ N (y i −⟨w,x i ⟩) 2 Мы получили обычную квадратичную функцию потерь. Итак, обучать вероятностную модель линейной регрессии с нормальным шумом — это то же самое, что учить «инженерную» модель с функцией потерь MSE. Вопрос на подумать. Какая вероятностная модель соответствует обучению линейной регрессии с функцией потерь MAE i=1 ∑ N ∣y i −⟨w,x i ⟩∣? Предсказание в вероятностных моделях Теперь представим, что параметры подобраны, и подумаем о том, как же теперь делать предсказания. Рассмотрим модель линейной регрессии y=⟨x,w⟩+ε,ε∼N(0,σ 2 ) Если w w известен, то для нового объекта x 0 x 0 соответствующий таргет имеет вид =⟨x 0 ,w⟩+ε∼N(⟨x 0 ,w⟩,σ 2 ) Таким образом, y 0 y 0 дан нам не точно, а в виде распределения (и логично: ведь мы оговорились выше, что ответы у нас искажены погрешностью, проинтерпретированной, как нормальный шум). Но что делать, если требуют назвать конкретное число? Кажется логичным выдать условное матожидание E(y 0 ∣x 0 )=⟨x 0 ,w⟩, тем более что оно совпадает с условной медианой и условной модой этого распределения. Если же медиана, мода и математическое ожидание различаются, то можно выбрать что-то из них с учётом особенностей задачи. Но на практике в схеме y∼f(x)+ε чаще всего рассматривают именно симметричные распределения с нулевым матожиданием, потому что для них f ( x ) f(x) совпадает с условным матожиданием E(y∣x) и является логичным точечным предсказанием. Приведём пример. Допустим шум ε ε был бы из экспоненциального распределения. Тогда f ( x ) f(x) была бы условным минимумом распределения. В принципе, можно придумать задачу, для которой такая постановка (предсказание минимума) была бы логичной. Но",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 4,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "что оно совпадает с условной медианой и условной модой этого распределения. Если же медиана, мода и математическое ожидание различаются, то можно выбрать что-то из них с учётом особенностей задачи. Но на практике в схеме y∼f(x)+ε чаще всего рассматривают именно симметричные распределения с нулевым матожиданием, потому что для них f ( x ) f(x) совпадает с условным матожиданием E(y∣x) и является логичным точечным предсказанием. Приведём пример. Допустим шум ε ε был бы из экспоненциального распределения. Тогда f ( x ) f(x) была бы условным минимумом распределения. В принципе, можно придумать задачу, для которой такая постановка (предсказание минимума) была бы логичной. Но это всё же довольно экзотическая ситуация. Приводим для сравнения модели с нормальным, лапласовским и экспоненциальным шумом: 9 Условное распределение на таргет, дискретный случай Допустим, мы имеем дело с задачей классификации с K K классами. Как мы можем её решать? Самый наивный вариант — научиться по каждому объекту x i x i предсказывать некоторое число для каждого класса, и у кого число больше — тот класс и выбираем! Наверное, так можно сделать, если мы придумаем хорошую функцию потерь. Но сразу в голову приходит мысль: почему бы не начать предсказывать не просто число, а вероятность? Таким образом, задача классификации сводится к предсказанию P(y i =k∣x i ) и как будто бы выбору класса с наибольшей вероятностью. Впрочем, как мы увидим дальше, всё не всегда работает так просто. Одну такую модель — правда, только для бинарной классификации — вы уже знаете. Это логистическая регрессия: P(y i =1∣x i ,w)= 1+e −⟨x i ,w⟩ 1 ,P(y i =0∣x i ,w)= 1+e −⟨x i ,w⟩ e −(x i ,w) = 1+e ⟨x i ,w⟩ 1 которую также можно записать в виде ∼Bern( 1+e −⟨x i ,w⟩ 1 ) где Bern(p) — распределение Бернулли с параметром p p. Нахождение вероятностей классов можно разделить на два этапа: Находим x i → логиты Находим x i логиты (−⟨x i ,w⟩,⟨x i ,w⟩) σ (σ(−⟨x i ,w⟩),σ(⟨x i ,w⟩)) где, напомним, σ σ — это сигмоида: σ(t)= 1+e −t 1 Сигмоида тут не просто так. Она обладает теми счастливыми свойствами, что монотонно возрастает; отображает всю числовую прямую на интервал ( 0 , 1 ) (0,1); σ(−x)=1−σ(x). Вот такой вид имеет её график: 9 Иными словами, с помощью сигмоиды можно делать «вероятности» из чего угодно, то есть более или менее для любого отображения f w f w (из признакового пространства в R R) с параметрами w w построить модель бинарной классификации: P(y i =0∣x i ,w)=σ(f w (−x i )),P(y i =1∣x i ,w)=σ(f w (x i )). Как и в случае логистической регрессии, такая модель равносильна утверждению о том, что log )=log p(y=0∣x i ,w) p(y=1∣x i ,w) . Похожим способом можно строить и модели для многоклассовой классификации. В этом нам поможет обобщение сигмоиды, которое называется softmax: softmax(t 1 ,…,t K )=( ∑ k=1 ,…, ∑ k=1 А именно, для любого отображения f w f w из пространства признаков в R K R K мы можем взять модель (P(y i =k∣x i ,w)) k=1 K =softmax(f w (x i",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 5,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "классификации: P(y i =0∣x i ,w)=σ(f w (−x i )),P(y i =1∣x i ,w)=σ(f w (x i )). Как и в случае логистической регрессии, такая модель равносильна утверждению о том, что log )=log p(y=0∣x i ,w) p(y=1∣x i ,w) . Похожим способом можно строить и модели для многоклассовой классификации. В этом нам поможет обобщение сигмоиды, которое называется softmax: softmax(t 1 ,…,t K )=( ∑ k=1 ,…, ∑ k=1 А именно, для любого отображения f w f w из пространства признаков в R K R K мы можем взять модель (P(y i =k∣x i ,w)) k=1 K =softmax(f w (x i )) Если все наши признаки — вещественные числа, а )=x i W — просто линейное отображение, то мы получаем однослойную нейронную сеть (P(y i =k∣x i ,w)) k=1 K =softmax(x i W) 9 Предостережение. Всё то, что мы описали выше, вполне работает на практике (собственно, классификационные нейросети зачастую так и устроены), но корректным не является. В самом деле, мы говорим, что строим оценки вероятностей P(y i =k∣x i ,w), но для подбора параметров используем не эмпирические вероятности, а только лишь значения argmax⁡k argmax P(y i =k∣x i ,w), то есть метки предсказываемых классов. Таким образом, при обучении мы не будем различать следующие две ситуации: 9 Это говорит нам о некоторой неполноценности такого подхода. Заметим ещё вот что. В случае бинарной классификации выбор предсказываемого класса как argmax⁡k argmax P(y i =k∣x i ,w) равносилен выбору того класса, для которого P(y i =k∣x i ,w)> 2 1 . Но если наши оценки вероятностей неадекватны, то этот вариант проваливается, и мы встаём перед проблемой выбора порога: каким должно быть значение t ^ t , чтобы мы могли приписать класс 1 тем объектам x i x i , для которых σ(f w (x i ))> t ? В одном из следующих параграфов мы обсудим, как всё-таки правильно предсказывать вероятности. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 3.3. Подбор гиперпараметров Как эффективно подбирать значения гиперпараметров модели и не переобучиться при этом Следующий параграф 4.2. Экспоненциальный класс распределений и принцип максимальной энтропии Самые главные семейства распределений в жизни любого data scientist’а",
    "metadata": {
      "title": "Вероятностный подход в ML",
      "url": "https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.1",
      "part": 6,
      "total_parts": 6,
      "source_file": "4.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Самые главные семейства распределений в жизни любого data scientist’а Мотивация: метод моментов Метод моментов — это ещё один способ, наряду с методом максимального правдоподобия, оценки параметров распределения по данным ,…,x N . Суть его в том, что мы выражаем через параметры распределения теоретические значения моментов =Ex k нашей случайной величины, затем считаем их выборочные оценки , приравниваем их все друг к другу и, решая полученную систему, находим оценки параметров. Можно доказать, что полученные оценки являются состоятельными, хотя могут быть смещены. Пример 1. Оценим параметры нормального распределения N(μ,σ 2 ) с помощью метода моментов. Теоретические моменты равны =μ,μ 2 =σ 2 +μ 2 Запишем систему: Из неё очевидным образом находим Легко видеть, что полученные оценки совпадают с оценками максимального правдоподобия Пример 2. Оценим параметр μ μ логнормального распределения exp ⁡ ( − ( log p(x)= x 2πσ 2 1 exp(− 2σ 2 (logx−μ) 2 ) при известном σ 2 σ 2 . Будет ли оценка совпадать с оценкой, полученной с помощью метода максимального правдоподобия? Теоретическое математическое ожидание равно exp exp(μ+ 2 σ 2 ), откуда мы сразу находим оценку μ ^ = log =log( Теперь запишем логарифм правдоподобия: log log l(X)=− i ∑ logx (logx i −μ) 2 +const Дифференцируя по μ μ и приравнивая производную к нулю, получаем log ⁡ x i μ MLE = N 1 i ∑ logx i что вовсе не совпадает с оценкой выше. Несколько приукрасив ситуацию, можно сделать вывод, что первые два выборочных момента позволяют если не править миром, то уверенно восстанавливать параметры распределений. А теперь давайте представим, что мы посчитали , а семейство распределений пока не выбрали. Как же совершить этот судьбоносный выбор? Давайте посмотрим на следующие три семейства и подумаем, в каком из них мы бы стали искать распределение, зная его истинные матожидание и дисперсию? 10 Почему-то хочется сказать, что в первом. Почему? Второе не симметрично — но почему мы так думаем? Если мы выберем третье, то добавим дополнительную информацию как минимум о том, что у распределения конечный носитель. А с чего бы? У нас такой инфомации вроде бы нет. Общая идея такова: мы будем искать распределение, которое удовлетворяет только явно заданным нами ограничениям и не отражает никакого дополнительного знания о нём. Но чтобы эти нестрогие рассуждения превратить в формулы, придётся немного обогатить наш математический аппарат и научиться измерять количество информации. Энтропия и дивергенция Кульбака-Лейблера Измерять «знание» можно с помощью энтропии Шэннона. Она определяется как log ⁡ P ( x ) H(P)=− x ∑ P(x)logP(x) для дискретного распределения и log H(p)=−∫p(x)logp(x)dx для непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу. Неформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении. Пример 1. Рассмотрим схему Бернулли с вероятностью успеха p p. Энтропия её результата равна log log ⁡ 2 p −(1−p)⋅log 2 (1−p)−p⋅log 2 p Давайте посмотрим на график этой функции: 10 Минимальное значение (нулевое) энтропия принимает при p∈{0,1}. В самом деле, для такого эксперимента мы всегда",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 1,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "∑ P(x)logP(x) для дискретного распределения и log H(p)=−∫p(x)logp(x)dx для непрерывного. В классическом определении логарифм двоичный, хотя, конечно, варианты с разным основанием отличаются лишь умножением на константу. Неформально можно представлять, что энтропия показывает, насколько сложно предсказать значение случайной величины. Чуть более строго — сколько в среднем бит нужно потратить, чтобы передать информацию о её значении. Пример 1. Рассмотрим схему Бернулли с вероятностью успеха p p. Энтропия её результата равна log log ⁡ 2 p −(1−p)⋅log 2 (1−p)−p⋅log 2 p Давайте посмотрим на график этой функции: 10 Минимальное значение (нулевое) энтропия принимает при p∈{0,1}. В самом деле, для такого эксперимента мы всегда можем наверняка сказать, каков будет его исход; обращаясь к другой интерпретации — чтобы сообщить кому-то о результате эксперимента, достаточно 0 0 бит (ведь получатель сообщения и так понимает, что вышло). Максимальное значение принимается в точке 1 2 2 1 , что вполне соответствует тому, что при предсказать исход эксперимента сложнее всего. Пример 2. Энтропия нормального распределения N(μ,σ 2 ) равна 1 2 log log(2πσ 2 )+ 2 1 , и чем меньше дисперсия, тем меньше энтропия, что и логично: ведь когда дисперсия мала, значения сосредоточены возле матожидания, и они становятся менее «разнообразными». Энтропия тесно связана с другим важным понятием из теории информации — дивергенцией Кульбака-Лейблера. Она определяется для p ( x ) p(x) q ( x ) q(x) как log KL(p∣∣q)=∫p(x)log q(x) p(x) dx в непрерывном случае и точно так же, но только с суммой вместо интеграла в дискретном. Дивергенцию можно представить в виде разности: log log KL(p∣∣q)=(−∫p(x)logq(x)dx)−(−∫p(x)logp(x)dx) Вычитаемое — это энтропия, которая, как мы уже поняли, показывает, сколько в среднем бит требуется, чтобы закодировать значение случайной величины. Уменьшаемое похоже по виду, и можно показать, что оно говорит о том, сколько в среднем бит потребуется на кодирование случайной величины с плотностью p p алгоритмом, оптимизированным для кодирования случайной величины q q. Иными словами, дивергенция Кульбака-Лейблера говорит о том, насколько увеличится средняя длина кодов для значений p p, если при настройке алгоритма кодирования вместо p p использовать q q. Более подробно вы можете почитать, например, в этом посте. Дивергенция Кульбака-Лейблера в некотором роде играет роль расстояния между распределениями. В частности, KL(p∣∣q)⩾0, причём дивергенция равна нулю, только если распределения совпадают почти всюду. Но при этом она не является симметричной: вообще говоря, KL(p∣∣q)  =KL(q∣∣p). Вопрос на подумать. Пусть p ( x ) p(x) — распределение, заданное на отрезке [ a , b ] [a,b]. Выразите энтропию через дивергенцию Кульбака-Лейблера p ( x ) p(x) с равномерным на отрезке распределением (x)= b−a 1 I [a,b] (x). Принцип максимальной энтропии Теперь наконец мы готовы сформулировать, какие распределения мы хотим искать. Принцип максимальной энтропии. Среди всех распределений на заданном носителе X X, удовлетворяющих условиям (x)=μ 1 , ..., (x)=μ k , где u i u i — некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории. Давайте рассмотрим несколько примеров, которые помогут ещё лучше",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 2,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": ") p(x) с равномерным на отрезке распределением (x)= b−a 1 I [a,b] (x). Принцип максимальной энтропии Теперь наконец мы готовы сформулировать, какие распределения мы хотим искать. Принцип максимальной энтропии. Среди всех распределений на заданном носителе X X, удовлетворяющих условиям (x)=μ 1 , ..., (x)=μ k , где u i u i — некоторые функции, мы хотим иметь дело с тем, которое имеет наибольшую энтропию. В самом деле, энтропия выражает нашу меру незнания о том, как ведёт себя распределение, и чем она больше — тем более «произвольное распределение», по крайней мере, в теории. Давайте рассмотрим несколько примеров, которые помогут ещё лучше понять, почему некоторые распределения так популярны: Пример 1. На конечном множестве 1 , … , n 1,…,n наибольшую энтропию имеет равномерное распределение (носитель — конечное множество из n n элементов, других ограничений нет). Доказательство: Пусть i=1,…,n — некоторое распределение, — равномерное. Запишем их дивергенцию Кульбака-Лейблера: log log ⁡ q i = KL(p∣∣q)= i ∑ p i logp logq log =−H(p)+logn =1 i ∑ p i Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что H ( p ) ⩽ log ⁡ n H(p)⩽logn. При этом равенство возможно, только если распределения совпадают. Пример 2. Среди распределений, заданных на всей вещественной прямой и имеющих заданные матожидание μ μ и дисперсию σ 2 σ 2 наибольшую энтропию имеет нормальное распределение N(μ,σ 2 ). Доказательство: Пусть p ( x ) p(x) — некоторое распределение, q(x)∼N(μ,σ 2 ). Запишем их дивергенцию Кульбака-Лейблера: log log KL(p∣∣q)=∫p(x)logp(x)dx−∫p(x)logq(x)dx= log =−H(p)−∫p(x)(− 2 1 log(2πσ 2 )− 2σ 2 1 (x−μ) 2 )dx= log =−H(p)+ 2 1 log(2πσ 2 )⋅ =1 ∫p(x)dx + 2σ 2 1 =Vp=σ 2 ∫(x−μ) 2 p(x)dx log =−H(p)+ =H(q) 2 1 log(2πσ 2 )+ 2 1 Так как дивергенция Кульбака-Лейблера всегда неотрицательна, получаем, что H(p)⩽H(q). При этом равенство возможно, только если распределения p p и q q совпадают почти всюду, а с точки зрения теории вероятностей такие распределения различать не имеет смысла. Пример 3. Среди распределений, заданных на множестве положительных вещественных чисел и имеющих заданное матожидание λ λ наибольшую энтропию имеет показательное распределение с параметром 1 λ λ 1 (его плотность равна exp p(x)= λ 1 exp(− λ 1 x)I (0;+∞) (x)). Все хорошо знакомые нам распределения, не правда ли? Проблема в том, что они свалились на нас чудесным образом. Возникает вопрос, можно ли их было не угадать, а вывести как-нибудь? И как быть, если даны не эти конкретные, а какие-то другие ограничения? Оказывается, что при некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе. Экспоненциальное семейство распределений Говорят, что семейство распределений относится к экспоненциальному классу, если оно может быть представлено в следующем виде: exp p(x∣θ)= h(θ) 1 g(x)⋅exp(θ T u(x)) где θ θ — вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства), h , g > 0 h,g>0, u u — некоторая вектор-функция, и, разумеется, сумма или интеграл по x x равняется единице. Последнее, в частности, означает, что exp h(θ)=∫g(x)exp(θ T u(x))dx (или сумма в дискретном случае). Пример 1. Покажем,",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 3,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "некоторых не очень обременительных ограничениях ответ можно записать с помощью распределений экспоненциального класса. Давайте же познакомимся с ними поближе. Экспоненциальное семейство распределений Говорят, что семейство распределений относится к экспоненциальному классу, если оно может быть представлено в следующем виде: exp p(x∣θ)= h(θ) 1 g(x)⋅exp(θ T u(x)) где θ θ — вектор вещественнозначных параметров (различные значения которых дают те или иные распределения из семейства), h , g > 0 h,g>0, u u — некоторая вектор-функция, и, разумеется, сумма или интеграл по x x равняется единице. Последнее, в частности, означает, что exp h(θ)=∫g(x)exp(θ T u(x))dx (или сумма в дискретном случае). Пример 1. Покажем, что нормальное распределение принадлежит экспоненциальному классу. Для этого мы должны представить привычную нам функцию плотности exp p(x∣μ,σ 2 )= 2π σ 1 exp(− 2σ 2 (x−μ) 2 ) в виде exp ⁡ ( ∑ i (параметр) i ⋅ (функция от x) i ) что-то, не зависящее от x p(x∣θ)= что-то, не зависящее от x g(x)⋅exp(∑ i (параметр) i ⋅(функция от x) i ) Распишем 1 2 π σ exp exp exp(− 2σ 2 (x−μ) 2 )= 2π σ 1 exp(− exp exp σexp(− 2σ 2 μ 2 ) exp(− Определим (x)=x,u 2 (x)=x exp h(θ)= 2π σexp(− 2σ 2 μ 2 ) Если теперь всё-таки честно выразить h h через θ θ (это мы оставляем в качестве лёгкого упражнения), то получится exp p(x∣μ,σ 2 )= h(θ) 1 exp(θ T u(x)) В данном случае функция g ( x ) g(x) просто равна единице. Пример 2. Покажем, что распределение Бернулли принадлежит экспоненциальному классу. Для этого попробуем преобразовать функцию вероятности (ниже x x принимает значения 0 0 или 1 1): exp ⁡ ( x log log P(x∣p)=p x (1−p) 1−x =exp(xlogp+(1−x)log(1−p)) Теперь мы можем положить u(x)=(x,1−x), θ=(p,1−p), и всё получится. Единственное, что смущает, — это то, что компоненты вектора u ( x ) u(x) линейно зависимы. Хотя это не является формальной проблемой, но всё же хочется с этим что-то сделать. Исправить это можно, если переписать exp ⁡ ( x log log (1−p) 1−x =(1−p)exp(xlogp+(−x)log(1−p))= exp ⁡ ( x log =(1−p)exp(xlog 1−p p ) и определить уже минимальное представление с u(x)=x, θ = log ⁡ p 1 − p θ=log 1−p p — мы ведь уже сталкивались с этим выражением, когда изучали логистическу регрессию, не так ли? Вопрос на подумать. Принадлежит ли к экспоненциальному классу семейство равномерных распределений на отрезках U[a,b]? Казалось бы, да: так как: exp ⁡ ( 0 ) p(x)= b−a 1 I [a,b] (x)exp(0) В чём может быть подвох? Как мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать Y ∣ X Y∣X. В том числе: нормальное; распределение Пуассона; экспоненциальное; биномиальное, мультиномиальное (с фиксированным числом испытаний); геометрическое; χ 2 χ 2 -распределение; бета-распределение; гамма-распределение; распределение Дирихле. К экспоненциальным семействам не относятся, к примеру: равномерное распределение на отрезке; t t-распределение Стьюдента; распределение Коши; смесь нормальных распределений. MLE для семейства из экспоненциального класса Возможно, вас удивил странный и на первый взгляд не очень естественный вид p(x∣θ).",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 4,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "I [a,b] (x)exp(0) В чём может быть подвох? Как мы увидели, к экспоненциальным семействам относятся как непрерывные, так и дискретные распределения. Вообще, к ним относится большая часть распределений, которыми Вам на практике может захотеться описать Y ∣ X Y∣X. В том числе: нормальное; распределение Пуассона; экспоненциальное; биномиальное, мультиномиальное (с фиксированным числом испытаний); геометрическое; χ 2 χ 2 -распределение; бета-распределение; гамма-распределение; распределение Дирихле. К экспоненциальным семействам не относятся, к примеру: равномерное распределение на отрезке; t t-распределение Стьюдента; распределение Коши; смесь нормальных распределений. MLE для семейства из экспоненциального класса Возможно, вас удивил странный и на первый взгляд не очень естественный вид p(x∣θ). Но всё не просто так: оказывается, что оценка максимального правдоподобия параметров распределений из экспоненциального класса устроена очень интригующе. Запишем функцию правдоподобия выборки X=(x 1 ,…,x exp p(X∣θ)=h(θ) −N ⋅( i=1 ∏ N g(x i ))⋅exp(θ T [ i=1 ∑ N u(x i )]) Её логарифм равен log log l(X∣θ)=−Nlogh(θ)+ i=1 ∑ N logg(x i )+θ T [ i=1 ∑ N u(x i )] Дифференцируя по θ θ, получаем log l(X∣θ)=−N∇ θ logh(θ)+[ i=1 ∑ N u(x i )] Тут нам потребуется следующая Лемма. ∇ θ log logh(θ)=Eu(x) Доказательство: Как мы уже отмечали в прошлом пункте: exp h(θ)=∫g(x)exp(θ T u(x))dx Следовательно, ∇ θ log exp exp logh(θ)= ∫g(x)exp(θ T u(x))dx ∇ θ ∫g(x)exp(θ T u(x))dx exp h(θ) ∫u(x)g(x)exp(θ T u(x))dx exp =∫u(x)⋅ h(θ) 1 g(x)exp(θ T u(x))dx=Eu(x) Кстати, можно ещё доказать, что log Cov logh(θ)=Cov(u i (x),u j (x)) Приравнивая l(X∣θ) к нулю и применяя лемму, мы получаем, что Eu(x)= N 1 [ i=1 ∑ N u(x i )] Таким образом, теоретические матожидания всех компонент (x) должны совпадать с их эмпирическими оценками, а метод максимального правдоподобия совпадает с методом моментов для (x) в качестве моментов. И в следующем пункте выяснится, что распределения из семейств, относящихся к экспоненциальному классу, это те самые распределения, которые имеют максимальную энтропию из тех, что имеют заданные моменты (x). **Пример.**Рассмотрим вновь логнормальное распределение: exp ⁡ ( − ( log p(x)= x 2πσ 2 1 exp(− 2σ 2 (logx−μ) exp log log 2πσ 2 1 exp(− 2σ 2 1 log 2 x+ σ 2 μ logx− exp exp log log 2πσ 2 exp( exp (x) logx (x) log exp exp −πθ 2 −1 ⋅exp− exp(θ 1 u 1 (x)+θ 2 u 2 (x)) Как видим, логнормальное распределение тоже из экспоненциального класса. Вас может это удивить: ведь выше мы обсуждали, что для него метод моментов и метод максимального правдоподобия дают разные оценки. Но никакого подвоха тут нет: мы просто брали не те моменты. В данном случае log ⁡ x u 1 (x)=logx, log ⁡ 2 x u 2 (x)=log 2 x, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться E log log ⁡ x i , E log log ⁡ 2 x i Elogx= N 1 i ∑ logx i ,Elog log 2 x i Матожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле: E log log Elogx= ∂θ 1 ∂",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 5,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "дают разные оценки. Но никакого подвоха тут нет: мы просто брали не те моменты. В данном случае log ⁡ x u 1 (x)=logx, log ⁡ 2 x u 2 (x)=log 2 x, их матожидания и надо брать; тогда для параметров, получаемых из MLE, должно выполняться E log log ⁡ x i , E log log ⁡ 2 x i Elogx= N 1 i ∑ logx i ,Elog log 2 x i Матожидания в левых частых мы должны выразить через параметры — и нам для этого совершенно не обязательно что-то интегрировать! В самом деле: E log log Elogx= ∂θ 1 ∂ logh(θ)= log ⁡ π + 1 2 log logπ+ 2 1 logθ )=− log log Elog 2 x= ∂θ 2 ∂ logh(θ)= Теорема Купмана-Питмана-Дармуа Теперь мы наконец готовы сформулировать одно из самых любопытных свойств семейств экспоненциального класса. В следующей теореме мы опустим некоторые не очень обременительные условия регулярности. Просто считайте, что для хороших дискретных и абсолютно непрерывных распределений, с которыми вы в основном и будете сталкиваться, это так. Теорема. Пусть exp p(x)= h(θ) 1 exp(θ T u(x)) — распределение, причём θ θ — вектор длины n n и (x)=α i для некоторых фиксированных i=1,…,n. Тогда распределение p ( x ) p(x) обладает наибольшей энтропией среди распределений с тем же носителем, для которых (x)=α i=1,…,n. При этом оно — единственное с таким свойством: в том смысле, что любое другое распределение, обладающее этим свойством, совпадает с ним почти всюду. Рассмотрим несколько примеров: Пример 1. Среди распределений на множестве {1,2,3,…} неотрицательных целых чисел с заданным математическим ожиданием μ μ найдём распределение с максимальной энтропией. В данном случае у нас лишь одна функция (x)=x, которая соответствует фиксации матожидания E x Ex. Плотность будет вычисляться только в точках x = k x=k, k=1,2,… и будет иметь вид exp =p(k)= h(θ) 1 exp(θk) В этой формуле уже безошибочно угадывается геометрическое распределение с p=1−e θ . Параметр p p можно подобрать из соображений того, что математическое ожидание равно μ μ. Матожидание геометрического распределения равно 1 p p 1 , так что . Окончательно, (1− μ 1 ) k−1 Пример 2. Среди распределений на всей вещественной прямой с заданным математическим ожиданием μ μ найдём распределение с максимальной энтропией. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.1. Вероятностный подход в ML Как описать привычные модели на языке статистики. Оптимизация функции потерь vs оценка максимального правоподобия Следующий параграф 4.3. Обобщённые линейные модели Как прокачать линейную модель с помощью распределений из экспоненциального класса",
    "metadata": {
      "title": "Экспоненциальный класс распределений и принцип максимальной энтропии",
      "url": "https://education.yandex.ru/handbook/ml/article/eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.2",
      "part": 6,
      "total_parts": 6,
      "source_file": "4.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как прокачать линейную модель с помощью распределений из экспоненциального класса Мотивация До сих пор мы рассматривали в основном модели вида y∼f(x)+ε с шумом ε ε из того или иного распределения. Но у этих моделей: шум не зависит от x x; y y может принимать любые значения. А что, если мы захотим предсказывать время ожидания доставки? Казалось бы, чем дольше время потенциального ожидания, тем больше его дисперсия. А как корректно предсказывать таргет, который принимает только целые значения? Один из подходов мы обсудим в этом параграфе. Грубо говоря, вместо того, чтобы прибавлять один и тот же шум, мы зафиксируем семейство распределений p(y∣μ(x)), в котором изменяемым параметром будет зависящее от x x математическое ожидание μ ( x ) μ(x). Вот как могут выглядеть такие модели для случаев, если p p нормальное с фиксированной дисперсией, экспоненциальное или пуассоновское соответственно: 11 Как видим, такой подход позволяет получать и модели с меняющейся дисперсией шума, и модели с целочисленным таргетом. Определение Мы рассмотрим достаточно широкий класс моделей — обобщённые линейные модели (generalized linear models, GLM). К ним относятся, в частности, линейная и логистическая регрессии. В итоге мы научимся подбирать подходящую регрессионную модель для самых разных типов данных. Вспомним, что вероятностную модель линейной регрессии можно записать как y∣x∼N(⟨x,w⟩,τ 2 ), а вероятностную модель логистической регрессии — как y∣x∼Bern(σ(⟨x,w⟩)), где Bern(p) — распределение Бернулли с параметром p p, а σ(u)= 1+e −u 1 . Итак, чем в этих терминах отличаются вероятностные модели линейной и логистической регрессии? Во первых, параметризованное семейство распределений для y ∣ x y∣x, а именно, N(∗,σ 2 ) в случае линейной регрессии и B e r n Bern в случае логистической. Во-вторых, в обоих случаях математическое ожидание условного распределения y ∣ x y∣x является функцией от ⟨ x , w ⟩ ⟨x,w⟩. На это можно посмотреть и по-другому: для каждой из задач выбрана функция g g такая, что g(E(y∣x))=⟨x,w⟩. Эта функция называется функцией связи (link function). В случае линейной регрессии g(u)=u. В самом деле, E(y∣x)=EN(⟨x,w⟩,τ 2 )=⟨x,w⟩. В случае логистической регрессии logit ( u ) = log ⁡ u 1 − u g(u)=σ −1 (u)=logit(u)=log 1−u u . Давайте это тоже проверим. В модели логистической регрессии условное распределение y ∣ x y∣x — это распределение Бернулли с вероятностью успеха σ(⟨x,w⟩), и этой же вероятности равно его математическое ожидание. Следовательно, g(σ(⟨x,w⟩))=σ −1 (σ(⟨x,w⟩))=⟨x,w⟩. Обобщая, можно сказать, что, если данные таковы, что E(Y∣X) не является линейной функцией от x x, мы линеаризуем E(Y∣X) с помощью функции связи g g. Замечание: Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного x x оценить все параметры распределения y ∣ x y∣x, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов w w, ни на итоговые предсказания E(Y∣X), которые выдаёт обученная модель. Продолжим. Задав эти две составляющие — параметризованное семейство распределений и функцию связи — мы получим обобщённую линейную модель (GLM). Для нового объекта x x она выдаст предсказание =E(y∣x)=g −1",
    "metadata": {
      "title": "Обобщённые линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshyonnye-linejnye-modeli",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.3",
      "part": 1,
      "total_parts": 4,
      "source_file": "4.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "g. Замечание: Вообще говоря, нормальное распределение определяется не только своим математическим ожиданием, но и стандартным отклонением. То есть, в отличие от логистической регрессии, модель линейной регрессии не позволяет для данного x x оценить все параметры распределения y ∣ x y∣x, и дисперсию приходится фиксировать изначально. К счастью, выбор её значения в нормальном распределении не влияет ни на оптимальный вектор весов w w, ни на итоговые предсказания E(Y∣X), которые выдаёт обученная модель. Продолжим. Задав эти две составляющие — параметризованное семейство распределений и функцию связи — мы получим обобщённую линейную модель (GLM). Для нового объекта x x она выдаст предсказание =E(y∣x)=g −1 (⟨x,w⟩), а выбор класса распределений y ∣ x y∣x потребуется нам для подбора весов w w. В принципе, можно выбрать любой класс распределений y ∣ x y∣x и любую монотонную функцию связи g g, получив некоторую вероятностную модель. Однако обычно для упрощения поиска оптимальных весов w w в GLM предполагают, что y ∣ x y∣x принадлежит одному из достаточно простых семейств экспоненциального класса. Что даёт нам принадлежность экспоненциальному классу? В контексте GLM обычно рассматривают подкласс экспоненциального класса, состоящий из семейств, представимых в виде exp p(y∣θ,ϕ)=exp( ϕ yθ−a(θ) +b(y,ϕ)) где θ θ и ϕ ϕ — скалярные параметры, причём ϕ ϕ — нечто фиксированное, обычно дисперсия, которая чаще всего полагается равной 1 1, а значения θ θ параметризуют распределения из семейства. Нетрудно переписать плотность в более привычном для нас виде, чтобы стало очевидно, что это семейство действительно из экспоненциального класса: exp exp exp p(y∣θ,ϕ)= exp( ϕ a(θ) ) 1 exp(b(y,ϕ))exp( ϕ yθ ) Действительно, если вспомнить, что φ φ — это константа, а не параметр, то получается очень похоже на exp p(y∣ν)= h(ν) 1 g(y)⋅exp(ν T u(y)) В частности, мы видим, что u ( y ) u(y) состоит из единственной компоненты (y), равной y ϕ ϕ y . По доказанной в предыдущем разделе лемме имеем тогда, что математическое ожидание μ μ такой случайной величины равно μ=ϕEu 1 (y)=ϕ ∂θ ∂ ( ϕ a(θ) )=a ′ (θ) До сих пор мы рассуждали о распределении p ( y ) p(y) без x x в условии. Что будет, если его добавить? Параметр ϕ ϕ мы договорились сохранять постоянным, тогда от x x должен зависеть единственный оставшийся параметр θ θ. Самый естественный в нашей ситуации вариант — это положить θ=⟨x,w⟩. В GLM мы вводили функцию g g, для которой g(E(y∣x))=⟨x,w⟩, то есть E(y∣x)=g −1 (⟨x,w⟩). Но ведь матожидание y y равно (θ), то есть (⟨x,w⟩). Это позволяет нам однозначно определить функцию связи g=(a ′ ) −1 . Такая функция связи называется канонической функцией связи (canonical link function). Примеры Поговорим немного о том, как на практике подбирать ϕ , a , b ϕ,a,b, чтобы по классу распределений y ∣ x y∣x определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров. Пример 1. Пусть мы решили применить к данным линейную регрессию. Тогда exp p(y∣x,w,σ 2 )= 2π σ 1 exp(− 2σ 2 (y−⟨x,w⟩) 2 ) Обозначим для краткости μ=⟨x,w⟩ и будем рассматривать p(y∣μ,σ 2 ). Мы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но",
    "metadata": {
      "title": "Обобщённые линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshyonnye-linejnye-modeli",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.3",
      "part": 2,
      "total_parts": 4,
      "source_file": "4.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Это позволяет нам однозначно определить функцию связи g=(a ′ ) −1 . Такая функция связи называется канонической функцией связи (canonical link function). Примеры Поговорим немного о том, как на практике подбирать ϕ , a , b ϕ,a,b, чтобы по классу распределений y ∣ x y∣x определить каноническую функцию связи. Чтобы разобраться, рассмотрим несколько примеров. Пример 1. Пусть мы решили применить к данным линейную регрессию. Тогда exp p(y∣x,w,σ 2 )= 2π σ 1 exp(− 2σ 2 (y−⟨x,w⟩) 2 ) Обозначим для краткости μ=⟨x,w⟩ и будем рассматривать p(y∣μ,σ 2 ). Мы уже знаем, что семейство нормальных распределений относится к экспоненциальному классу, но давайте выразим эту плотность в описанном выше более частном виде: exp log p(y∣μ,σ 2 )=exp(− 2σ 2 (y−μ) 2 −log(2πσ 2 )) В формуле экспоненциального семейства распределений единственная часть, не зависящая от θ θ, — это функция b b. Поскольку μ=a ′ (θ), функция b b также не должна зависеть от μ μ. Так что внутри экспоненты выделим в качестве функции b b всё, что не зависит от exp log p(y∣μ,σ 2 )=exp =ϕ σ 2 yμ =yθ − μ 2 /2 =a(θ) =b(y,ϕ) +log(2πσ 2 )) Эта формула уже похожа на формулу экспоненциального семейства распределений и видно, что ϕ = σ 2 ϕ=σ θ=g(μ)=μ (коэффициент при y y), a(θ)=μ 2 /2=θ 2 /2, log b(y,ϕ)=− 2σ 2 y 2 −log(2πσ 2 ). Каноническая функция связи является обратной к (θ)=θ, то есть ⟨x,w⟩=g(μ)=μ, как мы и привыкли. Пример 2. Проделаем то же самое, но теперь для распределения Бернулли. Пример 3. Хорошо, про линейную и логистическую регрессию мы и так знали. Давайте попробуем решить с помощью GLM новую задачу. Пусть мы хотим по каким-то признакам X X предсказать количество «лайков», которое пользователи поставят посту в социальной сети за первые 10 минут после публикации. Конечно, можно использовать для этого линейную регрессию. Однако предположение линейной регрессии, что Y ∣ X ∼ N Y∣X∼N, в данном случае странное по нескольким причинам. Во-первых, количество лайков заведомо не может быть отрицательным, а нормальное распределение всегда будет допускать ненулевую вероятность отрицательного значения. Во-вторых, количество лайков — всегда целое число. В-третьих, у распределения количества лайков, скорее всего, положительный коэффициент асимметрии (skewness). То есть, если модель предсказывает, что под постом будет 100 лайков, мы скорее можем ожидать, что под ним окажется 200 лайков, чем 0. Нормальное распределение симметрично и не может описать такие данные. С другой стороны, если мы предположим, что в первые 10 минут после публикации есть какая-то постоянная частота (своя для каждого поста, зависящая от x x), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем: 11 Но какая будет каноническая функция связи, если мы считаем, что Y ∣ X ∼ Poisson Y∣X∼Poisson? Аналогично первому и второму примерам: exp ⁡ ( y log ⁡ μ − μ − log ⁡ y ! ) p(y∣μ)= y! e −μ μ y =exp(ylogμ−μ−logy!) Откуда ϕ = 1 ϕ=1, log ⁡ μ θ=g(μ)=logμ, exp ⁡ ( θ ) a(θ)=μ=exp(θ), log ⁡ y ! b(y,ϕ)=−logy! Значит, эта модель (она называется пуассоновская",
    "metadata": {
      "title": "Обобщённые линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshyonnye-linejnye-modeli",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.3",
      "part": 3,
      "total_parts": 4,
      "source_file": "4.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "постоянная частота (своя для каждого поста, зависящая от x x), с которой пользователи ставят лайк, мы получим, что количество лайков имеет распределение Пуассона. Распределение Пуассона не имеет описанных выше проблем: 11 Но какая будет каноническая функция связи, если мы считаем, что Y ∣ X ∼ Poisson Y∣X∼Poisson? Аналогично первому и второму примерам: exp ⁡ ( y log ⁡ μ − μ − log ⁡ y ! ) p(y∣μ)= y! e −μ μ y =exp(ylogμ−μ−logy!) Откуда ϕ = 1 ϕ=1, log ⁡ μ θ=g(μ)=logμ, exp ⁡ ( θ ) a(θ)=μ=exp(θ), log ⁡ y ! b(y,ϕ)=−logy! Значит, эта модель (она называется пуассоновская регрессия), будет предсказывать с помощью формулы exp E(y∣x)=g −1 (⟨x,w⟩)=exp(⟨x,w⟩). Вопрос на подумать. В каких ситуациях была бы полезной функция связи complementary log-log link (cloglog) g ( x ) = log g(x)=log(−log(1−x))? Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.2. Экспоненциальный класс распределений и принцип максимальной энтропии Самые главные семейства распределений в жизни любого data scientist’а Следующий параграф 4.4. Как оценивать вероятности Как правильно оценить вероятности классов в задаче классификации",
    "metadata": {
      "title": "Обобщённые линейные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/obobshyonnye-linejnye-modeli",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.3",
      "part": 4,
      "total_parts": 4,
      "source_file": "4.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как правильно оценить вероятности классов в задаче классификации Мы уже упоминали, что оценивать вероятности классов как softmax(f w (x i )) для какой-то произвольной функции f w f w — это дело подозрительное. В этом разделе мы поговорим о том, как это делать хорошо и правильно. Что же такое вероятность класса, если объект либо принадлежит этому классу, либо нет? Ограничимся пока случаем двухклассовой классификации — с классами 0 и 1. Если утверждается, что мы предсказываем корректную вероятность класса 1 (обозначим её q ( x i ) q(x i )), то прогноз «объект x i x i принадлежит классу 1 с вероятностью 2 3 3 2 » должен сбываться в 2 3 3 2 случаев. То есть, условно говоря, если мы возьмём все объекты, то среди них что-то около двух третей действительно имеет класс 1. На математическом языке это можно сформулировать так: Если p ^ p — предсказанная вероятность класса 1, то P(y i =1∣q(x К сожалению, в реальной жизни p ^ p — это скорее всего вещественные числа, которые будут различными для различных y i y i , и никаких вероятностей мы не посчитаем, но мы можем разбить отрезок [ 0 , 1 ] [0,1] на бины, внутри каждого из которых уже вычислить, каковая там доля объектов класса 1, и сравнить эту долю со средним значением вероятности в бине: 12 У модели, которая идеально предсказывает вероятности (как обычно говорят, у идеально калиброванной модели) жёлтые точки на диаграмме калибровки должны совпадать с розовыми. А вот на картинке выше это не так: жёлтые точки всегда ниже розовых. Давайте поймём, что это значит. Получается, что наша модель систематически завышает предсказанную вероятность (розовые точки), и порог отсечения нам, выходит, тоже надо было бы сдвинуть вправо: 12 Но такая картинка, пожалуй, говорит о какой-то серьёзной патологии классификатора. Гораздо чаще встречаются следующие две ситуации: Слишком уверенный (overconfident) классификатор: 12 Такое случается с сильными классификаторыми (например, нейросетями), которые учились на метки классов, а не на вероятности: тем самым процесс обучения стимулировал их всегда давать ответ, как можно более близкий к 0 или 1. Неуверенный (underconfident) классификатор: 12 Такое может случиться, например, если мы слишком много обращаем внимания на трудные для классификации объекты на границе классов (как, скажем, в SVM), в каком-то смысле в ущерб более однозначно определяемым точкам. Этим же могут и грешить модели на основе бэггинга (например, случайный лес). Грубо говоря, среднее нескольких моделей предскажет что-то близкое к единице только если все слагаемые предскажут что-то, близкое к единице — но из-за дисперсии моделей это будет случаться реже, чем могло бы. Подробнее можно почитать в статье. Вам скажут: логистическая регрессия корректно действительно предсказывает вероятности Вам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера. Рассмотрим датасет c двумя классами (ниже на картинке обучающая выборка) 12 Обучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть L 2 L 2 -регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе",
    "metadata": {
      "title": "Как оценивать вероятности",
      "url": "https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.4",
      "part": 1,
      "total_parts": 5,
      "source_file": "4.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "к единице — но из-за дисперсии моделей это будет случаться реже, чем могло бы. Подробнее можно почитать в статье. Вам скажут: логистическая регрессия корректно действительно предсказывает вероятности Вам даже будут приводить какие-то обоснования. Важно понимать, что происходит на самом деле, и не дать ввести себя в заблуждение. В качестве противоядия от иллюзий предлагаем рассмотреть два примера. Рассмотрим датасет c двумя классами (ниже на картинке обучающая выборка) 12 Обучим на нём логистическую регрессию из sklearn безо всяких параметров (то есть L 2 L 2 -регуляризованную, но это не так важно). Классы не так-то просто разделить, вот и логистическая регрессия так себе справляется. Ниже изображена часть тестовой выборки вместе с предсказанными вероятностями классов для всех точек области 12 Видим, что модель не больно-то уверена в себе, и ясно почему: признаковое описание достаточно бедное и не позволяет нам хорошо разделить классы, хотя, казалось бы, это можно довольно неплохо сделать. Попробуем поправить дело, добавив полиномиальные фичи, то есть все для 0⩽j,k⩽5 в качестве признаков, и обучив поверх этих данных логистическую регрессию. Снова нарисуем некоторые точки тестовой выборки и предсказания вероятностей для всех точек области: 12 Видим, что у нас сочетание двух проблем: неуверенности посередине и очень уверенных ошибок по краям. Нарисуем теперь калибровочные кривые для обеих моделей: 12 Калибровочные кривые весьма примечательны; в любом случае ясно, что с предсказанием вероятностей всё довольно плохо. Посмотрим ещё, какие вероятности наши классификаторы чаще приписывают объектам: 12 Как и следовало ожидать, предсказания слабого классификатора тяготеют к серединке (та самая неуверенность), а среди предсказаний переобученного очень много крайне уверенных — и совсем не всегда правильных. Но почему же все твердят, что логистическая регрессия хорошо калибрована?! Попробуем понять и простить её. Как мы помним, логистическая регрессия учится путём минимизации функционала log log l(X,y)=− i=1 ∑ N (y i log(σ(⟨w,x i ⟩))+(1−y i )log(1−σ(⟨w,x i ⟩))) Отметим между делом, что каждое слагаемое — это кроссэнтропия распределения P P, заданного вероятностями P(0)=1−σ(⟨w,x i ⟩) и P(1)=σ(⟨w,x i ⟩), и тривиального распределения, которое равно y i y i с вероятностью 1 1. Допустим, что мы обучили по всему универсуму данных X X идеальную логистическую регрессию с идеальными весами w ∗ w ∗ . Пусть, далее, оказалось, что у нас есть n n объектов ,…,x n с одинаковым признаковым описанием (то есть по сути представленных одинаковыми векторами x i x i ), но, возможно, разными истинными метками классов ,…,y n . Тогда соответствующий им кусок функции потерь имеет вид log log i=1 ∑ n y i )log(σ(⟨w,x 1 ⟩))−( i=1 ∑ n (1−y i ))log(1−σ(⟨w,x 1 ⟩))= log log =−n( 2 1 p 0 log(σ(⟨w,x 1 ⟩))+p 1 log(1−σ(⟨w,x 1 ⟩))) где p j p j — частота j j-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда σ(⟨w,x 1 ⟩)=p 0 ,1−σ(⟨w,x 1 ⟩)=p 1 Результат, полученный для n n совпадающих точек будет приблизительно верным и для n n достаточно близких точек в случае, когда: признаковое описание данных достаточно хорошее — классы",
    "metadata": {
      "title": "Как оценивать вероятности",
      "url": "https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.4",
      "part": 2,
      "total_parts": 5,
      "source_file": "4.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "y i )log(σ(⟨w,x 1 ⟩))−( i=1 ∑ n (1−y i ))log(1−σ(⟨w,x 1 ⟩))= log log =−n( 2 1 p 0 log(σ(⟨w,x 1 ⟩))+p 1 log(1−σ(⟨w,x 1 ⟩))) где p j p j — частота j j-го класса среди истинных меток. В скобках также стоит кросс-энтропия распределения, задаваемого частотой меток истинных классов, и распределения, предсказываемого логистической регрессией. Минимальное значение кросс-энтропии (и минимум функции потерь) достигается, когда σ(⟨w,x 1 ⟩)=p 0 ,1−σ(⟨w,x 1 ⟩)=p 1 Результат, полученный для n n совпадающих точек будет приблизительно верным и для n n достаточно близких точек в случае, когда: признаковое описание данных достаточно хорошее — классы не перемешаны как попало и всё-таки близки к разделимым; модель не переобученная — то есть, предсказания вероятностей не скачут очень уж резко — вспомните второй пример. На всех этих точках модель будет выдавать примерно долю положительных, то есть тоже хорошую оценку вероятности. Как же всё-таки предсказать вероятности: методы калибровки Пусть наша модель (бинарной классификации) для каждого объекта x i x i выдаёт некоторое число q(x i )∈[0,1]. Как же эти числа превратить в корректные вероятности? Гистограммная калибровка. Мы разбиваем отрезок [ 0 , 1 ] [0,1] на бины ,…,B k (одинаковой ширины или равномощные) и хотим на каждом из них предсказывать всегда одну и ту же вероятность: θ j θ j , если q(x i )∈B j . Вероятности θ i θ i подбираются так, чтобы они как можно лучше приближали средние метки классов на соответствующих бинах. Иными словами, мы решаем задачу min j=1 i=1 N I{q(x i )∈B ,…,θ k ) min Вместо разности модулей можно рассматривать и разность квадратов. Метод довольно простой и понятный, но требует подбора числа бинов и предсказывает лишь дискретное множество вероятностей. Изотоническая регрессия. Этот метод похож на предыдущий, только мы будем, во-первых, настраивать и границы 0=b 0 ,b 1 ,…,b k =1 бинов ={t∣b j−1 ⩽b j }, а кроме того, накладываем условие ⩽…⩽θ k . Искать мы будем, приближая y i y i кусочно постоянной функцией g g от q ( x i ) q(x min ⁡ g i=1 ∑ N (y i −g(q(x i ))) 2 ⟶ g min 12 Минимизация осуществляется при помощи pool adjacent violators algorithm, и эти страницы слишком хрупки, чтобы выдержать его формулировку. Калибровка Платта представляет собой по сути применение сигмоиды поверх другой модели (то есть самый наивный способ получения «вероятностей»). Более точно, если q ( x i ) q(x i ) — предсказанная вероятность, то мы полагаем P(y i =1∣x i )=σ(aq(x i )+b)= 1+e −aq(x i )−b 1 где a a и b b подбираются методом максимального правдоподобия на отложенной выборке: log log min ⁡ a , b − i=1 log(σ(q(x i )))+(1−y i )log(1−σ(q(x i ))))⟶ a,b min Для избежания переобучения Платт предлагал также заменить метки (1−y i ) на регуляризованные вероятности таргетов: #{i∣y i =0}+2 1 ,t 1 = #{i∣y i =0}+2 #{i∣y i =1}+1 Калибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из",
    "metadata": {
      "title": "Как оценивать вероятности",
      "url": "https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.4",
      "part": 3,
      "total_parts": 5,
      "source_file": "4.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "P(y i =1∣x i )=σ(aq(x i )+b)= 1+e −aq(x i )−b 1 где a a и b b подбираются методом максимального правдоподобия на отложенной выборке: log log min ⁡ a , b − i=1 log(σ(q(x i )))+(1−y i )log(1−σ(q(x i ))))⟶ a,b min Для избежания переобучения Платт предлагал также заменить метки (1−y i ) на регуляризованные вероятности таргетов: #{i∣y i =0}+2 1 ,t 1 = #{i∣y i =0}+2 #{i∣y i =1}+1 Калибровка Платта неплохо справляется с выколачиванием вероятностей из SVM, но для более хитрых классификаторов может спасовать. В целом, можно показать, что этот метод хорошо работает, если для каждого из истинных классов предсказанные вероятности q ( x i ) q(x i ) распределы нормально с одинаковыми дисперсиями. Подробнее об этом вы можете почитать в этой статье. Там же описано обобщение данного подхода — бета-калибровка. С большим количеством других методов калибровки вы можете познакомиться в этой статье Как измерить качество калибровки Калибровочные кривые хорошо показывают, что есть проблемы, но как оценить наши усилия по улучшению предсказания вероятностей? Хочется иметь какую-то численную метрику. Мы упомянем две разновидности — прямое воплощение описанных выше идей. Expected/Maximum calibration error. Самый простой способ, впрочем — он наследник идеи с калибровочной кривой. А именно, разобьём отрезок [ 0 , 1 ] [0,1] на бины ,…,B k по предсказанным вероятностям и вычислим j=1 или max j=1,…,k max где ) — среднее значение y i y i , а ) — среднее значение q ( x i ) q(x i ) для x i x i , таких что q(x i )∈B j . Проблема этого способа в том, что мы можем очень по-разному предсказывать в каждом из бинов вероятности (в том числе константой) без ущерба для метрики. Brier score. Одна из популярных метрик, которая попросту измеряет разницу между предсказанными вероятностями и i=1 ∑ N (y i −q(x i )) 2 Казалось бы, в чём смысл? Немного подрастить мотивацию помогает следующий пример. Допустим, наши таргеты совершенно случайны, то есть P(y i =1∣x i )=P(y i ). Тогда хорошо калиброванный классификатор должен для каждого x i x i предсказывать вероятность 1 2 2 1 ; соответственно, его brier score равен 1 4 4 1 . Если же классификатор хоть в одной точке выдаёт вероятность , то в маленькой окрестности он должен выдавать примерно такие же вероятности. Поскольку же таргет случаен, локальный кусочек суммы из brier score будет иметь вид (1−p) 2 < 2 N ′ , что хуже, чем получил бы всегда выдающий 1 2 2 1 классификатор. Не обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss: log log i=1 logq(x i )+(1−y i )log(1−q(x i ))) Это же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета brier score и log-loss будут адекватными средствами оценки, но если нет — возможно всякое. Вопрос на засыпку: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности? Мы не будем про это рассказывать, но призываем читателя",
    "metadata": {
      "title": "Как оценивать вероятности",
      "url": "https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.4",
      "part": 4,
      "total_parts": 5,
      "source_file": "4.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "бы всегда выдающий 1 2 2 1 классификатор. Не обязательно брать квадратичную ошибку; сгодится и наш любимый log-loss: log log i=1 logq(x i )+(1−y i )log(1−q(x i ))) Это же и помогает высветить ограничения подхода, если вспомнить рассуждения о калиброванности логистической регрессии. Для достаточно гладких классификатора и датасета brier score и log-loss будут адекватными средствами оценки, но если нет — возможно всякое. Вопрос на засыпку: а как быть, если у нас классификация не бинарная, а многоклассовая? Что такое хорошо калиброванный классификатор? Как это определить численно? Как заставить произвольный классификатор предсказывать вероятности? Мы не будем про это рассказывать, но призываем читателя подумать над этим самостоятельно или, например, посмотреть туториал с ECML KDD 2020. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.3. Обобщённые линейные модели Как прокачать линейную модель с помощью распределений из экспоненциального класса Следующий параграф 4.5. Генеративный подход к классификации Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес",
    "metadata": {
      "title": "Как оценивать вероятности",
      "url": "https://education.yandex.ru/handbook/ml/article/kak-ocenivat-veroyatnosti",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.4",
      "part": 5,
      "total_parts": 5,
      "source_file": "4.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес Классификационные модели, которые мы рассматривали в предыдущих параграфах, нацелены непосредственно на оценку P(Y∣X). Такие модели называются дискриминативными. К ним относится, например, логистическая регрессия: она предлагает оценку (y=1∣x)=σ(w T ˆ x). В процессе обучения дискриминативные модели подбирают разделяющую поверхность (гиперплоскость в случае логистической регрессии). Новые объекты дискриминативная модель классифицирует в зависимости от того, по какую сторону от разделяющей поверхности они лежат. Например, обучившись на изображениях домашних кошек (y=0) и рысей (y=1), дискриминативная модель будет определять, новое изображение больше похоже на кошку или на рысь. При этом, если на вход такой модели дать изображение собаки (объект класса, которого не было в обучении, выброс), дискриминативная модель заведомо не сможет обнаружить, что это и не кошка, и не рысь, и отнесёт такой объект к одному из «знакомых» ей классов. В этом параграфе мы поговорим о другой группе моделей, которые нацелены на оценку P(X,Y)=P(X∣Y)P(Y). Такая модель описала бы, как обычно выглядят кошки, как они могут выглядеть, а каких кошек точно не бывает. Так же она описала бы и рысей. Она также определила бы по обучающим данным, насколько изображения кошек встречаются чаще, чем изображения рысей, т.е. оценила бы P ( Y ) P(Y). Генеративный и дискриминативный подходы к обучению Если модель позволила точно оценить распределение P(X∣Y), с её помощью можно генерировать объекты из этого условного распределения, в нашем примере — изображения кошек и рысей соответственно. А вместе распределение P(X,Y) дало бы нам возможность генерировать изображения и кошек, и рысей, причём именно в той пропорции, в которой они встречаются в реальном мире. Поэтому модели, оценивающие P(X,Y), называют генеративными. Ещё одно достоинство генеративных моделей — их способность находить выбросы в данных: объект x x можно считать выбросом, если P(x∣y) мало для каждого класса y y. Заметим, что находить выбросы с помощью генеративной модели можно и когда класс всего один — то есть никакие метки классов не доступны. Такая задача называется одноклассовой классификацией. Например, если у нас есть не размеченный датасет с аудиозаписями речи людей, то, обучив на нём генеративную модель, оценивающую в данном случае P(X∣Y)=P(X), мы сможем для нового аудио x x определить, похоже ли оно на аудиозапись человеческой речи (значение P ( x ) P(x) велико), или это что-то другое: синтезированная речь, посторонний шум и т.п. (значение P ( x ) P(x) мало). Если мы знаем, что «выбросы», с которыми модели предстоит сталкиваться, — это, как правило, синтезированная речь, то, мы можем дополнить датасет вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов. Чтобы использовать генеративную модель для классификации, необходимо выразить P(Y∣X) через P(X∣Y) и P ( Y ) P(Y). Сделать это позволяет формула Байеса: P(y∣x)= y ′ ∈Y ∑ P(y ′ )P(x∣y ′ ) P(x,y) = y ′ ∈Y ∑ P(y ′ )P(x∣y ′ ) P(y)P(x∣y) Классификация в генеративных моделях осуществляется с помощью байесовского классификатора: a ( x ) = arg ⁡ max arg ⁡ max arg ⁡ max a(x)=arg y∈Y max P(y∣x)=arg y∈Y max y ′ ∈Y ∑ P(y ′ )P(x∣y",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 1,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вторым классом, состоящим из синтезированной речи, и смоделировать также распределение этого класса. Это позволит существенно увеличить качество детектирования таких выбросов. Чтобы использовать генеративную модель для классификации, необходимо выразить P(Y∣X) через P(X∣Y) и P ( Y ) P(Y). Сделать это позволяет формула Байеса: P(y∣x)= y ′ ∈Y ∑ P(y ′ )P(x∣y ′ ) P(x,y) = y ′ ∈Y ∑ P(y ′ )P(x∣y ′ ) P(y)P(x∣y) Классификация в генеративных моделях осуществляется с помощью байесовского классификатора: a ( x ) = arg ⁡ max arg ⁡ max arg ⁡ max a(x)=arg y∈Y max P(y∣x)=arg y∈Y max y ′ ∈Y ∑ P(y ′ )P(x∣y ′ ) P(y)P(x∣y) =arg y∈Y max P(y)P(x∣y) Оценить P ( Y ) P(Y), как правило, несложно. Для этого используют частотные оценки, полученные в обучающей выборке: Выражение (1) (Y=y)= N #(Y=y) Отметим ещё раз, что использование генеративного подхода позволяет внедрять в модель априорные знания о P ( y ) P(y). Это не очень впечатляет, когда речь идёт о бинарной классификации, но всё меняется, если рассмотреть задачу ASR (автоматического распознавания речи), в которой по записи голоса восстанавливается произносимый текст. Таргетами здесь могут быть любые предложения или даже более развёрнутые тексты. При этом размеченных данных (запись, текст) обычно намного меньше, чем доступных текстов, и обученная на большом чисто текстовом корпусе языковая модель, которая будет оценивать вероятность того или иного предложения, может стать большим подспорьем, позволив из нескольких фонетически корректных наборов слов выбрать тот, который в большей степени похож на настоящее предложение. Но как смоделировать распределение P(X,Y)? Пространство всех возможных функций распределения P(X,Y) бесконечномерно, из-за чего оценить произвольное распределение с помощью конечной выборки невозможно. Поэтому перед оценкой P(X,Y) на это распределение накладывают дополнительные ограничения. Некоторые простые примеры таких ограничений мы рассмотрим в следующих разделах. Gaussian discriminant analysis Модель гауссовского (или квадратичного) дискриминантного анализа (GDA) строится в предположении, что распределение объектов каждого класса y y подчиняется многомерному нормальному закону со средним μ y μ y и ковариационной матрицей exp p(x∣y)= (2π) n/2 ∣Σ y ∣ 1/2 1 exp(− 2 1 (x−μ (x−μ y )) Тогда функция правдоподобия L(P(Y),μ,Σ)= i=1 ∏ N p(x )P(y i ) достигает максимума при i=1 i=1 i=1 i=1 )(x (Y), представленной выше см. выражение ( 1 ) (1). Рассмотрим, как выглядит разделяющая поверхность в модели GDA. На поверхности, разделяющей классы выполняется P(y i ∣x)=P(y j ∣x)⇔ p(x∣y i )P(y i )=p(x∣y j )P(y j )⇔ log log log log logp(x∣y i )+logP(y i )−logp(x∣y j )−logP(y j )=0⇔ Выражение (2) log log log log (x−μ (x−μ y i )−log(2π) n/2 ∣Σ y i ∣ 1/2 +logP(y i )+ 2 1 (x−μ (x−μ y j )+log(2π) n/2 ∣Σ y j ∣ 1/2 −logP(y j )=0 Поскольку левая часть уравнения (2) квадратична по x x, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён на рис. 13 Плотность классов и разделяющая поверхность в многоклассовой модели LDA см. рисунок. 13 Linear Discriminant Analysis В выражении (2) член второго порядка )x зануляется при . Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 2,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log log log (x−μ (x−μ y i )−log(2π) n/2 ∣Σ y i ∣ 1/2 +logP(y i )+ 2 1 (x−μ (x−μ y j )+log(2π) n/2 ∣Σ y j ∣ 1/2 −logP(y j )=0 Поскольку левая часть уравнения (2) квадратична по x x, разделяющая поверхность между двумя классами будет представлять из себя гиперповерхность порядка 2. Пример разделяющей поверхности многоклассовой модели GDA приведён на рис. 13 Плотность классов и разделяющая поверхность в многоклассовой модели LDA см. рисунок. 13 Linear Discriminant Analysis В выражении (2) член второго порядка )x зануляется при . Таким образом, если дополнительно предположить, что все классы имеют общую ковариационную матрицу Σ Σ, разделяющая поверхность между любыми двумя классами будет линейной (см. рисунок). Поэтому такая модель называется линейным дискриминантным анализом (LDA). На этапе обучения единственное отличие модели LDA от GDA состоит в оценке ковариационной матрицы: i=1 )(x Заметим, что в модели GDA для каждого класса требовалось оценить порядка d 2 d 2 параметров. Это может привести к переобучению в случае, если размерность пространства признаков велика, а некоторые классы представлены в обучающей выборке малым количеством объектов. В LDA для каждого класса требуется оценить лишь порядка d d параметров (значение P ( y ) P(y) и элементы вектора μ y μ y ), и ещё d 2 d 2 общих для всех классов параметров (элементы матрицы Σ Σ). Таким образом, основное преимущество модели LDA перед GDA — её меньшая склонность к переобучению, недостаток — линейная разделяющая поверхность. Метод наивного байеса Предположим, что признаки X X объектов каждого класса y y — независимые случайные величины: ∀y∈Y∀U,V:U⊔V={1,...d},∀x u ⊂R ∣U∣ ,x v ⊂R ∣V∣ P(X ∣Y=y)=P(X U ∈x u ∣Y=y)P(X V ∈x u ∣Y=y). В таком случае говорят, что величины X X условно независимы относительно Y Y. Тогда справедливо Выражение (3) P(X∣Y)=P(X 1 ,X 2 ,...,X d ∣Y)=P(X 1 ∣Y)P(X 2 ,...,X d ∣Y)=...=P(X 1 ∣Y)P(X 2 ∣Y)...P(X d ∣Y) То есть для того, чтобы оценить плотность многомерного распределения P(X∣Y) достаточно оценить плотности одномерных распределений P(X i ∣Y), см. рисунок. 13 На рисунке приведён пример условно независимых относительно Y Y случайных величин . Для оценки плотности двумерных распределений объектов классов достаточно оценить плотности маргинальных распределений, изображённые графиками вдоль осей. Рассмотрим пример. Пусть решается задача классификации отзывов об интернет-магазине на 2 категории: Y = 0 Y=0 — отрицательный отзыв, клиент остался не доволен, и Y = 1 Y=1 — положительный отзыв. Пусть признак X w X w равен 1, если слово w w присутствует в отзыве, и 0 иначе. Тогда условие выражения ( 3 ) (3) означает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон». На практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия выражения ( 3 ) (3), называют наивным байесовским классификатором (Naive Bayes classifier, NB). Обучение модели NB заключается в оценке распределений P ( Y ) P(Y) и P(X i ∣Y). Для P ( Y ) P(Y) можно использовать частотную оценку выражения",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 3,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "отзыве, и 0 иначе. Тогда условие выражения ( 3 ) (3) означает, что, в частности, наличие или отсутствие слова «дозвониться» в отрицательном отзыве не влияет на вероятность наличия в этом отзыве слова «телефон». На практике в процессе feature engineering почти всегда создаётся много похожих признаков, и условно независимые признаки можно встретить очень редко. Поэтому генеративную модель, построенную в предположении условия выражения ( 3 ) (3), называют наивным байесовским классификатором (Naive Bayes classifier, NB). Обучение модели NB заключается в оценке распределений P ( Y ) P(Y) и P(X i ∣Y). Для P ( Y ) P(Y) можно использовать частотную оценку выражения ( 1 ) (1). P(X i ∣y) — одномерное распределение. Рассмотрим несколько способов оценки одномерного распределения. Оценка одномерного распределения Пусть мы хотим оценить одномерное распределение P ( X ) P(X). Если распределение P ( X ) P(X) дискретное, требуется оценить его функцию массы, то есть вероятность того, что величина X X примет значение x j x j . Метод максимума правдоподобия приводит к частотной оценке: Выражение (4) (X=x j )= N #(X=x j ) Где N N — размер выборки, по которой оценивается распределение X X (количество объектов класса y y в случае оценки плотности класса y y). При этом может оказаться, что некоторое значение x j x j ни разу не встречается в обучающей выборке. Например, в случае классификации отзывов методом Наивного Байеса, слово «амбивалентно» не встретилось ни в одном положительном отзыве, но встретилось в отрицательных. Тогда использование оценки выражения ( 4 ) (4) приведёт к тому, что все отзывы с этим словом будут определяться NB как отрицательные с вероятностью 1. Чтобы избежать принятия таких радикальных решений при недостатке статистики, используют сглаживание Лапласа: (X=x j )= N+mα #(X=x j )+α , где m m — количество различных значений, принимаемых случайной величиной X X, α α — гиперпараметр. Для оценки плотности p p абсолютно непрерывного распределения в точке a a можно разделить количество объектов обучающей выборки в окрестности точки a a на размер этой окрестности: (a)= 2h j ∑ 1 a−h<X j <a+h = 2h j ∑ 1 −h<X j −a<h . Обычно объекты, лежащие дальше от точки a a, учитывают с меньшим весом. Таким образом, оценка плотности приобретает вид (a)= −a) , где функция K h K h , называемая ядром, обычно имеет носитель (−h,h) (см. рисунок ниже). Такой способ оценки плотности называют непараметрическим. 13 Результат оценки плотности с разными ядрами. Использованы изображения из: 13 При параметрической оценке плотности предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид exp p(x)= σ 2π 1 exp(− 2σ 2 (x−μ) 2 ) Таким образом, чтобы оценить плотность p ( x ) p(x), достаточно оценить параметры μ , σ μ,σ. Метод максимума правдоподобия в этом случае даст такие оценки: — выборочное среднее, j=1 — выборочное стандартное отклонение. Если в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс y y описать нормальным распределением со средним",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 4,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "предполагают, что искомое распределение лежит в параметризованном классе, и подбирают значения параметров при помощи метода максимума правдоподобия. Например, предположим, что искомое распределение нормальное. Тогда функция его плотности имеет вид exp p(x)= σ 2π 1 exp(− 2σ 2 (x−μ) 2 ) Таким образом, чтобы оценить плотность p ( x ) p(x), достаточно оценить параметры μ , σ μ,σ. Метод максимума правдоподобия в этом случае даст такие оценки: — выборочное среднее, j=1 — выборочное стандартное отклонение. Если в модели NB распределения всех признаков объектов каждого класса нормальные, оценив параметры этих распределений, мы сможем каждый класс y y описать нормальным распределением со средним μ p μ p и диагональной ковариационной матрицей, значения на диагонали которой обозначим σ p σ p . Таким образом, полученная модель (Gaussian Naive Bayes, GNB) эквивалентна модели GDA с дополнительным ограничением на диагональность ковариационных матриц. Наивный байесовский подход и логистическая регрессия Предположим теперь, что в модели GNB класса всего 2, причём соответствующие им ковариационные матрицы совпадают, как это было в модели LDA. Таким образом =σ. Посмотрим, как будет выглядеть P(Y∣X) в этом случае. По теореме Байеса имеем P(Y=1∣X)= P(Y=1)P(X∣Y=1)+P(Y=0)P(X∣Y=0) P(Y=1)P(X∣Y=1) Разделим числитель и знаменатель полученного выражения на числитель: exp P(Y=1∣X)= 1+ P(Y=1)P(X∣Y=1) P(Y=0)P(X∣Y=0) 1 = 1+exp(ln P(Y=1)P(X∣Y=1) P(Y=0)P(X∣Y=0) ) 1 Из условной независимости X i X i относительно Y Y получаем Формула (5) exp P(Y=1∣X)= 1+exp(ln P(Y=1) P(Y=0) + i=1 ∑ d ln P(X i ∣Y=1) P(X i ∣Y=0) ) 1 Перепишем сумму в знаменателе, воспользовавшись формулой плотности нормального распределения exp exp i=1 ∑ d ln P(X i ∣Y=1) P(X i ∣Y=0) = i=1 ∑ d ln 2πσ i 2 1 exp( 2σ i 2 −(X i −μ 1,i ) 2 ) 2πσ i 2 1 exp( 2σ i 2 −(X i −μ 0,i i=1 1,i ) 2 −(X i −μ 0,i ) 2 = i=1 0,i −μ 1,i 1,i 2 −μ 0,i 2 ) Подставляя это выражение в формулу (5) , получаем exp P(Y=1∣X)= 1+exp(ln P(Y=1) P(Y=0) + i=1 0,i −μ 1,i 1,i 2 −μ 0,i 2 )) 1 Таким образом, P(Y=1∣X) представляется в GNB с общей ковариационной матрицей в таком же виде, как в модели логистической регрессии: Формула (6) exp P(Y=1∣X)= 1+exp(w 0 + i=1 где в случае GNB =ln P(Y=0) P(Y=1) + i=1 1,i 2 −μ 0,i 0,i −μ 1,i i=1,…,l Однако это не значит, что модели эквивалентны: модель логистической регрессии накладывает менее строгие ограничения на распределение P(X,Y), чем GNB. Так, X i X i могут не являться условно независимыми относительно Y Y, а распределения P(X∣Y=y) могут не удовлетворять нормальному закону, но P(y∣X) может при этом всё равно представляться в виде формулы (6) . В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.4. Как оценивать вероятности Как правильно оценить вероятности классов в задаче классификации Следующий параграф 4.6. Байесовский подход к оцениванию Байесовская статистика. Априорные и апостериорные",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 5,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "не являться условно независимыми относительно Y Y, а распределения P(X∣Y=y) могут не удовлетворять нормальному закону, но P(y∣X) может при этом всё равно представляться в виде формулы (6) . В этом случае использование метода логистической регрессии предпочтительнее. С другой стороны, если есть основания полагать, что требования GNB выполняются, то от GNB можно ожидать более высокого качества классификации по сравнению с логистической регрессией. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.4. Как оценивать вероятности Как правильно оценить вероятности классов в задаче классификации Следующий параграф 4.6. Байесовский подход к оцениванию Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии",
    "metadata": {
      "title": "Генеративный подход к классификации",
      "url": "https://education.yandex.ru/handbook/ml/article/generativnyj-podhod-k-klassifikacii",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.5",
      "part": 6,
      "total_parts": 6,
      "source_file": "4.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии Априорное знание Начнём с простого вопроса: как нам внести в модель априорные знания. Представьте, что мы обучаем модель линейной регрессии y∼⟨x,w⟩+ε, ε∼N(0,σ 2 ). С помощью MLE мы получили некоторую оценку w ^ w на веса w w — всякие ли их значения мы встретим с покорностью и смирением? Наверное, мы удивимся, если какие-то компоненты вектора w ^ w будут очень большими по сравнению с элементами X X: пожалуй, наша физическая интуиция будет бунтовать против этого, мы задумаемся о том, что из-за потенциальных ошибок сокращения вычисление предсказаний ) окажутся неточным — в общем, хотелось бы по возможности избежать этого. Но как? Будь мы приверженцами чисто инженерного подхода, мы бы сделали просто: прибавили бы к функции потерь слагаемое α∥ω∥ 2 2 , или α ∥ ω ∥ 1 α∥ω∥ 1 , или ещё что-то такое — тогда процедура обучения стала бы компромиссом между минимизацией исходного лосса и этой добавки, что попортило бы слегка близость y∼⟨x,w⟩, но зато позволило бы лучше контролировать масштаб w ^ w . Надо думать, вы узнали в этой конструкции старую добрую регуляризацию. Но наша цель — зашить наше априорное знание о том, что компоненты w w не слишком велики по модулю, в вероятностную модель. Введение в модель априорного знания соответствует введению априорного распределения на w w. Какое распределение выбрать? Ну, наверное, компоненты w w будут независимыми (ещё нам не хватало задавать взаимосвязи между ними!), а каждая из них будет иметь какое-то непрерывное распределение, в котором небольшие по модулю значения более правдоподобны, а совсем большие очень неправдоподобны. Мы знаем такие распределения? Да, и сразу несколько. Например, нормальное. Логично было бы определить p(w)= i=1 ∏ D N(w i ∣0,τ 2 ) где τ 2 τ 2 — какая-то дисперсия, которую мы возьмём с потолка или подберём по валидационной выборке. Отметим, что выбор нормального распределение следует и из принципа максимальной энтропии: ведь у него наибольшая энтропия среди распределений на всей числовой оси с нулевым матожиданием и дисперсией τ 2 τ 2 . Контроль масштаба весов — это, вообще говоря, не единственное, что мы можем потребовать. Например, мы можем из каких-то физических соображений знать, что тот или иной вес в линейной модели непременно должен быть неотрицательным. Тогда в качестве априорного на этот вес мы можем взять, например, показательное распределение (которое, напомним, обладает максимальной энтропией среди распределений на положительных числах с данным матожиданием). Оцениваем не значение параметра, а его распределение Раз уж мы начали говорить о распределении на веса w w, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из которых мы пытаемся построить регрессию y∼ax+b: 14 Несмотря на то, что в каждом из случаев «точная формула» или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность — оценить распределение параметров. Так,",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 1,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "данным матожиданием). Оцениваем не значение параметра, а его распределение Раз уж мы начали говорить о распределении на веса w w, то почему бы не пойти дальше. Решая задачу классификации, мы уже столкнулись с тем, что может быть важна не только предсказанная метка класса, но и вероятности. Аналогичное верно и для задачи регрессии. Давайте рассмотрим две следующих ситуации, в каждой из которых мы пытаемся построить регрессию y∼ax+b: 14 Несмотря на то, что в каждом из случаев «точная формула» или градиентный спуск выдадут нам что-то, степень нашей уверенности в ответе совершенно различная. Один из способов выразить (не)уверенность — оценить распределение параметров. Так, для примеров выше распределения на параметр a a могли бы иметь какой-то такой вид: 14 Дальше мы постараемся формализовать процесс получения таких оценок. Построение апостериорного распределения Давайте ненадолго забудем про линейную регрессию и представим, что мы подобрали с пола монету, которая выпадает орлом с некоторой неизвестной пока вероятностью θ θ. До тех пор, пока мы не начали её подкидывать, мы совершенно ничего не знаем о θ θ, эта вероятность может быть совершенно любой — то есть априорное распределение на θ θ является равномерным (на отрезке [ 0 , 1 ] [0,1]): p(θ)=I [0;1] (θ) Теперь представим, что мы подкинули её n n раз, получив результаты Y=(y 1 ,…,y n ) ( 0 0 — решка, 1 1 — орёл), среди которых =n−∑ i=1 n y i решек и i=1 n y i орлов. Определённо наши познания о числе p p стали точнее: так, если n 1 n 1 мало, то можно заподозрить, что и p p невелико (уже чувствуете, запахло распределением!). Распределение мы посчитаем с помощью формулы Байеса: p(θ∣Y)= p(Y) p(θ,Y) = ∫p(Y∣ψ)p(ψ)dψ p(Y∣θ)p(θ) в нашем случае: p(θ∣Y)= ∫ 0 1 ∏ i=1 n ψ y i (1−ψ) 1−y i dψ ∏ i=1 n θ y i (1−θ) 1−y i I [0,1] (θ) (1−ψ) (1−θ) n 0 I [0,1] (θ) В этом выражении нетрудно узнать бета-распределение: Beta Beta(n 1 +1,n 0 +1). Давайте нарисует графики его плотности для нескольких конкретных значений Как можно заметить, с ростом n n мы всё лучше понимаем, каким может быть θ θ, при этом если орёл выпадал редко, то пик оказывается ближе к нулю, и наоборот. Ширина пика в каком-то смысле отражает нашу уверенность в том, какими могут быть значения параметра, и не случайно чем больше у нас данных — тем уже будет пик, то есть тем больше уверенности. Распределение p(θ∣Y) параметра, полученное с учётом данных, называется апостериорным. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные =(y 1 ′ ,…,y m ′ ), в которых m 0 m 0 решек и m 1 m 1 орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса: p(θ∣Y∪Y ′ )=p([θ∣Y]∣Y ′ )= p(Y ′ ) p(Y ′ ∣θ)p(θ∣Y) злой интеграл = = злой интеграл θ m 1 (1−θ) m 0 B(n 1 +1,n 0 +1) θ n 1 (1−θ) n 0 I",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 2,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "данных, называется апостериорным. Переход от априорного распределения к апостериорному отражает обновление нашего представления о параметрах распределения с учётом полученной информации, и этот процесс является сердцем байесовского подхода. Отметим, что если нам придут новые данные =(y 1 ′ ,…,y m ′ ), в которых m 0 m 0 решек и m 1 m 1 орлов, мы сможем ещё раз обновить распределение по той же формуле Байеса: p(θ∣Y∪Y ′ )=p([θ∣Y]∣Y ′ )= p(Y ′ ) p(Y ′ ∣θ)p(θ∣Y) злой интеграл = = злой интеграл θ m 1 (1−θ) m 0 B(n 1 +1,n 0 +1) θ n 1 (1−θ) n 0 I [0,1] (θ) константа ∼ Beta константа θ n 1 +m 1 (1−θ) n 0 +m 0 ∼Beta(n 1 +m 1 +1,n 0 +m 0 +1) Вопрос на подумать. Пусть p(y∣μ)=N(y∣μ,σ 2 ) — нормальное распределение с фиксированной дисперсией σ 2 σ 2 , а для параметра μ μ в качестве априорного выбрано также нормальное распределение N(μ∣λ,θ 2 ). Каким будет апостериорное распределение при условии данных Y=(y 1 ,…,y n )? Сопряжённые распределения В двух предыдущих примерах нам очень сильно повезло, что апостериорные распределения оказались нашими добрыми знакомыми. Если же взять случайную пару распределений p(y∣θ) и p ( θ ) p(θ), результат может оказаться совсем не таким приятным. В самом деле, нет никакой проблемы в том, чтобы посчитать числитель формулы Байеса, но вот интеграл в знаменателе может и не найтись. Поэтому выбирать распределения нужно с умом. Более того, поскольку апостериорное распределение само станет априорным, когда придут новые данные, хочется, чтобы априорное и апостериорное распределения были из одного семейства; пары (семейств) распределений p(y∣θ) и p ( θ ) p(θ), для которых это выполняется, называются сопряжёнными p ( θ ) p(θ) называется сопряжённым к p(y∣θ). Полезно помнить несколько наиболее распространённых пар сопряжённых распределений: p(y∣θ) — распределение Бернулли с вероятностью успеха p(θ) — бета распределение; p(y∣μ) — нормальное с матожиданием μ μ и фиксированной дисперсией p(θ) также нормальное; p(y∣λ) — показательное с параметром p(λ) — гамма распределение; p(y∣λ) — пуассоновское с параметром p(λ) — гамма распределение; p(y∣θ) — равномерное на отрезке [ 0 , θ ] [0,θ], p ( θ ) p(θ) — Парето; Возможно, вы заметили, что почти все указанные выше семейства распределений (кроме равномерного и Парето) относятся к экспоненциальному классу. И это не случайность! Экспоненциальный класс и тут лучше всех: оказывается, что для p(y∣θ) из экспоненциального класса можно легко подобрать сопряжённое p ( θ ) p(θ). Давайте же это сделаем. Пусть p(y∣θ) имеет вид exp p(y∣θ)= h(θ) 1 g(y)exp(θ T u(y)) Положим exp p(θ)= h ν (θ) 1 exp(η T θ)⋅f(η,ν) где f(η,ν) — множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение: exp exp злой интеграл = p(θ∣Y)= злой интеграл [∏ i=1 n h(θ) 1 g(y i )exp(θ T u(y i ))] h ν (θ) 1 exp(η T θ)⋅f(η,ν) exp что-то, где нет θ = что-то, где нет θ h ν+n (θ) 1 exp(θ T [η+∑ i=1 n u(y i )]) Это распределение действительно из того же семейства, что и p ( θ ) p(θ), только с новыми параметрами:",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 3,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "имеет вид exp p(y∣θ)= h(θ) 1 g(y)exp(θ T u(y)) Положим exp p(θ)= h ν (θ) 1 exp(η T θ)⋅f(η,ν) где f(η,ν) — множитель, обеспечивающий равенство единице интеграла от этой функции. Найдём апостериорное распределение: exp exp злой интеграл = p(θ∣Y)= злой интеграл [∏ i=1 n h(θ) 1 g(y i )exp(θ T u(y i ))] h ν (θ) 1 exp(η T θ)⋅f(η,ν) exp что-то, где нет θ = что-то, где нет θ h ν+n (θ) 1 exp(θ T [η+∑ i=1 n u(y i )]) Это распределение действительно из того же семейства, что и p ( θ ) p(θ), только с новыми параметрами: new =η+ i=1 ∑ n u(y i ),ν new =ν+n Пример. Пусть p(y∣q)=q y (1−q) 1−y подчиняется распределению Бернулли. Напомним, что оно следующим образом представляется в привычном для экспоненциального класса виде: exp log p(y∣q)= = h(q) 1 (1−q) exp =u 1 (y) y =θ log 1−q q Предлагается брать априорное распределение вида exp ⁡ ( η log что-то, где нет q p(q)= что-то, где нетq (1−q) ν exp(ηlog 1−q q ) Тогда апостериорное распределение будет иметь вид (проверьте, посчитав по формуле Байеса!) exp log что-то, где нет q p(q∣Y)= что-то, где нетq (1−q) ν+n exp([η+∑ i=1 n y i ]log 1−q q ) Превратив логарифм частного в сумму, а экспоненту суммы в произведение, легко убедиться, что получается то самое бета распределение, которое мы уже получали выше. Оценка апостериорного максимума (MAP) Апостериорное распределение — это очень тонкий инструмент анализа данных, но иногда надо просто сказать число (или же интеграл в знаменателе не берётся и мы не можем толком посчитать распределение). В качестве точечной оценки логично выдать самое вероятное значение θ ∣ Y θ∣Y (интеграл в знаменателе от θ θ не зависит, поэтому на максимизацию не влияет): argmax⁡θ argmax⁡θ MAP = θ argmax p(θ∣Y)= θ argmax p(Y∣θ)p(θ) Это число называется оценкой апостериорного максимума (MAP). Если же в формуле выше перейти к логарифмам, то мы получим кое-что, до боли напоминающее старую добрую регуляризацию (и не просто так, как мы вскоре убедимся!): argmax⁡θ argmax⁡θ log argmax p(Y∣θ)p(θ)= θ argmax log(p(Y∣θ)p(θ))= = argmax⁡θ ( log log argmax ( 2 1 logp(Y∣θ)+logp(θ)) Пример. Рассмотрим снова распределение Бернулли p(y∣q) и априорное распределение p ( q ) ∼ Beta p(q)∼Beta(q∣a,b). Тогда MAP-оценка будет равна argmax⁡q argmax⁡q argmax p(Y∣q)p(q)= q argmax q ∑ i=1 n y i (1−q) n−∑ i=1 n y i ⋅q a−1 (1−q) b−1 = argmax⁡q log log argmax ((a−1+ i=1 ∑ n y i )logq+(b−1+n− i=1 ∑ n y i )log(1−q)) Дифференцируя по q q и приравнивая производную к нулю, мы получаем a+b+n−2 a+∑ i=1 n y i −1 В отличие от оценки максимального правдоподобия i=1 n y i мы здесь используем априорное знание: параметры ( a − 1 ) (a−1) и ( b − 1 ) (b−1) работают как «память о воображаемых испытаниях», как будто бы до того, как получить данные y i y i , мы уже имели ( a − 1 ) (a−1) успехов и ( b − 1 ) (b−1) неудач. Связь MAP- и MLE-оценок Оценка максимального правдоподобия является частным случаем",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 4,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i=1 ∑ n y i )log(1−q)) Дифференцируя по q q и приравнивая производную к нулю, мы получаем a+b+n−2 a+∑ i=1 n y i −1 В отличие от оценки максимального правдоподобия i=1 n y i мы здесь используем априорное знание: параметры ( a − 1 ) (a−1) и ( b − 1 ) (b−1) работают как «память о воображаемых испытаниях», как будто бы до того, как получить данные y i y i , мы уже имели ( a − 1 ) (a−1) успехов и ( b − 1 ) (b−1) неудач. Связь MAP- и MLE-оценок Оценка максимального правдоподобия является частным случаем апостериорной оценки. В самом деле, если априорное распределение является равномерным, то есть p ( θ ) p(θ) не зависит θ θ (если веса θ θ вещественные, могут потребоваться дополнительные усилия, чтобы понять, как такое вообще получается), и тогда argmax⁡θ log argmax⁡θ ( log log MAP = θ argmax logp(Y∣θ)p(θ)= θ argmax logp(Y∣θ)+ =const logp(θ) = = argmax⁡θ log argmax logp(y∣θ)= θ MLE Байесовские оценки для условных распределений В предыдущих разделах мы разобрали, как байесовский подход работает для обычных, не условных распределений. Теперь вернёмся к чему-то более близкому к машинному обучению, а именно к распределениям вида y ∣ x , w y∣x,w, и убедимся, что для них байесовских подход работает точно так же, как и для обычных распределений. Имея некоторое распределение p(y∣x,w), мы подбираем для него априорное распределение на веса p ( w ) p(w) (и да, оно не зависит от x x: ведь априорное распределение существует ещё до появления данных) и вычисляем апостериорное распределение на веса: p(w∣X,y) Вычислять его мы будем по уже привычной формуле Байеса: p(w∣X,y)= p(y) p(y,w∣X) = p(y) p(y∣X,w)p(w) Повторим ещё разок, в чём суть байесовского подхода: у нас было некоторое априорное представление p ( w ) p(w) о распределении весов w w, а теперь, посмотрев на данные i=1 n , мы уточняем своё понимание, формулируя апостериорное представление p(w∣X,y). Если же нам нужна только точечная оценка, мы можем ограничиться оценкой апостериорного максимума (MAP): argmax⁡w argmax⁡w MAP = w argmax p(w∣X,y)= w argmax p(y∣X,w)p(w)= = argmax⁡w ( log log argmax ( 2 1 logp(y∣X,w)+logp(w)) что уже до неприличия напоминает регуляризованную модель Пример: линейная регрессия с L 2 L 2 -регуляризацией как модель с гауссовским априорным распределением на веса В модели линейной регрессии y=⟨x,w⟩+ε, ε∼N(0,σ 2 ) введём априорное распределение на веса вида p(w)=N(w∣0,τ 2 I)= j=1 ∏ D N(w j ∣0,τ 2 )= j=1 ∏ D p(w j ) Тогда MAP — точка минимума следующего выражения: − log log −logp(y∣X,w)−logp(w)=− i=1 ∑ N p(y i ∣x i ,w)− j=1 ∑ D p(w log log i=1 ∑ N (− 2 1 log(2πσ −(w,x i )) 2 )− j=1 ∑ D (− 2 1 log(2πτ не зависящие от w члены = 2σ 2 1 i=1 ∑ N (y i −(w,x j=1 ∑ D w j 2 + не зависящие от w члены Получается, что argmin⁡w MAP = w argmin ( 2 1 i=1 ∑ N (y i −(w,x ∥w∥ 2 ) а это же функция потерь для линейной регрессии с",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 5,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": ") Тогда MAP — точка минимума следующего выражения: − log log −logp(y∣X,w)−logp(w)=− i=1 ∑ N p(y i ∣x i ,w)− j=1 ∑ D p(w log log i=1 ∑ N (− 2 1 log(2πσ −(w,x i )) 2 )− j=1 ∑ D (− 2 1 log(2πτ не зависящие от w члены = 2σ 2 1 i=1 ∑ N (y i −(w,x j=1 ∑ D w j 2 + не зависящие от w члены Получается, что argmin⁡w MAP = w argmin ( 2 1 i=1 ∑ N (y i −(w,x ∥w∥ 2 ) а это же функция потерь для линейной регрессии с L 2 L 2 -регуляризацией! Напомним на всякий случай, что у этой задачи есть «точное» решение MAP =(X Для этого примера мы можем вычислить и апостериорное распределение p(w∣X,y). В самом деле, из написанного выше мы можем заключить, что log log log logp(w∣X,y)=log(p(y∣X,w)p(w))−logp(y)= не зависящие от w члены = 2σ 2 1 (y−Xw) T (y−Xw)+ 2τ 2 1 w T w+ не зависящие от w члены Таким образом, log logp(w∣X,y) — это квадратичная функция от w w, откуда следует, что апостериорное распределение является нормальным. Чтобы найти его параметры, нужно немного преобразовать полученное выражение: y−w T X T y−y T Wx+w T X T Xw)+ 2τ 2 1 w T w+const(w)= I)w− Wx+const(w)= (w− w MAP I)(w− w MAP )+const(w)= Таким образом, p(w∣X,y)=N( w MAP Как видим, от априорного распределения оно отличается корректировкой как матожидания MAP , так и ковариационной матрицы . Отметим, что X T X X T X — это, с точностью до численного множителя, оценка ковариационной матрицы признаков нашего датасета (элементы матрицы X T X X T X — это скалярные произведения столбцов X X, то есть столбцов значений признаков). Иллюстрация. Давайте на простом примере (датасет с двумя признаками) посмотрим, как меняется апостериорное распределение w w с ростом размера обучающей выборки: 14 Как видим, не только мода распределения, то есть MAP приближается к своему истинному значению, но и дисперсия распределения постепенно уменьшается. Ещё иллюстрация. Теперь рассмотрим задачу аппроксимации неизвестной функции одной переменной (чьи значения в обучающей выборке искажены нормальным шумом) многочленом третьей степени. Её, разумеется, тоже можно решать, как задачу линейной регрессии на коэффициенты многочлена. Давайте нарисуем, как будут выглядеть функции, сгенерированные из распределения p(w∣X,y) для разного объёма обучающей выборки: 14 Тут тоже видим, что функции не только становятся ближе к истинной, но и разброс их уменьшается. Пример: линейная регрессия с L 1 L 1 -регуляризацией как модель с лапласовским априорным распределением на веса Другое распределение, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения w j w j были правдоподобными, а большие не очень, — распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса. exp p(w)= j=1 ∏ D p(w j )= j=1 ∏ D 2 λ exp(−λ∣w m ∣) Проводя такое же вычисление, получаем, что argmin⁡w MAP = w argmin ( 2 1 i=1 ∑ N (y i −(w,x i )) 2 +λ j=1 ∑ D ∣w j ∣) а это же функция потерь для линейной регрессии с L 1",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 6,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "лапласовским априорным распределением на веса Другое распределение, которое тоже может кодировать наше желание, чтобы небольшие по модулю значения w j w j были правдоподобными, а большие не очень, — распределение Лапласа. Посмотрим, что будет, если его взять в качестве априорного распределения на веса. exp p(w)= j=1 ∏ D p(w j )= j=1 ∏ D 2 λ exp(−λ∣w m ∣) Проводя такое же вычисление, получаем, что argmin⁡w MAP = w argmin ( 2 1 i=1 ∑ N (y i −(w,x i )) 2 +λ j=1 ∑ D ∣w j ∣) а это же функция потерь для линейной регрессии с L 1 L 1 -регуляризацией! Как делать предсказания Все изложенные выше рассуждения проводились в ситуации, когда X=X train — обучающая выборка. Для неё мы можем посчитать p(w∣X train ,y train )= p(y) (y∣X,w)p(w) и точечную апостериорную оценку argmax⁡w MAP = w argmax p(y∣X,w)p(y). А теперь пусть нам дан новый объект ∈X. Какой таргет y 0 y 0 мы для него предскажем? Было бы естественным, раз уж мы предсказываем распределение для w w, и для y 0 y 0 тоже предсказывать распределение. Делается это следующим образом: p(y 0 ∣x 0 ,X train ,y train )=∫p(y 0 ∣x 0 ,w)p(w∣X train ,y train )dw Надо признать, что вычисление этого интеграла не всегда посильная задача, поэтому зачастую приходится «просто подставлять MAP ». В вероятностных терминах это можно описать так: вместо сложного апостериорного распределения p(w∣X train ,y train ) мы берём самое грубое на свете приближение p(w∣X train ,y train )≈δ(w− w MAP ), где δ ( t ) δ(t) — дельта-функция, которая не является честной функцией (а является тем, что математики называют обобщёнными функциями), которая определяется тем свойством, что ∫f(t)δ(t)dt=f(0) для достаточно разумных функций f f. Если не мудрствовать лукаво, то это всё значит, что p(y 0 ∣x 0 ,X train ,y train )≈p(y 0 ∣x 0 , w MAP ) Пример. Пусть y∼Xw+ε, ε∼N(0,σ) 2 — модель линейной регрессии с априорным распределением p(w)=N(0,τ 2 ) на параметры. Тогда, как мы уже видели раньше, p(w∣X,y)=N(w∣ w MAP Попробуем для новой точки x 0 x 0 посчитать распределение на y 0 y 0 . Рекомендуем читателю попробовать самостоятельно посчитать интеграл или же обратиться к пункту 7.6.2 книжки «Machine Learning A Probabilistic Perspective» автора Kevin P. Murphy, убедившись, что p(y 0 ∣x 0 ,X train ,y train )=N(y 0 ∣x 0 w MAP что, очевидно, более содержательно, чем оценка, полученная с помощью приближения p(w∣X train ,y train )≈δ(w− w MAP p(y 0 ∣x 0 , w MAP )=N(y 0 x 0 w MAP ,σ 2 ) Собственно, видно, что в этом случае Пример в примере. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше 3 3 (в которой мы строим модели с =1). Для N = 8 N=8 мы получали такую картинку: 14 Если оценить по приведённым выше формулам p(y 0 ∣x 0 ,X train ,y train ) для разных x 0 x 0 , то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 7,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": ")≈δ(w− w MAP p(y 0 ∣x 0 , w MAP )=N(y 0 x 0 w MAP ,σ 2 ) Собственно, видно, что в этом случае Пример в примере. Рассмотрим полюбившуюся уже нам задачу приближения функции многочленом степени не выше 3 3 (в которой мы строим модели с =1). Для N = 8 N=8 мы получали такую картинку: 14 Если оценить по приведённым выше формулам p(y 0 ∣x 0 ,X train ,y train ) для разных x 0 x 0 , то можно убедиться, что модель в большей степени уверена в предсказаниях для точек из областей, где было больше точек из обучающей выборки: 14 Байесовский подход и дообучение моделей До сих пор мы в основном рассуждали о моделях машинного обучения как о чём-то, что один раз обучается и дальше навсегда застывает в таком виде, но в жизни такое скорее редкость. Мы пока не будем обсуждать изменчивость истинных зависимостей во времени, но даже если истина неизменна, к нам могут поступать новые данные, которые очень хотелось бы использовать для дообучения модели. Обычные, не байесовские вероятностные модели не предоставляют таких инструментов. Оценку максимального правдоподобия придётся пересчитывать заново (хотя, конечно, можно схитрить, использовав старое значение в качестве начального приближения при итеративной оптимизации). Байесовский же подход позволяет оформить дообучения в виде простой и элегантной формулы: при добавлении новых данных N+1 ,y N+1 ),…,(x M ,y M ) имеем p(w∣(x i ,y i ) i=1 M )= p((y i ) i=N+1 M ) p((y i ) i=N+1 M ∣(x i ) i=N+1 M )p(w∣(x i ,y i ) i=1 N ) Байесовский подход к выбору модели: мотивация Нам часто приходится выбирать: дерево или случайный лес, линейная модель или метод ближайших соседей; да, собственно, и внутри наших вероятностных моделей есть параметры (скажем, дисперсия шума ), которые надо бы подбирать. Но как? В обычной ситуации мы выбираем модель, обученную на выборке train ,y train ) в зависимости от того, как она себя ведёт на валидационной выборке val ,y val ) (сравниваем правдоподобие или более сложные метрики) — или же делаем кросс-валидацию. Но как сравнивать модели, выдающие распределение? Ответим вопросом на вопрос: а как вообще сравнивать модели? Назначение любой модели — объяснять мир вокруг нас, и её качество определяется именно тем, насколько хорошо она справляется с этой задачей. Тестовая выборка — это хороший способ оценки, потому что она показывает, насколько вписываются в модель новые данные. Но могут быть и другие соображения, помогающие оценить качество модели. Пример №1 Аналитик Василий опоздал на работу. Своему руководителю он может предложить самые разные объяснения — и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания и потенциально позволяющая руководителю принять решение о том, карать ли Василия. Конечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров: Модель «Василий опоздал, потому что так получилось», то есть факт опоздания — это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 8,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на работу. Своему руководителю он может предложить самые разные объяснения — и это будет выработанная на одном обучающем примере модель, описывающая причины опоздания и потенциально позволяющая руководителю принять решение о том, карать ли Василия. Конечно, руководитель мог бы принять изложенную Василием модель к сведению, подождать, пока появятся другие опоздавшие, и оценить её, так скажем, на тестовой выборке, но стоит ли? Давайте рассмотрим несколько конкретных примеров: Модель «Василий опоздал, потому что так получилось», то есть факт опоздания — это просто ни от чего не зависящая случайная величина. Такая модель плоха тем, что (а) не предлагает, на самом деле, никакого объяснения тому факту, что Василий опоздал, а его коллега Надежда не опоздала и (б) совершенно не помогает решить, наказывать ли за опоздание. Наверное, такое не удовлетворит руководителя. Модель «Василий опоздал, потому что рядом с его домом открылся портал в другой мир, где шла великая битва орков с эльфами, и он почувствовал, что просто обязан принять в ней участие на стороне орков, которых привёл к победе, завоевав руку и сердце орочьей принцессы, после чего был перенесён обратно в наш скучный мир завистливым шаманом». Чем же она плоха? Битва с эльфами — это, безусловно, важное и нужное дело, и на месте руководителя мы бы дружно согласились, что причина уважительная. Но заметим, что в рамках этой модели можно объяснить множество потенциальных исходов, среди которых довольно маловероятным представляется наблюдаемый: тот, в котором Василий не погиб в бою, не остался со своей принцессой и не был порабощён каким-нибудь завистливым шаманом. Отметим и другой недостаток этой модели: её невозможно провалидировать. Если в совершенно случайной модели можно оценить вероятность опоздания и впоследствии, когда накопятся ещё примеры, проверить, правильно ли мы её посчитали, то в мире, где открываются порталы и любой аналитик может завоевать сердце орочьей принцессы, возможно всё, и даже если больше никто не попадёт в такую ситуацию, Василий всё равно сможет бить себя в грудь кулаком и говорить, что он избранный. Так что, наверное, это тоже не очень хорошая модель. Модель «Василий опоздал, потому что проспал» достаточно проста, чтобы в неё поверить, и в то же время даёт руководителю возможность принять решение, что делать с Василием. Пример №2 Обратимся к примеру из машинного обучения. Сравним три модели линейной регрессии: 14 Даже и не запрашивая тестовую выборку, мы можем сделать определённые выводы о качестве этих моделей. Средняя (квадратичная) явно лучше левой (линейной), потому что она лучше объясняет то, что мы видим: тот факт, что облако точек обучающей выборки выглядит вогнутым вниз. А что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой — многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как — многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки: 14 Обратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 9,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вогнутым вниз. А что с правым, почему мы можем утверждать, что он хуже? Есть много причин критиковать его. Остановимся вот на какой. На средней картинке у нас приближение квадратичной функцией, а на правой — многочленом довольно большой степени (на самом деле, десятой). А ради интереса: как выглядит график квадратичной функции и как — многочлена десятой степени со случайно сгенерированными коэффициентами? Давайте сгенерируем несколько и отметим их значения в точках обучающей выборки: 14 Обратите внимание на масштаб на графиках справа. И какова вероятность, что нам достался именно тот многочлен десятой степени, у которого значения в обучающих точках по модулю в пределах сотни? Очевидно, она очень мала. Поэтому мы можем сказать, что выбор в качестве модели многочлена десятой степени не очень обоснован. Попробуем резюмировать Слишком простая модель плохо объясняет наблюдаемые нами данные, тогда как слишком сложная делает это хорошо, но при этом описывает слишком многообразный мир, в котором имеющиеся у нас данные оказываются уже слишком частным случаем. В каком-то смысле наш способ выбора модели оказывается переформулировкой бритвы Оккама: из моделей, пристойно описывающих наблюдаемые явления, следует выбирать наиболее минималистичную. Байесовский подход к выбору модели: формализация Пусть у нас есть некоторое семейство моделей J J и для каждого j ∈ J j∈J задана какая-то своя вероятностная модель. В духе байесовского подхода было бы оценить условное распределение моделей p(j∣y,X)= j∈J ∑ p(j,y∣X) p(y∣X,j)p(j) и в качестве наилучшей модели взять её моду. Если же считать все модели равновероятными, то мы сводим всё к максимизации только лишь p(y∣X,j)=p j (y∣X): ȷ ^ = argmax⁡j argmax⁡j argmax ∫p j (y∣X,w)p j (w)dw= j argmax p j (y∣X) Величина (y∣X) называется обоснованностью (evidence, marginal likelihood) модели. Отметим, что такое определение вполне согласуется с мотивацией из предыдущего подраздела. Слишком простая модель плохо описывает наблюдаемые данные, и потому будет отвергнута. В свою очередь, слишком сложная модель способна описывать гораздо большее многообразие явлений, чем нам было бы достаточно. Таким образом, компромисс между качеством описания и сложностью и даёт нам оптимальную модель. Пример Вернёмся к нашей любимой задаче аппроксимации функции одной переменной многочленом небольшой степени по нескольким точкам, значение в которых было искажено нормальным шумом. Построим несколько моделей, приближающих многочленом степени не выше некоторого d e g deg (будет принимать значения 1, 3 и 6), положив в вероятностной модели =1. Мы не будем приводить полный вывод обоснованности для задачи регрессии p(y∣X,w)=N(y∣Xw,σ 2 I)p(w∣τ 2 I), а сразу выпишем ответ: p(y∣X)=N(0,σ 2 I+τ 2 XX T ) Посмотрим, какой будет обоснованность для разного числа обучающих точек: 14 Можно убедиться, что для регрессии по двум точкам наиболее обоснованная — линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте. Аппроксимация обоснованности и байесовский информационный критерий Точно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой. Напомним, что p(y∣X)=∫p(y∣X,w)p(w)dw Воспользуемся приближением Лапласа, то есть разложим p(y∣X,w) (как функцию от w w) вблизи своего максимума, то есть вблизи MLE в ряд Тейлора: log",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 10,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "обучающих точек: 14 Можно убедиться, что для регрессии по двум точкам наиболее обоснованная — линейная модель (и неудивительно), тогда как с ростом числа точек более обоснованной становится модель с многочленом третьей степени; слишком сложная же модель шестой степени всегда плетётся в хвосте. Аппроксимация обоснованности и байесовский информационный критерий Точно вычислить обоснованность может быть трудной задачей (попробуйте проделать это сами хотя бы для линейной регрессии!). Есть разные способы посчитать её приближённо; мы рассмотрим самый простой. Напомним, что p(y∣X)=∫p(y∣X,w)p(w)dw Воспользуемся приближением Лапласа, то есть разложим p(y∣X,w) (как функцию от w w) вблизи своего максимума, то есть вблизи MLE в ряд Тейлора: log log logp(y∣X,w)≈logp(y∣X, w )− 2 1 (w− )(w− w ), где линейный член отсутствует, поскольку разложение делается в точке локального экстремума, а ) — знакомая нам матрица Фишера log )=−E∇ w 2 logp(y∣X,w)∣ w =NI 1 ( w ). Далее, p ( w ) p(w) мы можем с точностью до второго порядка приблизить MAP ). Получается, что log p(y∣X)≈∫e logp(y∣X, w )− 2 N (w− )(w− w ) p( w MAP )dw= = e log logp(y∣X, w ) p( w MAP )∫e − 2 N (w− )(w− w ) dw= = e log logp(y∣X, w ) p( w MAP )⋅(2π) D/2 N D/2 exp ⁡ ( log log ⁡ N + всякие штуки ) =exp(logp(y∣X, w )− 2 D logN+всякие штуки) Несмотря на то, что MAP ) и , сгруппированные нами во «всякие штуки», существенным образом зависят от модели, при больших N N они вносят в показатель гораздо меньше вклада, чем первые два слагаемых. Таким образом, мы можем себе позволить вместо трудновычисляемых p(y∣X) использовать для сравнения модели байесовский информационный критерий байесовский информационный критерий(BIC): B I C = D log ⁡ N − 2 log BIC=DlogN−2logp(y∣X, w ) Фреквентисты против байесиан: кто кого? Мы с вами познакомились с двумя парадигмами оценивания: фреквентистской (frequentist, от слова \"frequency\", частота) — в которой считается, что данные являются случайным (настоящая случайность!) семплом из некоторого фиксированного распределения, которое мы стараемся оценить по этому семплу, и байесовской — в которой данные считаются данностью и в которой мы используем данные для обновления наших априорных представлений о распределении параметров (здесь случайности нет, а есть лишь нехватка знания). У обеих есть свои достоинства и недостатки, поборники и гонители. К недостаткам байесовской относится, безусловно, её вычислительная сложность: возможно, вы помните, в пучину вычислений сколь мрачных нас низвергла банальная задача линейной регрессии, и дальше становится только ещё трудней. Если мы захотим байесовский подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере. Одно известное свойство оценки максимального правдоподобия — асимптотическая нормальность. Если оценивать наши веса w w по различным наборам из N N обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка MLE тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при N → ∞ N→∞ MLE ∼N(w где w ∗ w ∗ — истинное",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 11,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "подход применять к более сложным моделям, например, нейросетям, нам придётся прибегать к упрощениям, огрублениям, приближениям, что, разумеется, ухудшает наши оценки. Но, если простить ему эту вынужденную неточность, он логичнее и честней, и мы продемонстрируем это на следующем примере. Одно известное свойство оценки максимального правдоподобия — асимптотическая нормальность. Если оценивать наши веса w w по различным наборам из N N обучающих примеров, причём считать, что наборы выбираются случайно (не будем уточнять, как именно), то оценка MLE тоже превращается в случайную величину, которая как-то распределена. Теория утверждает, что при N → ∞ N→∞ MLE ∼N(w где w ∗ w ∗ — истинное значение весов, а ) — матрица информации Фишера, которая определяется как log log )=E[( ∂w i ∂ logp(y∣X,w) logp(y∣X,w) w ∗ )] что при некоторых не слишком обременительных ограничениях равно log )=−E[ logp(y∣X,w) w ∗ ] При этом поскольку log log logp(y∣X,w)=∑ i=1 N logp(y∣X,w), матрица тоже распадается в сумму, и получается, что )=NI 1 (w ∗ ), то есть с ростом N N ковариация (NI 1 (w ∗ )) −1 оценки максимального правдоподобия стремится к нулю. На интуитивном уровне можно сказать, что матрица информации Фишера показывает, сколько информации о весах w w содержится в X X. Поговорим о проблемах. В реальной ситуации мы не знаем w ∗ w ∗ и тем более не можем посчитать матрицу Фишера, то есть мы с самого начала вынуждены лукавить. Ясно, что вместо w ∗ w ∗ можно взять просто w ^ w , а вместо ) — матрицу ), которую можно даже при желании определить как log log(p(y∣X,w)) w ∗ ) безо всякого математического ожидания. Итак, хотя мы можем теперь построить доверительный интервал для оцениваемых параметров, по ходу нами было сделано много упрощений: мы предположили, что асимптотическая оценка распределения уже достигнута, от w ∗ w ∗ перешли к w ^ w , а для полноты чувств ещё и избавились от математического ожидания. В байесовском подходе мы такого себе не позволяем. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.5. Генеративный подход к классификации Как использовать распределение меток классов в задаче классификации. LDA, QDA и наивный байес Следующий параграф 4.7. Модели с латентными переменными",
    "metadata": {
      "title": "Байесовский подход к оцениванию",
      "url": "https://education.yandex.ru/handbook/ml/article/bajesovskij-podhod-k-ocenivaniyu",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.6",
      "part": 12,
      "total_parts": 12,
      "source_file": "4.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Предположим, что мы делаем анализ данных для банка, и нам предоставили данные о годовых зарплатах клиентов. ml В этом графике заметны три моды, которым, наверное, соответствуют три кластера клиентов. Неопытный аналитик мог бы проигнорировать это и попытаться описать график в отчете для руководства с помощью двух чисел — средней зарплаты и стандартного отклонения зарплат. Однако данные с гистограммы ниже имеют точно такое же среднее и стандартное отклонение, как и мультимодальные данные выше. Распределение совсем другое, правда? Очевидно, что графики выглядят совершенно по-разному, и правильная интерпретация первого графика может принести бизнесу дополнительные деньги (скажем, если банк научится предлагать клиентам из каждого кластера более кастомизированные предложения). Так что не надо пытаться описывать мультимодальные данные с помощью унимодальных распределений. ml Проблема заключается в том, что нам неизвестно, к какому кластеру относится каждый клиент, и неизвестны характеристики кластеров — как же их тогда описать? Для каждого кластера можно попытаться задать свои параметры (среднее и дисперсию). Но как определить, из какого кластера конкретный клиент в выборке? Более того, один клиент может, например, «на 0.7» относиться к одному кластеру и «на 0.3» к другому. ml Как решать такую задачу «мягкой» кластеризации («мягкой», потому что один объект может относиться к нескольким кластерам)? Мы могли бы действовать итерационно. Сначала зададим начальное приближение на параметры распределений. Например, в нашем случае с клиентами банка из графика можно предположить, что среднее для первой кластера 30, для второго — 40, для третьей — 50, а стандартные отклонения у всех равняются, скажем, 10. Зная эти начальные параметры, мы можем для каждого клиента посчитать степень принадлежности к каждому из трёх кластеров (важно не забыть отнормировать эти числа, чтобы их сумма действительно равнялась единице). Дальше мы бы могли пересчитать наши средние и дисперсии, «взвешивая» вклад объектов пропорционально степени их принадлежности к каждому кластеру, и таким образом уточнить средние и дисперсии для всех трёх кластеров. Повторяя эти два шага последовательно, мы получили бы средние и дисперсии кластеров, а для каждого объекта — степени принадлежности к кластерам. Это — EM-алгоритм, подробнее о нём мы поговорим ниже. Кстати, оказывается, что и метод кластеризации K-средних во многом сродни EM-алгоритму (и на самом деле представляет собой его предельный случай). Действительно, мы сначала случайно расставляем центры кластеров. Затем мы для каждого объекта пересчитываем расстояние до центра каждого кластера, после чего получаем «вес» объекта в каждом кластере («вес» в том смысле, что чем ближе объект к центру кластера, тем больше этот объект учитывается при пересчете центра этого кластера) через нормировку расстояний. Теперь, если мы применяем настоящий метод K-средних, то приписываем объект к кластеру с самым большим «весом», после чего опять пересчитываем центры кластеров и потом опять измеряем вес для каждого объекта кластера. Строгое же применение EM-алгоритма даёт «мягкую» версию метода K-средних. Смеси распределений Говорят, что распределение p ( x ) p(x) является смесью распределений, если его плотность имеет вид p(x)= k=1 (x), k=1 ∑ K π k =1,π k ≥0, где: K K — число компонент; π k π k — вероятности компонент; (x) — функции правдоподобия, то есть функции вероятности компонент (в дискретном случае) или их плотности (в абсолютно непрерывном случае).",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 1,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "если мы применяем настоящий метод K-средних, то приписываем объект к кластеру с самым большим «весом», после чего опять пересчитываем центры кластеров и потом опять измеряем вес для каждого объекта кластера. Строгое же применение EM-алгоритма даёт «мягкую» версию метода K-средних. Смеси распределений Говорят, что распределение p ( x ) p(x) является смесью распределений, если его плотность имеет вид p(x)= k=1 (x), k=1 ∑ K π k =1,π k ≥0, где: K K — число компонент; π k π k — вероятности компонент; (x) — функции правдоподобия, то есть функции вероятности компонент (в дискретном случае) или их плотности (в абсолютно непрерывном случае). Проиллюстрируем это понятие на примере с банком. Будем считать, что распределения компонент смеси принадлежат некоторому параметрическому семейству: (x)=ϕ(x∣θ k ) (например, гауссовскому с параметром =(μ k ,σ k )). Мы можем говорить, что каждое из распределений (x) задаёт свой кластер, причём каждый кластер имеет некоторую априорную вероятность π k π k . Если у нас нет дополнительных данных, разумно положить Если же нам, к примеру, известно, что какой-нибудь кластер описывает сравнительно малочисленную группу людей, эти вероятности окажутся различными. Таким образом, мы проинтерпретировали нашу мягкую кластеризацию в терминах смеси распределений. Как генерировать из смеси распределений Рассмотрим следующий эксперимент: сначала из дискретного распределения ,…,π K } выбирается номер k k, а затем из распределения ϕ(x∣θ k ) выбирается значение x x. Покажем, что распределение переменной x x будет представлять собой смесь. Введем скрытую переменную z z, отвечающую за то, к какой компоненте смеси будет относиться очередной x x. Пусть она представляет собой K K-мерный бинарный случайный вектор, ровно одна компонента которого равна единице: z∈{0,1} K , k=1 ∑ K z k =1. Вероятность того, что единице будет равна k k-я компонента, положим равной p(z k =1)=π k . Запишем распределение сразу всего вектора: p(z)= k=1 Теперь, когда номер компоненты смеси известен, сгенерируем x x из распределения ϕ(x∣θ p(x∣z k =1)=ϕ(x∣θ k ), или, что то же самое, p(x∣z)= k=1 ∏ K [ϕ(x∣θ k )] z k . Проверим, что x x имеет нужное нам распределение. Запишем совместное распределение переменных x x и p(x,z)=p(z)p(x∣z)= k=1 ∏ K [π k ϕ(x∣θ k )] z k . Чтобы найти распределение переменной x x, нужно избавиться от скрытой переменной: p(x)= z ∑ p(x,z). Суммирование здесь ведется по всем возможным значениям z z, то есть по всем K K-мерным бинарным векторам с одной единицей: p(x)= z ∑ p(x,z)= k=1 ∑ K π k ϕ(x∣θ k ). Мы получили, что распределение сгенерированной переменной x x в описанном эксперименте представляет собой смесь K K компонент. Модели со скрытыми переменными Рассмотрим вероятностную модель с наблюдаемыми переменными X X и параметрами Θ Θ, для которой задано правдоподобие log logp(X∣Θ). Предположим, что в модели также существуют скрытые переменные Z Z, описывающие её внутреннее состояние и, возможно, недоступные для непосредственного наблюдения (как то, к какому из кластеров относится клиент). Тогда правдоподобие log logp(X∣Θ) называется неполным, а правдоподобие log logp(X,Z∣Θ) — полным. Они связаны соотношением log log logp(X∣Θ)=log{ Z ∑ p(X,Z∣Θ)}. Нашей основной целью будет создать хорошую модель X X, то есть оценить параметры",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 2,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "получили, что распределение сгенерированной переменной x x в описанном эксперименте представляет собой смесь K K компонент. Модели со скрытыми переменными Рассмотрим вероятностную модель с наблюдаемыми переменными X X и параметрами Θ Θ, для которой задано правдоподобие log logp(X∣Θ). Предположим, что в модели также существуют скрытые переменные Z Z, описывающие её внутреннее состояние и, возможно, недоступные для непосредственного наблюдения (как то, к какому из кластеров относится клиент). Тогда правдоподобие log logp(X∣Θ) называется неполным, а правдоподобие log logp(X,Z∣Θ) — полным. Они связаны соотношением log log logp(X∣Θ)=log{ Z ∑ p(X,Z∣Θ)}. Нашей основной целью будет создать хорошую модель X X, то есть оценить параметры Θ Θ. И оказывается, что с помощью введения скрытых переменных нередко удаётся существенно упростить правдоподобие и эффективно решить задачу. Рассмотрим пример со смесями распределений. В качестве наблюдаемых переменных здесь выступает выборка X={x 1 ,…,x ℓ }, в качестве скрытых переменных Z={z 1 ,…,z ℓ } — номера компонент, из которых сгенерированы объекты (здесь каждый из K-мерный вектор), в качестве параметров — априорные вероятности и параметры компонент Θ=(π 1 ,…,π K ,θ 1 ,…,θ K ). Неполное правдоподобие выглядит так: log log logp(X∣Θ)= i=1 ∑ l log{ k=1 ∑ K π k p(x i ∣θ k )}. Правдоподобие здесь имеет вид логарифма суммы. Если приравнять нулю его градиент, то получатся сложные уравнения, не имеющие аналитического решения. Данное правдоподобие сложно вычислять: оно не является выпуклым (а точнее, вогнутым) и может иметь много локальных экстремумов, поэтому применение обычных итерационных методов для его непосредственной максимизации приводит к медленной сходимости. Рассмотрим теперь полное правдоподобие: log log ⁡ π k + log logp(X,Z∣Θ)= i=1 ∑ l p(x i ,z i ∣Θ)= i=1 ∑ l k=1 ∑ K z ik {logπ k +logϕ(x i ∣θ k )}. Оно имеет вид суммы логарифмов, и это позволяет аналитически найти оценки максимального правдоподобия на параметры Θ Θ при известных переменных X X и Z Z. В общем случае Z Z также стараются выбирать таким способом, чтобы распределение p(X,Z∣Θ) оказалось «лучше» исходного. В каком именно смысле, мы увидим дальше. Проблема, впрочем, заключается в том, что нам не известны скрытые переменные Z Z, поэтому их необходимо оценивать одновременно с параметрами, что никак не легче максимизации неполного правдоподобия. Осуществить это позволяет EM-алгоритм. EM-алгоритм EM-алгоритм решает задачу максимизации полного правдоподобия путём попеременной оптимизации по параметрам и по скрытым переменным. Опишем сначала наивный способ оптимизации. Зафиксируем некоторое начальное приближение для параметров Θ old Θ old . При известных наблюдаемых переменных X X и параметрах Θ old Θ old мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения: Z ∗ = argmax⁡Z old ) = argmax⁡Z old argmax p(Z∣X,Θ old )= Z argmax p(X,Z∣Θ old ). Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров: Θ new = argmax⁡Θ new = Θ argmax p(X,Z ∗ ∣Θ). Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров Θ * Θ * . Данная процедура, однако, далека от идеальной — и ниже мы предложим решение, которое приводит к более качественным результатам. Воспользуемся байесовским подходом. Точечные оценки параметров несут меньше информации, чем их распределение; учтём это",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 3,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "old Θ old мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения: Z ∗ = argmax⁡Z old ) = argmax⁡Z old argmax p(Z∣X,Θ old )= Z argmax p(X,Z∣Θ old ). Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров: Θ new = argmax⁡Θ new = Θ argmax p(X,Z ∗ ∣Θ). Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров Θ * Θ * . Данная процедура, однако, далека от идеальной — и ниже мы предложим решение, которое приводит к более качественным результатам. Воспользуемся байесовским подходом. Точечные оценки параметров несут меньше информации, чем их распределение; учтём это и будем оптимизировать не Z Z, а условное распределение Z Z. Как и прежде, зафиксируем вектор параметров Θ old Θ old , но вместо точечной оценки вычислим апостериорное распределение на скрытых переменных old ) p(Z∣X,Θ old ), которое будет в некотором смысле оптимальным образом описывать распределение Z Z при известных X X и Θ Θ. В этом заключается E-шаг EM-алгоритма. Отметим, что вычислить p(Z∣XΘ) аналитически возможно не для всех распределений, и скрытые переменные стоит подбирать так, чтобы это всё-таки получилось. Теперь мы должны произвести оптимизацию по Θ Θ. Для этого возьмём логарифм полного правдоподобия log logp(X,Z∣Θ) и усредним его по всем возможным значениям скрытых переменных old old ) log old ) log Q(Θ,Θ old )=E Z∼p(Z∣X,Θ old ) logp(X,Z∣Θ)= Z ∑ p(Z∣X,Θ old )logp(X,Z∣Θ) Формально говоря, мы нашли матожидание логарифма полного правдоподобия по апостериорному распределению на скрытых переменных. На M-шаге новый вектор параметров находится как максимизатор данного матожидания: Θ new = argmax⁡Θ Q ( Θ , Θ old ) = argmax⁡Θ old ) log new = Θ argmax Q(Θ,Θ old )= Θ argmax Z ∑ p(Z∣X,Θ old )logp(X,Z∣Θ). EM-алгоритм состоит в чередовании E-шага и M-шага. Можно показать, что такой итерационной процесс всегда не уменьшает правдоподобие и сходится. Жёсткий EM-алгоритм Не всегда получается подобрать латентные переменные Z Z так, чтобы p(Z∣X,Θ) можно было выразить аналитически, то есть на E-шаге не удаётся минимизировать L(q,Θ) по q q. В такой ситуации иногда приходится брать оптимум не по всему пространству распределений, а только по некоторому семейству — например, параметрическому, в котором оптимизацию можно проводить градиентными методами. В максимально упрощённой ситуации мы возьмём семейство дельта-функций, то есть вместо распределения на Z Z будем брать просто точечную оценку. Такая модификация EM-алгоритма называется жёстким EM-алгоритмом. В начале параграфа мы упоминали кластеризацию методом K-средних и отмечали, что EM-алгоритм даёт «мягкую» версию алгоритма: на E-шаге мы не приписываем однозначно точку к какому-то из кластеров (то есть не берём точечную оценку скрытой переменной «номер кластера, к которому принадлежит точка»), а сопоставляем ей вероятности принадлежности каждому из кластеров (то есть распределение на скрытые переменные). Настоящий метод K-средних как раз таки соответствует жёсткому EM-алгоритму. Разделение смеси гауссиан Пусть теперь нам известно, что N N точек были семплированы из K разных гауссовских распределений и нам неизвестно, какая точка из какого распределения пришла в выборку. Нам нужно оценить параметры ( ) для первого распределения, ( ) для второго и, соответственно, ( ) для k k-го распределения. Если мы знаем, что точка x i x",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 4,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "точку к какому-то из кластеров (то есть не берём точечную оценку скрытой переменной «номер кластера, к которому принадлежит точка»), а сопоставляем ей вероятности принадлежности каждому из кластеров (то есть распределение на скрытые переменные). Настоящий метод K-средних как раз таки соответствует жёсткому EM-алгоритму. Разделение смеси гауссиан Пусть теперь нам известно, что N N точек были семплированы из K разных гауссовских распределений и нам неизвестно, какая точка из какого распределения пришла в выборку. Нам нужно оценить параметры ( ) для первого распределения, ( ) для второго и, соответственно, ( ) для k k-го распределения. Если мы знаем, что точка x i x i пришла из распределения z i z i , то её правдоподобие в равно: exp p(x i ∣z i ,θ)= 2π σ z i 1 exp(− Напомним, что z i z i — это латентная переменная, обозначающая номер гауссианы (от 1 до K K), из которой была просемплирована точка x i x i . Например, если точка x i x i просемплирована из распределения с номером 3, то в формулу вместо ( ) нужно подставлять Воспользуемся EM-алгоритмом для нахождения параметров ( ). Сначала инициализируем их: Θ old = μ 1 _ old , σ 1 _ old , μ 2 _ old , σ 2 _ old old , σ K _ old Θ old =μ 1_old ,σ 1_old ,μ 2_old ,σ 2_old ,…,μ K_old ,σ K_old Зная Θ _ old Θ_old, выполним E-шаг: нужно найти p(Z∣X,Θ) или что то же самое, для каждого объекта x i x i найти распределение на вероятности old ) p(z i =k∣x i ,Θ_old). Как найти апостериорную вероятность old ) p(z i =k∣x i ,Θ_old), если мы знаем x i x i и у нас есть приближение Θ _ old Θ_old? Ответ — по формуле Байеса: old old old old old p(z i =k∣x i ,Θ old )= p(x i ∣Θ old ) p(x i ∣z i =k,Θ old )⋅p(z i =k) = ∑ k=1 K p(x i ∣z i =k,Θ old )⋅p(z i =k) p(x i ∣z i =k,Θ old )⋅p(z i =k) где p(z i =k) — априорная вероятность того, что объекта x i x i получен из распределения с номером k k. На первом шаге априорную вероятность можно положить равной p(z i =k)= K 1 для всех гауссиан. Введём обозначение old old exp old ) 2 2 σ k old 2 ) u ik :=p(x i ∣z i =k,Θ old )= (2π) σ k old 1 exp(− 2σ k old 2 (x i −μ k old ) 2 ) — правдоподобие того, что объект x i x i пришел из нормального распределения с параметрами ( μ k old , σ k old 2 ) (μ k old ,σ k old 2 ). Тогда по формуле Байеса: old p(z i =k∣x i ,θ old )= ∑ k=1 Вот так для каждого объекта x i x i по начальному приближению θ old θ old мы посчитаем распределение p ( z i ) p(z i ) — с какими вероятностями объект x i x",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 5,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(2π) σ k old 1 exp(− 2σ k old 2 (x i −μ k old ) 2 ) — правдоподобие того, что объект x i x i пришел из нормального распределения с параметрами ( μ k old , σ k old 2 ) (μ k old ,σ k old 2 ). Тогда по формуле Байеса: old p(z i =k∣x i ,θ old )= ∑ k=1 Вот так для каждого объекта x i x i по начальному приближению θ old θ old мы посчитаем распределение p ( z i ) p(z i ) — с какими вероятностями объект x i x i приходит из той или иной компоненты смеси. Теперь выведем формулы для М-шага. θ = argmax⁡Θ E q ( Z ) log argmax⁡Θ log argmax E q(Z) logp(X∣Z,Θ)= Θ argmax i=1 ∑ N k=1 ∑ K p(z i =k∣x i ,Θ)⋅logp(x i ∣z i =k,Θ) log log i=1 ∑ N k=1 ∑ K p(z i =k∣x i ,Θ)⋅(log logσ k 2 )+const Запишем производную и приравняем к 0 0, чтобы найти экстремум: log q(Z) logp(X∣Z,Θ) =− i=1 ∑ N p(z i =k∣x i ,Θ)⋅ Отсюда i=1 N p(z i =k∣x i ,θ) ∑ i=1 N p(z i =k∣x i ,θ)⋅x i Мы получили конечную формулу для пересчета и предыдущему значению θ θ. Причем у этой формулы есть простая интерпретация — каждый объект мы взвешиваем с его вероятностью принадлежности к этому классу p(z i =k∣x,θ). Теперь посчитаем производную по (обратите внимание, что именно по квадрату log q(Z) logp(X∣Z,Θ) = i=1 ∑ N p(z i =k∣x i ,θ)⋅( )=0 Стало быть, i=1 N p(z i =k∣x i ,Θ) ∑ i=1 N p(z i =k∣x i ,Θ)(x i −μ k ) 2 Мы снова получили интерпретируемый результат: подсчитывая дисперсию для k k-ой гауссианы, мы учитываем вес каждого объекта при подсчете среднеквадратичноого отклонения. То есть веса — вероятности происхождения из той или иной компоненты смеси. Сравните эту формулу с формулой для подсчета выборочной дисперсии, где каждый из N N объектов вносит одинаковый вклад в дисперсию с весом −μ) 2 Вы можете «пощупать» EM-алгоритм в задаче разделения вероятностной смеси с помощью интерактивной визуализации — попробуйте сделать E и M шаги и последить за изменениями параметров: после одной итераций алгоритма можно выбрать точку на графике и наблюдать за вероятностью её принадлежности к разным кластерам. Вероятностный PCA Теперь давайте рассмотрим простой пример того, как введение латентных переменных может помочь выделять новые информативные признаки в данных. Предположим, что мы имеем выборку данных x i x i (вектор-строку), где каждый объект имеет D D признаков (предположим, что число D D очень большое). Это достаточно типичная ситуация, например, при работе с текстами или изображениями. Теперь введём следующую вероятностную модель +ε, где z i z i — латентный вектор-строка меньшей размерности T T, а ε∼N(0,σ 2 E), где E E — единичная матрица размером D × D D×D, σ σ — скаляр больший 0. Что означает эта модель? Она означает, что наши сложные многоразмерные данные x i x i могут иметь более простое малоразмерное представление z i z i , а",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 6,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "мы имеем выборку данных x i x i (вектор-строку), где каждый объект имеет D D признаков (предположим, что число D D очень большое). Это достаточно типичная ситуация, например, при работе с текстами или изображениями. Теперь введём следующую вероятностную модель +ε, где z i z i — латентный вектор-строка меньшей размерности T T, а ε∼N(0,σ 2 E), где E E — единичная матрица размером D × D D×D, σ σ — скаляр больший 0. Что означает эта модель? Она означает, что наши сложные многоразмерные данные x i x i могут иметь более простое малоразмерное представление z i z i , а отображение линейно с точностью до нормально распределенного шума. Заметим, что так как ε∼N(0,σ 2 E), отсюда следует, что ∼N(z i W T ,σ 2 E). Зададим априорное распределение на z i z i как стандартное нормальное ∼N(0,E) и распишем совместное распределение ) через условное и априорное: p(x i ,z i ∣W,σ)=p(x i ∣z i ,W,σ)p(z i ) Чтобы восстановить параметры W W, σ σ и латентные переменные z i z i , снова воспользуемся EM-алгоритмом. На E-шаге мы оцениваем распределение на z i z i при фиксированных W W и σ σ: По формуле Байеса распределение на z i z i при условии p(z i ∣x i ,W,σ)= p(x i ∣W,σ) p(x i ∣z i ,W,σ)⋅p(z i ) С точностью до констант и слагаемых, которые не зависят от z i z i , логарифм правдоподобия равен: log logp(z i ∣x i ,W,σ)∼− )(x log logp(z i ∣x i ,W,σ)∼− log logp(z i ∣x i ,W,σ)∼− W+σ 2 E)z Обозначим M:=W T W+σ 2 E, тогда log logp(z i ∣x i ,W,σ)∼− ⋅(σ log logp(z i ∣x i ,W,σ)∼− )⋅(σ 2 M) −1 ⋅(z Если теперь взять от этого экспоненту, увидим, что p(z i ∣x i ,W,σ)∼N(x i WM −1 ,σ 2 M). M-шаг. Теперь мы оптимизировать по W W и log log min ⁡ W , σ E p(Z∣X,W,σ) logp(X,Z∣W,σ)= i ∑ n E p(z i ∣x i ,W,σ) logp(x i ,z i ∣W,σ)→ W,σ min Приравняв производные к 0 0, можно найти: W new new =( i ∑ n (Ez i )x i T )⋅( i ∑ n E(z new new new T W new new [∣∣x i ∣∣ 2 −2x i W new Ez i T +tr(W new T W new E(z i T z i ))] Вероятностный PCA хорош тем, что: как любая байесовская модель, может служить промежуточным участком в более сложной вероятностной модели; если в данных есть пропуски, то вероятностный PCA легко обобщается и на этот случай с добавлением дополнительных скрытых переменных; так как параметры W , σ W,σ и оценки на z i z i получаются через итерационный EM-алгоритм, то вероятностный PCA может быть вычислительно эффективнее. Так, в вычислениях и промежуточных формулах нигде не используется матрица X∈R D×D , и все рассматриваемые матрицы имеют меньший размер. Связь с обычным PCA Как вероятностный PCA связан с обычным, который мы изучили в теме про разложение матриц? Напомним, что в обычном SVD-разложении мы полагали, что . Давайте опять",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 7,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "промежуточным участком в более сложной вероятностной модели; если в данных есть пропуски, то вероятностный PCA легко обобщается и на этот случай с добавлением дополнительных скрытых переменных; так как параметры W , σ W,σ и оценки на z i z i получаются через итерационный EM-алгоритм, то вероятностный PCA может быть вычислительно эффективнее. Так, в вычислениях и промежуточных формулах нигде не используется матрица X∈R D×D , и все рассматриваемые матрицы имеют меньший размер. Связь с обычным PCA Как вероятностный PCA связан с обычным, который мы изучили в теме про разложение матриц? Напомним, что в обычном SVD-разложении мы полагали, что . Давайте опять положим, что разница между есть гауссовский шум с нулевым средним ε∼N(0,σ 2 E): или =N(z Если зададим априорное распределение на z i z i как стандартное нормальное p(z i )∼N(0,E), тогда ∼N(0, ) и соответственно ∼N(0, E). Теперь сделаем обратную замену и убедимся, что оценка максимального правдоподобия в точности равна log log ⁡ det logp(x i ∣W,σ)=− 2 N logdet(W T W+σ 2 E)− W+σ +const (напомним, что это вектор-строки). Заметим, что число есть след матрицы, состоящей из этого числа, поэтому можно преобразовать вторую часть, как W+σ tr( W+σ tr((W T W+σ ))= tr((W T W+σ 2 E) −1 ⋅XX T ) Отсюда следует, что log log ⁡ det logp(x i ∣W,σ)=− 2 N logdet(W T W+σ 2 E)− 2 1 tr((W T W+σ 2 E) −1 ⋅XX T ) Приравняв производную по W W к нулю, найдем: Оценка максимума правдоподобия на D−T 1 j=T+1 Эту оценку можно интерпретировать как среднюю потерю дисперсии по всем проигнорированным сингулярным направлениям. Если же σ 2 σ 2 — константа, то при σ → 0 σ→0 получаем обычный PCA. Другой способ получить обычный PCA — это вместо обычного EM-алгоритма воспользоваться его жёсткой модификацией. Теперь предлагаем вам потренировать изученный материал на практике. Предлагаем вам выполнить лабораторную работу, которая покрывает большинство тем главы “Вероятностные модели”. Скачайте ноутбук с лабораторной работой. В нём вы найдете описания заданий и дополнительные материалы. Задания из лабораторной прикреплены к этому параграфу в виде задач в системе Яндекс Контест. Чтобы проверить себя, отправляйте решения по соответствующим задачам в систему. Успехов в практике! Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Выполните задачи урока 0 / 9 выполнено Сообщить об ошибке Предыдущий параграф 4.6. Байесовский подход к оцениванию Байесовская статистика. Априорные и апостериорные распределения на параметры моделей. MAP-оценки. Байесовский подход к выбору моделей. Байесовский подход для задачи линейной регресии Следующий параграф 5.1. Нейронные сети Краткий путеводитель по разделу",
    "metadata": {
      "title": "Модели с латентными переменными",
      "url": "https://education.yandex.ru/handbook/ml/article/modeli-s-latentnymi-peremennymi",
      "course": "ml",
      "chapter": "4. Вероятностные модели",
      "chapter_id": "4.7",
      "part": 8,
      "total_parts": 8,
      "source_file": "4.7.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе вы познакомитесь с нейронными сетями — семейством моделей, которое начиная с 2012-го постепенно добивается превосходства во всё новых и новых приложениях, во многих став де-факто стандартом. Они были придуманы ещё в 70-х, но техническая возможность и понимание того, как обучать нейросети большого размера, появились лишь примерно к 2011 году, и это дало мощный толчок к их развитию. Совокупность нейросетевых подходов и сама наука о нейросетях носит название глубинного обучения или deep learning. Глубинное обучение основано на двух идеях. Во-первых, это стремление к переходу от построения сложных пайплайнов, каждая компонента которых тренируется сама по себе решать кусочек задачи, к end-to-end обучению всей системы, как одного целого. Так, мы можем обучать не каждый слой отдельно, а все вместе, и не учить какие-нибудь линейные модели поверх случайных лесов, а работать с одной цельной моделью Во-вторых, это обучение представлений объектов — информативных признаковых описаний, учитывающих структуру данных, построенных лишь на основе самих данных, зачастую неразмеченных. Это позволяет автоматизировать процесс отбора признаков: теперь вместо того, чтобы руками экспертов искать «более информативное» или «более простое» признаковое описание наших объектов, мы можем получить их автоматически, притом используя не только данные, доступные нам для задачи, но и, к примеру, все тексты мира. Обучению представлений будет посвящён отдельный параграф, а в этом мы постараемся убедить вас, что нейросети — это гибкий инструмент для решения самых разных задач и для работы с самыми разными типами данных. Надо признать, впрочем, что современные модели весьма сложны и мало напоминают своих элегантных предшественников из 2012 года. Развитие нейросетей во многом мотивируется нуждами и возможностями индустрии, ростом производительности компьютеров и объёмов доступных данных. При этом теория отчаянно не поспевает за практикой. В глубинном обучении весьма распространён чисто инженерный подход к построению алгоритмов, изобилие деталей, основанных на интуиции и обосновывающихся фразой «просто потому, что так работает, а иначе — нет», поэтому ряд учёных продолжает относиться к нейросетям скептически. Однако результаты, достигнутые с их помощью за последние 10 лет, столь впечатляющие, что их нельзя игнорировать. Особенно существенный прогресс был достигнут в области анализа данных, обладающих некоторой внутренней структурой: текстов, изображений, видео, облаков точек, графов и так далее. Тем не менее, есть и подходы к теоретическому осмыслению того, почему нейросети работают так хорошо, и мы познакомим вас с ними в отдельном параграфе, посвящённой теории машинного обучения. Но довольно предисловий! Давайте набросаем план нашего вторжения в мир глубинного обучения: Первое знакомство с полносвязными нейросетями. Вы впервые встретитесь с самой простой нейросетевой архитектурой, узнаете, как строятся и как применяются нейронные сети. Обратное распространение ошибки. Вы узнаете, как легко и быстро дифференцировать по параметрам сложного вычислительного графа, после чего будете (теоретически) понимать, как обучить несложную нейросеть. Тонкости обучения. Нейросети — могущественный, но капризный инструмент, и чем сложнее глубинная модель, тем труднее обучить её. В этом разделе мы начнём знакомить вас с разными приёмами, которые позволяют повысить вероятность успеха, а также с регуляризационными техниками для нейросетей. Свёрточные нейросети. Классический пример структурированных данных — это изображения. В этом параграфе вы познакомитесь с базовым инструментарием для работы с ними — свёртками, пулингом и со свёрточными сетями в целом. В первых",
    "metadata": {
      "title": "Нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejronnye-seti",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.1",
      "part": 1,
      "total_parts": 2,
      "source_file": "5.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и как применяются нейронные сети. Обратное распространение ошибки. Вы узнаете, как легко и быстро дифференцировать по параметрам сложного вычислительного графа, после чего будете (теоретически) понимать, как обучить несложную нейросеть. Тонкости обучения. Нейросети — могущественный, но капризный инструмент, и чем сложнее глубинная модель, тем труднее обучить её. В этом разделе мы начнём знакомить вас с разными приёмами, которые позволяют повысить вероятность успеха, а также с регуляризационными техниками для нейросетей. Свёрточные нейросети. Классический пример структурированных данных — это изображения. В этом параграфе вы познакомитесь с базовым инструментарием для работы с ними — свёртками, пулингом и со свёрточными сетями в целом. В первых четырёх главах вы познали основы глубинного обучения, но в основном имели дело с ситуацией, когда и данные, и выходы представляют из себя непритязательные векторы. А что делать, если мы должны работать с чем-то более сложно устроенным? Оказывается, за счёт своей гибкости и возможности сочетать в себе самые разные компоненты нейросети отлично справляются с данными, имеющими однородную структуру (изображениями, текстами, облаками точек … …). Для работы с каждым из этих типов данных требуются специфические инженерные решения, и мы постараемся познакомить вас с ними, разверзнув перед вами всю бездну способностей нейросетей! Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 4.7. Модели с латентными переменными Следующий параграф 5.2. Первое знакомство с полносвязными нейросетями Основные понятия глубинного обучения. Базовые слои и функции активации",
    "metadata": {
      "title": "Нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejronnye-seti",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.1",
      "part": 2,
      "total_parts": 2,
      "source_file": "5.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Основные понятия глубинного обучения. Базовые слои и функции активации Основные определения Искусственная нейронная сеть (далее — нейронная сеть) — это сложная дифференцируемая функция, задающая отображение из исходного признакового пространства в пространство ответов, все параметры которой могут настраиваться одновременно и взаимосвязанно (то есть сеть может обучаться end-to-end). В частном (и наиболее частом) случае представляет собой последовательность дифференцируемых параметрических преобразований. Внимательный читатель может заметить, что под указанное выше определение нейронной сети подходят и логистическая, и линейная регрессия. Это верное замечание: и линейная, и логистическая регрессии могут рассматриваться как нейронные сети, задающие отображения в пространство ответов и логитов соответственно. Сложную функцию удобно представлять в виде суперпозиции простых, и нейронные сети обычно предстают перед программистом в виде конструктора, состоящего из более-менее простых блоков (слоёв, layers). Вот две простейшие их разновидности: Линейный слой (linear layer, dense layer) — линейное преобразование над входящими данными. Его обучаемые параметры — это матрица W W и вектор x↦xW+b ( W∈R d×k ,x∈R d ,b∈R k ). Такой слой преобразует d d-мерные векторы в k k-мерные. Функция активации (activation function) — нелинейное преобразование, поэлементно применяющееся к пришедшим на вход данным. Благодаря функциям активации нейронные сети способны порождать более информативные признаковые описания, преобразуя данные нелинейным образом. Может использоваться, например, ReLU (rectified linear unit) ReLU ( x ) = max ( 0 , x ) ReLU(x)=max(0,x) или уже знакомая вам из логистической регрессии сигмоида σ(x)= 1+e −x 1 . К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее. Даже самые сложные нейронные сети обычно собираются из относительно простых блоков, подобных этим. Таким образом, их можно представить в виде вычислительного графа (computational graph), где промежуточным вершинам соответствуют преобразования. На иллюстрации ниже приведён вычислительный граф для логистической регрессии. 16 Не правда ли, похоже на слоёный пирог из преобразований? Отсюда и слои. Графы могут быть и более сложными, в том числе нелинейными: 16 Давайте разберёмся, что тут происходит. Input — это вход нейросети, который получает исходные данные. Обычно требуется, чтобы они имели вид матрицы («объекты-признаки») или тензора (многомерной матрицы). Вообще говоря, входов может быть несколько: например, мы можем подавать в нейросеть картинку и какие-нибудь ещё сведения о ней — преобразовывать их мы будем по-разному, поэтому логично предусмотреть два входа в графе. Дальше к исходным данным X 0 X 0 применяются два линейных слоя, которые превращают их в промежуточные (внутренние, скрытые) представления . В литературе они также называются активациями (не путайте с функциями активации). Каждое из представлений подвергается нелинейному преобразованию, превращаясь в новые промежуточные представления соответственно. Переход от X 0 X 0 к двум новым матрицам (или тензорам) можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных. Затем представления конкатенируются (то есть признаковые описания всех объектов объединяются). Дальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на выход сети, то есть отдаётся обратно пользователю. Нейросеть, в которой есть только линейные слои и различные функции активации, называют полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP). Посмотрим, что происходит с размерностями, если на вход подаётся матрица N × d N×d: 16 А вот",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 1,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "промежуточные представления соответственно. Переход от X 0 X 0 к двум новым матрицам (или тензорам) можно рассматривать как построение двух новых (возможно, более информативных) признаковых описаний исходных данных. Затем представления конкатенируются (то есть признаковые описания всех объектов объединяются). Дальше следует ещё один линейный слой и ещё одна активация, и полученный результат попадает на выход сети, то есть отдаётся обратно пользователю. Нейросеть, в которой есть только линейные слои и различные функции активации, называют полносвязной (fully connected) нейронной сетью или многослойным перцептроном (multilayer perceptron, MLP). Посмотрим, что происходит с размерностями, если на вход подаётся матрица N × d N×d: 16 А вот и настоящий пример из реальной жизни. GoogLeNet (она же Inception-v1), показавшая SotA-результат на ILSVRC 2014 (ImageNet challenge), выглядит так: 16 Здесь каждый кирпичик — это некоторое относительно простое преобразование, а белым помечены входы и выходы вычислительного графа. Современные же сети часто выглядят и ещё сложней, но всё равно они собираются из достаточно простых кирпичиков-слоёв. Примечание. Впрочем, в общем случае нейронная сеть — это просто некоторая сложная функция (или, что эквивалентно, граф вычислений). Поэтому в некоторых (очень нетривиальных) случаях нет смысла разбивать её на слои. В качестве иллюстрации ниже приведены структуры агностических нейронных сетей WANN, представленных в работе Weight Agnostic Neural Networks, NeurIPS 2019. 16 Forward & backward propagation Информация может течь по графу в двух направлениях. Применение нейронной сети к данным (вычисление выхода по заданному входу) часто называют прямым проходом, или же forward propagation (forward pass). На этом этапе происходит преобразование исходного представления данных в целевое и последовательно строятся промежуточные (внутренние) представления данных — результаты применения слоёв к предыдущим представлениям. Именно поэтому проход называют прямым. 16 При обратном проходе, или же backward propagation (backward pass), информация (обычно об ошибке предсказания целевого представления) движется от финального представления (а чаще даже от функции потерь) к исходному через все преобразования. Механизм обратного распространения ошибки, играющий важнейшую роль в обучении нейронных сетей, как раз предполагает обратное движение по вычислительному графу сети. С ним вы познакомитесь в следующем параграфе. 16 Архитектуры для простейших задач Как мы уже упоминали выше, нейросети — это универсальный конструктор, который из простых блоков позволяет собрать орудия для решения самых разных задач. Давайте посмотрим на конкретные примеры. Безусловно, мир намного разнообразнее того, что мы покажем вам в этом параграфе, но с чего-то ведь надо начинать, не так ли? В тех несложных ситуациях, которые мы сейчас рассмотрим, архитектура будет отличаться лишь самыми последними этапами вычисления (у сетей будут разные «головы»). Для иллюстрации приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных: 16 Бинарная классификация Для решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от 0 0 до 1 1, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв =σ(f(X m )), где f f — некоторая функция, превращающая представление X m X m в число (если X m X m — матрица, то подойдёт f(X m )=X m w+b, где w w — вектор-столбец), а σ σ — наша любимая сигмоида. При этом X m X m может получаться как угодно,",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 2,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "приведём примеры нескольких игрушечных архитектур для решения игрушечных задач классификации и регрессии на двумерных данных: 16 Бинарная классификация Для решения задачи бинарной классификации подойдёт любая архитектура, на выходе у которой одно число от 0 0 до 1 1, интерпретируемое как «вероятность класса 1». Обычно этого добиваются, взяв =σ(f(X m )), где f f — некоторая функция, превращающая представление X m X m в число (если X m X m — матрица, то подойдёт f(X m )=X m w+b, где w w — вектор-столбец), а σ σ — наша любимая сигмоида. При этом X m X m может получаться как угодно, лишь бы хватало оперативной памяти и не было переобучения. В качестве функции потерь удобно брать уже знакомый нам log loss. Многоклассовая классификация Работая с другими моделями, мы порой вынуждены были выдумывать сложные стратегии многоклассовой классификации; нейросети позволяют это делать легко и элегантно. Достаточно построить сеть, которая будет выдавать K K неотрицательных чисел, суммирующихся в 1 (где K K — число классов); тогда им можно придать смысл вероятностей классов и предсказывать тот класс, «вероятность» которого максимальна. Превратить произвольный набор из K K чисел в набор из неотрицательных чисел, суммирующихся в 1, позволяет, к примеру, функция softmax softmax(x 1 ,…,x K )=( ,…, Наиболее популярные архитектуры для многоклассовой классификации имеют вид y ^ = softmax =softmax(f(X m )), где f f — функция, превращающая X m X m в матрицу B × K B×K (где B B — размер батча), а X m X m может быть получен любым приятным вам образом. Но какой будет функция потерь для такой сети? Мы должны научиться сравнивать «распределение вероятностей классов» с истинным (в котором на месте истинного класса стоит 1, а в остальных местах 0). Сделать это позволяет кросс-энтропия, она же negative log-likelihood — некоторый аналог расстояния между распределениями: log ,y)=− B 1 i=1 ∑ B k=1 ∑ K y ik log y ik , где снова B B — размер батча, а K K — число классов. Легко видеть, что при K = 2 K=2 получается та самая функция потерь, которую мы использовали для обучения бинарной классификации. (Множественная) регрессия С помощью нейросетей легко создать модель, которая предсказывает не одно число, а сразу несколько. Например, координаты ключевых точек лица — кончика носа, уголков рта и так далее. Достаточно сделать, чтобы последнее представление было матрицей B × M B×M, где B B — размер батча, а M M — количество предсказываемых чисел. Особенностью большинства моделей регрессии является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например, M S E MSE по всей матрице B × M B×M. Всё вместе Если вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация). Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 3,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "является то, что после последнего слоя (часто линейного) не ставят функций активации. Вы тоже этого не делайте, если только чётко не понимаете, зачем вам это. В качестве функции потерь можно брать, например, M S E MSE по всей матрице B × M B×M. Всё вместе Если вы используете нейросети, то ваши таргеты могут иметь и различную природу. Например, можно соорудить одну-единственную сеть, которая по фотографии нескольких котиков определяет их количество (регрессия) и породу каждого из них (многоклассовая классификация). Лосс для такой модели может быть равен (взвешенной) сумме лоссов для каждой из задач (правда, не факт, что это хорошая идея). Так что, по крайней мере в теории, сетям подвластны любые задачи. На практике, конечно, всё гораздо хитрей: для обучения слишком сложной сети у вас может не хватить данных или вычислительных мощностей. Популярные функции активации Для начала поговорим о том, зачем они нужны. Казалось бы, можно последовательно выстраивать лишь линейные слои, но так не делают: после каждого линейного слоя обязательно вставляют функцию активации. Но зачем? Попробуем разобраться. Рассмотрим нейронную сеть из двух линейных слоёв. Что произойдёт, если между ними будет отсутствовать нелинейная функция активации? out =(X Линейная комбинация линейных отображений есть линейное отображение, то есть два последовательных линейных слоя эквивалентны одному линейному слою. Добавление функций активации после линейного слоя позволяет получить нелинейное преобразование, и подобной проблемы уже не возникает. Вдобавок правильный выбор функции активации позволяет получить преобразование, обладающее подходящими свойствами. В качестве функции активации может использоваться, например, уже знакомая вам из логистической регрессии сигмоида exp ⁡ ( − x ) σ(x)= 1+exp(−x) 1 или ReLU (Rectified linear unit) ReLU ( x ) = max ( 0 , x ) ReLU(x)=max(0,x). К более глубокому рассмотрению разновидностей и свойств различных функций активации вернёмся позднее. Примечание. На самом деле бывают ситуации, когда два линейных слоя подряд — это полезно. Например, если вы понимаете, что у вас очень много параметров, а информации в данных не так много, вы можете заменить линейный слой, превращающий m m-мерные векторы в n n-мерные, на два, вставив посередине k k-мерное представление, где k ≪ m , n k≪m,n: 16 С точки зрения линейной алгебры это примерно то же самое, что потребовать, чтобы матрица исходного линейного слоя имела ранг не выше k k. И с точки зрения сужения «информационного канала» это иногда может сработать. Но в любом случае вы должны понимать, что два линейных слоя подряд стоит ставить, только если вы хорошо понимаете, чего хотите добиться. Вернёмся к функциям активации. Вот наиболее популярные: 16 Рассмотрим их подробнее. ReLU, Rectified linear unit Формула: ReLU ( x ) = max ReLU(x)=max(0,x), ReLU ReLU:R→[0,+∞). ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением. Плюсы: простота вычисления активации и производной. Минусы: область значений является смещённой относительно нуля; для отрицательных значений производная равна нулю, что может привести к затуханию градиента. ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды. Leaky ReLU Формула: Leaky ReLU ( x ) = max const",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 4,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "популярные: 16 Рассмотрим их подробнее. ReLU, Rectified linear unit Формула: ReLU ( x ) = max ReLU(x)=max(0,x), ReLU ReLU:R→[0,+∞). ReLU это простая кусочно-линейная функция. Одна из наиболее популярных функций активации. В нуле производная доопределяется нулевым значением. Плюсы: простота вычисления активации и производной. Минусы: область значений является смещённой относительно нуля; для отрицательных значений производная равна нулю, что может привести к затуханию градиента. ReLU и её производная очень просты для вычисления: достаточно лишь сравнить значение с нулём. Благодаря этому использование ReLU позволяет достигать прироста в скорости до четырёх-шести раз относительно сигмоиды. Leaky ReLU Формула: Leaky ReLU ( x ) = max const Leaky ReLU(x)=max(αx,x),α=const,0<α≪1 Leaky ReLU Leaky ReLU:R→(−∞,+∞). Гиперпараметр α α обеспечивает небольшой уклон слева от нуля, что позволяет получить более симметричную относительно нуля область значений. Также меньше провоцирует затухание градиента благодаря наличию ненулевого градиента и слева, и справа от нуля. PReLU, Parametric ReLU Формула: PReLU ( x ) = max PReLU(x)=max(αx,x),0<α≪1 PReLU PReLU:R→(−∞,+∞). Аналогична Leaky ReLU, но параметр α α настраивается градиентными методами. ELU ELU – это гладкая аппроксимация ReLU. Обладает более высокой вычислительной сложностью, достаточно редко используется на практике. Sigmoid, сигмоида Формула: exp σ(x)= 1+exp(−x) σ:R→(0,1). Исторически одна из первых функций активации. Рассматривалась в том числе и как гладкая аппроксимация порогового правила, эмулирующая активацию естественного нейрона. Плюсы: Минусы: область значений смещена относительно нуля; сигмоида (как и её производная) требует вычисления экспоненты, что является достаточно сложной вычислительной операцией. Её приближённое значение вычисляется на основе ряда Тейлора или с помощью полиномов, Stack Overflow question 1, question 2; на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента; максимальное значение производной составляет 0.25 0.25, что также приводит к затуханию градиента. На практике сигмоида редко используется внутри сетей, чаще всего в случаях, когда внутри модели решается задача бинарной классификации (например, вероятность забывания информации в LSTM). Tanh, гиперболический тангенс Формула: tanh ⁡ ( x ) = exp ⁡ ( x ) − exp ⁡ ( − x ) exp ⁡ ( x ) + exp tanh(x)= exp(x)+exp(−x) exp(x)−exp(−x) , tanh tanh:R→(−1,1). Плюсы: как и сигмоида, имеет ограниченную область значений; в отличие от сигмоиды, область значений симметрична. Минусы: требует вычисления экспоненты, что является достаточно сложной вычислительной операцией; на «хвостах» обладает практически нулевой производной, что может привести к затуханию градиента. Вопрос на подумать. А почему симметричность области значений может быть ценным свойством? Немного о мощи нейросетей Рассмотрим для начала задачу регрессии. Ясно, что линейная модель (то есть однослойная нейросеть) может приблизить только линейную функцию, но уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art. Теорема Цыбенко (1989). Для любой непрерывной функции f(x):R m →R и для любого ε > 0 ε>0 найдётся число N N, а также числа …,w ,…,b ,…,α N , для которых f(x)− i=1 ∑ N α i σ(⟨x,w i ⟩+b i ) <ε для любых x x из единичного куба [0,1] В сумме из теоремы Цыбенко легко",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 5,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "уже двухслойная нейросеть может приблизить почти что угодно. Есть ряд теорем на эту тему, мы упомянем одну из них. Обратите внимание на год: как мы уже упоминали, нейросети начали серьёзно изучать задолго до того, как они начали превращаться в state of the art. Теорема Цыбенко (1989). Для любой непрерывной функции f(x):R m →R и для любого ε > 0 ε>0 найдётся число N N, а также числа …,w ,…,b ,…,α N , для которых f(x)− i=1 ∑ N α i σ(⟨x,w i ⟩+b i ) <ε для любых x x из единичного куба [0,1] В сумме из теоремы Цыбенко легко опознать двуслойную нейросеть с сигмоидной функцией активации. В самом деле, сперва мы превращаем x x в ⟨x,w i ⟩+b i — это можно представить в виде одной матричной операции (линейный слой!): x↦x (1) =x⋅( )+( где w i w i — вектор-столбцы, а каждое из b i b i прибавляется к i i-му столбцу, после чего поэлементно берём от x ( 1 ) x (1) сигмоиду (активация) (2) =σ(x (1) ), после чего вычисляем i=1 (2) =(α 1 ,…,α N )⋅x, и это второй линейный слой (без свободного члена). Правда, теорема не очень помогает находить такие функции, но это уже другое дело. В любом случае — если дать нейросети достаточно данных, она действительно может выучить почти что угодно. Упражнение. Мы не будем приводить результатов, касающихся классификации, но рекомендуем воспользоваться замечательной песочницей. Убедитесь сами, что при использовании одного скрытого слоя из двух нейронов и сигмоиды в качестве функции активации, можно неплохо классифицировать данные со сложной, совсем даже не линейной границей между классами. Вы также можете поиграть с разными функциями активации. А для получения решения нам необходим метод автоматической настройки всех параметров нейронной сети — метод обратного распространения ошибки, или же error backpropagation. Рассмотрим его в деталях в следующем параграфе. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 5.1. Нейронные сети Краткий путеводитель по разделу Следующий параграф 5.3. Метод обратного распространения ошибки Как эффективно посчитать градиенты по весам нейронной сети",
    "metadata": {
      "title": "Первое знакомство с полносвязными нейросетями",
      "url": "https://education.yandex.ru/handbook/ml/article/pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.2",
      "part": 6,
      "total_parts": 6,
      "source_file": "5.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Как эффективно посчитать градиенты по весам нейронной сети Нейронные сети обучаются с помощью тех или иных модификаций градиентного спуска, а чтобы применять его, нужно уметь эффективно вычислять градиенты функции потерь по всем обучающим параметрам. Казалось бы, для какого-нибудь запутанного вычислительного графа это может быть очень сложной задачей, но на помощь спешит метод обратного распространения ошибки. Метод обратного распространения ошибки (backward propagation) Открытие метода обратного распространения ошибки стало одним из наиболее значимых событий в области искусственного интеллекта. В актуальном виде он был предложен в 1986 году Дэвидом Э. Румельхартом, Джеффри Э. Хинтоном и Рональдом Дж. Вильямсом, а также независимо и одновременно красноярскими математиками С. И. Барцевым и В. А. Охониным. С тех пор для нахождения градиентов параметров нейронной сети используется метод вычисления производной сложной функции, и оценка градиентов параметров сети стала хоть и сложной инженерной задачей, но уже не искусством. Несмотря на простоту используемого математического аппарата, появление этого метода привело к значительному скачку в развитии искусственных нейронных сетей. Суть метода можно записать одной формулой, тривиально следующей из формулы производной сложной функции: если f(x)=g m (g m−1 (…(g 1 (x))…)), то m−1 ∂g m ∂g m−2 ∂g m−1 . Уже сейчас мы видим, что градиенты можно вычислять последовательно, в ходе одного обратного прохода, начиная с m−1 ∂g m и умножая каждый раз на частные производные предыдущего слоя. Backward propagation в одномерном случае В одномерном случае всё выглядит особенно просто. Пусть w 0 w 0 — переменная, по которой мы хотим продифференцировать , причём сложная функция имеет вид f(w 0 )=g m (g m−1 (…g 1 (w 0 )…)), где все g i g i скалярные. Тогда )=g m ′ (g m−1 (…g 1 (w 0 )…))⋅g m−1 ′ (g m−2 (…g 1 (w 0 )…))⋅…⋅g 1 ′ (w 0 ) Суть этой формулы такова. Если мы уже совершили прямой проход (forward propagation), значит мы уже знаем ),g 2 (g 1 (w 0 )),…,g m−1 (…g 1 (w 0 )…), Поэтому мы можем действовать следующим образом: берём производную g m g m в точке m−1 (…g 1 (w 0 )…); умножаем на производную g m − 1 g m−1 в точке m−2 (…g 1 (w 0 )…); и так далее, пока не дойдём до производной g 1 g 1 в точке w 0 w 0 . Проиллюстрируем это на картинке, расписав по шагам дифференцирование по весам w i w i функции потерь логистической регрессии на одном объекте (то есть для батча размера 1): 17 Собирая все множители вместе, получаем: =(−y)⋅e −y(w 1+e −y(w ⋅(−y)⋅e −y(w 1+e −y(w ⋅(−y)⋅e −y(w 1+e −y(w Таким образом, сперва совершается forward propagation для вычисления всех промежуточных значений (да, все промежуточные представления нужно будет хранить в памяти), а потом запускается backward propagation, на котором в один проход вычисляются все градиенты. Почему же нельзя просто пойти и начать везде вычислять производные? В параграфе, посвящённом матричным дифференцированиям, мы поднимаем вопрос о том, что вычислять частные производные по отдельности — это зло, лучше пользоваться матричными вычислениями. Но есть и ещё одна причина: даже и с матричной производной в принципе не всегда хочется",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 1,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "17 Собирая все множители вместе, получаем: =(−y)⋅e −y(w 1+e −y(w ⋅(−y)⋅e −y(w 1+e −y(w ⋅(−y)⋅e −y(w 1+e −y(w Таким образом, сперва совершается forward propagation для вычисления всех промежуточных значений (да, все промежуточные представления нужно будет хранить в памяти), а потом запускается backward propagation, на котором в один проход вычисляются все градиенты. Почему же нельзя просто пойти и начать везде вычислять производные? В параграфе, посвящённом матричным дифференцированиям, мы поднимаем вопрос о том, что вычислять частные производные по отдельности — это зло, лучше пользоваться матричными вычислениями. Но есть и ещё одна причина: даже и с матричной производной в принципе не всегда хочется иметь дело. Рассмотрим простой пример. Допустим, что r+1 — два последовательных промежуточных представления N × M N×M и N × K N×K, связанных функцией r+1 =f r+1 (X r ). Предположим, что мы как-то посчитали производную r+1 ∂L функции потерь L L, тогда i,j r+1 ∂X ij r+1 ∂L И мы видим, что, хотя оба градиента r+1 — это просто матрицы, в ходе вычислений возникает «четырёхмерный кубик» r+1 . Его болезненно даже хранить: уж больно много памяти он требует —м MK по сравнению с безобидными N M + N K NM+NK, требуемыми для хранения градиентов. Поэтому хочется промежуточные производные r+1 рассматривать не как вычисляемые объекты r+1 , а как преобразования, которые превращают r+1 Целью следующих параграфов будет именно это: понять, как преобразуется градиент в ходе error backward propagation при переходе через тот или иной слой. Вы спросите себя: надо ли мне сейчас пойти и прочитать параграф учебника про матричное дифференцирование? Короткий ответ: Зависит от ваших знаний. Длинный ответ: Найдите производную функции по вектору — матрица размера n × n f(x)=x T Ax, A∈Mat n R — матрица размера n×n А как всё поменяется, если A A тоже зависит от x x? Чему равен градиент функции, если A A является скаляром? Если вы готовы прямо сейчас взять ручку и бумагу и посчитать всё, то вам, вероятно, не надо читать про матричные дифференцирования. Но мы советуем всё-таки заглянуть в этот параграф, если обозначения, которые мы будем дальше использовать, покажутся вам непонятными: единой нотации для матричных дифференцирований человечество пока, увы, не изобрело, и переводить с одной на другую не всегда легко. А мы же сразу перейдём к интересующей нас вещи: к вычислению градиентов сложных функций. Градиент сложной функции Напомним, что формула производной сложной функции выглядит следующим образом: (u∘v)](h)=[D v(x 0 ) u]([D x 0 v](h)) Теперь разберёмся с градиентами. Пусть f(x)=g(h(x)) – скалярная функция. Тогда f](x−x 0 )=⟨∇ x 0 f,x−x 0 ⟩. С другой стороны, h(x 0 ) g]([D x 0 h](x−x 0 ))=⟨∇ h x 0 g,[D x 0 h](x−x 0 )⟩=⟨[D x 0 h] ∗ ∇ h(x 0 ) g,x−x 0 ⟩. То есть f=[D x 0 h] ∗ ∇ h(x 0 ) g — применение сопряжённого к h линейного отображения к вектору h(x 0 ) g. Эта формула — сердце механизма обратного распространения ошибки. Она говорит следующее: если мы каким-то образом получили градиент функции потерь по переменным из некоторого промежуточного представления X k X k нейронной сети и при",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 2,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "функция. Тогда f](x−x 0 )=⟨∇ x 0 f,x−x 0 ⟩. С другой стороны, h(x 0 ) g]([D x 0 h](x−x 0 ))=⟨∇ h x 0 g,[D x 0 h](x−x 0 )⟩=⟨[D x 0 h] ∗ ∇ h(x 0 ) g,x−x 0 ⟩. То есть f=[D x 0 h] ∗ ∇ h(x 0 ) g — применение сопряжённого к h линейного отображения к вектору h(x 0 ) g. Эта формула — сердце механизма обратного распространения ошибки. Она говорит следующее: если мы каким-то образом получили градиент функции потерь по переменным из некоторого промежуточного представления X k X k нейронной сети и при этом знаем, как преобразуется градиент при проходе через слой f k f k между X k − 1 X k−1 и X k X k (то есть как выглядит сопряжённое к дифференциалу слоя между ними отображение), то мы сразу же находим градиент и по переменным из X k − 1 X k−1 : 17 Таким образом слой за слоем мы посчитаем градиенты по всем X i X i вплоть до самых первых слоёв. Далее мы разберёмся, как именно преобразуются градиенты при переходе через некоторые распространённые слои. Градиенты для типичных слоёв Рассмотрим несколько важных примеров. Пример №1 f(x)=u(v(x)), где x x — вектор, а v ( x ) v(x) – поэлементное применение v(x 1 ) ⋮ v(x N ) Тогда, как мы знаем, f](h)=⟨∇ x 0 f,h⟩=[∇ x 0 f] T h. Следовательно, v(x 0 ) u]([D x 0 v](h)) =[∇ v(x )⊙h)= = i ∑ [∇ v(x =⟨[∇ v(x 0 ) u]⊙v ′ (x 0 ),h⟩. где ⊙ ⊙ означает поэлементное перемножение. Окончательно получаем f=[∇ v(x 0 ) u]⊙v ′ (x 0 )=v ′ (x 0 )⊙[∇ v(x 0 ) u] Отметим, что если x x и h ( x ) h(x) — это просто векторы, то мы могли бы вычислять всё и по формуле )⋅( ∂z j ∂h ). В этом случае матрица ) была бы диагональной (так как z j z j зависит только от x j x j : ведь h h берётся поэлементно), и матричное умножение приводило бы к тому же результату. Однако если x x и h ( x ) h(x) — матрицы, то ) представлялась бы уже «четырёхмерным кубиком», и работать с ним было бы ужасно неудобно. Пример №2 f(X)=g(XW), где X X и W W — матрицы. Как мы знаем, f](X−X 0 )=tr([∇ X 0 f] T (X−X 0 )). Тогда g]([D X 0 (∗W)](H))=[D X 0 W g](HW)= =tr([∇ X 0 W g] T ⋅(H)W)= =tr(W[∇ X 0 W (g)] T ⋅(H))=tr([[∇ X 0 W g]W T ] T (H)) Здесь через ∗ W ∗W мы обозначили отображение Y ↪ Y W Y↪YW, а в предпоследнем переходе использовалось следующее свойство следа: tr(ABC)=tr(CAB), где A , B , C A,B,C — произвольные матрицы подходящих размеров (то есть допускающие перемножение в обоих приведённых порядках). Следовательно, получаем f=[∇ X 0 W (g)]⋅W T Пример №3 f(W)=g(XW), где W W и X X — матрицы. Для приращения H=W−W 0 имеем f](H)=tr([∇ W 0 f] T (H))",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 3,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "0 (∗W)](H))=[D X 0 W g](HW)= =tr([∇ X 0 W g] T ⋅(H)W)= =tr(W[∇ X 0 W (g)] T ⋅(H))=tr([[∇ X 0 W g]W T ] T (H)) Здесь через ∗ W ∗W мы обозначили отображение Y ↪ Y W Y↪YW, а в предпоследнем переходе использовалось следующее свойство следа: tr(ABC)=tr(CAB), где A , B , C A,B,C — произвольные матрицы подходящих размеров (то есть допускающие перемножение в обоих приведённых порядках). Следовательно, получаем f=[∇ X 0 W (g)]⋅W T Пример №3 f(W)=g(XW), где W W и X X — матрицы. Для приращения H=W−W 0 имеем f](H)=tr([∇ W 0 f] T (H)) Тогда g]([D W 0 (X∗)](H))=[D XW 0 g](XH)= =tr([∇ XW 0 g] T ⋅X(H))=tr([X T [∇ XW 0 g]] T (H)) Здесь через X ∗ X∗ обозначено отображение Y ↪ X Y Y↪XY. Значит, f=X T ⋅[∇ XW 0 (g)] Пример №4 f(X)=g(softmax(X)), где X X — матрица N × K N×K, а softmax — функция, которая вычисляется построчно, причём для каждой строки softmax(x)=( ,…, В этом примере нам будет удобно воспользоваться формализмом с частными производными. Сначала вычислим для одной строки x x, где через s l s l мы для краткости обозначим softmax(x) . Нетрудно проверить, что (1−s j=l, j  =l Так как softmax вычисляется независимо от каждой строчки, то (1−s r=i,j=l, r=i,j  =l, r  =i , где через s r l s rl мы обозначили для краткости softmax(X) rl . Теперь пусть =∇g= ∂s rl ∂L (пришедший со следующего слоя, уже известный градиент). Тогда r,l Так как =0 при r ≠ i r  =i, мы можем убрать суммирование по =−s i1 s ij ∇ i1 −…+s ij (1−s ij )∇ ij −…−s =−s Таким образом, если мы хотим продифференцировать f f в какой-то конкретной точке X 0 X 0 , то, смешивая математические обозначения с нотацией Python, мы можем записать: =−softmax(X 0 )⊙sum (softmax(X 0 )⊙∇ softmax(X 0 ) g, axis=1)+ softmax(X 0 )⊙∇ softmax(X 0 ) g Backward propagation в общем виде Подытожим предыдущее обсуждение, описав алгоритм error backward propagation (алгоритм обратного распространения ошибки). Допустим, у нас есть текущие значения весов и мы хотим совершить шаг SGD по мини-батчу X X. Мы должны сделать следующее: Совершить forward propagation, вычислив и запомнив все промежуточные представления X=X 0 ,X 1 ,…,X m = y . Вычислить все градиенты с помощью backward propagation. С помощью полученных градиентов совершить шаг SGD. Проиллюстрируем алгоритм на примере двухслойной нейронной сети со скалярным output. Для простоты опустим свободные члены в линейных слоях. 17 Обучаемые параметры – матрицы U U и W W. Как найти градиенты по ним в точке L=∇ W 0 ( 2 1 L∘h∘[W↦g(XU 0 )W])= =g(XU 0 ) T ∇ g(XU 0 )W 0 (L∘h)= k×N g(XU N×1 h ′ (∫ 0 1 g(XU 0 )W 0 ) ⊙ N×1 ∇ h(∫ 0 1 g(XU 0 )W 0 ) L Итого матрица k × 1 k×1, как и L=∇ U 0 ( 2 1 L∘h∘[Y↦YW 0 ]∘g∘[U↦XU])= L∘h∘[Y↦YW 0 ]∘g)= (XU 0 )⊙∇ g(XU 0 ) [∈",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 4,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сети со скалярным output. Для простоты опустим свободные члены в линейных слоях. 17 Обучаемые параметры – матрицы U U и W W. Как найти градиенты по ним в точке L=∇ W 0 ( 2 1 L∘h∘[W↦g(XU 0 )W])= =g(XU 0 ) T ∇ g(XU 0 )W 0 (L∘h)= k×N g(XU N×1 h ′ (∫ 0 1 g(XU 0 )W 0 ) ⊙ N×1 ∇ h(∫ 0 1 g(XU 0 )W 0 ) L Итого матрица k × 1 k×1, как и L=∇ U 0 ( 2 1 L∘h∘[Y↦YW 0 ]∘g∘[U↦XU])= L∘h∘[Y↦YW 0 ]∘g)= (XU 0 )⊙∇ g(XU 0 ) [∈ 0 1 L∘h∘[Y↦YW 0 ]) = … = XTD×N =…= D×N X T ⋅ 2 1 N×K g ′ (XU 0 ) ⊙ N×K ∫ 0 1 N×1 h ′ (∫ 0 1 g(XU 0 )W 0 ) ⊙ N×1 ∇ h(∫ 0 1 g(XU 1×K W T Итого D × K D×K, как и U 0 U 0 Схематически это можно представить следующим образом: 17 Backward propagation для двухслойной нейронной сети Если вы не уследили за вычислениями в предыдущем примере, давайте более подробно разберём его чуть более конкретную версию (для g = h = σ g=h=σ) Рассмотрим двуслойную нейронную сеть для классификации. Мы уже встречали её ранее при рассмотрении линейно неразделимой выборки. Предсказания получаются следующим образом: =σ(X 1 W 2 )=σ((σ(X 0 W 1 ))W 2 ). Пусть — текущее приближение матриц весов. Мы хотим совершить шаг по градиенту функции потерь, и для этого мы должны вычислить её градиенты по в точке Прежде всего мы совершаем forward propagation, в ходе которого мы должны запомнить все промежуточные представления: =σ(X =σ(X =σ(σ(X . Они понадобятся нам дальше. Для полученных предсказаний вычисляется значение функции потерь: log log l=L(y, y )=ylog( y )+(1−y)log(1− y ). Дальше мы шаг за шагом будем находить производные по переменным из всё более глубоких слоёв. Градиент L L по предсказаниям имеет вид 1−y = y (1− y ) y− y , где, напомним, =σ(X 3 )=σ((σ(X 0 W 0 1 ))W 0 2 ) (обратите внимание на то, что тут именно те, из которых мы делаем градиентный шаг). Следующий слой — поэлементное взятие σ σ. Как мы помним, при переходе через него градиент поэлементно умножается на производную σ σ, в которую подставлено предыдущее промежуточное представление: l=σ ′ (X 3 )⊙∇ y l=σ(X 3 )(1−σ(X 3 ))⊙ y (1− =σ(X 3 )(1−σ(X 3 ))⊙ σ(X 3 )(1−σ(X 3 )) y−σ(X l=(X l=(X 2 ) T ⋅(y−σ(X 3 ))= =(σ(X ⋅(y−σ(σ(X Аналогичным образом l=∇ X 3 l⋅(W 0 2 ) T =(y−σ(X 3 ))⋅(W =(y−σ(X 2 W 0 2 ))⋅(W 0 2 ) T Следующий слой — снова взятие l=σ ′ (X 1 )⊙∇ X 2 l=σ(X 1 )(1−σ(X 1 ))⊙((y−σ(X 2 W 0 2 ))⋅(W =σ(X 1 )(1−σ(X 1 ))⊙((y−σ(σ(X 1 )W 0 2 ))⋅(W 0 2 ) T ) Наконец, последний слой — это умножение . Тут мы дифференцируем только по l=(X l=(X 0 ) T ⋅(σ(X 1 )(1−σ(X 1 ))⊙(y−σ(σ(X 1 )W 0 2 ))⋅(W =(X 0",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 5,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "y−σ(X l=(X l=(X 2 ) T ⋅(y−σ(X 3 ))= =(σ(X ⋅(y−σ(σ(X Аналогичным образом l=∇ X 3 l⋅(W 0 2 ) T =(y−σ(X 3 ))⋅(W =(y−σ(X 2 W 0 2 ))⋅(W 0 2 ) T Следующий слой — снова взятие l=σ ′ (X 1 )⊙∇ X 2 l=σ(X 1 )(1−σ(X 1 ))⊙((y−σ(X 2 W 0 2 ))⋅(W =σ(X 1 )(1−σ(X 1 ))⊙((y−σ(σ(X 1 )W 0 2 ))⋅(W 0 2 ) T ) Наконец, последний слой — это умножение . Тут мы дифференцируем только по l=(X l=(X 0 ) T ⋅(σ(X 1 )(1−σ(X 1 ))⊙(y−σ(σ(X 1 )W 0 2 ))⋅(W =(X 0 ) T ⋅(σ(X 0 W 0 1 )(1−σ(X 0 W 0 1 ))⊙(y−σ(σ(X ))⋅(W 0 2 ) T ) Итоговые формулы для градиентов получились страшноватыми, но они были получены друг из друга итеративно с помощью очень простых операций: матричного и поэлементного умножения, в которые порой подставлялись значения заранее вычисленных промежуточных представлений. Автоматизация и autograd Итак, чтобы нейросеть обучалась, достаточно для любого слоя k−1 ↦X k с параметрами W k W k уметь: превращать L в k−1 L (градиент по выходу в градиент по входу); считать градиент по его параметрам При этом слою совершенно не надо знать, что происходит вокруг. То есть слой действительно может быть запрограммирован как отдельная сущность, умеющая внутри себя делать forward propagation и backward propagation, после чего слои механически, как кубики в конструкторе, собираются в большую сеть, которая сможет работать как одно целое. Более того, во многих случаях авторы библиотек для глубинного обучения уже о вас позаботились и создали средства для автоматического дифференцирования выражений (autograd). Поэтому, программируя нейросеть, вы почти всегда можете думать только о forward-проходе, прямом преобразовании данных, предоставив библиотеке дифференцировать всё самостоятельно. Это делает код нейросетей весьма понятным и выразительным (да, в реальности он тоже бывает большим и страшным, но сравните на досуге код какой-нибудь разухабистой нейросети и код градиентного бустинга на решающих деревьях и почувствуйте разницу). Но это лишь начало Метод обратного распространения ошибки позволяет удобно посчитать градиенты, но дальше с ними что-то надо делать, и старый добрый SGD едва ли справится с обучением современной сетки. Так что же делать? О некоторых приёмах мы расскажем в следующем параграфе. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 5.2. Первое знакомство с полносвязными нейросетями Основные понятия глубинного обучения. Базовые слои и функции активации Следующий параграф 5.4. Тонкости обучения Инициализация весов. Регуляризация нейросетей. Dropout и Batchnorm",
    "metadata": {
      "title": "Метод обратного распространения ошибки",
      "url": "https://education.yandex.ru/handbook/ml/article/metod-obratnogo-rasprostraneniya-oshibki",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.3",
      "part": 6,
      "total_parts": 6,
      "source_file": "5.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Инициализация весов. Регуляризация нейросетей. Dropout и Batchnorm Если открыть случайную научную статью по глубинному обучению и попробовать воспроизвести её результаты, можно запросто потерпеть крах, и даже код на github, если он есть, может не помочь. А дело в том, что обучение сложной модели — это сложная инженерная задача, в которой успеху сопутствует огромное число разных хаков, и изменение какого-нибудь безобидного параметра может очень сильно повлиять на результат. В этом параграфе мы познакомим вас с некоторыми из таких приёмов. Инициализируем правильно Как вы уже успели заметить, нейронные сети — достаточно сложные модели, чувствительные к изменениям архитектуры, гиперпараметров, распределения данных и другим вещам. Поэтому значительную роль играет начальная инициализация весов вашей сети. Стоит отметить, что здесь речь идет именно о начальной инициализации параметров сети, вопрос дообучения (и использования предобученных сетей в качестве backbone) в данном параграфе рассматриваться не будет. Нейронные сети включают в себя различные преобразования, и инициализация по-хорошему также должна зависеть от типа используемого преобразования. На практике вопрос часто остается без внимания, так как в большинстве современных фреймворков уже реализованы методы инициализации, зависящие от используемой функции активации и гиперпараметров слоя, и пользователь может не задумываться об этом. Но всё же важно понимать, какие соображения привели к появлению тех или иных стратегий инициализации. Давайте разберём несколько методов инициализации и обсудим их свойства. Наивный подход №0: инициализация нулем/константой Казалось бы, инициализация параметров слоя нулями — это достаточно просто и лаконично. Но инициализация нулём (как и любой другой константой) ведёт к катастрофе! Вот пример того, что может получиться: 18 Стоит, впрочем, отметить, что из-за численных ошибок значения параметров могут всё-таки сдвинуться с мёртвой точки, и тогда нейросеть что-нибудь выучит: 18 Здесь также стоит привести цитату из замечательной Deep Learning book (страница 301): Perhaps the only property known with complete certainty is that the initial parameters need to “break symmetry” between different units. If two hidden units with the same activation function are connected to the same inputs, then these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way. Эвристический подход №1: инициализация случайными числами Если константная инициализация не подходит, можно инициализировать нейросеть случайными числами. Допустим, веса пришли из распределения с нулевым средним и дисперсией σ 2 σ 2 , например, из нормального распределения N(0,σ 2 ). Пусть теперь на вход линейному слою с весами w w размерности n in n in пришел вектор x x аналогичной размерности. Замечание. Можем считать, что мы рассматриваем лишь одну компоненту следующего промежуточного представления z z. Все компоненты x x распределены одинаковым образом и обладают нулевым средним. Тогда дисперсия их произведения y y имеет вид: Var ( y ) = Var Var Var ( x i ) + Var ( w i ) Var ( x i ) ] Var(y)=Var(w T x)= i=1 ∑ n [E[x i ] 2 Var(w i )+E[w i ] 2 Var(x i )+Var(w i )Var(x i )] Первое и второе слагаемые равны нулю так как математические ожидание",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 1,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "n in n in пришел вектор x x аналогичной размерности. Замечание. Можем считать, что мы рассматриваем лишь одну компоненту следующего промежуточного представления z z. Все компоненты x x распределены одинаковым образом и обладают нулевым средним. Тогда дисперсия их произведения y y имеет вид: Var ( y ) = Var Var Var ( x i ) + Var ( w i ) Var ( x i ) ] Var(y)=Var(w T x)= i=1 ∑ n [E[x i ] 2 Var(w i )+E[w i ] 2 Var(x i )+Var(w i )Var(x i )] Первое и второе слагаемые равны нулю так как математические ожидание и весов, и значений x x равны нулю. Замечание. Стоит заметить, что это будет верно и для промежуточных слоев в случае использования симметричной относительно нуля функции активации, например, tanh. Поскольку все веса пришли из одного распределения, можно выразить дисперсию результата следующим образом: Var Var ( w ) Var ( x ) , Var(w T x)=n in Var(w)Var(x), где Var ( x ) Var(x) — это дисперсия любой компоненты x x (как было оговорено ранее, они распределены одинаково), а Var Var(w)=σ 2 — дисперсия компоненты w w. Следовательно, дисперсия результата линейно зависит от дисперсии входных данных с коэффициентом n in Var ( w ) n in Var(w). Увеличение дисперсии промежуточных представлений с каждым новым преобразованием (слоем) может вызвать численные ошибки или насыщение функций активации (таких как tanh и sigmoid), что не лучшим образом скажется на обучении сети. Снижение дисперсии может привести к почти нулевым промежуточным представлениям (плюс «линейному» поведению tanh и sigmoid в непосредственной близости от нуля), что тоже негативно повлияет на результаты обучения. Поэтому для начальной инициализации весов имеет смысл использовать распределение, дисперсия которого позволила бы сохранить дисперсию результата. Например, ∀i,w i ∼N(0, n in 1 ), или же в общем случае ∀ i , Var ∀i,Var(w i )= n in 1 Данный подход часто упоминается как calibrated random numbers initialization. Подход №2: Xavier & Normalized Xavier initialization Если обратиться к предыдущему подходу, можно обнаружить, что все выкладки верны как для «прямого» прохода (forward propagation), так и для обратного (backward propagation). Дисперсия градиента при этом меняется в n out Var ( w ) n out Var(w) раз, где n out n out — размерность следующего за x x промежуточного представления. И если мы хотим, чтобы сохранялись дисперсии и промежуточных представлений, и градиентов, у нас возникают сразу два ограничения: ∀ i , Var ∀i,Var(w Var out . ∀i,Var(w i )= n out 1 . Легко заметить, что оба этих ограничения могут быть выполнены только в случае, когда размерность пространства не меняется при отображении, что случается далеко не всегда. В работе Understanding the difficulty of training deep feedforward neural networks за авторством Xavier Glorot и Yoshua Bengio в качестве компромисса предлагается использовать параметры из распределения с дисперсией ∀ i , Var out . ∀i,Var(w i )= n in +n out 2 . Подробный вывод данного результата можно найти в оригинальной статье в формулах 2-12. Обратите внимание: эта инициализация хорошо подходит именно для tanh, так как в выводе явно учитывается симметричность функции активации",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 2,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "out 1 . Легко заметить, что оба этих ограничения могут быть выполнены только в случае, когда размерность пространства не меняется при отображении, что случается далеко не всегда. В работе Understanding the difficulty of training deep feedforward neural networks за авторством Xavier Glorot и Yoshua Bengio в качестве компромисса предлагается использовать параметры из распределения с дисперсией ∀ i , Var out . ∀i,Var(w i )= n in +n out 2 . Подробный вывод данного результата можно найти в оригинальной статье в формулах 2-12. Обратите внимание: эта инициализация хорошо подходит именно для tanh, так как в выводе явно учитывается симметричность функции активации относительно нуля. В случае использования равномерного распределения U U для инициализации весов с учетом описанных выше ограничений мы получим normalized Xavier initialization: out out ] . ∀i,w i ∼U[− n in +n out 6 , n in +n out 6 ]. Замечание. Здесь используется тот факт, что дисперсия непрерывного равномерного распределения Var Var[U[a,b]]= 12 1 (b−a) 2 . Сравнение подобной инициализации для поведения промежуточных представлений (сверху) и градиентов (снизу) проиллюстрированы ниже (иллюстрации из оригинальной статьи): 18 18 Подход №3: Kaiming initialization Вы могли обратить внимание, что Xavier initialization во многом опиралась на поведение функции активации tanh. Данный тип инициализации и впрямь лучше подходит для нее, но само использование гиперболического тангенса приводит к некоторым сложностям (например, к затуханию градиентов). В 2015 году в работе Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification за авторством Kaiming He, Xiangyu Zhang, Shaoqing Ren и Jian Sun были рассмотрены особые свойства функции активации ReLU, в частности, существенно смещенная относительно нуля область значений. Пусть представление на входе было получено после применения данной функции активации к предыдущему представлению z prev z prev : x = ReLU ( z prev ) , x=ReLU(z prev ), где z prev z prev , в свою очередь, — это выход предыдущего линейного слоя с нулевым средним для каждой компоненты весов, то есть, в частности, E ( z prev ) = 0 E(z prev )=0 В таком случае дисперсия выхода следующего линейного слоя примет вид: Var Var Var ( x i ) + Var ( w i ) Var Var(w T x)= i=1 ∑ n [E[x i ] 2 Var(w i )+E[w i ] 2 Var(x i )+Var(w i )Var(x i )]= =(E[x i ] 2 +V(x i ))V(w i ). В данном случае первый член не может быть проигнорирован, так как ReLU имеет ассиметричную область значений, а значит, распределения x i x i будут смещёнными. С учетом того, что Var Var(x)=E[x i 2 ]−E[x i ] 2 , выражение выше примет итоговый вид: Var Var Var Var(w T x)=Var(w T x)=n in Var(w i )E(x i 2 ). С учётом поведения ReLU и того, что E ( z prev ) = 0 E(z prev )=0, можно сказать, что Var ( z prev ) , E(x i 2 )= 2 1 Var(z prev ), то есть Var Var ( w i ) Var ( z prev ) . Var(w T x)= 2 1 n in Var(w i )Var(z prev ).",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 3,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "x i x i будут смещёнными. С учетом того, что Var Var(x)=E[x i 2 ]−E[x i ] 2 , выражение выше примет итоговый вид: Var Var Var Var(w T x)=Var(w T x)=n in Var(w i )E(x i 2 ). С учётом поведения ReLU и того, что E ( z prev ) = 0 E(z prev )=0, можно сказать, что Var ( z prev ) , E(x i 2 )= 2 1 Var(z prev ), то есть Var Var ( w i ) Var ( z prev ) . Var(w T x)= 2 1 n in Var(w i )Var(z prev ). Получается, что использование ReLU приводит к необходимости инициализировать веса из распределения, чья дисперсия удовлетворяет следующему ограничению: Var ∀i, 2 1 n in Var(w i )=1. Например, подходит нормальное распределение N(0, n in 2 ). Данный способ инициализации (и его сравнение с Xavier initialization) проиллюстрирован ниже: Источник Промежуточные выводы Рассмотренные способы инициализации используют достаточно много предположений, но все-таки они работают и позволяют нейронным сетям в некоторых случаях значительно быстрее сходиться. Понимание принципов работы даже таких небольших механизмов – ключ к глубокому освоению области глубокого обучения 😃 Методы оптимизации в нейронных сетях Так как мы договорились, что нейросети представляют собой параметризованные дифференцируемые функции и для каждого параметра мы можем посчитать градиент, то, так же как и линейные модели, их можно настраивать с помощью градиентных методов. В параграфе про линейные модели мы под этим подразумевали обычно стохастический градиентный спуск на батчах, и это совершенно подходящий способ и для нейросетей тоже. Но существует множество модификаций и эвристик, позволяющих ускорить его сходимость, познакомиться с которыми вы можете в специальном параграфе, посвящённом методам оптимизации. Регуляризация нейронных сетей Смысл термина регуляризация (англ. regularization) гораздо шире привычного вам прибавления L 1 L 1 - или L 2 L 2 -нормы вектора весов к функции потерь. Фактически он объединяет большое количество техник для борьбы с переобучением и для получения более подходящего решения с точки зрения эксперта. Каждая из них позволяет навязать модели определённые свойства, пусть даже и ценой некоторого снижения качества предсказания на обучающей выборке. Например, уже знакомая читателю L 1 L 1 - или L 2 L 2 -регуляризация в задаче линейной регрессии (регуляризация Тихонова) позволяет исключить наименее значимые признаки (для линейной модели) или же получить устойчивое (хоть и смещённое) решение в случае мультиколлинеарных признаков. В нейронных сетях техники регуляризации можно разделить на три обширные группы: связанные с изменением функции потерь; связанные с изменением структуры сети; связанные с изменением данных. Рассмотрим каждую из них подробнее. Регуляризация через функцию потерь Изменение функции потерь — классический способ получить решение, удовлетворяющее определённым условиям. В глубинном обучении часто используется техника Weight Decay, очень близкая к регуляризации Тихонова. Она представляет собой аналогичный штраф за высокие значения весов нейронной сети с коэффициентом регуляризации λ λ: L with regularization = L original with regularization =L original +λ∣∣W∣∣ 2 Данная техника регуляризации была совмещена с методом градиентной оптимизации Adam, в результате чего был получен метод AdamW (описанный в параграфе параграфе про методы оптимизации). Также достаточно часто в качестве регуляризационного члена встречается энтропия распределения, предсказанного нейронной сетью.",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 4,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "изменением данных. Рассмотрим каждую из них подробнее. Регуляризация через функцию потерь Изменение функции потерь — классический способ получить решение, удовлетворяющее определённым условиям. В глубинном обучении часто используется техника Weight Decay, очень близкая к регуляризации Тихонова. Она представляет собой аналогичный штраф за высокие значения весов нейронной сети с коэффициентом регуляризации λ λ: L with regularization = L original with regularization =L original +λ∣∣W∣∣ 2 Данная техника регуляризации была совмещена с методом градиентной оптимизации Adam, в результате чего был получен метод AdamW (описанный в параграфе параграфе про методы оптимизации). Также достаточно часто в качестве регуляризационного члена встречается энтропия распределения, предсказанного нейронной сетью. Представьте, что вы рекомендуете пользователю товары по истории его взаимодействия с сервисом, семплируя товары для показа в соответствии с распределением предсказанной релевантности. Вам может быть важно, чтобы рекомендации не были фиксированными (менялись при обновлении страницы), ведь это повысит вероятность того, что пользователь найдёт что-то интересное, а вы узнаете о нём что-нибудь новое. В такой ситуации при обучении модели вы можете потребовать, чтобы распределение предсказаний не сходилось к вырожденному, и в качестве дополнительной штрафной функции может выступать энтропия этого распределения. Энтропия дифференцируема, как и сами предсказанные величины, и может быть использована в качестве регуляризационного члена. Для задачи классификации он будет выглядеть следующим образом: =f(x;θ), L with regularization = L original log with regularization =L original −λ k ∑ p k log p k , где λ λ — коэффициент регуляризации, p ^ p — предсказанные вероятности. Тем самым эксперт привносит своё знание непосредственно в процесс обучения модели в подходящей математической форме: «предсказания должны быть разнообразными» —> «распределение не должно быть вырожденным» —> «энтропия не должна быть слишком низкой». Регуляризация через ограничение структуры модели Внесение подходящих преобразований в структуру сети также может быть хорошим способом добиться желаемых результатов. Огромное влияние на развитие нейронных сетей оказали техники dropout (2014) и batch normalization (2015), позволившие сделать нейронные сети более устойчивыми к переобучению и многократно ускорить их сходимость соответственно. Dropout Обратимся к простым полносвязным (FC/Dense) сетям из нескольких слоёв. Каждый из слоёв порождает новое признаковое описание x k x k объекта x in x in , который пришёл на вход: k−1 ). Но как можно гарантировать, что модель будет эффективно использовать все доступные параметры, а не переобучится под использование лишь небольшого их подмножества, поделив для себя внутреннее представление на сигнал и шум? x overfitted k = [ signal , noise ] x overfitted k =[signal,noise] Для этого можно было бы случайным образом «выключать» доступ к некоторым координатам внутренних представлений на этапе обучения. Тогда при выключении «полезных» координат произойдёт резкое изменение предсказаний модели, что приведёт к увеличению ошибки, а полученные градиенты этой ошибки укажут, как её исправить с использованием (и изменением) других координат. Сравнение тока информации по исходной модели и по модели с «выключенными» координатами внутренних представлений можно проиллюстрировать с помощью классической картинки: 18 Обратите внимание: «выключать» можно как оригинальные признаки, так и признаки, возникающие на любом другом уровне представления объектов. С точки зрения ( k + 1 ) (k+1)-го слоя нейронной сети данные приходят откуда-то извне: при k = 0 k=0 — из реального мира,",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 5,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "некоторым координатам внутренних представлений на этапе обучения. Тогда при выключении «полезных» координат произойдёт резкое изменение предсказаний модели, что приведёт к увеличению ошибки, а полученные градиенты этой ошибки укажут, как её исправить с использованием (и изменением) других координат. Сравнение тока информации по исходной модели и по модели с «выключенными» координатами внутренних представлений можно проиллюстрировать с помощью классической картинки: 18 Обратите внимание: «выключать» можно как оригинальные признаки, так и признаки, возникающие на любом другом уровне представления объектов. С точки зрения ( k + 1 ) (k+1)-го слоя нейронной сети данные приходят откуда-то извне: при k = 0 k=0 — из реального мира, а при k > 1 k>1 — с предыдущих слоёв. Технически это осуществляется следующим образом: некоторые координаты внутреннего представления домножаются на ноль. То есть добавляется ещё одно преобразование, которое представляет собой домножение выхода предыдущего слоя на маску из нулей и единиц. mask , x k+1 = 1−p 1 x k ⊙mask, mask i ∼ Bernoulli mask i ∼Bernoulli(1−p). где ( 1 − p ) (1−p) (вероятность обнуления координаты) — это гиперпараметр слоя. Отметим, что во многих фреймворках для глубинного обучения в качестве параметра слоя указывается именно вероятность обнуления, а не выживания. Данная маска участвует и при подсчёте градиентов: mask ∇ x k L= 1−p 1 ∇ x k+1 L⊙mask Как правило, маска генерируется независимо на каждом шаге градиентного спуска. Важно отметить, что на этапе предсказания dropout ничего не меняет, то есть k+1 =x k . Множитель 1 1 − p 1−p 1 нужен для того, чтобы распределение x k + 1 x k+1 на этапе предсказания совпадало с распределением на этапе обучения. В самом деле, если даже математическое ожидание x k x k было равно нулю, выборочная дисперсия x k ⊙ mask x k ⊙mask ниже, чем у x k x k : ведь часть значений обнулилась. На этапе предсказания dropout «выключается»: внутренние представления используются как есть, без умножения на маску. А чтобы слой знал, обучается он сейчас или предсказывает, в нейросетевых библиотеках в классе слоя обычно реализовано переключение между этими режимами (например, булев флаг training в pytorch-модулях). Стоит отметить, что dropout может применяться и к входным данным (то есть слой dropout может стоять первым в сети), и это может приводить к получению более качественных результатов. Например, если в данных множество мультикоррелирующих признаков или присутствует шум, наличие dropout позволит избежать обусловливания модели на лишь их подмножество и позволит учитывать их все. Так, подобный подход может быть использован, если данные представляют собой сильно разреженные векторы высокой размерности (скажем, сведения об интересе пользователя к тем или иным товарам). Batch normalization Появление техники batch normalization привело к значительному ускорению обучения нейронных сетей. В данном параграфе мы рассмотрим лишь основные принципы работы batch normalization. Дискуссия о свойствах и причинах эффективности batch normalization всё ещё ведётся, рекомендуем обратить внимание на статью с NeurIPS 2018. Нам, впрочем, кажется, что, несмотря на активную критику в его адрес, полезно знать и предложенное авторами подхода объяснение необходимости batch normalization. Использование batch normalization гарантирует, что каждая компонента представления на выходе будет иметь контролируемое среднее и дисперсию. Достигается это следующим образом:",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 6,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "собой сильно разреженные векторы высокой размерности (скажем, сведения об интересе пользователя к тем или иным товарам). Batch normalization Появление техники batch normalization привело к значительному ускорению обучения нейронных сетей. В данном параграфе мы рассмотрим лишь основные принципы работы batch normalization. Дискуссия о свойствах и причинах эффективности batch normalization всё ещё ведётся, рекомендуем обратить внимание на статью с NeurIPS 2018. Нам, впрочем, кажется, что, несмотря на активную критику в его адрес, полезно знать и предложенное авторами подхода объяснение необходимости batch normalization. Использование batch normalization гарантирует, что каждая компонента представления на выходе будет иметь контролируемое среднее и дисперсию. Достигается это следующим образом: Сперва идёт собственно слой batch normalization, на котором текущий батч приводится к нулевому среднему и единичной дисперсии: k+1 где μ μ и σ 2 σ 2 — среднее и дисперсия признаков по обрабатываемому батчу, а ε ε — гиперпараметр слоя, небольшое положительное число, добавляемое для улучшения численной устойчивости. Отметим, что μ μ и σ σ, будучи функциями от X k X k , тоже участвуют в вычислении градиентов. В ходе предсказания (или, как ещё говорят, инференса, от английского inference) используются фиксированные значения , которые были получены в ходе обучения как скользящее среднее всех μ μ и σ 2 σ 2 . Более точно: на каждой итерации forward pass мы вычисляем λ+μ(1−λ) λ+σ 2 (1−λ), где λ λ также является гиперпараметром слоя. Далее идёт слой channelwise scaling, который позволяет выучить оптимальное шкалирование для всех признаков X k + 2 X k+2 k+2 =βX k+1 +γ, где β β и γ γ — обучаемые параметры, позволяющие настраивать в ходе обучения оптимальные значения матожидания и дисперсии выходного слоя X k + 2 X k+2 . Ниже приведён алгоритм из оригинальноq статьи 2015 года за авторством Сергея Иоффе и Кристиана Сегеди: 18 Причина популярности batch normalization заключается в значительном ускорении обучения нейронных сетей и в улучшении их сходимости в целом. Рассмотрим график из оригинальной статьи: 18 Как видно на иллюстрации выше, использование batch normalization позволило ускорить обучение в несколько раз и даже добиться лучших результатов, чем SotA-подход 2014 года Inception (структура которого была приведена на одной из иллюстраций в начале этого параграфа). Значительное ускорение достигается в том числе благодаря использованию более высокого learning rate: благодаря нормировке связь между слоями не нарушается столь сильно. Стоит заметить, что причины столь эффективной работы batch normalization до сих пор являются поводом для дискуссий и строгого теоретического объяснения эффекта от batch normalization ещё нет. Несмотря на это, он перевернул область глубинного обучения и вошёл в стандартный инструментарий при обучении нейронных сетей. Примечание: стоит заметить, что в настоящее время существуют и другие способы нормировать промежуточные представления: instance normalization, layer normalization и так далее. В завершение рекомендуем ознакомиться со статьёй о работе метода обратного распространения ошибки в слое batch normalization. Регуляризация через изменение данных Внесение изменений в данные (аугментация данных) также является популярной техникой регуляризации. Рассмотрим её на примере. Пусть перед нами фотография самолёта. Добавим мелкодисперсный шум к изображению. Мы всё ещё сможем увидеть на фотографии самолёт, но, с точки зрения модели машинного обучения (в данном случае — нейронной сети), полученное",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 7,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "область глубинного обучения и вошёл в стандартный инструментарий при обучении нейронных сетей. Примечание: стоит заметить, что в настоящее время существуют и другие способы нормировать промежуточные представления: instance normalization, layer normalization и так далее. В завершение рекомендуем ознакомиться со статьёй о работе метода обратного распространения ошибки в слое batch normalization. Регуляризация через изменение данных Внесение изменений в данные (аугментация данных) также является популярной техникой регуляризации. Рассмотрим её на примере. Пусть перед нами фотография самолёта. Добавим мелкодисперсный шум к изображению. Мы всё ещё сможем увидеть на фотографии самолёт, но, с точки зрения модели машинного обучения (в данном случае — нейронной сети), полученное изображение является новым объектом! Повернём изображение самолёта на 10 градусов по часовой стрелке. В нашем распоряжении ещё одно изображение с известной целевой меткой (например, меткой класса «самолёт»), в котором присутствует поворот. Таким образом, внесение новых данных позволяет дать модели понять, какие преобразования над данными являются допустимыми, и она уже будет более устойчивой к наличию небольшого шума в данных или к поворотам (к которым чувствительна операция свёртки). Вдобавок аугментации позволяют значительно увеличить объём обучающей выборки. Особую популярность аугментации приобрели в области компьютерного зрения. В качестве примера приведём отличную библиотеку, позволяющую производить аугментацию изображений. Стоит обратить внимание, что используемые аугментации должны быть адекватны решаемой задаче. Инвертирование цветов на фотографии, внесение значительного количества шумов или переворот изображения могут привести и к негативным результатам (по сути, просто сделают выборку более зашумлённой или даже заставят сеть учиться на данных, которые она никогда не встретит в реальности), так как обобщающая способность сети ограниченна. Можно сказать, что аугментированные данные должны принадлежать к той же генеральной совокупности, что и оригинальный датасет. Итак, эксперт может привнести своё понимание задачи и на уровне аугментации данных: если данное преобразование является допустимым (то есть преобразованный объект мог бы попасть в обучающую выборку и самостоятельно — как фотография с другого устройства или запись речи другого человека с опечаткой), то модель должна быть устойчива к данным с подобными преобразованиями. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 5.3. Метод обратного распространения ошибки Как эффективно посчитать градиенты по весам нейронной сети Следующий параграф 6.1. Свёрточные нейросети",
    "metadata": {
      "title": "Тонкости обучения",
      "url": "https://education.yandex.ru/handbook/ml/article/tonkosti-obucheniya",
      "course": "ml",
      "chapter": "5. Глубинное обучение - введение",
      "chapter_id": "5.4",
      "part": 8,
      "total_parts": 8,
      "source_file": "5.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе мы на примере задачи распознавания изображений познакомимся со свёрточными нейронными сетями, уже ставшими стандартом в области. Для начала мы разберёмся, с какого рода данными придётся работать, затем попробуем решить задачу «в лоб» при помощи знакомых вам полносвязных сетей и поймём, чем это чревато. А после чего рассмотрим свёртки и попробуем выработать нужную интуицию. Формат данных Картинки в большинстве случаев представляют собой упорядоченный набор пикселей, где каждый пиксель — это вектор из трех «каналов»: интенсивность красного, интенсивность зелёного, интенсивность синего. 2 Каждая интенсивность характеризуется числом от 0 до 1, но для привычных нам изображений этот интервал равномерно дискретизирован для экономии памяти, чтобы уместиться в 8 бит (от 0 до 255). При этом нулевая интенсивность (0, 0, 0), соответствует чёрному цвету, а максимальная интенсивность (255, 255, 255) — белому. Когда мы наблюдаем изображение на мониторе компьютера, мы видим эти пиксели «уложенными» в строки одинаковой длины (человек не сможет воспринять картинку, вытянутую в один вектор). Длину каждой такой строки называют шириной W картинки, а количество строк — высотой H. Резюмирую, мы можем рассматривать картинку, как тензор HxWx3, состоящий из чисел uint8. 1 Существует множество разных форматов хранения картинок: вместо трех интенсивностей мы можем использовать триплет «оттенок, насыщенность, интенсивность», а сами картинки хранить, например, как тензор CxHxW. MLP Наверное, самый простой способ построить нейронную сеть для решения задачи классификации на наших данных — это «развернуть» нашу картинку в вектор, а затем использовать обычную многослойную сеть с кросс-энтропией в качестве лосса. Однако, такой подход имеет несколько недостатков. Недостаток №1: количество параметров В первом слое у нас получается HxWxCxCout параметров, где Cout — это количество нейронов в первом слое. Если поставить Cout слишком маленьким, мы рискуем потерять много важной информации, особенно, если рассматривать картинки размером, например, 1920x1080. Если же выставить Cout большим, рискуем получить слишком много параметров (а это только первый слой), а с этим и все вытекающие проблемы — переобучение, сложность оптимизации и так далее. Недостаток №2: структура данных никак не учитывается. Что здесь имеется в виду под «структурой»? Попробуем объяснить на примере. Для этого рассмотрим картинку щеночка: puppy Если мы сдвинем картинку на несколько пикселей, то мы все еще будем уверены в том, что это щенок: shifted Точно также мы останемся неизменны в своем мнении, если картинку отмасштабировать: scaled или повернуть/развернуть: rotated flipped Получается, что нейронная сеть должна «сама» понять, что ее ответ должен быть инвариантен к описанным преобразованиям. Но, обычно, это достигается за счет увеличения количества нейронов в скрытых слоях (как мы можем помнить из universal approximation theorem), что и так для нас — головная боль из-за первого пункта. С частью этих проблем нам поможет новый «строительный блок» — свёртка. О ней в следующем разделе. Свёртки Строгое определение свёртки мы дадим ниже, а вначале разберёмся в мотивации. Давайте попробуем решить хотя бы проблему инвариантности к сдвигу. Щенок может быть где угодно на картинке, и мы не можем наверняка сказать, в какой части изображения наша модель «лучше всего» научилась видеть щенков. Поэтому для надёжного предсказания будет логично посдвигать картинку на все возможные смещения (пустоты заполним нулями): gif Затем для каждого смещения",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 1,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(как мы можем помнить из universal approximation theorem), что и так для нас — головная боль из-за первого пункта. С частью этих проблем нам поможет новый «строительный блок» — свёртка. О ней в следующем разделе. Свёртки Строгое определение свёртки мы дадим ниже, а вначале разберёмся в мотивации. Давайте попробуем решить хотя бы проблему инвариантности к сдвигу. Щенок может быть где угодно на картинке, и мы не можем наверняка сказать, в какой части изображения наша модель «лучше всего» научилась видеть щенков. Поэтому для надёжного предсказания будет логично посдвигать картинку на все возможные смещения (пустоты заполним нулями): gif Затем для каждого смещения мы предскажем вероятность наличия щенка на картинке. Получившиеся предсказания можно уже агрегировать как удобно: среднее, максимум и так далее. Давайте взглянем на эту операцию под другим углом. Рассмотрим картинку, размером в 3 раза превышающую оригинальную, в центре которой находится наше изображение щеночка: padded Возьмём окно размером с исходную картинку, и будем его сдвигать на все возможные смещения внутри нового изображения: gif Легко видеть, что получается то же самое, как если бы мы картинку сдвигали относительно окна. Представим себе самую простую модель, основанную на данном принципе — что-то вроде ансамбля линейных. Каждую из сдвинутых картинок вытянем в вектор и скалярно умножим на вектор весов (для простоты один и тот же для всех сдвигов) — получим линейный оператор, для которого есть специальное имя — свёртка. Это один из важнейших компонент в свёрточных нейронных сетях. Веса свёртки, упорядоченные в тензор (в нашем случае размерности HxWx3), составляют её ядро. Область картинки, которая обрабатывается в текущий момент, обычно называется окном свёртки. Обратите внимание, что обычно такие свёртки называются двумерными — так как окно свёртки пробегает по двум измерениям картинки (при этом все цветовые каналы участвуют в вычислениях). Следующая картинка поможет разобраться (внимание: на ней нет изображения весов оператора): 11 Каждый «кубик» на картинке — это число. Большой черный тензор слева — это изображение щеночка X X. Фиолетовым на нем выделено окно, из которого мы достаем все пиксели и разворачиваем в вектор (аналогично операции flatten в numpy) v v. Далее этот вектор умножается на вектор весов класса «щенок» w 1 w 1 , и получается число k 1 k 1 — логит интересующего класса. Добавив остальные классы, получим матрицу весов W W — прямо как в мультиномиальной логистической регрессии. Эту операцию мы повторяем для каждого возможного сдвига окна свёртки. Результаты домножения удобно бывает скомпоновать в двумерную табличку, которую при желании можно трактовать, как некоторую новую картинку (в серых тонах, потому что канал уже только один). Воспользуемся этим, чтобы получше осознать, что происходит в ходе свёртки. Вопрос на подумать. Какой геометрический смысл имеет свёртка с ядром А с ядром Вопрос на подумать. На краях картинок из ответа к предыдущему вопросу заметны тёмные рамки. Что это такое? Откуда они берутся? Решив проблему обеспечения устойчивости к сдвигу картинки и имея на руках наш огромный свёрточный фильтр, давайте попробуем теперь справиться с первой проблемой — количество параметров. Самое простое, что можно придумать, — это уменьшить размер окна с HxW до, допустим, kxk (обычно нечётное и k∈[3,11]). В этом случае получается",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 2,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "(в серых тонах, потому что канал уже только один). Воспользуемся этим, чтобы получше осознать, что происходит в ходе свёртки. Вопрос на подумать. Какой геометрический смысл имеет свёртка с ядром А с ядром Вопрос на подумать. На краях картинок из ответа к предыдущему вопросу заметны тёмные рамки. Что это такое? Откуда они берутся? Решив проблему обеспечения устойчивости к сдвигу картинки и имея на руках наш огромный свёрточный фильтр, давайте попробуем теперь справиться с первой проблемой — количество параметров. Самое простое, что можно придумать, — это уменьшить размер окна с HxW до, допустим, kxk (обычно нечётное и k∈[3,11]). В этом случае получается радикальное снижение количества параметров и сложности вычислений. gif К сожалению, с таким подходом возникает новая проблема: предсказание для какого-то окна никак не учитывает контекст вокруг него. Получается, мы можем получить разумные предсказания только в случае, если распознаваемый объект обладает признаками, которые «помещаются» в окно свёртки (например, лого автомобиля при классификации марок машин), либо объекты заметно отличаются по своей текстуре (шерсть кошки vs кирпич, например). На картинке ниже сделана попытка изобразить проблему: very Область картинки, на которую «смотрит» наша нейронная сеть, называется receptive field — и про него приходится часто думать в задачах компьютерного зрения. Давайте и мы подумаем, как его можно было бы увеличить, не увеличивая размер ядра. Вспомним, что в нашей нейронке сейчас есть только один слой, сразу предсказывающий класс. Выглядит так, что мы можем применить уже знакомую технику стекинга слоев: пусть на первой стадии мы делаем C 1 C 1 разных свёрток с фильтрам размером kxk. Результаты каждой свёртки можно упорядочить в виде новой «картинки», а из этих «картинок» сложить трёхмерный тензор. Получаем так называемую карту признаков размером HxWxC_1. Применим к ней поэлементно нелинейность и воспользуемся K новыми свёртками для получения предсказаний для каждого пикселя. На таком шаге получается, что наш receptive field для финальных нейронов вырос от kxk до (2k-1)x(2k-1) (пояснение на картинке). Повторяя такую операцию, мы можем добиться, чтобы наши финальные нейроны уже могли «видеть» почти всю нужную информацию для хорошего предикта. Более того, у нас возникает меньшее количество параметров и падает сложность вычислений в сравнении с использованием одной большой полносвязной сети. Как это схематично выглядит: 15 Промежуточный тензор L 1 L 1 , полученный при помощи C 1 C 1 свёрток, можно себе представить, как новую картинку, у которой уже C 1 C 1 каналов. На следующей картинке можно отследить, как меняется receptive field в зависимости от глубины: 16 На картинке схематично изображен «плоский» двумерный тензор (количество каналов = 1), к которому последовательно применили три свёртки 3x3. В каждом случае рассматривается пиксель в центре. Каждый соответствующий тензор помечен, как L i L i . Если рассматривать первую свёртку ( X → L 1 X→L 1 ), то размер receptive field равен размеру е окна = 3. Рассмотрим вторую свёртку . В ее вычислении участвуют пиксели из квадрата 3х3, причём каждый из них, в свою очередь, был получен при помощи предыдущей свёртки X → L 1 X→L 1 . Получается, что receptive field композиции свёрток X→L 1 →L 2 — это объединение receptive fields свёртки X",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 3,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "тензор (количество каналов = 1), к которому последовательно применили три свёртки 3x3. В каждом случае рассматривается пиксель в центре. Каждый соответствующий тензор помечен, как L i L i . Если рассматривать первую свёртку ( X → L 1 X→L 1 ), то размер receptive field равен размеру е окна = 3. Рассмотрим вторую свёртку . В ее вычислении участвуют пиксели из квадрата 3х3, причём каждый из них, в свою очередь, был получен при помощи предыдущей свёртки X → L 1 X→L 1 . Получается, что receptive field композиции свёрток X→L 1 →L 2 — это объединение receptive fields свёртки X → L 1 X→L 1 по всем пикселям из окна свёртки , образуя новый, размером 5x5. Аналогичные рассуждения можно повторить и для всех последующих свёрток. Ещё один способ увеличить receptive field — это использовать dilated convolution, в которых окно свёртки (то есть те пиксели картинки, на которые умножается ядро) не обязано быть цельным, а может идти с некоторым шагом (вообще говоря, даже разным по осям H и W). Проиллюстрируем, как будет выглядеть окно для обычной свёртки и для свёртки с шагом dilation=2: dilated Если установить параметр dilation=(1,1), получится обычная свёртка. Итак, свёртки помогли нам решить сразу две проблемы: устойчивости к сдвигу и минимизации числа параметров. Теперь давайте попробуем определить оператор более формально. Формальное определение свёртки 18 Вопрос на подумать. Пусть у нас есть тензор размером HxWxC_{in}, к которому одновременно применяется C o u t C out свёрток, размер окна каждой равен kxk. Посчитайте количество обучаемых параметров. Как изменится формула, если к свёртке добавить смещение (bias)? Во сколько раз изменится количество параметров, если увеличить размер окна в 2 раза? А если увеличить количество каналов out в два раза? А если увеличить размер входного тензора в 2 раза по высоте и ширине? Вопрос на подумать. Оцените количество операций сложений-умножений для предыдущего упражнения. Как оно поменяется, если увеличить в два раза размер окна? Количество каналов? Размер входного тензора? Вопрос на подумать. Пусть последовательно применяется N N свёрток k × k k×k. Посчитайте размер receptive field для последнего оператора. Свёртки не только для изображений Нетрудно видеть, что аналоги двумерной свёртки можно определить и для тензоров другой размерности, в любой ситуации, когда для нас актуально поддерживать устойчивость модели к сдвигам данных. Например, это актуально для работы с текстами. Обычно текст разбивается на последовательные токены (например, на слова или какие-то subword units), и каждому из этих токенов ставится в соответствие вектор (более подробно об этом вы можете почитать в параграфе про работу с текстами или в разделе про вложения слов учебника по NLP Лены Войта). cnn Представим теперь, что мы хотим определить, позитивно или негативно окрашен этот текст. Мы можем предположить, что эмоциональная окраска локальна и может проявляться на любом участке текста, и тогда нам нужна модель, которая может «посмотреть» отдельно на каждый последовательный фрагмент текста некоторой длины. И здесь тоже может сработать свёртка: cnn Существуют свёртки и для тензоров более высокой размерности, например, для видео (где прибавляется ещё координата «время»). Поворот, отражение, масштабирование А что делать с остальными проблемами: поворотом, отражением, масштабированием? К сожалению, на",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 4,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "можете почитать в параграфе про работу с текстами или в разделе про вложения слов учебника по NLP Лены Войта). cnn Представим теперь, что мы хотим определить, позитивно или негативно окрашен этот текст. Мы можем предположить, что эмоциональная окраска локальна и может проявляться на любом участке текста, и тогда нам нужна модель, которая может «посмотреть» отдельно на каждый последовательный фрагмент текста некоторой длины. И здесь тоже может сработать свёртка: cnn Существуют свёртки и для тензоров более высокой размерности, например, для видео (где прибавляется ещё координата «время»). Поворот, отражение, масштабирование А что делать с остальными проблемами: поворотом, отражением, масштабированием? К сожалению, на момент написания параграфа (вторая половина 2021 года), автору не было известно об успешном опыте решения этих проблем в архитектуре сети. При этом оказывается, что приведенного оператора уже достаточно, чтобы нейронная сеть могла хорошо обобщать на невиданные ранее картинки (лишь бы свёрток было больше и сеть глубже). В качестве потенциально интересного (но пока не проявившего себя на практике) направления исследований можно упомянуть капсульные нейросети. Кроме того, вам может быть интересно познакомиться с геометрическим глубинным обучением. В качестве короткого введения рекомендуем посмотреть вот этот keynote с ICLR 2021, которое ставит своей целью исследование общих принципов, связывающих устойчивость к различным преобразованием и современные нейросетевые архитектуры (авторы сравнивают свои идеи с эрлангенской программой Феликса Кляйна — отсюда название). Свёрточный слой и обратное распространение ошибки Поговорим о том, как через свёрточный слой протекают градиенты. Нам нужно будет научиться градиент по выходу превращать в градиент по входу и в градиент по весам из ядра. Начнём с иллюстрации для одномерной свёртки с одним входным каналом, ядром длины 3 3 с дополнением по бокам нулями. Заметим, что её можно представить в виде матричного умножения: ,…,x d )∗(w =(0,x 1 ,…,x d ,0)⋅ =(x 1 ,…,x Обозначим последнюю матрицу через W ^ W , а ядро свёртки через W W. Что происходит с градиентом при переходе через матричное умножение, мы уже отлично знаем. Градиент по весам равен L=∇ Разберёмся, что из себя представляет умножение на W ^ T W T справа. Эта матрица имеет вид Она тоже соответствует свёртке, только: с симметричным исходному ядром с дополнением вектора нулями (это как раз соответствует неполным столбцам: можно считать, что «выходящие» за границы матрицы и отсутствующие в ней элементы умножаются на нули). Вопрос на подумать. Поменяется ли что-нибудь, если исходный вектор не дополнять нулями? Общий случай Рассмотрим теперь двумерную свёртку, для простоты нечётного размера и без свободного члена (X∗W) ijc = p=1 ∑ c in k 1 =−k ∑ k k 2 =−k ∑ k W k+1+k 1 ,k+1+k 2 ,p c X i+k 1 ,j+k 1 ,p Продифференцируем по X s t l X stl stl ∂L = i,j,c ∑ ∂X stl ∂(X∗W) ijc ⋅ ∂(X∗W) ijc ∂L Разберёмся с производной stl ∂(X∗W) ijc . Во всей большой сумме из определения свёртки для (X∗W) ijc элемент X s t l X stl может встретиться в позициях i+k 1 ,j+k 2 ,l при i+k 1 =s, j+k 2 =t и всевозможных c c, причём это возможно лишь если =s−i∈{−k,…,k}, =t−j∈{−k,…,k} (для всех",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 5,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "∑ c in k 1 =−k ∑ k k 2 =−k ∑ k W k+1+k 1 ,k+1+k 2 ,p c X i+k 1 ,j+k 1 ,p Продифференцируем по X s t l X stl stl ∂L = i,j,c ∑ ∂X stl ∂(X∗W) ijc ⋅ ∂(X∗W) ijc ∂L Разберёмся с производной stl ∂(X∗W) ijc . Во всей большой сумме из определения свёртки для (X∗W) ijc элемент X s t l X stl может встретиться в позициях i+k 1 ,j+k 2 ,l при i+k 1 =s, j+k 2 =t и всевозможных c c, причём это возможно лишь если =s−i∈{−k,…,k}, =t−j∈{−k,…,k} (для всех остальных (X∗W) ijc производная по X s t l X stl нулевая). Соответствующий коэффициент при X s t l X stl будет равен k+1+k 1 ,k+1+k 2 ,c . Таким образом, производная будет иметь вид: out stl ∂L = c=1 ∑ c out k 1 =−k ∑ k k 2 =−k ∑ k W k+1+k 1 ,k+1+k 2 ,c ⋅ ∂(X∗W) s−k 1 ,t−k 2 ,c ∂L Легко заметить, что это тоже свёртка, но поскольку индексы W и в ∂(X∗W) ∂L стоят с разными знаками, получаем, что ∇ X L = W [::-1,::-1,:] L=W[::-1,::-1,:]∗∇ X∗W L Продифференцируем по i,j,c ∑ ∂W ab q ∂(X∗W) ijc ⋅ ∂(X∗W) ijc ∂L В формуле для (X∗W) ijc элемент может встретиться в позициях k+1+k 1 ,k+1+k 2 q , для k+1+k 1 =a, k+1+k 2 =b, с коэффициентами i+k 1 ,j+k 2 ,p (для любых p p). Значит, производная будет иметь вид: p=1 ∑ c in i=1 ∑ H j=1 ∑ W X a−k−1,b−k−1,p ⋅ ∂(X∗W) a−k−1,b−k−1,q ∂L В этой формуле тоже нетрудно узнать свёртку: L=X∗∇ X∗W L Вопрос на подумать. Если всё-таки есть свободные члены, как будет выглядить градиент по b c b c ? Остальные важные блоки свёрточных нейронных сетей Наигравшись с нашими мысленными экспериментами, давайте обратимся к опыту инженеров и исследователей, который копился с 2012 года – alexnet. Он поможет нам разобраться с тем, как эффективней всего строить картиночные нейронки. Здесь будут перечислены самые важные (на момент написания и по мнению автора) блоки. Max pool Каждая из C C свёрток очередного свёрточного слоя — это новая карта признаков для нашего изображения, и нам, конечно, хотелось бы, чтобы таких карт было побольше: ведь это позволит нам выучивать больше новых закономерностей. Но для картинок в высоком разрешении это может быть затруднительно: слишком уж много будет параметров. Выходом оказалось использование следующей эвристики: сначала сделаем несколько свёрток с C 1 C 1 каналами, а затем как-нибудь уменьшим нашу карту признаков в 2 раза и одновременно увеличим количество свёрток во столько же. Посчитаем, как в таком случае изменится число параметров: было H×W×K×K×C 1 ×C 1 , стало (H/2)×(W/2)×K×K×(C 1 ×2)×(C 1 ×2)=H×W×K×K×C 1 ×C 1 , то есть, ничего не изменилось, а количество фильтров удвоилось, что приводит к выучиванию более сложных зависимостей. Осталось разобраться, как именно можно понижать разрешение картинки. Тривиальный способ — взять все пиксели с нечетными индексами. Такой подход будет работать, но, как может подсказать здравый смысл, выкидывать пиксели — значит терять информацию, а",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 6,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "несколько свёрток с C 1 C 1 каналами, а затем как-нибудь уменьшим нашу карту признаков в 2 раза и одновременно увеличим количество свёрток во столько же. Посчитаем, как в таком случае изменится число параметров: было H×W×K×K×C 1 ×C 1 , стало (H/2)×(W/2)×K×K×(C 1 ×2)×(C 1 ×2)=H×W×K×K×C 1 ×C 1 , то есть, ничего не изменилось, а количество фильтров удвоилось, что приводит к выучиванию более сложных зависимостей. Осталось разобраться, как именно можно понижать разрешение картинки. Тривиальный способ — взять все пиксели с нечетными индексами. Такой подход будет работать, но, как может подсказать здравый смысл, выкидывать пиксели — значит терять информацию, а этого не хотелось бы делать. Здесь есть много вариантов: например, брать среднее/максимум по обучаемым весам в окне 2x2, которое идет по карте признаков с шагом 2. Экспериментально выяснилось, что максимум — хороший выбор, и, в большинстве архитектур, используют именно его. Обратите внимание, что максимум берется для каждого канала независимо. Еще одно преимущество — увеличение receptive field. Получается, что он увеличивается в 2 раза: 21 Операция понижения разрешения со взятием максимума в окне называется max pooling, а со взятием среднего — average pooling. Вопрос на подумать. Как будет преобразовываться градиент во время error backpropagation для maxpool с окном и шагом 2x2? А для average pool? Кстати, ещё один способ уменьшать размер карт признаков по ходу применения свёрточной сети — использование strided convolution, в которых ядро свёртки сдвигается на каждом шаге на некоторое большее единицы число пикселей (возможно, разное для осей H и W; обычная свёртка получается, если установить параметр stride=(1,1)). gif Global average pool Как свёрточные слои, так и пулинг превращают картинку в «стопку» карт признаков. Но если мы решаем задачу классификации или регрессии, то в итоге нам надо получить число (или вектор логитов, если речь про многоклассовую классификацию). Один из способов добиться этого — воспользоваться тем, что свёртка без дополнения нулями и пулинг уменьшают размер карты признаков, и в итоге при должном терпении и верном расчёте мы можем получить тензор 1x1xC (финальные, общие признаки изображения), к которому уже можно применить один или несколько полносвязных слоёв. Или же можно, не дождавшись, пока пространственные измерения схлопнутся, «растянуть» всё в один вектор и после этого применить полносвязные слои (именно так, как мы не хотели делать, не правда ли?). Примерно так и происходило в старых архитектурах (alexnet, vgg). Вопрос на подумать. Попробуйте соорудить конструкцию из свёточных слоёв и слоёв пулинга, превращающую изображение размера 128x128x3 в тензор размера 1x1xC. Но у такого подхода есть как минимум один существенный недостаток: для каждого размера входящего изображения нам придётся делать новую сетку. Позднее было предложено следующее: после скольких-то свёрточных слоёв мы будем брать среднее вдоль пространственных осей нашего последнего тензора и усреднять их активации, а уже после этого строить MLP. Это и есть Global Average Pooling. У такого подхода есть несколько преимуществ: радикально меньше параметров; теперь мы можем применять нейронку к картинку любого размера; мы сохраняем «магию» инвариантности предсказаний к сдвигам. 23 Residual connection Оказывается, что, если мы будем бесконтрольно стекать наши свёртки, то, несмотря на использование relu и batch normalization, градиенты все равно будут затухать, и на",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 7,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "минимум один существенный недостаток: для каждого размера входящего изображения нам придётся делать новую сетку. Позднее было предложено следующее: после скольких-то свёрточных слоёв мы будем брать среднее вдоль пространственных осей нашего последнего тензора и усреднять их активации, а уже после этого строить MLP. Это и есть Global Average Pooling. У такого подхода есть несколько преимуществ: радикально меньше параметров; теперь мы можем применять нейронку к картинку любого размера; мы сохраняем «магию» инвариантности предсказаний к сдвигам. 23 Residual connection Оказывается, что, если мы будем бесконтрольно стекать наши свёртки, то, несмотря на использование relu и batch normalization, градиенты все равно будут затухать, и на первых слоях будут почти нулевыми. Интересное решение предлагают авторы архитектуры resnet: давайте будем «прокидывать» признаки на предыдущем слое мимо свёрток на следующем: 24 Таким образом получается, что градиент доплывет даже до самых первых слоев, что существенно ускоряет сходимость и качество полученной модели. Вопрос: почему именно сумма? Может, лучше конкатенировать? Авторы densenet именно такой подход и предлагают (с оговорками), получая результаты лучше, чем у resnet. Однако, такой подход получается вычислительно сложным и редко используется на практике. Регуляризация Несмотря на наши ухищрения со свёртками, в современных нейронных сетях параметров все равно оказывается больше, чем количество картинок. Поэтому часто оказывается важным использовать различные комбинации регуляризаторов, которых уже стало слишком много, чтобы все описывать в этом параграфе, так что мы рассмотрим лишь несколько наиболее важных. Классические Почти все регуляризаторы, которые использовались в классической машинке и полносвязных сетях, применимы и здесь: l1/l2, dropout и так далее. Вопрос на подумать. Насколько разумно использовать dropout в свёрточных слоях? Как можно модифицировать метод, чтобы он стал «более подходящим»? Аугментации Это один из самых мощных инструментов при работе с картинками. Помогает, даже если картинок несколько тысяч, а нейронная сеть с миллионами параметров. Мы уже выяснили, что смещение\\поворот\\прочее не меняют (при разумных параметрах) факта наличия на картинке того или иного объекта. На самом деле, есть огромное множество операций, сохраняющих это свойство: сдвиги, повороты и отражения; добавление случайного гауссового шума; вырезание случайной части картинки (cutout); перспективные преобразования; случайное изменение оттенка\\насыщенности\\яркости для всей картинки; и многое другое. Пример хорошой библиотеки с аугментациями: Albumentations. Label smoothing Часто оказывается, что нейронная сеть делает «слишком уверенные предсказания»: 0.9999 или 0.00001. Это становится головной болью, если в нашей разметке есть шум — тогда градиенты на таких объектах могут сильно портить сходимость. Исследователи пришли к интересной идее: давайте предсказывать не one-hot метку, а ее сглаженный вариант. Итак, пусть у нас есть K K классов: ohot =(0,0,…,1,…,0) k−1 ε , k−1 ε ,…,1−ε, k−1 ε ,…, k−1 ohot Обычно берут ε = 0.1 ε=0.1. Тем самым модель штрафуется за слишком уверенные предсказания, а шумные лейблы уже не вносят такого большого вклада в градиент. Mixup Самый интересный вариант. А что будет, если мы сделаем выпуклую комбинацию двух картинок и их лейблов: ml где α α обычно семплируется из какого-нибудь Бета распределения. Оказывается, что такой подход заставляет модель выучивать в каком-то смысле более устойчивые предсказания, так как мы форсируем некую линейность в отображении из пространства картинок в пространство лейблов. На практике часто оказывается, что это дает значимое улучшение в качестве",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 8,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ε , k−1 ε ,…,1−ε, k−1 ε ,…, k−1 ohot Обычно берут ε = 0.1 ε=0.1. Тем самым модель штрафуется за слишком уверенные предсказания, а шумные лейблы уже не вносят такого большого вклада в градиент. Mixup Самый интересный вариант. А что будет, если мы сделаем выпуклую комбинацию двух картинок и их лейблов: ml где α α обычно семплируется из какого-нибудь Бета распределения. Оказывается, что такой подход заставляет модель выучивать в каком-то смысле более устойчивые предсказания, так как мы форсируем некую линейность в отображении из пространства картинок в пространство лейблов. На практике часто оказывается, что это дает значимое улучшение в качестве модели. Бонус №1: знаковые архитектуры в мире свёрточных нейронных сетей для задачи классификации изображений Дисклеймер: это мнение одного автора. Приведённые в этом разделе вехи связаны преимущественно с архитектурами моделей, а не способом их оптимизации. Здесь перечислены знаковые архитектуры, заметно повлиявшие на мир свёрточных нейронных сетей в задаче классификации картинок (и не только). К каждой архитектуре указана ссылка на оригинальную статью, а также комментарий автора параграфа с указанием ключевых нововведений. Значение метрики error rate на одном из влиятельных датасетов imagenet указано для финального ансамбля из нейросетей, если не указано иное. Зачем это полезно изучить (вместе с чтением статей)? Основных причин две: Общее развитие. Полезно понимать, откуда взялись и чем мотивированы те или иные компоненты. Этот вопрос задают на собеседовании, когда не знают, что еще спросить 😃 lenet (1998) Ссылка на статью 7 слоев Первая свёрточная нейронная сеть, показавшая SOTA (State Of The Art) результаты на задаче классификации изображений цифр MNIST. В архитектуре впервые успешно использовались свёрточные слои с ядром 5x5. В качестве активации использовался tanh, а вместо max pool в тот момент использовался average. alexnet (2012) Ссылка на статью 11 слоев Первая CNN (Convolutional Neural Network), взявшая победу на конкурсе imagenet. Автор предложил использовать ReLU вместо сигмоид (чтобы градиенты не затухали) и популяризовал max-pool вместо average. Что самое важное, обучение модели было перенесено на несколько GPU, что позволило обучать достаточно большую модель за относительное небольшое время (6 дней на двух видеокартах того времени). Также автор обратил внимание, что глубина нейросети важна, так как выключение хотя бы одного слоя стабильно ухудшало качество на несколько процентов. network in network (2013) Ссылка на статью В статье не привели интересных SOTA результатов, но зато ввели два очень популярных впоследствии модуля. Первый — это GAP (Global Average Pooling), который стоит после последнего свёрточного слоя и усредняет все активации вдоль пространственных осей. Второй — стекинг 1x1 свёрток поверх 3x3, что эквивалентно тому, что вместо линейной свёртки используется полносвязный слой. vgg (2014) Ссылка на статью 19 слоев Авторы предложили декомпозировать большие свёртки (5x5, 7x7 и выше) на последовательное выполнение свёрток 3x3 с нелинейностями между ними. Впоследствии, за нечастым исключением, свёртки 3x3 стали стандартом в индустрии (вместе со свёртками 1x1). googleLeNet aka Inception (2014) Ссылка на статью 22 слоя Ввели inception слой, просуществовавший довольно продолжительное время. Сейчас сам слой уже не используется, но идея лежащая в его основе, эксплуатируется. Идея следующая: будем параллельно применять свёртки с разным пространственными размерами ядер, чтобы можно было одновременно обрабатывать как low-, так и",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 9,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "поверх 3x3, что эквивалентно тому, что вместо линейной свёртки используется полносвязный слой. vgg (2014) Ссылка на статью 19 слоев Авторы предложили декомпозировать большие свёртки (5x5, 7x7 и выше) на последовательное выполнение свёрток 3x3 с нелинейностями между ними. Впоследствии, за нечастым исключением, свёртки 3x3 стали стандартом в индустрии (вместе со свёртками 1x1). googleLeNet aka Inception (2014) Ссылка на статью 22 слоя Ввели inception слой, просуществовавший довольно продолжительное время. Сейчас сам слой уже не используется, но идея лежащая в его основе, эксплуатируется. Идея следующая: будем параллельно применять свёртки с разным пространственными размерами ядер, чтобы можно было одновременно обрабатывать как low-, так и high-level признаки. Еще полезной для сообщества оказалась идея с dimensionality reduction: перед тяжелой операцией поставим свёртку 1x1, чтобы уменьшить количество каналов и кратно ускорить вычисление. batch normalization (2015) Ссылка на статью Авторы внедрили вездесущую batch normalization, которая стабилизирует сходимость, позволяя увеличить шаг оптимизатора и скорость сходимости. Применив идею к архитектуре inception, они превзошли человека на imagenet. kaiming weight initialization (2015) Ссылка на статью В статье предложили использовать инициализацию весов, берущую во внимание особенность активации ReLU (в предыдущих работах предполагалось, что Var[x]=E[x 2 ], что, очевидно, нарушается для =max(0,x)). Применение этой и других «свистелок» на VGG19 позволило существенно уменьшить ошибку на imagenet. ResNet (2015) Ссылка на статью 152 слоя Архитектура, которая на момент написания этого параграфа до сих пор бейзлайн и отправная точка во многих задачах. Основная идея — использование skip connections, что позволило градиенту протекать вплоть до первых слоев. Благодаря этому эффекту получилось успешно обучать очень глубокие нейронные сети, например, с 1202 слоями (впрочем, результаты на таких моделях менее впечатляющие, чем на 152-слойной). После этой статьи также стали повсеместно использоваться GAP и уменьшение размерности свёртками 1x1. MobileNet (2017) Ссылка на статью Очень популярная модель для быстрого инференса (на мобильных устройствах или gpu). По качеству хоть и немного проигрывает «монстрам», но в индустрии, оказывается, зачастую этого достаточно (особенно если брать последние варианты модели). Основная деталь — это использование depthwise convolutions: параллельный стекинг свёрток 3x3x1x1 — то есть таких, в которых вычисление для каждого с out с out канала просходит только на основе признаков одного c in c in канала. Чтобы скомбинировать фичи между каналами, используется классическая 1x1 свёртка. EfficientNet (2019) Ссылка на статью Одна из первых моделей, полученных при помощи NAS (Neural Architecture Search), которая взяла SOTA на imagenet. После этого, модели, где компоненты подбирались вручную, уже почти не показывали лучших результатов на классических задачах. Бонус №2: не классификацией единой Свёрточными нейронными сетями можно решать большой спектр задач, например: Сегментация. Если убрать в конце слои GlobalAveragePool или flatten, то можно делать предсказания для каждого пикселя в отдельности (подумайте, что делать, если в сети есть maxpool) — получаем сегментацию картинки. Проблема — долгая и дорогая разметка. Детекция. Часто намного дешевле получить разметку объектов обрамляющими прямоугольниками. Здесь уже можно для каждого пикселя предсказывать размеры прямоугольника, который обрамляет объект, к которому принадлежит пиксель. Проблемы — нужен этап агрегации прямоугольников + много неоднозначностей во время разметки + много эверистик на всех этапах + данных нужно больше. Понимание видео. Добавляем в тензор новый канал — временной,",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 10,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нейронными сетями можно решать большой спектр задач, например: Сегментация. Если убрать в конце слои GlobalAveragePool или flatten, то можно делать предсказания для каждого пикселя в отдельности (подумайте, что делать, если в сети есть maxpool) — получаем сегментацию картинки. Проблема — долгая и дорогая разметка. Детекция. Часто намного дешевле получить разметку объектов обрамляющими прямоугольниками. Здесь уже можно для каждого пикселя предсказывать размеры прямоугольника, который обрамляет объект, к которому принадлежит пиксель. Проблемы — нужен этап агрегации прямоугольников + много неоднозначностей во время разметки + много эверистик на всех этапах + данных нужно больше. Понимание видео. Добавляем в тензор новый канал — временной, считаем четырехмерные свёртки — и получаем распознавание сцен на видео. Metric learning. Часто мы не можем собрать все интересующие нас классы, например, в задаче идентификации человека по лицу (или товара на полке). В этом случае используют такой трюк: научим модель в некотором смысле (обычно по косиносному расстоянию) разделять эмбеддинги существующих классов (уникальных людей). Если на руках была репрезентативная выборка, то модель, скорее всего (а обычно — всегда), выучит генерировать дискриминативные эмбеддинги, которые уже позволят различать между собой ранее невиданные лица. и многое другое Итого Мы разобрались, что для картинок эффективно использовать свёрточные фильтры в качестве основных операторов. Выяснили, какие основные блоки есть почти в каждой картиночной нейронной сети и зачем они там нужны. Разобрались, какие методы регуляризаторы сейчас самые популярные и какая за ними идея. И наконец — рассмотрели знаковые архитектуры в мире свёрточных нейронных сетей. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 5.4. Тонкости обучения Инициализация весов. Регуляризация нейросетей. Dropout и Batchnorm Следующий параграф 6.2. Нейросети для работы с последовательностями",
    "metadata": {
      "title": "Свёрточные нейросети",
      "url": "https://education.yandex.ru/handbook/ml/article/svyortochnye-nejroseti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.1",
      "part": 11,
      "total_parts": 11,
      "source_file": "6.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом разделе вы познакомитесь с нейросетями для работы с данными, имеющими вид последовательностей некоторых токенов. Это может быть музыка или видео, временные ряды или траектория движения робота, последовательности аминокислот в белке или много чего ещё, но одним из самых богатых источников таких данных является Natural Language Processing (NLP). Как следует из названия, Natural Language Processing (обработка естественного языка) — это область data science, посвященная анализу текстов, написанных на естественных (человеческих) языках. С задачами обработки текста мы встречаемся каждый день, например, когда просим Siri или Алису включить любимую песню или добавить напоминание в календарь, когда используем автодополнение при вводе поискового запроса или проверяем орфографию и пунктуацию с помощью специальных программ. Вот ещё несколько примеров задач, относящихся к обработке естественного языка: классификация документов (по темам, рубрикам, жанрам и так далее); определение спама; определение частей речи; исправление орфографических ошибок и опечаток; поиск ключевых слов, синонимов / антонимов в тексте; распознавание именованных сущностей (имен, названий географических объектов, дат, номеров телефонов, адресов); определение эмоциональной окраски текста (sentiment analysis); поиск релевантных документов по запросу, а также их ранжирование; задача суммаризации (автоматическое составление краткого пересказа текста); автоматический перевод с одного языка на другой (машинный перевод); диалоговые системы и чат-боты; вопросно-ответные системы (выбор ответа из нескольких предложенных вариантов или вопросы с открытым ответом); кроме того, к NLP также относят задачу распознавания речи (Automated Speech Recognition, ASR). Для работы с такими данными есть несколько возможных режимов: Many-to-one. На вход подается последовательность объектов, на выходе один объект. Пример 2: классификация текстов или видео. Пример 2: тематическая классификация. По предложению нефиксированной длины генерируем вектор вероятностей упоминания заранее фиксированных тем во входном предложении. Размерность выходного вектора постоянна и равна количеству тем. One-to-many. На вход подается один объект, на выходе последовательность объектов. Пример: генерация заголовка к изображению (image captioning). Many-to-many. На входе и выходе последовательности нефиксированной длины. Примеры: машинный перевод, суммаризация текста, генерация заголовка к статье. Синхронизированный вариант many-to-many. На входе и выходе последовательности одинаковой длины, токены входной явно сопоставлены соответствующим токенам выходной. Пример: генерация покадровых субтитров к видео, PoS-tagging (part of speech tagging, для каждого слова в предложении предсказываем, что это за часть речи). sec Мы начнём с архитектур, в которых размер выхода предсказуемым образом зависит от размера входа: many-to-one и синхронизованном варианте many-to-many — но в итоге доберёмся и до остальных. Word Embeddings Перед тем, как рассказать об архитектурах, которые часто используются для работы с текстами, надо разобраться, каким образом можно кодировать текстовые данные: ведь нужно их превратить во что-то векторное, прежде чем подавать на вход нейросети. К векторизации текстов есть два базовых подхода: векторизовать текст целиком, превращая его в один вектор; векторизовать отдельные структурные единицы, превращая текст в последовательность векторов. Первые, статистические подходы к векторизации следовали первому подходу и рассматривали текст как неупорядоченный набор («мешок») токенов (обычно токенами являются слова). Тем самым, тексты «Я не люблю ML» и «Я люблю не ML» получали одинаковые векторизации, то есть по ходу терялась существенная информация. Поэтому мы лишь коротко упомянем о них. Обратимся теперь к другому подходу и подумаем, как сопоставить векторы (эмбеддинги) словам. Допустим, что у нас одно и то же",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 1,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "векторное, прежде чем подавать на вход нейросети. К векторизации текстов есть два базовых подхода: векторизовать текст целиком, превращая его в один вектор; векторизовать отдельные структурные единицы, превращая текст в последовательность векторов. Первые, статистические подходы к векторизации следовали первому подходу и рассматривали текст как неупорядоченный набор («мешок») токенов (обычно токенами являются слова). Тем самым, тексты «Я не люблю ML» и «Я люблю не ML» получали одинаковые векторизации, то есть по ходу терялась существенная информация. Поэтому мы лишь коротко упомянем о них. Обратимся теперь к другому подходу и подумаем, как сопоставить векторы (эмбеддинги) словам. Допустим, что у нас одно и то же слово будет представлено одним и тем же вектором во всех текстах и в любых позициях. Как заключить в векторе его смысл, содержающуюся в нём информацию? Ответ предвосхищает одну из основных идей обучения представлений: нужно использовать контекст. Если, читая книгу на иностранном языке, вы встречаете незнакомое слово, вы нередко можете угадать его значение по контексту, что оно значит. Можно сказать, что смысл слова — это те слова, которые встречаются с ним рядом. Одним из воплощений такого подхода является Word2vec. Впервые он был предложен Т.Миколовым в 2013 году в статье Efficient Estimation of Word Representations in Vector Space. Для обучения авторы предложили две стратегии: Skip-gram и CBOW (Сontinuous bag-of-words): В архитектуре CBOW модель учится предсказывать данное (центральное) слово по контексту (например, по двум словам перед данным и двум словам после него). В архитектуре Skip-gram модель учится по слову предсказывать контекст (например, каждого из двух соседей слева и справа); cbow Авторы предложили для каждого слова w w обучать два эмбеддинга: , первое из которых используется, когда w w является центральным, а второе — когда оно рассматривается, как часть контекста. В модели CBOW при фиксированном контексте context context вычисляются логиты context logits u =⟨ w∈ context после чего «вероятности» всевозможных слов u u быть центральным словом для контекста context context вычисляются как softmax softmax(logits). Модель учится с помощью SGD на кросс-энтропию полученного распределения с истинным рапределением центральных слов. CBOW В модели Skip-gram по данному центральному слову u u для каждой позиции контекста context context независимо предсказывается распределение вероятностей. В качестве функции потерь выступает сумма кросс-энтропий распределений слов контекста с их истинными распределениями. Skip Размерность эмбеддинга в каждой из архитектур — это гиперпараметр и подбирается эмпирически. В оригинальной статье предлагается взять размерность эмбеддинга 300. Полученные представления центральных слов могут дальше использоваться в качестве эмбеддингов слов, которые сохраняют семантическую связь слов друг с другом. Мы не будем здесь останавливаться подробно на деталях работы Word2vec и его современных модификациях и предложим читателю обратиться к соответствующей лекции в учебнике Лены Войта по NLP. А мы лишь продемонстрируем, что он работает. Примеры. Возьмём несколько слов и посмотрим, как выглядят топ-10 слов, ближайших к ним в пространстве эмбеддингов (обученных на одном из датасетов Quora Questions с помощью word2vec): quantum: electrodynamics, computation, relativity, theory, equations, theoretical, particle, mathematical, mechanics, physics; personality: personalities, traits, character, persona, temperament, demeanor, narcissistic, trait, antisocial, charisma; triangle: triangles, equilateral, isosceles, rectangle, circle, choke (догадаетесь, почему?), quadrilateral, hypotenuse, bordered, polygon; art: arts, museum, paintings, painting, gallery, sculpture,",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 2,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "будем здесь останавливаться подробно на деталях работы Word2vec и его современных модификациях и предложим читателю обратиться к соответствующей лекции в учебнике Лены Войта по NLP. А мы лишь продемонстрируем, что он работает. Примеры. Возьмём несколько слов и посмотрим, как выглядят топ-10 слов, ближайших к ним в пространстве эмбеддингов (обученных на одном из датасетов Quora Questions с помощью word2vec): quantum: electrodynamics, computation, relativity, theory, equations, theoretical, particle, mathematical, mechanics, physics; personality: personalities, traits, character, persona, temperament, demeanor, narcissistic, trait, antisocial, charisma; triangle: triangles, equilateral, isosceles, rectangle, circle, choke (догадаетесь, почему?), quadrilateral, hypotenuse, bordered, polygon; art: arts, museum, paintings, painting, gallery, sculpture, photography, contemporary, exhibition, artist. Вопрос на подумать. В реальных текстах наверняка будут опечатки, странные слова и другие подобные неприятности. Word2vec же учится для фиксированного словаря. Что делать, если на этапе применения вам попадается неизвестное слово? Да и вообще, хорошо ли учить вложения для редких слов или слов с нетривиальными опечатками, которые, может быть, только раз встретятся в тексте? Вопрос на подумать. В некоторых случаях всё же полезно уметь строить эмбеддинги не отдельных слов, а текстов (например, для поиска похожих документов). Можете ли вы, вдохновившись идеей word2vec, придумать более тонкий способ сделать это, чем BoW или TF-IDF? Рекуррентные нейронные сети Итак, мы представили текст в виде последовательности векторов, соответствующих словам или их кусочкам. Как с ней работать? Один из вариантов мы уже рассматривали: можно посмотреть на последовательность из k k векторов размерности d d как на «изображение» k × 1 k×1 с d d «каналами», после чего использовать уже знакомые нам свёрточные нейросети, только с одномерными свёртками вместо двумерных. В каких-то случаях это действительно будет работать, но всё же есть несколько сомнительных моментов: Хотя изображения тоже могут быть разного размера, всё же в датасете редко попадаются рядом картинки 1920 × 1080 1920×1080 и 3 × 3 3×3, а среди, скажем, отзывов на ресторан могут попадаться как труды, сопоставимые по размеру с «Войной и миром», так и безликие «Да, вроде норм». И если обработать слишком длинное предложение нам поможет (с потерей информации, конечно) global pooling, слишком короткое может что-нибудь поломать, особенно если мы забываем про паддинг. Слегка философское соображение. Изображение однородно, в нём нет предпочтительных направлений, тогда как текст пишется и читается последовательно. Нам может показаться, что это стоит использовать: при обработке очередного токена обращаться к предыдущим, как к его контексту. В последнем соображении уже непосредственно видна идея рекуррентных нейронных сетей (recurrent networks, RNN): ecurrent Давайте разберёмся, что тут происходит. Чтобы хранить информацию о предыдущих токенах, мы вводим понятие внутренней памяти или скрытого состояния (hidden state, векторы h n h n ). В простейшем случае оно выражается одним вектором фиксированной размерности. На каждом (дискретном) шаге в сеть подаются данные (например, эмбеддинг токена), при этом происходит обновление скрытого состояния. Пример: h n = tanh =tanh(h n−1 после чего по скрытому состоянию предсказывается выходной сигнал, к примеру, следующим образом: Обратите внимание, что веса W i W i одинаковы на всех итерациях, то есть вы можете представлять себе, что очередные n−1 подаются на вход одного и того же слоя, зацикленного на себе. Рекуррентную сеть можно",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 3,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "информацию о предыдущих токенах, мы вводим понятие внутренней памяти или скрытого состояния (hidden state, векторы h n h n ). В простейшем случае оно выражается одним вектором фиксированной размерности. На каждом (дискретном) шаге в сеть подаются данные (например, эмбеддинг токена), при этом происходит обновление скрытого состояния. Пример: h n = tanh =tanh(h n−1 после чего по скрытому состоянию предсказывается выходной сигнал, к примеру, следующим образом: Обратите внимание, что веса W i W i одинаковы на всех итерациях, то есть вы можете представлять себе, что очередные n−1 подаются на вход одного и того же слоя, зацикленного на себе. Рекуррентную сеть можно обучать на ошибку, равную суммарному отклонению по всем выходных сигналам y n y n нашей сети. Вопрос на подумать. Как инициализировать веса W i W i мы, наверное, понимаем (про это можно почитать в параграфе про тонкости обучения нейросетей). А как инициализировать начальное скрытое состояние h 0 h 0 ? Можно ли инициализировать его нулём? Нетрудно представить себе и нейросеть с несколькими рекуррентными слоями: первый слой RNN будет принимать на вход исходную последовательность, вторая RNN — выходы первой сети, третья — выходы второй и т.д. Такие сети называют глубокими рекуррентными сетями. Вот пример схему глубокой рекуррентной сети: recurrent Вы, наверное, заметили, что описанная выше архитектура RNN решает синхронизованную версию задачи many-to-many. Её, впрочем, легко переделать для решения задачи many-to-one: достаточно убрать все выходы, кроме последнего: recurrent Bidirectional RNN Стандартная RNN учитывает только предыдущий контекст. Но ведь слово в предложении связано не только с предыдущими, но и с последующими словами. В таких случаях имеет смысл использовать двунаправленную рекуррентную сеть (bidirectional RNN, BRNN). Как следует из названия, в bidirectional RNN есть две рекуррентных подсети: прямая (forward, токены в нее подаются от первого к последнему) и обратная (backward, токены подаются в обраттном порядке). Вот пример такой архитектуры: recurrent Конечно, формула для y n y n может быть и другой. Например, выходы обеих рекуррентных сетей могут агрегироваться путем усреднения, или суммирования, или любым другим способом. Обратите внимание, что двунаправленная рекуррентная сеть работает с входом фиксированного размера, и по-прежнему не может решать не синхронизованный вариант задачи many-to-many. Backward RNN должна точно знать, где заканчивается входная последовательность, чтобы начать её обрабатывать с конца. Зато такая архитектура может помочь в решении задачи определения именованных сущностей или частей речи, использоваться в качестве энкодера в машинном переводе и так далее. Взрыв и затухание градиента в RNN При всех неоспоримых плюсах описанной выше глубокой рекуррентной архитектуры, на практике обычно используется её модифицированный вариант, который позволяет бороться с проблемой затухания или зашкаливания (взрыва) градиентов. Давайте разберёмся подробнее, почему она возникает. Рассмотрим функцию потерь =L(y n , y ^ n ), измеряющую отклонение предсказанного n n-го выхода от истинного (напомним, что архитектура many-to-many обучается на n=1 N L n , а архитектура many-to-one — на L N L N ). Выход y n y n зависит от скрытого состояния h n h n , а то, в свою очередь, от всех ,i<n. Обновление градиента при переходе через преобразование h i = tanh =tanh(h i−1 ) имеет, как мы хорошо знаем, вид tanh",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 4,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "её модифицированный вариант, который позволяет бороться с проблемой затухания или зашкаливания (взрыва) градиентов. Давайте разберёмся подробнее, почему она возникает. Рассмотрим функцию потерь =L(y n , y ^ n ), измеряющую отклонение предсказанного n n-го выхода от истинного (напомним, что архитектура many-to-many обучается на n=1 N L n , а архитектура many-to-one — на L N L N ). Выход y n y n зависит от скрытого состояния h n h n , а то, в свою очередь, от всех ,i<n. Обновление градиента при переходе через преобразование h i = tanh =tanh(h i−1 ) имеет, как мы хорошо знаем, вид tanh i−1 L=(∇ h i−1 L)W 1 T ⊙tanh ′ (h i−1 То есть в ходе вычисления (n−1) раз будем умножать на . Если у есть собственные значения, по модулю большие 1 1, и нам не посчастливится попадать в их окрестность, градиент будет стремиться к бесконечности («взрываться»). Такие градиенты делают обучение нестабильным, а в крайнем случае значения весов могут стать настолько большими, что произойдет численное переполнение, и значения весов перестанут обновляться. Если же у есть маленькие собственные значения, градиент может затухать. В любом случае, эти проблемы делают получение информации от далеких по времени состояний затруднительным. Теоретические выкладки о том, почему RNN без модификаций не могут достаточно хорошо учитывать долговременные зависимости, появились ещё в 90х. Их можно прочесть в статье Y.Bengio, 1994 или диссертации Josef Hochreiter, 1991. Но как бороться с этой проблемой? Простым инженерным решением является gradient clipping. Эта техника устанавливает максимально возможное значение градиента и заменяет все значения выше выбранного порога на это значение. При обратном распространении ошибки пробрасывается «ограниченный» градиент: otherwise ∥∇L∥ clip ={ ∥∇L∥, if ∥∇L∥<τ τ, otherwise где τ τ — гиперпараметр, подбираемый порог. Но сам по себе gradient clipping это довольно грубый инструмент. Поэтому были придуманы сложные модификации рекуррентных сетей, позволяющие им выучивать длинные зависимости. LSTM Вдохновение при написании этого параграфа черпалось из статьи в блоге исследователя Кристофера Олаха, из него же взяты иллюстрации. Сеть с долговременной и кратковременной памятью (Long short term memory, LSTM) частично решает проблему исчезновения или зашкаливания градиентов в процессе обучения рекуррентных сетей методом обратного распространения ошибки. Эта архитектура была предложена Hochreiter & Schmidhuber в 1997 году. LSTM построена таким образом, чтобы учитывать долговременные зависимости. Рассмотрим подробнее архитектуру LSTM. Все рекуррентные сети можно представить в виде цепочки из повторяющихся блоков. В RNN таким блоком обычно является один линейный слой с гиперболическим тангенсом в качестве функции активации. В LSTM повторяющийся блок имеет более сложную структуру, состоящую не из одного, а из четырех слоев. Кроме скрытого состояния h n h n , в LSTM появляется понятие состояния блока (cell state, c n c n ). Cell state c n c n будет играть роль внутренней, закрытой информации LSTM-блока, тогда как скрытое состояние h n h n теперь становится передаваемым наружу (не только в следующий блок, но и на следующий слой или выход всей сети) значением. LSTM может добавлять или удалять определенную информацию из cell state с помощью специальных механизмов, которые называются gates (ворота или вентили в русскоязычной литературе). Рассмотрим этот механизм подробнее. Основное назначение",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 5,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сложную структуру, состоящую не из одного, а из четырех слоев. Кроме скрытого состояния h n h n , в LSTM появляется понятие состояния блока (cell state, c n c n ). Cell state c n c n будет играть роль внутренней, закрытой информации LSTM-блока, тогда как скрытое состояние h n h n теперь становится передаваемым наружу (не только в следующий блок, но и на следующий слой или выход всей сети) значением. LSTM может добавлять или удалять определенную информацию из cell state с помощью специальных механизмов, которые называются gates (ворота или вентили в русскоязычной литературе). Рассмотрим этот механизм подробнее. Основное назначение вентиля — контролировать количество проходящей через него информации. Для этого матрица, проходящая по каналу, который контролирует вентиль, поточечно умножается на выражение вида σ(W 1 h n−1 +W 2 x n ) Сигмоида выдает значение от 0 0 до 1 1. Оно означает, какая доля информации сможет пройти через вентиль. Рассмотрим типы гейтов в том порядке, в каком они применяются в LSTM. Forget gate (вентиль забывания). Он позволяет на основе предыдущего скрытого состояния h t − 1 h t−1 и нового входа x t x t определить, какую долю информации из c t − 1 c t−1 (состояния предыдущего блока) стоит пропустить дальше, а какую забыть. LSTM: вентиль забывания.Источник Доля f t f t сохраняемой информации из c t − 1 c t−1 вычисляется следующим образом: =σ(h t−1 Дальше f t f t поэлементно умножается на c t − 1 c t−1 . Следующий шаг — определить, что нового мы внесём в cell state. Для этого у нас есть отличная кандидатура — уже привычное: C t ~ = tanh =tanh(h t−1 Но мы не уверены, что вся эта информация достаточно релевантна и достойна переноса в cell state, и хотим взять лишь некоторую её долю. Какую именно — поможет узнать наш следующий персонаж. Input gate (вентиль входного состояния). Вычислим =σ(h t−1 LSTM: вентиль входного состояния. Источник и умножим почленно на , чтобы получить информацию, которая поступит в cell state от h t − 1 h t−1 и x t x t . А именно, новое состояние cell state будет равно: t−1 где ⊙ ⊙ — это поэлементное умножение. Первое слагаемое отвечает за «забывание» нерелевантной информации из c t − 1 c t−1 , а второе — за привнесение новой, релевантной. LSTM: обновление состояния блока. Источник Как мы уже отмечали, роль выходного вектора LSTM-блока будет играть h n h n . Он вычисляется по cell state с помощью последнего вентиля. Output gate (вентиль выходного состояния). Он отвечает на вопрос о том, сколько информации из cell state следует отдавать на выход из LSTM-блока. Доля вычисляется следующим образом: =σ(h t−1 Теперь пропускаем cell state через гиперболический тангенс, чтобы значения были в диапазоне от − 1 −1 до 1 1, и умножаем полученный вектор на o_n, чтобы отфильтровать информацию из cell state, которую нужно подать на выход: tanh ⊙tanh(c t ) LSTM: вентиль выходного состояния. Источник Описанная архитектура выглядит несколько сложно. Кроме того, вычисление четырех различных типов гейтов может быть вычислительно невыгодным. Поэтому были разработаны различные",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 6,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "по cell state с помощью последнего вентиля. Output gate (вентиль выходного состояния). Он отвечает на вопрос о том, сколько информации из cell state следует отдавать на выход из LSTM-блока. Доля вычисляется следующим образом: =σ(h t−1 Теперь пропускаем cell state через гиперболический тангенс, чтобы значения были в диапазоне от − 1 −1 до 1 1, и умножаем полученный вектор на o_n, чтобы отфильтровать информацию из cell state, которую нужно подать на выход: tanh ⊙tanh(c t ) LSTM: вентиль выходного состояния. Источник Описанная архитектура выглядит несколько сложно. Кроме того, вычисление четырех различных типов гейтов может быть вычислительно невыгодным. Поэтому были разработаны различные вариации LSTM, одна из самых популярных (Gated Recurrent Unit, GRU) освещена ниже. Gated Recurrent Unit (GRU) Gated Recurrent Unit был предложен в статье Cho et al. в 2014 году. GRU объединяет input gate и forget gate в один update gate, также устраняет разделение внутренней информации блока на hidden и cell state. Вот общий вид GRU-блока: GRU. Источник Внимательно посмотрев на структуру LSTM, можно заметить, что функции forget gate и input gate похожи. Первый механизм определяет, какие значения c t − 1 c t−1 надо забыть, а второй — какие значения нового вектора нужно использовать для обновления старого cell state c t − 1 c t−1 . Давайте объединим эти функции воедино: грубо говоря, будем забывать только те значения, которые собираемся обновить. Такую роль в GRU выполняет update gate ( =σ(h t−1 Новый тип гейта, который появляется в GRU — reset gate ( r t r t ). Он определяет, какую долю информации из h t − 1 h t−1 с прошлого шага надо «сбросить», инициализировать заново. =σ(h t−1 Теперь мы вычисляем потенциальное обновление для скрытого состояния =tanh((r t ⊙h t−1 и, наконец, решаем, что из старого забыть, а что из нового добавить: =(1−z t )⊙h t−1 В итоге GRU имеет меньше параметров, чем LSTM (в GRU нет output gate) и при прочих равных, быстрее учится. GRU и LSTM показывают сопоставимое качество на многих задачах, включая генерацию музыки, распознавание речи, многие задачи обработки естественного языка. Модификации RNN, которые помогают лучше моделировать долгосрочные зависимости (LSTM, GRU) — важная веха развития нейросетей в NLP. Следующий большой этап в развитии — механизм внимания — мы рассмотрим чуть ниже. Seq2seq Вы, должно быть обратили внимание, что мы пока не касались задач, связанных с порождением последовательностей (синхронизованный варианты many-to-many не в счёт). Действительно: имевшиеся у нас пока инструменты не позволяли генерировать последовательности произвольной длины. Но как тогда переводить с одного языка на другой? Ведь мы не знаем, какой должна быть длина перевода фразы, да и однозначного соответствия между словами исходного предложения и его перевода обычно нет. Естественным решением для задачи sequence-to-sequence (seq2seq) является использование архитектуры энкодер-декодер, состоящей из кодировщика (энкодера) для кодирования информации об исходной последовательности в контекстном векторе (context vector) и декодировщика (декодера) для превращения закодированной энкодером информации в новую последовательность. prob Очевидным выбором на роль энкодера и декодера являются рекуррентные сети, например, LSTM. Простейшая архитектура будет иметь вид: Teacher Рассмотрим подробнее энкодер и декодер. Энкодер читает входное предложение токен за токеном и обрабатывает их",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 7,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "тогда переводить с одного языка на другой? Ведь мы не знаем, какой должна быть длина перевода фразы, да и однозначного соответствия между словами исходного предложения и его перевода обычно нет. Естественным решением для задачи sequence-to-sequence (seq2seq) является использование архитектуры энкодер-декодер, состоящей из кодировщика (энкодера) для кодирования информации об исходной последовательности в контекстном векторе (context vector) и декодировщика (декодера) для превращения закодированной энкодером информации в новую последовательность. prob Очевидным выбором на роль энкодера и декодера являются рекуррентные сети, например, LSTM. Простейшая архитектура будет иметь вид: Teacher Рассмотрим подробнее энкодер и декодер. Энкодер читает входное предложение токен за токеном и обрабатывает их с помощью блоков рекуррентной сети. Hidden state последнего блока становится контекстным вектором. Часто энкодер читает предложение в обратном порядке. Это делается для того, чтобы последний токен, который видит энкодер, совпал (или примерно совпал) с первыми токенами, которые будет генерировать декодер. Таким образом, декодеру проще начать процесс воссоздания предложения. Несколько первых правильных токенов сильно упрощают процесс дальнейшей генерации. Архитектура декодера аналогична энкодеру. При этом каждый блок декодера должен учитывать токены, сгенерированные к текущему моменту, и также информацию о предложении на исходном языке. Вектор скрытого состояния в нулевом блоке декодера ( g 0 g 0 ) инициализируется с помощью контекстного вектора. Таким образом, декодер получит сжатое представление исходного предложения. Предложение генерируется следующим образом: в первый блок подаем метку начала последовательности (например, -токен, begin of sentence), на выходе первого блока получаем первый токен новой последовательности, и затем подаем его на вход следующего блока декодера. Повторяем аналогичную процедуру до тех пор, пока не сгенерируется метка конца последовательности (например, , end of sentence) или не будет достигнута максимально возможная длина предложения. Таким образом, декодер работает в режиме языковой модели, генерируя предложение токен за токеном и учитывая предыдущий контекст. Разумеется, энкодер может быть и более сложным. Например, можно использовать многослойную двунаправленную сеть, лишь бы выходом её был один вектор контекста. С декодером сложнее: он должен порождать слова по одному, в одном направлении. Далее мы очень коротко остановимся на нетривиальных моментах обучения и применения такой модели. Тонкости применения В предыдущих разделах мы не останавливались подробно на том, что происходит с выходами y n y n , но сейчас всё-таки попробуем разобраться. Если мы решаем задачу машинного перевода, то на очередном этапе декодер выдаёт нам условное распределение p(y n ∣x,y <n ) на словах (или каких-то subword unit, например, BPE), из которого мы будем выбирать самое вероятное слово y n y n и подавать его на вход следующего блока. Но эта, жадная, стратегия может и подвести. Легко представить себе ситуацию, в которой самое вероятное на данный момент слово приведёт дальше к менее вероятной подпоследовательности: why Чтобы справиться с этим, на этапе применения модели используют beam search. В каждый момент времени мы поддерживаем некоторое количество B B самых вероятных гипотез, на n n-м шаге пытаясь продолжать все сохранённые, а из продолжений выбирая топ- B B по метрике t=1 ∏ n p(y t ∣x,y <t ) beam Число B B нет смысла делать большим (это и вычислительно будет тяжко, и может привести к более плохим результатам), можете брать в пределах",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 8,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "блока. Но эта, жадная, стратегия может и подвести. Легко представить себе ситуацию, в которой самое вероятное на данный момент слово приведёт дальше к менее вероятной подпоследовательности: why Чтобы справиться с этим, на этапе применения модели используют beam search. В каждый момент времени мы поддерживаем некоторое количество B B самых вероятных гипотез, на n n-м шаге пытаясь продолжать все сохранённые, а из продолжений выбирая топ- B B по метрике t=1 ∏ n p(y t ∣x,y <t ) beam Число B B нет смысла делать большим (это и вычислительно будет тяжко, и может привести к более плохим результатам), можете брать в пределах 10 10. Тонкости обучения Как уже было сказано выше, на каждом шаге декодер предсказывает распределение вероятностей p(y n ∣x,y <n ). Вся модель учится на сумму по всем n n кросс-энтропиям этих распределений с истинными y n y n . Одна из сложностей такого обучения состоит в том, что единожды ошибившись и предсказав неправильный y ^ n y n вместо истинного y n y n , модель скорее всего и следующие токены предскажет неверно, а это сделает всё дальнейшее обучение малополезным: ведь мы будем учить декодер предсказывать правильное продолжение неправильного начала. Одним из способов борьбы с этим является teacher forcing. Суть его в том, что на этапе обучения мы подаём на вход декодера не предсказанный им на предыдущем этапе токен, а истинный: teacher А как же one-to-many? У нас остался лишь один неразобранный тип задач: one-to-many. К счастью, чтобы с ним справиться, ничего нового не нужно: достаточно уже знакомой модели энкодер-декодер, лишь с корректировкой энкодера. Рассмотрим для примера задачу генерации подписей к изображениям (image captioning). Если мы уже умеем как-то превращать картинки в векторы, то эти векторы мы можем напрямую подавать в декодер в качестве векторов контекста: one Более подробно о том, как строить векторизации для изображений, вы узнаете в параграфе про обучение представлений. А если у вас есть все данные мира, то вы можете в качестве энкодера взять свёрточную нейросеть и обучать её вместе с декодером end-to-end: one Механизм внимания (attention) Как человек переводит предложения с одного языка на другой? Обычно переводчик уделяет особое внимание слову, которое записывает в данный момент. Хочется сообщить аналогичную интуицию нейронным сетям. Рассмотрим, как можно реализовать такой механизм на примере машинного перевода. Внимательно посмотрим на seq2seq модель для машинного перевода. Вся информация о предложении на исходном языке заключена в контекстном векторе, но разные слова в предложении могут иметь разную смысловую значимость и следовательно, должны учитываться с разными весами. Кроме того, при генерации разных частей перевода следует обращать внимание на разные части исходного предложения. Например, первое слово переведенной фразы нередко связано с первыми словами в предложении, поданном на вход энкодеру, а порой одно слово перевода передаёт смысл нескольких слов, разбросанных по исходному предложению (вдруг кто-нибудь сталкивался с отделяемыми приставками в немецком?). Механизм внимания (attention) реализует эту интуицию путем предоставления декодеру информации обо всех токенах исходного предложения на каждом шаге генерации. Рассмотрим классическую модель внимания, предложенную Bahdanau et al. в 2014 году. Обозначим скрытые состояния энкодера ,…,h n ), а скрытые состояния декодера ,…,s m ).",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 9,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "учитываться с разными весами. Кроме того, при генерации разных частей перевода следует обращать внимание на разные части исходного предложения. Например, первое слово переведенной фразы нередко связано с первыми словами в предложении, поданном на вход энкодеру, а порой одно слово перевода передаёт смысл нескольких слов, разбросанных по исходному предложению (вдруг кто-нибудь сталкивался с отделяемыми приставками в немецком?). Механизм внимания (attention) реализует эту интуицию путем предоставления декодеру информации обо всех токенах исходного предложения на каждом шаге генерации. Рассмотрим классическую модель внимания, предложенную Bahdanau et al. в 2014 году. Обозначим скрытые состояния энкодера ,…,h n ), а скрытые состояния декодера ,…,s m ). Важно отметить, что , это контекстный вектор. На каждом шаге декодера будем считать attention scores, умножая s i s i на вектор скрытого состояния каждого блока энкодера ,…,h n ). Таким образом, получаем n n значений, указывающих, насколько каждый из токенов c номерами ( 0... n ) (0...n) из исходного предложения важен для генерации токена i i из перевода: =[⟨s i ,h 0 ⟩,⟨s i ,h 1 ⟩,…,⟨s i ,h n ⟩]= =[s i h 0 T ,…,s i h n T ] (здесь , как обычно, являются строками, так что — скаляр). Теперь превращаем эти значения в attention distribution, применив к ним softmax: α i = softmax =softmax(e i ) Используем α i α i в качестве весов для нахождения окончательного вектора внимания j=0 Теперь в декодере на шаге i вместо вектора скрытого состояния ,...,h n ) будем использовать вектор ] -- конкатенацию скрытого состояния блока и соответствующего attention вектора. Таким образом, на каждом шаге декодер получает информацию о важности всех токенов входного предложения. Данная схема вычисления attention представлена на следующем рисунке. Вычисление attention в seq2seq модели Существует много разных видов механизмов внимания, например: Базовый dot-product, рассмотренный ранее: =[s i h j T ] j=0 n Мультипликативный: =[s i Wh j T ] j=0 n , где W W — обучаемая матрица весов. MLP: e i j = tanh =tanh(h )v, где — обучаемые матрицы весов, v v — обучаемый вектор весов Важной особенностью механизма внимания является то, что его веса несут в себе информацию о связях слов в двух языках, участвующих в переводе. Визуализировав веса механизма внимания, получаем таблицу взаимосвязей между словами: Пример визуализации весов attention Весьма логично, что слово dogs теснее всего связано со словом собак, а слову очень соответствуют целых два слова: very и much. Self-attention В предыдущем разделе мы обсуждали применение механизма внимания во время работы декодера, но оказывается, что и энкодеру это может быть полезно. Механизм внутреннего внимания (self-attention) используется, чтобы посмотреть на другие слова во входной последовательности во время кодирования конкретного слова. Изначально этот механизм был представлен в статье Attention is all you need как элемент архитектуры «трансформер» (Transformer). Эффективность трансформера демонстировалась на примере задачи машинного перевода. Сейчас трансформеры и self-attention обрели огромную популярность и используются не только в NLP, но и в других областях (например, в компьютерном зрении: Vision Transformer, Video Transformer, Multimodal Transformer for Video Retrieval и так далее). Более подробный обзор архитектуры Трансформер оставим курсу Лены Войта по NLP, а",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 10,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "работы декодера, но оказывается, что и энкодеру это может быть полезно. Механизм внутреннего внимания (self-attention) используется, чтобы посмотреть на другие слова во входной последовательности во время кодирования конкретного слова. Изначально этот механизм был представлен в статье Attention is all you need как элемент архитектуры «трансформер» (Transformer). Эффективность трансформера демонстировалась на примере задачи машинного перевода. Сейчас трансформеры и self-attention обрели огромную популярность и используются не только в NLP, но и в других областях (например, в компьютерном зрении: Vision Transformer, Video Transformer, Multimodal Transformer for Video Retrieval и так далее). Более подробный обзор архитектуры Трансформер оставим курсу Лены Войта по NLP, а пока остановимся на механизме внутреннего внимания. Пусть на вход нейросети пришли два предложения «Мама мыла раму. Она держала в руках тряпку». Местоимение «она» относится к маме или к раме? Для человека это очень простой вопрос, но для модели машинного обучения — нет. Self-attention помогает выучить взаимосвязи между токенами в последовательности, моделируя «смысл» других релевантных слов в последовательности при обработке текущего токена. Что происходит внутри self-attention-модуля? Для начала, из входного вектора (например, эмбеддинга каждого токена) формируются три вектора: Query (запрос), Key (ключ) и Value (значение). Они получаются с помощью умножения входного вектора на матрицы , веса которых учатся вместе со всеми остальными параметрами модели с помощью обратного распространения ошибки. Выделение этих трех абстракций нужно, чтобы разграничить эмбеддинги, задающие «направление» внимания (query, key) и смысловую часть токена (value). Вектор query задает модальность «начальной точки» механизма внутреннего внимания (от какого токена направлено внимание), вектор key — модальность «конечной точки» (к какому токену направлено внимание). Таким образом, один и тот же токен может выступать как «начальной», так и «конечной» точкой направления внимания: self-attention вычисляется между всеми токенами в выбранном фрагменте текста. Процесс происходит так: по очереди фиксируется каждый токен (становится query) и просчитывается степень его связанности со всеми оставшимися токенами. Для этого поочередно key-вектора всех токенов скалярно умножаются на query-вектор текущего токена. Полученные числа будут показывать, насколько важны остальные токены при кодировании query токена в конкретной позиции. Дальше полученные числа надо нормализовать и пропустить через софтмакс, чтобы получить распределение. Затем подсчитывается взвешенная сумма value векторов,где в качестве весов используются полученные на предыдущем шаге вероятности. Полученный вектор и будет выходом слоя внутреннего внимания для одного токена. Изложенную выше схему вычисления self-attention вектора для одного токена можно представить простой схемой: Вычисление self-attention для одного токена. Источник На практике self-attention не вычисляется для каждого токена по отдельности, вместо этого используются матричные вычисления. Например, вместо вычисления query, key и value векторов для каждого токена, настакаем эмбеддинги входных токенов в матрицу X X и посчитаем матрицы Q=W Q ∗X, K=W K ∗X и V=W V ∗X. Затем происходит повторение описанных в предыдущем абзаце шагов, только для матриц. Посчитаем итоговую матрицу Z Z, подав матрицы Q, K и V в формулу: norm const ) V ˙ Z=softmax( norm const В оригинальной статье Vaswani et al., 2017 в качестве нормализующей константы выбрали число 8 (квадратный корень размерности key-векторов). Нормализация приводила к более стабильным градиентам в процессе обучения. Интересно, что обычно используют параллельно несколько self-attention блоков. Такая схема называется multi-head self-attention. Вычисление",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 11,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и value векторов для каждого токена, настакаем эмбеддинги входных токенов в матрицу X X и посчитаем матрицы Q=W Q ∗X, K=W K ∗X и V=W V ∗X. Затем происходит повторение описанных в предыдущем абзаце шагов, только для матриц. Посчитаем итоговую матрицу Z Z, подав матрицы Q, K и V в формулу: norm const ) V ˙ Z=softmax( norm const В оригинальной статье Vaswani et al., 2017 в качестве нормализующей константы выбрали число 8 (квадратный корень размерности key-векторов). Нормализация приводила к более стабильным градиентам в процессе обучения. Интересно, что обычно используют параллельно несколько self-attention блоков. Такая схема называется multi-head self-attention. Вычисление self-attention происходит несколько раз с разными матрицами весов, затем полученные матрицы конкатенируются и умножаются на еще одну матрицу весов W O W O (см. схему). Это позволяет разным self-attention головам фокусироваться на разных взаимосвязях, например, одна голова может отвечать за признаковые описания, другая за действия, третья за отношения «объект-субъект». Разные головы могут вычисляться параллельно, при этом входная матрица эмбеддингов отображается в разные подпространства представлений, что значительно обогащает возможности внутреннего внимания моделировать взаимосвязи между словами. В виде формулы вычисление multihead self-attention можно представить так: MultiHead(Q,K,V)=Concat(head 1 ,head 2 ,...,head где norm const ) V ˙ head i (Q,K,V)=softmax( norm const Схема вычисления multi-head self-attention. Источник Есть много реализаций self-attention (PyTorch, TensorFlow). Также советуем ознакомиться с jupyter-ноутбуком от Гарвардской NLP-группы, в котором представлена реализация архитектуры «трансформер» с подробными объяснениями. Еще один отличный источник, позволяющий подробнее разобраться с self-attention и трансформером, - это статья Jay Alammar под названием «Illustrated Transformer». Особенности работы с текстами Предобработка текстов Перед тем, как применять описанные выше архитектуры (или даже использовать простые подходы, вроде TF-IDF или word2vec), нужно разобраться, как делать предобработку текстов. Первым делом надо научиться представлять связный текст в виде последовательности. Для начала имеет смысл разбить текст на предложения, а дальше уже на слова или символьные n-граммы. Этот процесс называется токенизацией. Можно делать токенизацию вручную, например, с помощью регулярных выражений, или воспользоваться готовыми методами из библиотеки NLTK. Представим, что мы получили упорядоченный список слов, из которых состоит текст. Но это еще не все. Обычно тексты содержат разные грамматические формы одного и того же слова. Привести все словоформы к начальной форме можно с помощью лемматизации. Лемматизация - это алгоритм приведения слова к его начальной форме с использованием морфологическего анализа и знаний об особенностях конкретного языка. Пример работы лемматизатора: «собаки, собака, с собакой, собаками -> собака» Другой способ приведения всех словоформ к одной форме - это стемминг. Стемминг — это более грубый процесс на основе эвристик, который действует без знания контекста, словарей и морфологии. Стеммер не поймет, что слова с чередованием имеют один и тот же корень (только если прописать в явном виде такую эвристику) или что слова «есть», «буду» и «был» - это формы глагола «быть». Стемминг - менее аккуратный процесс по сравнению с лемматизацией, зато гораздо более быстрый. Еще один важный этап предобработки текстов - это удаление стоп-слов. Стоп-словами называют междометия, союзы, предлоги, артикли, в общем все слова, которые будут вносить шум в работу алгоритма машинного обучения. Иногда дополнительно убирают слова общей лексики, оставляя только специфические",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 12,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— это более грубый процесс на основе эвристик, который действует без знания контекста, словарей и морфологии. Стеммер не поймет, что слова с чередованием имеют один и тот же корень (только если прописать в явном виде такую эвристику) или что слова «есть», «буду» и «был» - это формы глагола «быть». Стемминг - менее аккуратный процесс по сравнению с лемматизацией, зато гораздо более быстрый. Еще один важный этап предобработки текстов - это удаление стоп-слов. Стоп-словами называют междометия, союзы, предлоги, артикли, в общем все слова, которые будут вносить шум в работу алгоритма машинного обучения. Иногда дополнительно убирают слова общей лексики, оставляя только специфические термины. Универсального списка слов не существует, но для начала можно использовать список стоп-слов из библиотеки NLTK. Аугментации для текстов Аугментации данных часто используются, чтобы увеличить количество данных в обучающей выборке, а также повысить обобщаемость модели. И если для компьютерного зрения аугментации относительно простые и могут выполняться на лету (масштабирование, обрезка, вращение, добавление шума и т.д.), то для текстов в виду грамматической структуры, синтаксиса и особенностей языка все не так просто. Аугментации текста менее «автоматические», в идеале нужно понимать смысл фразы и иметь под рукой отлично работающий механизм перефразирования. Рассмотрим несколько популярных способов аугментации текстовых данных: Обратный перевод. Переводим исходный текст на какой-то язык, и затем переводим его обратно. Это помогает сохранить контекст, но при этом получить синонимичную формулировку. Замены слова на синонимичное/близкое по смыслу. Для этого можно использовать словари синонимов либо искать близкое слово в пространстве эмбеддингов, минимизируя расстояние между соответствующими векторами. В качестве таких эмбеддингов можно взять привычный word2vec, fasttext или контекстуализированные эмбеддинги на основе претренированных моделей (BERT, ELMO, GPT-2/GPT-3 и так далее). Вставка синонима слова в случайное место в предложении. Замена сокращения на полное наименование и обратно. Для английского языка этот способ более актуален, чем для русского. Случайная вставка/удаление/замена/перемена местами слов в предложении. Случайная перестановка местами предложений. Случайное изменение букв на произвольные/ближайшие на клавиатуре, добавление/исправление орфографических и пунктуационных ошибок, изменение регистра. MixUp для текстов. В задаче классификации смешиваем признаковые описания двух объектов и с такими же весами смешиваем их метки классов, получаем новый объект с признаками x i j x ij и меткой класса =λx i +(1−λ)x =λy i +(1−λ)y j Для текстов признаковые описания можно смешивать на уровне слов (выбирать ближайшее слово в пространстве word embeddings) или на уровне предложений. Еще один вариант: сэмплировать слова из двух разных текстов с вероятностями λ λ и 1 − λ 1−λ. 9. Аугментации с использованием синтаксического дерева предложения. 10. Генерация текста языковыми моделями. Например, генерация текста с помощью упоминавшейся ранее модели GPT-3. Подробнее про некоторые методы аугментации текстов можно почитать в статье Easy Data Augmentation (EDA). Многие из описанных выше и в статье методов реализованы в библиотеке NLPAug, использование которой сильно упрощает задачу аугментации текстовых данных на практике. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 6.1. Свёрточные нейросети Следующий параграф 6.3. Трансформеры",
    "metadata": {
      "title": "Нейросети для работы с последовательностями",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-raboty-s-posledovatelnostyami",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.2",
      "part": 13,
      "total_parts": 13,
      "source_file": "6.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Наверное, ни один из рассказов про современные нейросети не обойдётся без упоминания трансформер-моделей: в самом деле, почти все нашумевшие достижения в глубинном обучении последних лет так или иначе опираются на эту архитектуру. Что же в ней такого особенного и почему трансформеры успешно применяются в самых разных задачах? Давайте разбираться. Зачем нам внимание Для начала вспомним, что основным подходом для работы с последовательностями до 2017 года (выхода оригинальной статьи про архитектуру «трансформер») было использование рекуррентных нейронных сетей, или RNN. Однако у такого подхода есть несколько известных минусов: Во-первых, RNN содержат всю информацию о последовательности в скрытом состоянии, которое обновляется с каждым шагом. Если модели необходимо «вспомнить» что-то, что было сотни шагов назад, то эту информацию необходимо хранить внутри скрытого состояния и не заменять чем-то новым. Следовательно, придется иметь либо очень большое скрытое состояние, либо мириться с потерей информации. Во-вторых, обучение рекуррентных сетей сложно распараллелить: чтобы получить скрытое состояние RNN-слоя для шага i + 1 i+1, вам необходимо вычислить состояние для шага i i. Таким образом, обработка батча примеров длиной 1000 1000 должна потребовать 1000 1000 последовательных операций, что занимает много времени и не очень эффективно работает на GPU, созданных для параллельных вычислений. Обе этих проблемы затрудняют применение RNN к по-настоящему длинным последовательностям: даже если вы дождетесь конца обучения, ваша модель по своей конструкции будет так или иначе терять информацию о том, что было в начале текста. Хочется иметь способ «читать» последовательность так, чтобы в каждый момент времени можно было обратиться к произвольному моменту из прошлого за константное время и без потерь информации. Таким способом и является лежащий в основе трансформеров механизм self-attention, о котором далее пойдет речь. Как мы узнаем позже, благодаря своей универсальности и масштабируемости этот механизм оказался применим к множеству задач помимо обработки естественного языка. Ниже приведено устройство архитектуры «трансформер» из оригинальной статьи: ml Слева на схеме представлено устройство энкодера. Он по очереди применяется к исходной последовательности из N N блоков: ml Каждый блок выдаёт последовательность такой же длины. В нём есть два важных слоя, multi-head attention и feed-forward. После каждого из них к выходу прибавляется вход (это стандартный подход под названием residual connection) и затем активации проходят через слой layer normalization: на рисунке эта часть обозначена как “Add & Norm”. У декодера схема похожая, но внутри каждого из N N блоков два слоя multi-head attention, в одном из которых используются выходы энкодера. Давайте подробнее обсудим каждую из составляющих частей этого механизма. Слой внимания Первая часть transformer-блока — это слой self-attention. От обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым. Если говорить более подробно, то в вычислении внимания для последовательности будет участвовать три обучаемых матрицы , W K , W V . Представление x i x i каждого элемента входной последовательности мы умножаем на , W K , W V , получая вектор-строки i — номер элемента), которые соответственно называются запросами, ключами и значениями (query, key и value). Их роли можно условно описать следующим образом: q i q",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 1,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "слой self-attention. От обычного внимания его отличает то, что выходом являются новые представления для элементов той же последовательности, что мы подали на вход, причем каждый элемент этой последовательности напрямую взаимодействует с каждым. Если говорить более подробно, то в вычислении внимания для последовательности будет участвовать три обучаемых матрицы , W K , W V . Представление x i x i каждого элемента входной последовательности мы умножаем на , W K , W V , получая вектор-строки i — номер элемента), которые соответственно называются запросами, ключами и значениями (query, key и value). Их роли можно условно описать следующим образом: q i q i — запрос к базе данных; k i k i — ключи хранящихся в базе значений, по которым будет осуществляться поиск; v i v i — сами значения. queries Близость запроса к ключу можно определять, например, с помощью скалярного произведения: self-attention weights i = softmax self-attention weights i =softmax( ,…), где C C — некоторая нормировочная константа. Именно так и делали в исходной статье; в качестве нормировочной константы брался корень d k d k из размерности ключей и значений. Теперь мы складываем значения v i v i с полученными коэффициентами. Это и будет выходом слоя self-attention. В векторизованном виде можно записать: self-attention softmax self-attention(Q,K,V)=softmax( d k QK T )V, где Q Q, K K, V V — матрицы запросов, ключей и значений соответственно, в которых по строкам записаны , а softmax softmax берётся построчно. Особенности слоя внимания в декодере Как мы уже упоминали выше, в декодере один из attention-слоёв является слоем кросс-внимания (cross-attention), в котором запросы берутся из выходной последовательности, а ключи и значения — из входной (то есть из результатов работы энкодера). decoder Также стоит учитывать, что в описанном выше виде внимания каждый токен будет «смотреть» на всю последовательность, что нежелательно для декодера. Действительно, на этапе генерации мы будем порождать по одному токену за шаг, и доступ к последующим шагам на этапе обучения приведёт к утечке информации в декодере и низкому качеству модели. Чтобы избежать этой проблемы, при обучении к вниманию нужно применять авторегрессивную маску, вручную обращая в − ∞ −∞ веса до softmax для токенов из будущего, чтобы после softmax их вероятности стали нулевыми. Как можно увидеть на рисунке внизу, эта маска имеет нижнетреугольный вид. Источник Multi-head attention Один набор Q Q, K K и V V может отражать только один вид зависимостей между токенами, и матрицы извлекают лишь ограниченный набор информации из входных представлений. Чтобы скомпенсировать эту неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты. Рисунок ниже показывает, как выглядит multi-head attention: ml Эффективность Подход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. В частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся. Однако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 2,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "неоптимальность, авторы архитектуры предложили подход с несколькими «головами» внимания (multi-head attention): по сути вместо одного слоя внимания мы применяем несколько параллельных с разными весами, а потом агрегируем результаты. Рисунок ниже показывает, как выглядит multi-head attention: ml Эффективность Подход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. В частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся. Однако стоит помнить о затратах памяти и времени: поскольку каждый элемент последовательности взаимодействует с каждым, легко показать, что сложность self-attention составляет O ( n 2 ) O(n 2 ) по длине последовательности, а простые реализации, формирующие полную матрицу внимания, будут расходовать ещё и O ( n 2 ) O(n 2 ) памяти. С оптимизацией вычислительной сложности внимания связано множество работ как инженерного, так и архитектурного плана: в частности, есть подходы, которые позволяют сократить время работы self-attention до линейного или существенно уменьшают константы за счёт учёта иерархии памяти GPU. Например, на графиках ниже сравнивается время работы и потребление памяти трансформера со стандартным вниманием и с механизмом из статьи Longformer: longformer Полносвязный слой и нормализация Вторая часть трансформерного блока называется feed-forward network (FFN) и представляет собой два обычных полносвязных слоя, применяемых независимо к каждому элементу входной последовательности. В последних архитектурах размер промежуточного представления (то есть выхода первого слоя) бывает весьма большим — в 4 раза больше выходов блока. Из-за этого вычислительной стоимостью FFN не стоит пренебрегать: несмотря на квадратичную асимптотику внимания, в больших моделях или на коротких последовательностях FFN может занимать существенно больше времени по сравнению с self-attention. В виде формулы применение FFN можно представить так: FFN ( x ) = act FFN(x)=act(xW Промежуточные активации act act в FFN бывают разными: начиналось всё с широко известной ReLU, но в какой-то момент сообщество перешло на GELU (Gaussian Error Linear Unit) с формулой x Φ ( x ) xΦ(x), где Φ Φ — функция распределения стандартной нормальной случайной величины. Сравнение ReLU, ELU и GELU. Источник Скажем ещё пару слов о layer normalization: как было показано в ряде работ, их положение внутри residual-ветки довольно важно. В стандартной архитектуре используется формулировка PostLN, где нормализация применяется после остаточной связи. Однако такое применение нормализации оказывается довольно нестабильным при обучении моделей с большим числом слоёв: вместо этого предлагается использовать PreLN (справа на рисунке снизу), где нормализация применяется ко входу residual-ветки. ml Кодирование позиций Внимательный читатель может заметить, что все операции внутри трансформер-блока, строго говоря, инвариантны к порядку элементов в последовательности. Например, результат внимания зависит от скалярных произведений между эмбеддингами токенов, но расположение этих токенов внутри текста значения не имеет. Таким образом, итоговые представления каждого токена на выходе из модели будут одинаковыми вне зависимости от порядка слов, что вряд ли нас устроит. Как с этим справиться? На помощь приходит такая вещь, как позиционные эмбеддинги. Это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах. Исторически первым подходом были фиксированные эмбеддинги, однозначно кодирующие позицию",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 3,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Внимательный читатель может заметить, что все операции внутри трансформер-блока, строго говоря, инвариантны к порядку элементов в последовательности. Например, результат внимания зависит от скалярных произведений между эмбеддингами токенов, но расположение этих токенов внутри текста значения не имеет. Таким образом, итоговые представления каждого токена на выходе из модели будут одинаковыми вне зависимости от порядка слов, что вряд ли нас устроит. Как с этим справиться? На помощь приходит такая вещь, как позиционные эмбеддинги. Это вспомогательные представления, которые прибавляются к обычным эмбеддингам токенов входной последовательности и позволяют слоям внимания различать одинаковые токены на разных местах. Исторически первым подходом были фиксированные эмбеддинги, однозначно кодирующие позицию тригонометрическими функциями (ниже p o s pos — номер позиции, i i — индекс элемента в векторе, кодирующем эту позицию, d d — размерность эмбеддинга): sin ⁡ ( p o s 1000 cos ⁡ ( p o s 1000 (pos,2i) PE (pos,2i+1) =sin( 10000 2i/d pos ), =cos( 10000 2i/d pos ). С момента появления архитектуры «трансформер», однако, появилось множество других способов кодировать позиции токенов. Например, можно просто сделать позиционные эмбеддинги обучаемыми наряду с эмбеддингами токенов. Иной подход — напрямую учесть тот факт, что нам важны не абсолютные позиции токенов, а расстояние между ними, и обучать относительные позиционные представления: подобный подход заметно улучшает качество на чувствительных к порядку слов задачах, а его более современные модификации регулярно используются в самых мощных моделях. Позиционное кодирование ALiBi: метод добавляет необучаемые константы к весам внимания в зависимости от расстояния между токенами ключа и значения. Источник Про BERT и GPT Несомненно, трансформер-модели не были бы так интересны, если бы практически все задачи NLP сейчас не решались бы с помощью этой архитектуры. Главными факторами, повлиявшими на бурный рост популярности идеи self-attention, послужили два семейства хорошо всем известных архитектур — BERT и GPT, которые в некотором роде являются энкодером и декодером трансформера, которые зажили своей жизнью. Модель GPT (Generative Pretrained Transformer) хронологически появилась раньше. Она представляет собой обычную языковую модель, реализованную в виде последовательности слоев декодера трансформера. В качестве задачи при обучении выступает обычное предсказание следующего токена (то есть многоклассовая классификация по словарю). Важно, что в качестве маски внимания как раз выступает нижнетреугольная матрица: в противном случае возникла бы утечка в данных из-за того, что токены из «прошлого» будут видеть «будущее». Полученную модель можно использовать для генерации текстов и всех задач, которые на это опираются. Даже ChatGPT, обученная на специальных инструкциях, по своей сути незначительно отличается от базовой модели. Иллюстрация задачи при обучении GPT. Источник Как понятно из названия, модель Bidirectional Encoder Representations from Transformers (или BERT) отличается от GPT двунаправленностью внимания: это значит, что при обработке входной последовательности все токены могут использовать информацию друг о друге. Это делает такую архитектуру более удобной для задач, где нужно сделать предсказание относительно всего входа целиком без генерации, например, при классификации предложений или поиске пар похожих документов. Важно, что при этом BERT не учится генерировать тексты с нуля: одна из его задач при обучении — это masked language modeling (предсказание случайно замаскированных слов по оставшимся, изображено на рисунке ниже), а вторая — next sentence prediction (предсказание по паре текстовых фрагментов,",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 4,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "из названия, модель Bidirectional Encoder Representations from Transformers (или BERT) отличается от GPT двунаправленностью внимания: это значит, что при обработке входной последовательности все токены могут использовать информацию друг о друге. Это делает такую архитектуру более удобной для задач, где нужно сделать предсказание относительно всего входа целиком без генерации, например, при классификации предложений или поиске пар похожих документов. Важно, что при этом BERT не учится генерировать тексты с нуля: одна из его задач при обучении — это masked language modeling (предсказание случайно замаскированных слов по оставшимся, изображено на рисунке ниже), а вторая — next sentence prediction (предсказание по паре текстовых фрагментов, следуют они друг за другом или нет). Пример masked language modeling. Заметим, что самое ключевое отличие в моделях BERT и GPT (а не в задачах для обучения или применениях) можно свести к использованию разных видов внимания, изображенных на рисунке снизу. Отличия между вниманием в BERT и GPT. Источник Тонкости обучения К сожалению, если вы просто напишете код Transformer-нейросети и попробуете сразу обучить что-то содержательное, используя привычные для других архитектур гиперпараметры, то вас с большой вероятностью постигнет неудача. Оптимизационный процесс для таких моделей зачастую требуется изменить, и недостаточное внимание к этому может повлечь за собой существенные потери в итоговом качестве или вообще привести к нестабильному обучению. Первый момент, на который стоит обратить внимание, — размер батча для обучения. Практически все современные Transformer-модели обучаются на больших батчах, которые для самых больших языковых моделей могут достигать миллионов токенов. Разумеется, ни одна современная GPU не может обработать столько данных за один шаг: на помощь приходят распределенное обучение и чуть более универсальный трюк с аккумуляцией градиентов по микробатчам. Также в последних статьях зачастую прибегают к увеличению размера батча по ходу обучения: идея заключается в том, что на ранних этапах важнее быстрее совершить много шагов градиентного спуска, а на поздних становится важнее иметь точную оценку градиента. Размер батча может играть большую роль даже для сравнительно маленьких моделей. Источник Второй немаловажный фактор — выбор оптимизатора и расписания для learning rate. Обучить трансформер стандартным SGD, скорее всего, не выйдет: в оригинальной статье в качестве оптимизатора использовался Adam, и де-факто он остаётся стандартом до сих пор. Однако стоит заметить, что для больших размеров батча Adam порой работает плохо: из-за этого порой приходится прибегать к алгоритмам наподобие LAMB, нормализующим обновления весов для каждого слоя. Трансформеры не для текстов Разумеется, успех этого семейства архитектур на множестве текстовых задач не мог остаться незамеченным для исследователей в других доменах. Одним из наиболее ярких примеров областей, в которой Transformer-модели нашли новое приложение, несомненно, является компьютерное зрение. К примеру, архитектура ViT (Vision Transformer) в свое время побила рекорды качества по классификации изображений, задействуя идею self-attention для картинок, разделенных на множество «лоскутных» (patches) сегментов квадратной формы. Как пишут авторы статьи, идея использовать Transformer-архитектуру в зрении пришла к ним после наблюдения за успехами таких моделей в NLP: использование такого общего подхода, как self-attention, позволяет избежать необходимости явно закладывать в архитектуру особенности задачи (это ещё называют inductive bias) при достаточном времени обучения, числе параметров и размере выборки. Vi Также именно на трансформерах базируется генеративная часть DALL-E — модели,",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 5,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "примеров областей, в которой Transformer-модели нашли новое приложение, несомненно, является компьютерное зрение. К примеру, архитектура ViT (Vision Transformer) в свое время побила рекорды качества по классификации изображений, задействуя идею self-attention для картинок, разделенных на множество «лоскутных» (patches) сегментов квадратной формы. Как пишут авторы статьи, идея использовать Transformer-архитектуру в зрении пришла к ним после наблюдения за успехами таких моделей в NLP: использование такого общего подхода, как self-attention, позволяет избежать необходимости явно закладывать в архитектуру особенности задачи (это ещё называют inductive bias) при достаточном времени обучения, числе параметров и размере выборки. Vi Также именно на трансформерах базируется генеративная часть DALL-E — модели, положившей начало активным исследованиям последних лет в генерации изображений по тексту. Концептуально DALL-E довольно проста: её можно рассматривать как авторегрессивную «языковую модель», генерирующую изображение по одному «визуальному токену» за шаг. Применяют трансформеры и к обучению с подкреплением: ярким примером является работа Decision Transformer, в которой предлагают использовать авторегрессивное моделирование с использованием этой архитектуры для построения агента. Авторы показали, что такой же подход, который используют для генерации текстов, можно использовать для предсказания действий в динамической среде: как показано на рисунке ниже, модель последовательно принимает стандартные тройки из закодированных состояний, текущих действий и наград и в качестве ответа на каждом шаге выдаёт следующее действие. Архитектура Decision Transformer. Источник Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 6.2. Нейросети для работы с последовательностями Следующий параграф 6.4. Графовые нейронные сети",
    "metadata": {
      "title": "Трансформеры",
      "url": "https://education.yandex.ru/handbook/ml/article/transformery",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.3",
      "part": 6,
      "total_parts": 6,
      "source_file": "6.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Наряду с обработкой табличных, текстовых, аудио данных и изображений, в глубинном обучении довольно часто приходится решать задачи на данных, имеющих графовую структуру. К таким данным относятся, к примеру, описания дорожных и компьютерных сетей, социальных графов и графов цитирований, молекулярных графов, а также графов знаний, описывающих взаимосвязи между сущностями, событиями и абстрактными категориями. graph В этом параграфе мы с вами познакомимся с основными задачами, которые возникают при обработке графов, а также поговорим о графовых свертках и графовых нейронных сетях — специальном классе обучаемых преобразований, способных принимать в качестве входа графы и решать задачи на них. Описание графовых данных Граф G=(V,E) принято представлять двумя множествами: множеством V V, содержащим вершины и их признаковые описания, а также множеством E E, содержащим связи между вершинами (то есть рёбра) и признаковые описания этих связей. Для простоты математических выкладок и изложения дальнейшего материала давайте считать, что мы всегда работаем с ориентированными графами. Если граф содержит ненаправленное ребро, мы его заменяем на пару направленных ребер. Кроме того, давайте обозначать окрестность вершины как N(v)={ v ^ ∣( v ^ ,v)∈E}. 2 Графовые данные довольно разнообразны. Они могут отличаться между собой в следующих моментах: По размеру, т.е. количеству вершин и/или ребер. По наличию признаковых описаний вершин и рёбер. В зависимости от решаемой задачи, графы могут содержать информацию только в вершинах, только в ребрах, либо же и там и там. Кроме того, графы могут быть гомо- и гетерогенными — в зависимости от того, имеют ли вершины и ребра графа одну природу либо же нет. Например, социальные графы содержат огромное количество вершин и ребер, часто измеряющееся в тысячах, содержат информацию в вершинах и очень редко в ребрах, а также являются гомогенными, так как все вершины имеют один тип. В то же время, молекулярные графы — это пример графов с, как правило, средним количеством вершин и ребер; вершины и связи в молекулярных графах имеют признаковое описание (типы атомов и ковалентных связей, а также информацию о зарядах и т.п.), но при этом также являются гомогенными графами. К классу гетерогенных графов относятся, например, графы знаний, описывающие некоторую систему, различные сущности в ней и взаимодействия между этими сущностями. Вершины (сущности) и связи (ребра) такого графа могут иметь различную природу: скажем, вершинами могут быть сотрудники и подразделения компании, а рёбра могут отвечать отношениям «Х работает в подразделении Y», «X и Z коллеги» и так далее. 6.4.1 Задачи на графах Разнообразие графовых данных закономерно породило множество разнообразных задач, которые решаются на этих данных. Среди них можно встретить классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных уровнях: классифицировать можно весь граф (graph-level), а можно отдельные его вершины (node-level) или связи (edge-level). Так, в качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства. На node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 1,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных уровнях: классифицировать можно весь граф (graph-level), а можно отдельные его вершины (node-level) или связи (edge-level). Так, в качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства. На node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки неразмеченных вершин. Например, предсказать интересы нового пользователя по интересам его друзей. 1 Часто бывает такое, что граф приходит полностью неразмеченным и хочется без учителя разделить на компоненты. Например, имея граф цитирований, выделить в нем подгруппы соавторов или выделить области исследования. В таком случае принято говорить о node-level кластеризации графа. graph Наконец, довольно интересна задача предсказания пропущенных связей в графе. В больших графах часто некоторые связи отсутствуют. Например, в социальном графе пользователь может добавить не всех знакомых в друзья. А в графе знаний могут быть проставлены только простые взаимосвязи, а высокоуровневые могут быть пропущены. 1 В конце, хотелось бы отметить очень важные особенности всех задач, связанных с графами. Алгоритмы решения этих задач должны обладать двумя свойствами. Во-первых, графы в датасетах, как правило, могут отличаться по размерам: как по количеству вершин, так и по количеству связей. Алгоритмы решения задач на графах должны уметь принимать графы различных размеров. Во-вторых, алгоритмы должны быть инварианты к перестановкам порядка вершин. То есть если взять тот же граф и перенумеровать его вершины, то алгоритмы должны выдавать те же предсказания с учетом этой перестановки. Графовые нейронные сети Развитие глубинного обучения повлияло на подходы к решению задач на графовых данных. Был предложен концепт графовых нейронных сетей, которые в последнее время либо полностью заменили классические алгоритмы обработки графов, либо породили мощные синергии с этими алгоритмами. 1 Графовые нейронные сети по принципу работы и построения идейно очень похожи на сверточные нейронные сети. Более того, забегая немного вперед, графовые нейроные сети являются обобщением сверточных нейронных сетей. На вход графовой нейронной сети подается граф. В отличие от сверточных нейронных сетей, которые требуют, чтобы все картинки в батче были одинакового размера, графовые нейронные сети допускают разные размеры у объектов батча. Кроме того, в отличие от картинок, у которых информация довольно однородна (это, как правило, несколько цветовых каналов) и хранится в пикселях, у графов информация может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы. =Emb(V),h 0 e =Emb(E) Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 2,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы. =Emb(V),h 0 e =Emb(E) Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей и обновляют информацию в вершине. То же самое делают и обычные свертки. Поэтому такие слои и называются графовыми свертками. Графовая свертка принимает на вход граф со скрытыми состояниями у вершин и ребер и выдает тот же граф, но уже с обновленными более информативными скрытыми состояниями. В отличие от сверточных нейронных сетей, при обработке графа pooling слои вставляют редко, в основном в graph-level задачах, при этом придумать разумную концепцию графового пулинга оказалось нелегко. Если вам станет интересно, вы можете познакомиться с несколькими вариантами графовых пулингов в следующих статьях: Learning Spectral Clustering Kernel k-means, Spectral Clustering and Normalized Cuts Weighted Graph Cuts without Eigenvectors В большинстве же архитектур пулинги не используются, и структура графа на входе и выходе графовой нейронной сети совпадает. 1 Полученная после череды сверток информация с вершин и ребер в конце обрабатывается с помощью полносвязных сетей для получения ответа на задачу. Для node-level классификации и регрессии полносвязная сеть применяется к скрытым состояниям вершин , а для edge-level, соответственно, к скрытым состояниям ребер . Для получения ответа на graph-level уровне информация с вершин и ребер сначала агрегируется с помощью readout операции. На месте readout операции могут располагаться любые инвариантные к перестановкам операции: подсчет максимума, среднего или даже обучаемый self-attention слой. gnn Как говорилось ранее, графовые нейронные сети являются обобщением сверточных. Если представить пиксели изображения вершинами графа, соединить соседние по свертке пиксели ребрами и предоставить относительную позицию пикселей в информации о ребре, то графовая свертка на таком графе будет работать так же, как и свертка над изображением. gnn К графовым нейронным сетям, как и к сверточным, применим термин receptive field. Это та область графа, которая будет влиять на скрытое состояние вершины после N сверток. Для графов receptive field после N графовых сверток — это все вершины и ребра графа, до которых можно дойти от фиксированной вершины не более чем за N переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток. receptive Многие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 3,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток. receptive Многие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только для ребер или же и там и там. Кроме того, стоит иметь ввиду, что графы различных размеров будут неравноценно влиять на среднее и дисперсию в BatchNorm слое. Более стабильной альтернативой BatchNorm в обработке графов, например, являются LayerNorm и GraphNorm, которые производят нормировку активаций по каждому графу независимо. LayerNorm, по сути, применяет BatchNorm для каждого графа: Var Var[x]+ϵ x−E[x] ⊙γ+β A вот GraphNorm содержит несколько обучаемых параметров и является более гибким вариантом нормализации: Var Var[x−α⊙E[x]]+ϵ x−α⊙E[x] ⊙γ+β Парадигмы построения графовых сверток Важно отметить, что в отличие от свертки, применяемой для изображений, являющейся четко определенной операцией, графовая свертка представляет собой именно концепт, абстрактную операцию, обновляющую скрытые представления объектов графа, используя доступную информацию с соседей и ребер. На практике, конкретный механизм графовой свертки разрабатывается для конкретной задачи, и различные реализации графовых сверток могут очень сильно отличаться между собой. И если зайти на сайты популярных фреймворков глубинного обучения на графах (например, PyG), то можно обнаружить десятки различных реализаций графовых сверток. Во-первых, графовые свертки отличаются между собой по тому набору информации, которые они могут использовать. Есть свертки, которые используют только скрытые представления вершин, игнорируя информацию на ребрах. Существуют свертки, которые по разному обрабатывают информацию от ребер различного типа. А есть свертки, которые используют информацию с ребер и вершин, обновляя одновременно и те и другие. Во-вторых, и что более важно, графовые свертки можно разделить на два семейства, которые отличаются математической парадигмой, в которой они работают. Есть spatial (пространственный) и spectral (спектральный) подходы. Пространственные свертки основываются на message-passing парадигме, в то время как спектральные работают с графовым лапласианом и его собственными векторами. На практике, спектральные свертки чаще применяются и показывают лучшие результаты в задачах связанных с обработкой одного большого графа, где важно понимать относительное месторасположение вершины в этом большом графе. Например, графа соцсетей или графа цитирований. Пространственные свертки показывают хорошие результаты в остальных задачах, где для решения задачи важно находить локальные подструктуры внутри графа. Несмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой работе. Давайте разберемся с этими двумя парадигмами. gnn Пространственная парадигма Пространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа. Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей. 1 Итерация работы Message passing подхода для одной вершины можно описать",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 4,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "задачи важно находить локальные подструктуры внутри графа. Несмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой работе. Давайте разберемся с этими двумя парадигмами. gnn Пространственная парадигма Пространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа. Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей. 1 Итерация работы Message passing подхода для одной вершины можно описать следующим абстрактным алгоритмом. Для каждой вершины v v собираются все тройки ) состоящие из скрытых представлений текущей вершин x v x v и ее соседа x w x w , а также из типа ребра e w v e wv ,соединяющего текущую вершину и её соседа. Ко всем этим тройкам применяется обучаемое преобразование M M (от слова message), которая считает сообщение — информацию, которая идет от соседа к вершине. Посчитанные сообщения агрегируются в одно, обозначаемое m v m v . Сообщения могут быть сагрегированы любой ассоциативной операцией, например взятием поэлементного минимума, максимума или среднего. Далее, агрегированное сообщение и текущее внутреннее состояние вершины подаются на вход обучаемой операции U U (от слова update), которая обновляет внутреннее состояние вершины. Конкретные имплементации операций M , U M,U непосредственно зависят от алгоритма и той задачи, которую он решает. 1 Одним из самых известных классических алгоритмов, построенных на пространственной парадигме, является PageRank. Алгоритм PageRank проходит по графу веб страниц и выставляет каждой веб-странице значение ее \"важности\" P G PG, которое впоследствии можно использовать для ранжирования поисковой выдачи. Формула подсчета PageRank выражается через коэффициент затухания d d, а также значения PageRank соседей N ( A ) N(A) вершины и количество исходящих ссылок из этих соседей L N ( A ) LN(A) следующим образом: PR(A)=(1−d)+d B∈N(A) ∑ LN(B) PR(B) В такой постановке операции подсчета сообщений M M и операции обновления U U имеют следующий вид: M(B)= LN(B) PR(B) m a = B∈N(A) ∑ M(B) U(A)=(1−d)+dm a PR(A)=U(A) Графовые свертки, работающие на парадигме передачи сообщений, как правило делают M M и U U обучаемыми преобразованиями. Рассмотрим несколько конкретных примеров архитектур. GraphSAGE Свертка GraphSAGE работает по следующему принципу. Для каждой вершины вычисляется набор скрытых представлений соседних вершин , из которых идут связи в текущую. Далее, собранная информация агрегируется с помощью некоторой коммутативной операции A G G R AGGR в вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины , они домножаются на обучаемую матрицу W W и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу. t+1 =AGGR({h w t ,w∈N(v)}) h v t+1 =σ(W t+1 CONCAT(m v t+1 ,h v t )) Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 5,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины , они домножаются на обучаемую матрицу W W и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу. t+1 =AGGR({h w t ,w∈N(v)}) h v t+1 =σ(W t+1 CONCAT(m v t+1 ,h v t )) Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной архитектуры в задачах, связанных с выучиванием представлений вершин, однако использование данной свертки можно встретить и в других задачах, связанных с обработкой графов, не содержащих дополнительной информации о рёбрах. GAT Свертка GAT (Graph ATtention) является развитием идеи GraphSAGE. В качестве механизма агрегации эта архитектура предлагает использовать механизм внимания, у которого матрицы преобразования для ключей, значений и запросов совпадают и обозначены в формуле буквой W W. Как и в GraphSAGE, агрегированное сообщение проходит через сигмоиду, но не домножается перед этим на обучаемую матрицу. =softmax(act(a T CONCAT(Wh v t ,Wh ∗ t ))) h v t+1 =σ w∈N(v) Здесь act — некоторая функция активации. Как и в случае механизма внимания для последовательностей, в момент обновления представления для вершины v v attention «смотрит» на все остальные вершины w w и генерирует веса α v w α vw , которые указывают, информация из каких вершин w w «важнее» для нас. Благодаря мощности и гибкости механизм внимания, эта свертка показала отличные результаты на множестве задач и является одной из самых популярных сверток. По умолчанию, эта свертка, как и GraphSAGE, использует только признаки вершин, однако, в некоторых проектах можно встретить модификации свертки, в которых механизм внимания учитывает ещё и информацию для ребер. RGCN Наконец, есть специально разработанные свертки для обработки графов, ребра которых могут быть нескольких типов. Одна из них называется RGCN (Relational Graph Convolutional Networks). Она суммирует скрытые представления соседей, однако каждое представление соседа домножается на матрицу, зависящую от типа ребра, которое соединяет соседа с текущей вершиной. Если в графе присутствует ребра N N типов, то данная свертка будет учить N N матриц - по одной для каждого типа связи. t+1 =σ r∈R ∑ w∈N r (v) ∑ c i,r Спектральная парадигма Противоположностью пространственной парадигме является спектральная (spectral) парадигма. В своей постановке спектральная парадигма опирается на анализ процесса диффузии сигнала внутри графа и анализирует матрицы, описывающих граф — матрицу смежности и матрицу, которая называется Лапласианом графа. spectral Лапласиан графа — это матрица L = D − A L=D−A, где D D — диагональная матрица, хранящая в i i-й диагональной ячейке количество исходящих из i i-й вершины рёбер, а A A — матрица смежности графа, ( i , j ) (i,j)-й элемент которой равен числу рёбер, соединяющих i i-ю и j j-ю вершину. Лапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений,",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 6,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "граф — матрицу смежности и матрицу, которая называется Лапласианом графа. spectral Лапласиан графа — это матрица L = D − A L=D−A, где D D — диагональная матрица, хранящая в i i-й диагональной ячейке количество исходящих из i i-й вершины рёбер, а A A — матрица смежности графа, ( i , j ) (i,j)-й элемент которой равен числу рёбер, соединяющих i i-ю и j j-ю вершину. Лапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений, описывают разрезы графа — его разделения пополам таким образом, чтобы между разделенным половинами было как можно меньше ребер. Так, собственный вектор, соответствующий наименьшему положительному собственному значению, будет описывать кластеризацию графа на два подграфа. Все индексы, соответствующие положительным элементам вектора задают вершины, которые должны оказаться в первом кластере, а отрицательные элементы будут соответствовать вершинам, которые должны оказаться во втором кластере. Этим свойством Лапласиана графа пользуются для того, чтобы проводить кластеризацию графа без учителя. Для этого надо: Посчитать Лапласиан L L матрицы A A Посчитать k k собственных векторов, соответствующих наименьшим собственным значениям Сформировать из них матрицу размера N × k N×k, каждая строка которой описывает вершину k k признаками Кластеризовать объекты, описываемые этой матрицей (например, c помощью K-Means) Таким образом, спектральный подход отлично подходит для того, чтобы находить в графе компоненты, вершины которых связаны друг с другом и имеют похожие свойства. GCN Свертка GCN, основанная на спектральной парадигме, использует только скрытые состояния вершин h h и матрицу смежности A A — она учитывает лишь наличие или отсутствие ребра в графе, но не признаки ребер. С математической точки зрения, GCN очень проста и представляет собой один шаг итеративного процесса поиска собственных значений Лапласиана графа: мы берем скрытые представления вершин и домножаем их на нормированную матрицу смежности — матрицу A A, домноженную слева и справа на матричный корень матрицы D D. Этот шаг применяется ко всем каналам скрытого представления вершины. После этого шага, обновленные скрытые представления ещё домножаются на обучаемую матрицу t+1 =θD −1/2 (A+I)D −1/2 h t Здесь h j h j — это матрица размера (число вершин) × ×(длина вектора представления), то есть к каждому «каналу» представлений свёртка применяется отдельно. Если же мы хотим работать с несколькими каналами, то есть вместо h t h t у нас матрица H t H t размера (число вершин) × ×(число каналов), и ещё добавить нелинейность f f, формула переписывается следующим образом: t+1 =f(D −1/2 (A+I)D −1/2 H t Θ). Авторы данной свертки показали отличное качество работы в задачах классификации вершин графов цитирования и графа знаний. Однако, различные модификации данной свертки применяются и в других задачах, например, для выучивания векторных представлений вершин и для кластеризации вершин графа. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 6.3. Трансформеры Следующий параграф 6.5. Нейросети для облаков точек",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.4",
      "part": 7,
      "total_parts": 7,
      "source_file": "6.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Наряду с обработкой табличных, текстовых, аудио данных и изображений, в глубинном обучении довольно часто приходится решать задачи на данных, имеющих графовую структуру. К таким данным относятся, к примеру, описания дорожных и компьютерных сетей, социальных графов и графов цитирований, молекулярных графов, а также графов знаний, описывающих взаимосвязи между сущностями, событиями и абстрактными категориями. graph В этом параграфе мы с вами познакомимся с основными задачами, которые возникают при обработке графов, а также поговорим о графовых свертках и графовых нейронных сетях — специальном классе обучаемых преобразований, способных принимать в качестве входа графы и решать задачи на них. Описание графовых данных Граф G=(V,E) принято представлять двумя множествами: множеством V V, содержащим вершины и их признаковые описания, а также множеством E E, содержащим связи между вершинами (то есть рёбра) и признаковые описания этих связей. Для простоты математических выкладок и изложения дальнейшего материала давайте считать, что мы всегда работаем с ориентированными графами. Если граф содержит ненаправленное ребро, мы его заменяем на пару направленных ребер. Кроме того, давайте обозначать окрестность вершины как N(v)={ v ^ ∣( v ^ ,v)∈E}. 2 Графовые данные довольно разнообразны. Они могут отличаться между собой в следующих моментах: По размеру, т.е. количеству вершин и/или ребер. По наличию признаковых описаний вершин и рёбер. В зависимости от решаемой задачи, графы могут содержать информацию только в вершинах, только в ребрах, либо же и там и там. Кроме того, графы могут быть гомо- и гетерогенными — в зависимости от того, имеют ли вершины и ребра графа одну природу либо же нет. Например, социальные графы содержат огромное количество вершин и ребер, часто измеряющееся в тысячах, содержат информацию в вершинах и очень редко в ребрах, а также являются гомогенными, так как все вершины имеют один тип. В то же время, молекулярные графы — это пример графов с, как правило, средним количеством вершин и ребер; вершины и связи в молекулярных графах имеют признаковое описание (типы атомов и ковалентных связей, а также информацию о зарядах и т.п.), но при этом также являются гомогенными графами. К классу гетерогенных графов относятся, например, графы знаний, описывающие некоторую систему, различные сущности в ней и взаимодействия между этими сущностями. Вершины (сущности) и связи (ребра) такого графа могут иметь различную природу: скажем, вершинами могут быть сотрудники и подразделения компании, а рёбра могут отвечать отношениям «Х работает в подразделении Y», «X и Z коллеги» и так далее. 6.4.1 Задачи на графах Разнообразие графовых данных закономерно породило множество разнообразных задач, которые решаются на этих данных. Среди них можно встретить классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных уровнях: классифицировать можно весь граф (graph-level), а можно отдельные его вершины (node-level) или связи (edge-level). Так, в качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства. На node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 1,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "классические постановки классификации, регрессии и кластеризации, но есть и специфичные задачи, не встречающиеся в других областях — например, задача восстановления пропущенных связей внутри графа или генерации графов с нужными свойствами. Однако даже классические задачи могут решаться на различных уровнях: классифицировать можно весь граф (graph-level), а можно отдельные его вершины (node-level) или связи (edge-level). Так, в качестве примера graph-level задач можно привести классификацию и регрессию на молекулярных графах. Имея датасет с размеченными молекулами, можно предсказывать их принадлежность к лекарственной категории и различные химико-биологические свойства. На node-level, как правило, классифицируют вершины одного огромного графа, например, социального. Имея частичную разметку, хочется восстановить метки неразмеченных вершин. Например, предсказать интересы нового пользователя по интересам его друзей. 1 Часто бывает такое, что граф приходит полностью неразмеченным и хочется без учителя разделить на компоненты. Например, имея граф цитирований, выделить в нем подгруппы соавторов или выделить области исследования. В таком случае принято говорить о node-level кластеризации графа. graph Наконец, довольно интересна задача предсказания пропущенных связей в графе. В больших графах часто некоторые связи отсутствуют. Например, в социальном графе пользователь может добавить не всех знакомых в друзья. А в графе знаний могут быть проставлены только простые взаимосвязи, а высокоуровневые могут быть пропущены. 1 В конце, хотелось бы отметить очень важные особенности всех задач, связанных с графами. Алгоритмы решения этих задач должны обладать двумя свойствами. Во-первых, графы в датасетах, как правило, могут отличаться по размерам: как по количеству вершин, так и по количеству связей. Алгоритмы решения задач на графах должны уметь принимать графы различных размеров. Во-вторых, алгоритмы должны быть инварианты к перестановкам порядка вершин. То есть если взять тот же граф и перенумеровать его вершины, то алгоритмы должны выдавать те же предсказания с учетом этой перестановки. Графовые нейронные сети Развитие глубинного обучения повлияло на подходы к решению задач на графовых данных. Был предложен концепт графовых нейронных сетей, которые в последнее время либо полностью заменили классические алгоритмы обработки графов, либо породили мощные синергии с этими алгоритмами. 1 Графовые нейронные сети по принципу работы и построения идейно очень похожи на сверточные нейронные сети. Более того, забегая немного вперед, графовые нейроные сети являются обобщением сверточных нейронных сетей. На вход графовой нейронной сети подается граф. В отличие от сверточных нейронных сетей, которые требуют, чтобы все картинки в батче были одинакового размера, графовые нейронные сети допускают разные размеры у объектов батча. Кроме того, в отличие от картинок, у которых информация довольно однородна (это, как правило, несколько цветовых каналов) и хранится в пикселях, у графов информация может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы. =Emb(V),h 0 e =Emb(E) Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 2,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "может также храниться в вершинах и/или ребрах. Причем в одних задачах информация может быть только в вершинах, в других только в ребрах, а в третьих и там, и там. Сама информация может быть довольно разнородной: это могут быть и вещественные значения, и дискретные значения, в зависимости от природы графа и от типа решаемой задачи. Поэтому, довольно часто первым слоем в графовых нейронных сетях идут Embedding слои, которые переводят дискретные токены в вещественные векторы. =Emb(V),h 0 e =Emb(E) Однако, сама суть работы у графовых и сверточных сетей совпадает. В графовой нейронной сети по очереди применяются слои, которые собирают информацию с соседей и обновляют информацию в вершине. То же самое делают и обычные свертки. Поэтому такие слои и называются графовыми свертками. Графовая свертка принимает на вход граф со скрытыми состояниями у вершин и ребер и выдает тот же граф, но уже с обновленными более информативными скрытыми состояниями. В отличие от сверточных нейронных сетей, при обработке графа pooling слои вставляют редко, в основном в graph-level задачах, при этом придумать разумную концепцию графового пулинга оказалось нелегко. Если вам станет интересно, вы можете познакомиться с несколькими вариантами графовых пулингов в следующих статьях: Learning Spectral Clustering Kernel k-means, Spectral Clustering and Normalized Cuts Weighted Graph Cuts without Eigenvectors В большинстве же архитектур пулинги не используются, и структура графа на входе и выходе графовой нейронной сети совпадает. 1 Полученная после череды сверток информация с вершин и ребер в конце обрабатывается с помощью полносвязных сетей для получения ответа на задачу. Для node-level классификации и регрессии полносвязная сеть применяется к скрытым состояниям вершин , а для edge-level, соответственно, к скрытым состояниям ребер . Для получения ответа на graph-level уровне информация с вершин и ребер сначала агрегируется с помощью readout операции. На месте readout операции могут располагаться любые инвариантные к перестановкам операции: подсчет максимума, среднего или даже обучаемый self-attention слой. gnn Как говорилось ранее, графовые нейронные сети являются обобщением сверточных. Если представить пиксели изображения вершинами графа, соединить соседние по свертке пиксели ребрами и предоставить относительную позицию пикселей в информации о ребре, то графовая свертка на таком графе будет работать так же, как и свертка над изображением. gnn К графовым нейронным сетям, как и к сверточным, применим термин receptive field. Это та область графа, которая будет влиять на скрытое состояние вершины после N сверток. Для графов receptive field после N графовых сверток — это все вершины и ребра графа, до которых можно дойти от фиксированной вершины не более чем за N переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток. receptive Многие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 3,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "переходов. Знание receptive field полезно при проектировании нейронной сети - имея представление о том, с какой окрестности вершины надо собрать информацию для решения задачи, можно подбирать нужное количество графовых сверток. receptive Многие техники стабилизации обучения и повышения обобщаемости, такие как Dropout, BatchNorm и Residual Connections, применимы и к графовым нейронным сетям. Однако стоит помнить про их особенности. Эти операции могут независимо применяться (или не применяться) к вершинам и ребрам. Так, если вы применяете Dropout, то вы вправе поставить для вершин и для рёбер различные значения dropout rate. Аналогично и для Residual Connections - они могут применяться только для вершин, только для ребер или же и там и там. Кроме того, стоит иметь ввиду, что графы различных размеров будут неравноценно влиять на среднее и дисперсию в BatchNorm слое. Более стабильной альтернативой BatchNorm в обработке графов, например, являются LayerNorm и GraphNorm, которые производят нормировку активаций по каждому графу независимо. LayerNorm, по сути, применяет BatchNorm для каждого графа: Var Var[x]+ϵ x−E[x] ⊙γ+β A вот GraphNorm содержит несколько обучаемых параметров и является более гибким вариантом нормализации: Var Var[x−α⊙E[x]]+ϵ x−α⊙E[x] ⊙γ+β Парадигмы построения графовых сверток Важно отметить, что в отличие от свертки, применяемой для изображений, являющейся четко определенной операцией, графовая свертка представляет собой именно концепт, абстрактную операцию, обновляющую скрытые представления объектов графа, используя доступную информацию с соседей и ребер. На практике, конкретный механизм графовой свертки разрабатывается для конкретной задачи, и различные реализации графовых сверток могут очень сильно отличаться между собой. И если зайти на сайты популярных фреймворков глубинного обучения на графах (например, PyG), то можно обнаружить десятки различных реализаций графовых сверток. Во-первых, графовые свертки отличаются между собой по тому набору информации, которые они могут использовать. Есть свертки, которые используют только скрытые представления вершин, игнорируя информацию на ребрах. Существуют свертки, которые по разному обрабатывают информацию от ребер различного типа. А есть свертки, которые используют информацию с ребер и вершин, обновляя одновременно и те и другие. Во-вторых, и что более важно, графовые свертки можно разделить на два семейства, которые отличаются математической парадигмой, в которой они работают. Есть spatial (пространственный) и spectral (спектральный) подходы. Пространственные свертки основываются на message-passing парадигме, в то время как спектральные работают с графовым лапласианом и его собственными векторами. На практике, спектральные свертки чаще применяются и показывают лучшие результаты в задачах связанных с обработкой одного большого графа, где важно понимать относительное месторасположение вершины в этом большом графе. Например, графа соцсетей или графа цитирований. Пространственные свертки показывают хорошие результаты в остальных задачах, где для решения задачи важно находить локальные подструктуры внутри графа. Несмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой работе. Давайте разберемся с этими двумя парадигмами. gnn Пространственная парадигма Пространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа. Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей. 1 Итерация работы Message passing подхода для одной вершины можно описать",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 4,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "задачи важно находить локальные подструктуры внутри графа. Несмотря на принципиальную противоположность этих двух подходов, активно предпринимаются попытки их совмещения в одну парадигму, например, в этой работе. Давайте разберемся с этими двумя парадигмами. gnn Пространственная парадигма Пространственная (spatial) парадигма основывает на алгоритме передачи сообщений (message passing) между вершинами графа. Концепт этого подхода заключается в следующем - каждая вершина графа имеет внутреннее состояние. Каждую итерацию это внутреннее состояние пересчитывается, основываясь на внутренних состояниях соседей по графу. Каждый сосед влияет на состояние вершины, так же как и вершина влияет на состояния соседей. 1 Итерация работы Message passing подхода для одной вершины можно описать следующим абстрактным алгоритмом. Для каждой вершины v v собираются все тройки ) состоящие из скрытых представлений текущей вершин x v x v и ее соседа x w x w , а также из типа ребра e w v e wv ,соединяющего текущую вершину и её соседа. Ко всем этим тройкам применяется обучаемое преобразование M M (от слова message), которая считает сообщение — информацию, которая идет от соседа к вершине. Посчитанные сообщения агрегируются в одно, обозначаемое m v m v . Сообщения могут быть сагрегированы любой ассоциативной операцией, например взятием поэлементного минимума, максимума или среднего. Далее, агрегированное сообщение и текущее внутреннее состояние вершины подаются на вход обучаемой операции U U (от слова update), которая обновляет внутреннее состояние вершины. Конкретные имплементации операций M , U M,U непосредственно зависят от алгоритма и той задачи, которую он решает. 1 Одним из самых известных классических алгоритмов, построенных на пространственной парадигме, является PageRank. Алгоритм PageRank проходит по графу веб страниц и выставляет каждой веб-странице значение ее \"важности\" P G PG, которое впоследствии можно использовать для ранжирования поисковой выдачи. Формула подсчета PageRank выражается через коэффициент затухания d d, а также значения PageRank соседей N ( A ) N(A) вершины и количество исходящих ссылок из этих соседей L N ( A ) LN(A) следующим образом: PR(A)=(1−d)+d B∈N(A) ∑ LN(B) PR(B) В такой постановке операции подсчета сообщений M M и операции обновления U U имеют следующий вид: M(B)= LN(B) PR(B) m a = B∈N(A) ∑ M(B) U(A)=(1−d)+dm a PR(A)=U(A) Графовые свертки, работающие на парадигме передачи сообщений, как правило делают M M и U U обучаемыми преобразованиями. Рассмотрим несколько конкретных примеров архитектур. GraphSAGE Свертка GraphSAGE работает по следующему принципу. Для каждой вершины вычисляется набор скрытых представлений соседних вершин , из которых идут связи в текущую. Далее, собранная информация агрегируется с помощью некоторой коммутативной операции A G G R AGGR в вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины , они домножаются на обучаемую матрицу W W и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу. t+1 =AGGR({h w t ,w∈N(v)}) h v t+1 =σ(W t+1 CONCAT(m v t+1 ,h v t )) Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 5,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "вектор фиксированного размера. В качестве операции агрегации авторы предлагают использовать операции взятия средних или максимальных значений скрытых представлений объектов из набора. Далее агрегированный вектор объединяется со скрытым представлением вершины , они домножаются на обучаемую матрицу W W и к результату умножения поэлементно применяется сигмоида. Обучаемые параметры данного слоя, как и в случае GCN, содержат только одну матрицу. t+1 =AGGR({h w t ,w∈N(v)}) h v t+1 =σ(W t+1 CONCAT(m v t+1 ,h v t )) Данная свертка использует только скрытые представления вершин, однако уделяет больше внимания локальному окружению вершины, нежели её глобальному положению во всем графе. Авторы показали высокое качество данной архитектуры в задачах, связанных с выучиванием представлений вершин, однако использование данной свертки можно встретить и в других задачах, связанных с обработкой графов, не содержащих дополнительной информации о рёбрах. GAT Свертка GAT (Graph ATtention) является развитием идеи GraphSAGE. В качестве механизма агрегации эта архитектура предлагает использовать механизм внимания, у которого матрицы преобразования для ключей, значений и запросов совпадают и обозначены в формуле буквой W W. Как и в GraphSAGE, агрегированное сообщение проходит через сигмоиду, но не домножается перед этим на обучаемую матрицу. =softmax(act(a T CONCAT(Wh v t ,Wh ∗ t ))) h v t+1 =σ w∈N(v) Здесь act — некоторая функция активации. Как и в случае механизма внимания для последовательностей, в момент обновления представления для вершины v v attention «смотрит» на все остальные вершины w w и генерирует веса α v w α vw , которые указывают, информация из каких вершин w w «важнее» для нас. Благодаря мощности и гибкости механизм внимания, эта свертка показала отличные результаты на множестве задач и является одной из самых популярных сверток. По умолчанию, эта свертка, как и GraphSAGE, использует только признаки вершин, однако, в некоторых проектах можно встретить модификации свертки, в которых механизм внимания учитывает ещё и информацию для ребер. RGCN Наконец, есть специально разработанные свертки для обработки графов, ребра которых могут быть нескольких типов. Одна из них называется RGCN (Relational Graph Convolutional Networks). Она суммирует скрытые представления соседей, однако каждое представление соседа домножается на матрицу, зависящую от типа ребра, которое соединяет соседа с текущей вершиной. Если в графе присутствует ребра N N типов, то данная свертка будет учить N N матриц - по одной для каждого типа связи. t+1 =σ r∈R ∑ w∈N r (v) ∑ c i,r Спектральная парадигма Противоположностью пространственной парадигме является спектральная (spectral) парадигма. В своей постановке спектральная парадигма опирается на анализ процесса диффузии сигнала внутри графа и анализирует матрицы, описывающих граф — матрицу смежности и матрицу, которая называется Лапласианом графа. spectral Лапласиан графа — это матрица L = D − A L=D−A, где D D — диагональная матрица, хранящая в i i-й диагональной ячейке количество исходящих из i i-й вершины рёбер, а A A — матрица смежности графа, ( i , j ) (i,j)-й элемент которой равен числу рёбер, соединяющих i i-ю и j j-ю вершину. Лапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений,",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 6,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "граф — матрицу смежности и матрицу, которая называется Лапласианом графа. spectral Лапласиан графа — это матрица L = D − A L=D−A, где D D — диагональная матрица, хранящая в i i-й диагональной ячейке количество исходящих из i i-й вершины рёбер, а A A — матрица смежности графа, ( i , j ) (i,j)-й элемент которой равен числу рёбер, соединяющих i i-ю и j j-ю вершину. Лапласиан графа имеет неотрицательные собственные значения. Количество нулевых собственных значений всегда совпадает с количеством компонент связности. Потрясающим свойством Лапласиана является то, что его собственные векторы, соответствующие положительным собственным значениям, в порядке возрастания собственных значений, описывают разрезы графа — его разделения пополам таким образом, чтобы между разделенным половинами было как можно меньше ребер. Так, собственный вектор, соответствующий наименьшему положительному собственному значению, будет описывать кластеризацию графа на два подграфа. Все индексы, соответствующие положительным элементам вектора задают вершины, которые должны оказаться в первом кластере, а отрицательные элементы будут соответствовать вершинам, которые должны оказаться во втором кластере. Этим свойством Лапласиана графа пользуются для того, чтобы проводить кластеризацию графа без учителя. Для этого надо: Посчитать Лапласиан L L матрицы A A Посчитать k k собственных векторов, соответствующих наименьшим собственным значениям Сформировать из них матрицу размера N × k N×k, каждая строка которой описывает вершину k k признаками Кластеризовать объекты, описываемые этой матрицей (например, c помощью K-Means) Таким образом, спектральный подход отлично подходит для того, чтобы находить в графе компоненты, вершины которых связаны друг с другом и имеют похожие свойства. GCN Свертка GCN, основанная на спектральной парадигме, использует только скрытые состояния вершин h h и матрицу смежности A A — она учитывает лишь наличие или отсутствие ребра в графе, но не признаки ребер. С математической точки зрения, GCN очень проста и представляет собой один шаг итеративного процесса поиска собственных значений Лапласиана графа: мы берем скрытые представления вершин и домножаем их на нормированную матрицу смежности — матрицу A A, домноженную слева и справа на матричный корень матрицы D D. Этот шаг применяется ко всем каналам скрытого представления вершины. После этого шага, обновленные скрытые представления ещё домножаются на обучаемую матрицу t+1 =θD −1/2 (A+I)D −1/2 h t Здесь h j h j — это матрица размера (число вершин) × ×(длина вектора представления), то есть к каждому «каналу» представлений свёртка применяется отдельно. Если же мы хотим работать с несколькими каналами, то есть вместо h t h t у нас матрица H t H t размера (число вершин) × ×(число каналов), и ещё добавить нелинейность f f, формула переписывается следующим образом: t+1 =f(D −1/2 (A+I)D −1/2 H t Θ). Авторы данной свертки показали отличное качество работы в задачах классификации вершин графов цитирования и графа знаний. Однако, различные модификации данной свертки применяются и в других задачах, например, для выучивания векторных представлений вершин и для кластеризации вершин графа. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 6.3. Трансформеры Следующий параграф 6.5. Нейросети для облаков точек",
    "metadata": {
      "title": "Графовые нейронные сети",
      "url": "https://education.yandex.ru/handbook/ml/article/nejroseti-dlya-oblakov-tochek",
      "course": "ml",
      "chapter": "6. Глубинное обучение - архитектуры",
      "chapter_id": "6.5",
      "part": 7,
      "total_parts": 7,
      "source_file": "6.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Объекты, с которыми работает машинное обучение, очень разнообразны и часто состоят из большого количества низкоуровневых сигналов: это может быть цвет отдельного пикселя, амплитуда звукового сигнала в отдельно взятый момент времени или буква в тексте. Каждый из таких слабых сигналов в отдельности несёт крайне мало информации про объект, но все вместе слабые сигналы складываются в музыку, изображение или текст. В сыром виде такие объекты сложно анализировать, поэтому наша цель — находить хорошие представления данных, удобные для анализа и решения разных задач. Любой способ построения признакового описания объекта мы будем назвать алгоритмом построения представлений, а настройку такого алгоритма по данным — обучением. Так, все методы понижения размерности, например, SVD — это методы обучения представлений, а методы обучения представлений часто можно проинтерпретировать как методы понижения размерности. Представим, что нам хочется уметь искать похожие музыкальные треки и использовать эту технологию в музыкальном сервисе для функции «играть похожие треки». Каждый трек в нашей базе хранится в формате WAV с частотой 44kHz и длится 3 минуты. Другими словами, трек будет описываться вектором из 7920000 чисел (44000 Hz * 60 секунд * 3 минуты). Однако небольшие изменения треков (сдвиг по времени, изменение громкости) могут соответствовать существенному изменению положения вектора представлений в пространстве. Поэтому простые расстояния в таком пространстве, вероятно, не будут отражать представление людей о схожести треков. Искать похожие треки, используя такие, сырые представления, проблематично, и необходимо научится строить представления, с помощью которых будет удобно решать разные высокоуровневые задачи. Выученные представления или модели для вычисления представлений можно использовать для: Поиска изображений: по представлению изображения искать похожие изображения. Рекомендаций: по представлениям пользователя определять наиболее интересные фильмы или товары. Чат-ботов: по представлению диалога уметь продолжать диалог, отвечать на вопросы и так далее. Систем видеосвязи: уметь восстанавливать кадры по компактным представлениям, сохраняя высокое качество картинки. и многого другого. Для каждой из этих задач будут свои хорошо отлаженные трюки, однако ключевых идей, которые часто переиспользуются, повторяются и модифицируются, не очень много. Мы постараемся рассказать про такие идеи. Предупреждение: перед прочтением этой главы стоит освежить в памяти главу про нейронные сети. Нейронные сети и выучивания представлений Нейронные сети можно рассматривать, как механизм автоматического выучивания представлений, поэтому современные методы выучивания представлений существенно сконцентрированы на использовании нейросетей. Напомним, что нейронная сеть состоит из набора дифференцируемых преобразований, примененных друг за другом к объекту x x для получения предсказания целевой переменной y y. Обычно преобразования содержат обучаемые параметры, которые настраиваются в процессе обучения по данным. Преобразования в литературе часто называют слоями. Результат применения преобразования к его входу мы будем называть скрытыми представлениями или активациями. Representations Активации любого слоя можно использовать как представления объекта. Представления с разной глубины нейронной сети будут обладать разными свойствами. Рассмотрим свёрточные нейронные сети для изображений. Активации первых слоёв обычно видят только маленькие части исходной картинки, другими словами, имеют маленький receptive field. Такие активации могут реагировать — принимать большие значения — только на низкоуровневые детали, маленькие фрагменты изображения. По мере увеличения глубины receptive field становится больше, а активации начинают реагировать на более высокоуровневые абстракции, такие как формы и части объектов. Активации последних слоёв имеют большой receptive field и реагируют на уже на объекты",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 1,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "входу мы будем называть скрытыми представлениями или активациями. Representations Активации любого слоя можно использовать как представления объекта. Представления с разной глубины нейронной сети будут обладать разными свойствами. Рассмотрим свёрточные нейронные сети для изображений. Активации первых слоёв обычно видят только маленькие части исходной картинки, другими словами, имеют маленький receptive field. Такие активации могут реагировать — принимать большие значения — только на низкоуровневые детали, маленькие фрагменты изображения. По мере увеличения глубины receptive field становится больше, а активации начинают реагировать на более высокоуровневые абстракции, такие как формы и части объектов. Активации последних слоёв имеют большой receptive field и реагируют на уже на объекты и группы объектов. Источник На изображении ниже показаны части картинок (патчи), каждая группа из 9 изображений максимизирует значение определенной активации в обученной нейронной сети. Размер патча зависит от receptive field активации, а максимизация ведется по датасету реальных изображений: выбираем топ-9 патчей из датасета по значению активации. Для активаций, взятых с ранних слоев, нейроны реагируют на низкоуровневые детали. По мере увеличения глубины нейроны начинают реагировать на более высокоуровневые объекты. Источник Большая часть методов, которые мы рассмотрим ниже, за исключением матричной факторизации (в зависимости от того, как на это взглянуть), будут использовать активации нейросети в качестве представлений. Поэтому, обучить представления и обучить нейросеть это почти синонимы. Большинство отличий будет состоять в том, как эти нейронные сети обучаются и какую архитектуру имеют. Дообучение Нейронные сети можно обучать из случайной инициализации, а можно стартовать с вектора весов, обученного на внешнем датасете. К примеру, если вы решаете задачу классификации изображений, часто инициализация части вашей нейронной сети весами, предобученными на популярном датасете ImageNet, ускорит и улучшит обучение. Такой процесс называется fine-tuning («дообучение» / «файнтюнинг»): Источник Как можно усложнять эту схему: Добавлять в модель много новых, обучающихся с нуля слоёв (на картинке мы добавляем один, но можно и больше); Не обязательно копировать все слои, можно копировать только сколько-то первых. Дообучать как все веса модели, так и какую-то часть. К примеру, можно заморозить скопированные слои и дообучать только новые части модели. Для файнтюнинга часто используют постепенное увеличение (warm-up) learning rate на первых эпохах обучения. Это позволяет сетке «привыкнуть» к новой задаче и архитектуре. Пример: Источник Prior В некотором смысле хорошая инициализация работает как праер на функции, которые могут быть выучены после дообучения. Поэтому дообучение часто требует в разы меньше данных, чем обучение со случайной инициализации. Supervised обучение Обучение представлений через решение supervised задачи Обучение представлений через решение supervised задач — это простой и популярный способ обучения представлений. Рассмотрим его на примере задачи поиска изображений (image retrieval) Задача: Рассмотрим задачу поиска изображений. Каждое изображение хочется закодировать вектором признаков (представлением) так, чтобы вектора признаков похожих изображений были близки. При чем тут обучение представлений? Image retrieval часто рассматривается как задача обучения представлений. Хочется получить алгоритм, который по изображению выдаст вектор (представление объекта) так, чтобы близость векторов по какой-то простой (скажем, евклидовой) метрике означала схожесть объектов. Идея: Возьмём активации с последнего слоя из нейросети, предобученной на большом размеченном датасете. Для задач зрения почти всегда имеется ввиду предобучение на задаче классификации. Также мы предполагаем, что высокоуровневая разметка собрана человеком. Почему такие представления могут",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 2,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "его на примере задачи поиска изображений (image retrieval) Задача: Рассмотрим задачу поиска изображений. Каждое изображение хочется закодировать вектором признаков (представлением) так, чтобы вектора признаков похожих изображений были близки. При чем тут обучение представлений? Image retrieval часто рассматривается как задача обучения представлений. Хочется получить алгоритм, который по изображению выдаст вектор (представление объекта) так, чтобы близость векторов по какой-то простой (скажем, евклидовой) метрике означала схожесть объектов. Идея: Возьмём активации с последнего слоя из нейросети, предобученной на большом размеченном датасете. Для задач зрения почти всегда имеется ввиду предобучение на задаче классификации. Также мы предполагаем, что высокоуровневая разметка собрана человеком. Почему такие представления могут адекватно решать задачу поиска изображений? Классификационная сеть будет неявно поощрять, чтобы у похожих изображений векторы активаций тоже были близки. К примеру, перед последнем слоем классификационной сети активации кошек и собак должны быть распознаны линейными классификаторами — активации картинок одного класса должны быть близки друг к другу. А за счет похожих паттернов визуально похожие коты будут находиться ближе друг к другу, чем непохожие. Решение Для начала нам нужно обучить нейросеть на большом размеченном датасете картинок/текста/звука/... (pretext problem) Обычно лучше всего работает предобучение на задачах классификации. Почему так происходит? Пока непонятно. Возможно, это связано с тем, что для классификации удобнее собирать датасеты, а возможно это хорошие свойства задачи или CrossEntropy функции потерь. Для изображений часто используется предобучение на датасете ImageNet — классификация на 1000 классов, 1.3M изображений в обучающей выборке, ~ 150 GiB. Для текстов, обычно решают задачу языкового моделирования, на набирающем популярность датасет The Pile ~ 825 GiB. Затем мы дообучаем нейросеть на более похожем на нашу задачу размеченном датасете (если такой есть; если нет пропускаем этот шаг). После — оставляем только первые L L слоёв. Aктивации слоя L L берём в качестве представлений объектов Агрегируем активации по пространственным координатам, чтобы получить вектор для каждого объекта. Часто используется покомпонентное среднее или максимум (скажем, глобальный пулинг для изображений). Наконец, используем признаки или предобученные веса для решения целевой задачи (downstream problem). Об алгоритме Supervised подход можно применять для различных типов данных. Всё, что нужно — это большой размеченный датасет, хоть и отдалённо, но похожий на целевые данные. Для музыки это может быть задача классификации жанра, для видео — задача классификация действий, для текста — классификация тематик. Достоинства и недостатки 👍 Благодаря выученным представлениям мы сможем решать целевую задачу, не имея для неё большого датасета; 👎 Нужен большой размеченный датасет, близкий для целевой задачи; 👎 Оптимальные представления для датасета, на котором мы предобучаемся, могут сколь угодно плохо подойти для целевой задачи. К примеру, представления, полученные на ImageNet, плохо подойдут для медицинских изображений (Raghu2019). Эксперименты На рисунке ниже показан пример того, как представления помогают решать задачу поиска. Запрос находится в самом левом столбике. Зеленым отмечены верно найденные изображения, красным — неверно найденные, синим — изображения из стоп-листа. Как видно, система вполне неплохо решает задачу поиска изображений. Подробнее про такой подход для поиска изображений можно почитать в работах (Babenko 2014, Babenko 2015). Источник Советы Статья Big Transfer (BiT): General Visual Representation Learning (Kolesnikov at el. 2019) даёт ряд важных советов, о том, на что именно стоит",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 3,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сколь угодно плохо подойти для целевой задачи. К примеру, представления, полученные на ImageNet, плохо подойдут для медицинских изображений (Raghu2019). Эксперименты На рисунке ниже показан пример того, как представления помогают решать задачу поиска. Запрос находится в самом левом столбике. Зеленым отмечены верно найденные изображения, красным — неверно найденные, синим — изображения из стоп-листа. Как видно, система вполне неплохо решает задачу поиска изображений. Подробнее про такой подход для поиска изображений можно почитать в работах (Babenko 2014, Babenko 2015). Источник Советы Статья Big Transfer (BiT): General Visual Representation Learning (Kolesnikov at el. 2019) даёт ряд важных советов, о том, на что именно стоит обратить внимание при supervised обучении c целью переноса представлений и весов модели на новые задачи. Рассмотрим их подробнее. Большие и разнообразные датасеты: Увеличение размера pretex-датасета вносит существенный вклад в качество решения downstream задачи. Авторы продемонстрировали, что при предобучении переход от 1М изображений (ImageNet) к 14М изображений (ImageNet21k) к 300M изображений (JFT), стабильно улучшает качество дообучения на новой задаче с маленьким числом размеченных примеров. Да, тут мы заходим на территорию, когда ImageNet рассматривается как маленький датасет. Большие pretext модели: Увеличение датасета при недостаточном размере модели может навредить. Нужно одновременно иметь большие модели и большие датасеты. Одно из возможных объяснений такое: с увеличением датасета модель должна предсказывать правильные ответы на трейне для огромного числа точек. При этом нельзя работать очень плохо хоть на каких-то точках, ведь когда гибкости недостаточно, моделька настраивается «так себе» во многих областях пространства, что может ухудшить финальное качество алгоритма. Долгое обучение: Большие модели требуют много шагов оптимизации. Обучение метрических эмбедингов с использование разметки (triplet loss) Мотивация: После supervised обучения расстояния между эмбеддингами не обязаны хорошо отражать треубуемую для решения нашей задачи «похожесть». Поэтому хочется, чтобы «похожесть» моделировалась известным расстоянием (к примеру евклидовым). Для этого была предложена триплетная фунция потерь или triplet loss (Schroff at el. 2015). Триплетный лосс обучается на тройках объектов (якорный объект, негативный к якорному, позитивный к якорному). Информация о позитивных и негативных объектах – это один из видов разметки. Этот лосс может использоваться как для обучения с нуля, так и для дообучения. Отметим, что объекты не обязательно должны быть из одного домена: к примеру, якорный объект может быть картинкой, а позитивные и негативые объекты текстами. Таким образом, мы сможем находить «подходящие» тексты к картинкам и наоборот. Как будет устроено обучение рассмотрим тройки объектов ( obj i , pos i , neg i ) (obj i ,pos i ,neg i ), где pos i pos i — позитивный пример к obj i obj i , neg i neg i — негативный пример к obj i obj i будем притягивать e m b ( obj i ) emb(obj i ) и e m b ( pos i ) emb(pos i ) и отталкивать e m b ( obj i ) emb(obj i ) и e m b ( neg i ) emb(neg i ) одним из популярных лоссов для решения такой задачи является triplet loss: obj pos obj neg [∥emb θ (obj i )−emb θ (pos i )∥ 2 2 −∥emb θ (obj i )−emb θ (neg",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 4,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "— позитивный пример к obj i obj i , neg i neg i — негативный пример к obj i obj i будем притягивать e m b ( obj i ) emb(obj i ) и e m b ( pos i ) emb(pos i ) и отталкивать e m b ( obj i ) emb(obj i ) и e m b ( neg i ) emb(neg i ) одним из популярных лоссов для решения такой задачи является triplet loss: obj pos obj neg [∥emb θ (obj i )−emb θ (pos i )∥ 2 2 −∥emb θ (obj i )−emb θ (neg i )∥ 2 2 +α] + . e m b emb представляется нейронной сетью emb θ (⋅) α α — параметер зазора — в некотором смысле усложняет задачу: при alpha=0 достаточно, чтобы позитивный эмбединг был ближе якорному, чем негативный; с параметром 0.5 alpha=0.5 мы начинаем требовать, чтобы позитивный был ближе, чем негативный, как минимум на 0.5 0.5. лосс L L оптимизируем по параметрам θ θ. triplet Алгоритм формирования троек Обучение с триплет лоссом сильно зависит от алгоритма формирования троек. Если формировать тройки случайно, то большинство троек будут слишком легкими, не информативными. Негативные объекты будет слишком легко отличить от позитивных, поэтому обучающего сигнала от таких троек будет мало. Поэтому хочется собрать наиболее сложные тройки из всех объектов в датасете или минибатче. Такой процесс называется hard negative/positive mining и часто используется для обучения с триплетной функцией потерь. 🧪 Примеры: Примеры Диалоговая система: obj i obj i — фраза; pos i pos i — подходящий ответ; neg i neg i — ответ не в тему. Верификация лица: obj i obj i — лицо которое хотим верифицировать; pos i pos i — тот же человек, что и в obj i obj i , но с других ракукрсов, в другом освещении, ...; neg i neg i — лица других людей. triplet Достоинства и недостатки 👍 Обучение метрических эмбедингов (metric learning), в отличие от supervised подхода, использует информацию о метрике, что позволяет выучить более релевантные представления для целевой задачи. 👎 Все еще требует разметки (на тройки объектов). 👎 Обучение с триплетным лоссом часто ведет себя нестабильно (еще нестабильнее, чем обучение нейросетей для других задач). Self-supervised обучение В этом разделе мы хотим показать, что нейронные сети и представления можно предобучать без рукотвороной разметки. Мотивация Мы разобрали supervised предобучение нейронных сетей и их использование для извлечения признаков. Однако supervised подходы не всегда эффективны. Supervised обучение требует больших размеченных датасетов. Разметка данных — это трудоёмкий и дорогой процесс, на выходе от которого всё равно получается шумная, и зачастую смещенная разметка. Поэтому от ручной разметки данных хочется уйти или хотя бы постараться её минимизировать. Этого можно добиться, если научиться использовать неразмеченные данные для предобучения. Неразмеченные данные генерируются в огромном количестве, и их значительно проще собирать. Это позволит нам обучаться на огромных коллекциях данных, размер которых был бы недостижим при необходимости сбора разметки. Также в каждом объекте, изображении, звуке или тексте содержится в разы больше информации, которую можно учитывать при обучении, чем закодировано в одном таргете. К примеру, один из тысячи",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 5,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "датасетов. Разметка данных — это трудоёмкий и дорогой процесс, на выходе от которого всё равно получается шумная, и зачастую смещенная разметка. Поэтому от ручной разметки данных хочется уйти или хотя бы постараться её минимизировать. Этого можно добиться, если научиться использовать неразмеченные данные для предобучения. Неразмеченные данные генерируются в огромном количестве, и их значительно проще собирать. Это позволит нам обучаться на огромных коллекциях данных, размер которых был бы недостижим при необходимости сбора разметки. Также в каждом объекте, изображении, звуке или тексте содержится в разы больше информации, которую можно учитывать при обучении, чем закодировано в одном таргете. К примеру, один из тысячи классов можно закодировать всего десятью битами, а изображение содержит мегабайты внутренней полезной информации, котрую можно использовать для обучения. Поэтому подходы, которые могут обучаться без разметки, но с использованием внтурненнией информации, потенциально могут выучивать более хорошие представления, чем supervised подходы. 💡Основная идея self-supervised обучения — обучение через решение синтетических supervised задач (pretext problems), источником разметки в которых является сам объект (текст, изображение, или видео). Отсюда и приставка \"self\" в названии подхода. Примеры pretext задач предсказание объекта по его компактному описанию; предсказание слова по контексту; предсказание закрытой части изображения по открытой; предсказание будущих кадров по прошлым. Если всё это кажется вам supervised-задачами, вы правы! Приставлка self- означает отсутствие внешней разметки. Признаки и веса, выученные для решения, казалось бы, бесполезных pretext задач, на практике работают как очень хороший претрейнинг для решения supervised задач (downstream problems). Это позволяет достигать отличного качества, используя в сотни раз меньше размеченных данных по сравнению с чисто supervised подходами. Осталось ответить на вопрос: какие pretext задачи использовать? Универсального ответа нет, но оказывается, что многие pretext задачи используют контекст для обучения. Подробнее об этом расскажем далее. Использование контекста для обучения Почему контекст так важен для обучения? Обучение людей, как и обучение алгоритмов, неразрывно связано с использованием контекста. При изучении иностранного языка часто прибегают к упражнениям вида «Вставте правильные слова в текст». Чтобы выполнить такое упражнение, человеку нужно учитывать контекст и предсказывать значения незнакомых слов, если это необходимо для понимания текста. context Предложенная профессором Южно-Калифорнийского университета Стивином Крашенйном «гипотезы входного материала» (input hypothesis) предполагают, что для эффективного изучения языка человеку нужно читать и слушать текст, который немного превышает его текущий уровень. Скажем, содержит 10-15% незнакомых слов, но при этом остается понятным. Такой способ обучения требует восстановления значения незнакомых слов из контекста. Визуальный контекст также широко используется при обучении детей. Вы можете помнить упражнения, в которых нужно было найти лишний предмет, закончить рисунок или раскрасить изображение. Такие задания требуют учета визуального контекста для решения задачи: важно уметь понимать принадлежность разных рисунков к одной группе, генерировать изображение, наблюдая только некоторую его часть и так далее. context Self-supervised обучение представлений и моделей глубокого обучения использует похожие идеи обучения из контекста. К примеру, модель word2vec (Mikolov et al. 2013) и BERT (Devlin et al. 2018) выучивают эмбединги слов, решая задачу предсказания слов по контексту. С философией word2vec вы уже познакомились в параграфе про нейросети для работы с последовательностями. А некоторые модели для картинок решают пазлы (Doersch, Noroozi et al. 2017), дополняют изображения или звуки (van",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 6,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "рисунок или раскрасить изображение. Такие задания требуют учета визуального контекста для решения задачи: важно уметь понимать принадлежность разных рисунков к одной группе, генерировать изображение, наблюдая только некоторую его часть и так далее. context Self-supervised обучение представлений и моделей глубокого обучения использует похожие идеи обучения из контекста. К примеру, модель word2vec (Mikolov et al. 2013) и BERT (Devlin et al. 2018) выучивают эмбединги слов, решая задачу предсказания слов по контексту. С философией word2vec вы уже познакомились в параграфе про нейросети для работы с последовательностями. А некоторые модели для картинок решают пазлы (Doersch, Noroozi et al. 2017), дополняют изображения или звуки (van den Oord et al. 2018), раскрашивают фотографии (Zhang et al. 2016) и ищут похожие объекты (Chen et al. 2020). Последнюю из них — SimCLR – мы подробно разберём ниже. Self-supervised предобучение для изображений SimCLR — метод, который первым продемонстрировал, что self-supervised предобучение может достигать того же качества что и supervised обучение. Он основан на контрастивной функции потерь (contrastive loss), и в некотором смысле решает задачу поиска похожих объектов. Также мы разберем метод self-supervised предобучения для vision tranformer, который, в некотором смысле, дорисовывает картинку, а также демонстрирует, что методы self-supervised предобучения для изображений и текстов во многом похожи. Стоит отметить, что pretext задачи, которые мы будем обсуждать ниже, не являются «серебряной пулей». Известно, что такие задачи работают как хороший претрейнинг. Другими словами, позволяют получить хорошее качество для некоторых downstream задач (классификация, детекция, сегментация) после дообучения на небольшом количестве размеченных примеров. Хорошего понимания, почему эти методы работают, в области пока нет. Скорее всего, разные типы задач (downstream problems) будут требовать разных методов претрейнинга, но это мы поймём только в ближайшие несколько лет. A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) SimCLR решает синтетическую задачу поиска похожих изображений. Вот как он работает на верхнем уровне: Для каждого изображения в минибатче генерируются две аугментации; Выбирается одно из изображений; одна из его аугментаций считается запросом, вторая — позитивным ответом, аугментации остальных объектов — негативными примерами; Цель модели — для каждого «запроса» найти позитивный пример. Выученные веса могут быть использованны для «дообучения»/«файнтюнинга» сети под финальную задачу. Иллюстрация задачи SimCLR для одного запросса из минибатча Лосс SimCLR оптимизирует контрастив лосс (contrastive loss), который фактически является кросс энтропией на positive-negative разметке: exp exp positive query =−log ∑ z∈Neg(query) exp(sim(emb(query),emb(z))) exp(sim(emb(query),emb(positive))) , где sim(⋅,⋅) — это косинусное расстояние, a лосс L L работает следующим образом: Контрастивная функция потерь positive query притягивает друг к другу эмбединги запроса e m b ( query ) emb(query) и позитивного примера e m b ( positive ) emb(positive), в то же самое время отталкивая эмбединги негативных примеров emb(z); Максимум s i m ( query , positive ) sim(query,positive) будет достигается в точке query = positive query=positive, поэтому эмбединги аугментаций одной и той же картинки будут притягиваться; Знаменатель требует, чтобы негативные эмбединги были далеко от запроса. Интуиция: На контрастивную функцию потерь можно смотреть как на поиск ответа по запросу, который ведется только среди всех эмбедингов в текущем минибатче. Такая задача требует сохранения информации про контент на изображении (что, вообще говоря, не очень просто)",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 7,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "m b ( query ) emb(query) и позитивного примера e m b ( positive ) emb(positive), в то же самое время отталкивая эмбединги негативных примеров emb(z); Максимум s i m ( query , positive ) sim(query,positive) будет достигается в точке query = positive query=positive, поэтому эмбединги аугментаций одной и той же картинки будут притягиваться; Знаменатель требует, чтобы негативные эмбединги были далеко от запроса. Интуиция: На контрастивную функцию потерь можно смотреть как на поиск ответа по запросу, который ведется только среди всех эмбедингов в текущем минибатче. Такая задача требует сохранения информации про контент на изображении (что, вообще говоря, не очень просто) и в то же время понижения размерности, так как эмбединги f ( ⋅ ) f(⋅) обычно имеют сравнительно низкую размерность. Размер минибатча: Размер минибатча влияет на количество отрицательных примеров. Чем больше отрицательных примеров — тем более сложную задачу мы ставим перед нейросетью. Существует некоторый баланс между сложностью задачи и качеством выученных представлений. Слишком простые задачи (то есть маленькие батчи) обычно не позволяют выучить хороших представлений: простая задача может хорошо решаться даже с помощью «плохих» представлений. Поэтому SimCLR обучается хорошо только на очень больших мини-батчах (с тысячами примеров). Что нам нужно иметь перед началом обучения неразмеченный датасет изображений X=x 0 ,...,x N операцию аугментации изображения aug(x j ) энкодер (⋅):Image→R M (типичные значения M~2048) проекция (⋅):R M →R K (типичные значения K~128) ✍️ В примере сверху emb(x)=g⊙f(x) Как мы обучаемся Семплируем мини-батч объектов ∼X; Для каждого объекта в минибатче Х ^ Х ^ : — Cемплируем две аугментации =aug( Х ^ i ),aug( Х ^ i ); — Вычисляем эмбединги ),f θ (v i ′ ); — Вычисляем проекции ),g ψ (v i ′ ); Вычисляем contrastive loss L=∑ , используя sim(⋅,⋅)= ∥∥u∥∥ ∥∥v∥∥ exp exp exp =−log [exp(sim(q,z j ))+exp(sim(q,z j ′ ))] exp(sim(q,p)) — В L L два слагаемых из-за того, что в паре (изображение, аугментация), вообще говоря, любой элемент можно выбрать в качестве запроса (другой тогда будет позитивным примером). Тем самым из одного мини-батча картинок мы можем сделать два мини-батча для обучения SimCLR. — Функция потерь L L вычисляется для низкоразмерных проекций Делаем шаг по градиенту θ,ψ L, повторяем с шага 1 пока не сойдёмся; Используем (⋅) для генерации эмбедингов или файнтюнинга под supervised задачу. Почему это вообще работает? Точно никто не знает, но приведем следующую гипотезу: Контрастивная функция потерь требует различать аугментации разных изображений. При этом эмбеддинги должны содержать информацию о контенте изображения, чтобы осуществлять поиск аугментаций одинаковых изображений по ключу. Этот процесс позволяет создать представления изображений, сохраняющие достаточно много информации про контент, чтобы решать не только задачу поиска аугментаций, но и другие задачи. Результаты Претрейнинг, который мы обсудили выше, позволяет эффективно дообучать модели и получать качество, сравнимое с supervised обучением, используя в 100 раз меньше размеченных примеров. simclrv2 Оговорка в том, что эти результаты получены второй весрсией метода SimCLRv2 (Chen at. el, 2020). SimCLRv2 TLDR; модели больше, глубже сеть проекции, улучшение качества происходит за счет дистиляции. Vision Transformer и BERT-like обучение Одна из самых популярных self-supervised задач в NLP — это предсказание замаскированных токенов (masked tokens",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 8,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "одинаковых изображений по ключу. Этот процесс позволяет создать представления изображений, сохраняющие достаточно много информации про контент, чтобы решать не только задачу поиска аугментаций, но и другие задачи. Результаты Претрейнинг, который мы обсудили выше, позволяет эффективно дообучать модели и получать качество, сравнимое с supervised обучением, используя в 100 раз меньше размеченных примеров. simclrv2 Оговорка в том, что эти результаты получены второй весрсией метода SimCLRv2 (Chen at. el, 2020). SimCLRv2 TLDR; модели больше, глубже сеть проекции, улучшение качества происходит за счет дистиляции. Vision Transformer и BERT-like обучение Одна из самых популярных self-supervised задач в NLP — это предсказание замаскированных токенов (masked tokens prediction Devlin at el. 2019). При обучении такая модель (обычно transformer) видит текст, в котором некоторые токены заменены на специальный токен [MASK]; задача модели — правильно предсказать замаскированные токены по контексту. Оказывается, такой претрейнинг позволяет очень хорошо адаптировать модель для решения разных задач, таких как классификация текстов, используя при этом мало размеченных примеров. Можно ли использовать такой self-supervised подход для изображений? Оказывается, что да! В этом помогает vision transformer. В последнее время модели на основе vision transformer (ViT) (Dosovitskiy at el. 2020) бурно развиваются и компьютерного зрения. vit В supervised режиме для задачи классификации vision transformer обучается следующим образом: Изображение нарезается на квадратные патчи одинакового размера; Затем патчи вытягиваются в последовательность; Каждый патч вытягивается в столбец пикселей; Каждый стобец проецируется обучаемой матрицей; К каждому вектору с шага 4 добавляются positional encoding (без позиционных эмбеддингов трансформер не учитывает позицию токена в последовательности, а positional encoding кодирует позицию токена и позволяют трансформеру учитывать эту информацию); Векторы с шага 5 подаются в трансформер; Классификационный токен на выходе предсказывает распределение на классы; Вычисляется кросс-энтропия, делается шаг по её градиенту. ViT не используют локальные операции, такие как свёртки. Как следствие, такие модели требуют заметно больше данных и параметров для обучения (300M изображений по сравнению со стандартным размером размеченного датасета 1.3М). Но оказывается, что BERT-like self-supervised обучение применимо и для моделей vision transformer, и позволяет обучать их без использовния гиганских датасетов. Какие self-supervised задачи на замаскированных патчах решают авторы статьи: Предсказание среднего цвета в замаскированном патче; Предсказание патча низкого разрешения и одновременное предсказание цвета; Предсказание патча высокого разрешения разрешение с использованием L 2 L 2 -лосса. Во всех случаях обучается и файнтюнится вся сеть целиком. Этот интересный пример показывает, что pretext-задачи придуманные, для NLP-сетей могут быть применены и к задачам зрения. Послесловие Глубинное обучение — в существенной степени наука о представлениях сложных объектов. В этом параграфе мы лишь слегка затронули несколько важных тем: supervised предобучение, self-supervised предобучение, и metric learning. Self-supervised предобучение — это важный новый раздел глубинного обучения, который, вероятно, поможет серьезно сократить количество необходимой разметки во многих приложениях. Генеративные модели VAE/inverse-GANs также широко используются для получения и обработки представлений. О них вы сможете прочитать в следующих параграфах. Почитать по теме Contrastive Representation Learning, Lilian Weng, May 2021. Self-Supervised Representation Learning, Lilian Weng, Nov 2019. Самообучение (Self-Supervision), Александр Дьяконов, Июнь 2020. Self-Supervised Learning | ICLR, Yann LeCun, May 2020. Self-Supervised Learning | UC Berkeley, CS294-158 Deep Unsupervised Learning, Aravind Srinivas, Spring 2020. Unsupervised Representation Learning",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 9,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "мы лишь слегка затронули несколько важных тем: supervised предобучение, self-supervised предобучение, и metric learning. Self-supervised предобучение — это важный новый раздел глубинного обучения, который, вероятно, поможет серьезно сократить количество необходимой разметки во многих приложениях. Генеративные модели VAE/inverse-GANs также широко используются для получения и обработки представлений. О них вы сможете прочитать в следующих параграфах. Почитать по теме Contrastive Representation Learning, Lilian Weng, May 2021. Self-Supervised Representation Learning, Lilian Weng, Nov 2019. Самообучение (Self-Supervision), Александр Дьяконов, Июнь 2020. Self-Supervised Learning | ICLR, Yann LeCun, May 2020. Self-Supervised Learning | UC Berkeley, CS294-158 Deep Unsupervised Learning, Aravind Srinivas, Spring 2020. Unsupervised Representation Learning | DeepMind x UCL. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 6.5. Нейросети для облаков точек Следующий параграф 7.2. Дистилляция знаний",
    "metadata": {
      "title": "Обучение представлений",
      "url": "https://education.yandex.ru/handbook/ml/article/obuchenie-predstavlenij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.1",
      "part": 10,
      "total_parts": 10,
      "source_file": "7.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе вы познакомитесь с продвинутой техникой машинного обучения, получившей название дистилляции знаний. Дистилляция знаний (knowledge distillation) — это способ обучения в первую очередь нейросетевых моделей машинного обучения, направленный на передачу знаний от модели-учителя к модели-ученику. Источник Слишком абстрактное определение? Соглашусь, но в последние годы дистилляция знаний как поле исследований сильно разрослась и стала включать в себя множество новых и, возможно, даже неожиданных сценариев применения. Так, авторы статьи 2020 года утверждают, что смогли добиться примерной инвариантности выходов полносвязной сети к сдвигу входа-картинки с помощью дистилляции в неё знаний из сверточной сети. Таким образом получается, что дистилляция знаний может применяться для того, чтобы передавать так называемые inductive biases от одной сети к другой. Схожие доводы встречаются и в статьях безумно популярного на момент написания данного параграфа направления трансформеров для компьютерного зрения. Так, использование дистилляции знаний оказалась важным компонентом для получения хорошего качества предсказания на ImageNet от ViT без использования дополнительных данных. Впоследствии данный подход использовался и в других трансформерах для компьютерного зрения, например, в LeViT. Тем не менее, среди всего разнообразия применений дистилляции знаний наиболее ярко выделяется одно — сжатие моделей. Сжатие моделей Задача сжатия моделей проистекает из следующего наблюдения. Неоднократно было замечено, что в широком диапазоне практически значимых задач машинного обучения точность предсказания модели существенно зависит от её размера. При этом зачастую данная зависимость выглядит достаточно тривиально: последовательное увеличение размеров модели позволяет последовательно улучшать точность её предсказаний. Однако такой безграничный рост приводит к ряду проблем, связанных с практическим применением итоговых моделей. Сюда относятся рост времени обучения больших моделей и повышенные аппетиты таких моделей к размерам и качеству обучающей выборки. Кроме того, большие модели нередко требуют более дорогостоящего вычислительного оборудования для эффективного применения, особенно если мы говорим об обработке большого количества запросов в сжатые сроки. А для некоторых сценариев, таких как предсказание в реальном времени и/или на мобильных устройствах, применение большой модели может оказаться вовсе невозможным. Эти проблемы породили каждая свою ветвь исследований. Так в последние годы де-факто стандартным способом обучения даже относительно компактных моделей стало использование mixed-precision training, которое позволяет ускорить обучение более или менее любых сетей на современных графических процессорах, при этом практически без потерь в итоговом качестве. Для борьбы с недостатком обучающих данных была предложена целая плеяда методов self-supervised pretraining, и новые появляются до сих пор. Сжатие моделей же концентрируется на решении проблем, связанных с этапом применения уже обученных моделей. Как можно догадаться из названия, задача сжатия моделей заключается в том, чтобы взять большую модель, и сжать её в как можно более компактную модель при этом по возможности минимально пожертвовав качеством предсказания. Исторически задачу сжатия моделей пытались решать множеством разных способов. Классическим примером здесь может служить прунинг, где модель обучается специальным образом (например, с использованием L2 регуляризации) так, чтобы часть весов в итоге можно было занулить и исключить из итоговой модели. Однако методы данного семейства, как правило, страдают от двух основных проблем. Во-первых, простое удаление части весов каждого из слоёв обычно показывает лишь незначительное ускорение в применении итоговой модели за исключением случаев использования специализированной аппаратуры Во-вторых, наивный прунинг нередко приводит к существенной просадке в качестве предсказания сжатой",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 1,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "её в как можно более компактную модель при этом по возможности минимально пожертвовав качеством предсказания. Исторически задачу сжатия моделей пытались решать множеством разных способов. Классическим примером здесь может служить прунинг, где модель обучается специальным образом (например, с использованием L2 регуляризации) так, чтобы часть весов в итоге можно было занулить и исключить из итоговой модели. Однако методы данного семейства, как правило, страдают от двух основных проблем. Во-первых, простое удаление части весов каждого из слоёв обычно показывает лишь незначительное ускорение в применении итоговой модели за исключением случаев использования специализированной аппаратуры Во-вторых, наивный прунинг нередко приводит к существенной просадке в качестве предсказания сжатой модели, причём соотношение степени сжатия и качества итоговой модели едва ли возможно контролировать. Чтобы обойти данные ограничения, и была предложена техника дистилляции знаний. Хинтоновская дистилляция знаний Первой статьёй, в которой можно встретить дистилляцию знаний в современном виде является статья Хинтона и др. 2014 года. В ней авторы рассматривают задачу классификации картинок и предлагают использовать предсказания большой заранее обученной модели-учителя в качестве новой, мягкой, разметки, которую будет пытаться повторить компактный ученик. Формулировка Авторы предлагают использовать дивергенцию Кульбака-Лейблера между предсказаниями учителя и ученика в качестве дополнительного слагаемого к стандартной функции потерь — кросс-энтропии между предсказанием ученика и жёсткой разметкой: log i=1 ∑ N (− j=1 ∑ K y ij logp ij +λD KL (p i ∥q i ))= log log i=1 ∑ N (− j=1 ∑ K y ij logp ij +λ j=1 ∑ K q ij log log log i=1 ∑ N ( j=1 ∑ K y ij logp ij +λ j=1 ∑ K q ij logp ij ) Здесь через L K D L KD обозначена функция потерь для дистилляции знаний. Под N N подразумевается число объектов, а под K K — классов, представленных в обучающей выборке. Через y i j y ij обозначена жёсткая разметка: если i -ый объект принадлежит j -ому классу , 0 , в противном случае если i-ый объект принадлежит j-ому классу, в противном случае. Через p i j p ij обозначены вероятности классов, предсказанные моделью-учеником, а через q i j q ij — моделью-учителем. Коэффициент λ λ позволяет настраивать баланс между решением исходной задачи и повторением предсказаний учителя. В последнем переходе учтено, что логарифм частного раскладывается в разность логарифмов, после чего один из членов можно исключить, поскольку он не зависит от оптимизируемых значений p i j p ij . В дальнейшем для упрощения выкладок под L K D L KD будет подразумеваться именно последнее выражение. Мотивация Свой выбор функции потерь авторы мотивируют следующим образом. Широко известным фактом является то, что при классификации картинок на достаточно больших и разнообразных датасетах большие нейронные сети стабильно показывают лучшие результаты по сравнению с компактными. Однако также хорошо известно, что даже сравнительно небольшие нейронные сети способны приближать очень широкий спектр функций. В таком случае можно предположить, что проблема обучения компактных сетей заключается не в том, что компактная модель не способна приблизить ту же функцию, что и большая, а в том, что компактная модель не способна самостоятельно выучить данную функцию из исходных данных. В таком случае потенциально мы можем подтолкнуть компактную",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 2,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Мотивация Свой выбор функции потерь авторы мотивируют следующим образом. Широко известным фактом является то, что при классификации картинок на достаточно больших и разнообразных датасетах большие нейронные сети стабильно показывают лучшие результаты по сравнению с компактными. Однако также хорошо известно, что даже сравнительно небольшие нейронные сети способны приближать очень широкий спектр функций. В таком случае можно предположить, что проблема обучения компактных сетей заключается не в том, что компактная модель не способна приблизить ту же функцию, что и большая, а в том, что компактная модель не способна самостоятельно выучить данную функцию из исходных данных. В таком случае потенциально мы можем подтолкнуть компактную модель к выучиванию более информативного представления путем модификации функции потерь. Как этого добиться? Давайте возьмем заведомо более информативное представление, выученное большой моделью-учителем, и добавим в функцию потерь слагаемое, которое будет учить модель-ученика повторять его. В случае решения задачи классификации KL-дивергенция является именно таким слагаемым. Есть и другой способ взглянуть на хинтоновскую дистилляцию знаний. Минимизация L K D L KD отличается от стандартного обучения, тем, что мы дополнительно минимизируем расстояние между предсказаниями ученика и учителя. От стандартной разметки такая целевая переменная отличается наличием так называемых теневых знаний (dark knowledge), которые состоят из вероятностей принадлежности объекта ко всем классам, помимо истинного. Благодаря теневым знаниям модели-ученику во время обучения доступна дополнительная информация о взаимоотношениях между представленными в обучающей выборке классами, а также схожести отдельных объектов и классов. Чтобы проверить данную гипотезу, авторы проводят следующий эксперимент. Сначала они обучают модель-учителя классифицировать картинки на датасете MNIST. После этого авторы обучают компактную модель-ученика с помощью ранее полученного учителя на тех же данных, но опуская при этом все картинки цифры 3 3. После этого авторы показывают, что, если исправить коэффициент сдвига для данного класса в последнем слое сети-ученика с помощью небольшой валидационной выборки, сеть способна верно определить 98.6 % 98.6% картинок тройки, несмотря на то, что во время обучения она не видела ни одного примера. Также косвенным подтверждением данной гипотезы можно считать и тот факт, что при использовании довольно популярной сейчас техники сглаживания разметки (label smoothing), эффективность дистилляции знаний заметно падает. Именно теневые знания на данный момент являются де-факто стандартным объяснением эффекта Хинтоновской дистилляции знаний. Использование температуры при подсчете KL-дивергенции В качестве дополнительной эвристики авторы также предлагают перед взятием KL-дивергенции сглаживать распределения учителя и ученика с помощью температуры T T, то есть вместо считать KL-дивергенцию между , где: p i T = softmax =softmax( T z i ) Здесь с помощью z i z i обозначены логиты классов, предсказанные моделью-учеником. Формула для выглядит аналогично. Зачем нужна температура? Давайте рассмотрим формулу дополнительного слагаемого функции потерь. Для простоты выкладок я ограничусь функцией потерь для единственного объекта под номером i i, а также опущу постоянный множитель λ N N λ , который также не существенен для данного повествования. log j=1 ∑ K q ij logp ij Вспомним, что коэффициенты q i j q ij приходят из предобученной модели-учителя, а значит являются константными с точки зрения процесса оптимизации. В таком случае несложно видеть, что мы имеем дело с чем-то очень близким к стандартной кросс-энтропийной функции потерь, но таргет y i j",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 3,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "классов, предсказанные моделью-учеником. Формула для выглядит аналогично. Зачем нужна температура? Давайте рассмотрим формулу дополнительного слагаемого функции потерь. Для простоты выкладок я ограничусь функцией потерь для единственного объекта под номером i i, а также опущу постоянный множитель λ N N λ , который также не существенен для данного повествования. log j=1 ∑ K q ij logp ij Вспомним, что коэффициенты q i j q ij приходят из предобученной модели-учителя, а значит являются константными с точки зрения процесса оптимизации. В таком случае несложно видеть, что мы имеем дело с чем-то очень близким к стандартной кросс-энтропийной функции потерь, но таргет y i j y ij — это уже не one-shot закодированные номера классов, а что-то более интересное. В таком случае компоненты предсказания ученика, которые отвечают классам, оценённым учителем, как наиболее вероятные, получат большие веса и сформируют каркас итоговой функции потерь. В то же время все прочие компоненты получат околонулевые коэффициенты и влияния на функцию потерь практически не окажут. В какой-то степени эффект от этого может быть позитивным. Действительно, так как для преобразования предсказания нейронной сети в распределение вероятностей мы используем softmax softmax, итоговая модель не может предсказать строго нулевую вероятность. Поэтому типичное предсказание обученной сети содержит в себе множество практически нулевых значений. При этом порядок между данными значениями определяется в первую очередь не похожестью объекта на представителей данных классов, а конкретным исходом стохастического процесса обучения данной модели. В таком случае нам вовсе не хотелось бы вынуждать ученика воспроизводить данный порядок, если ценой тому будет ухудшение точности предсказания истинного класса. С другой стороны, нейронные сети являются зачастую излишне уверенными в себе классификаторами: их предсказание часто содержит ярко выраженный максимум, вероятность которого близка к единице даже в тех случаях, когда модели стоило бы усомниться. К сожалению, для нас это значит, что при дистилляции знаний из такой модели мы рискуем попасть в ситуацию, что итоговые веса q i j q ij настолько малы для всех классов, кроме истинного, что наше дополнительное слагаемое по сути повторяет стандартную кросс энтропию и не способно внести хоть сколь-нибудь заметный вклад в обучение модели-ученика. Этот эффект можно нивелировать путем сглаживания предсказания учителя таким образом, чтобы сделать распределение q i j q ij ближе к равномерному, для чего, собственно и используется температура. В таком случае функция потерь задается следующим образом: log log i=1 ∑ N ( j=1 ∑ K y ij logp ij +λ j=1 ∑ K q ij T logp ij T ) Но в данную формулу незаметно закралась одна неприятная деталь. Давайте рассмотрим градиент второго слагаемого в скобках. Как и в прошлый раз, для простоты выкладок я ограничусь случаем единственного объекта под номером i i и опущу константный множитель log j=1 ∑ K q ij T logp ij T Здесь легко узнаётся формула кросс-энтропийной функции потерь, градиент которой по логитам считается следующим образом: Можно видеть, что при изменении температуры T T баланс между слагаемыми функции потерь (качеством решения задачи и качеством повторения предсказания учителя) нарушается. Действительно, если раньше мы настраивали его путём выбора подходящего коэффициента λ λ, то теперь мы приходим к тому, что при изменении температуры коэффициент λ λ",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 4,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "деталь. Давайте рассмотрим градиент второго слагаемого в скобках. Как и в прошлый раз, для простоты выкладок я ограничусь случаем единственного объекта под номером i i и опущу константный множитель log j=1 ∑ K q ij T logp ij T Здесь легко узнаётся формула кросс-энтропийной функции потерь, градиент которой по логитам считается следующим образом: Можно видеть, что при изменении температуры T T баланс между слагаемыми функции потерь (качеством решения задачи и качеством повторения предсказания учителя) нарушается. Действительно, если раньше мы настраивали его путём выбора подходящего коэффициента λ λ, то теперь мы приходим к тому, что при изменении температуры коэффициент λ λ необходимо также менять: иначе при взятии градиента одно слагаемое функции потерь будет разделено на T T, а другое останется неизменным. Разумным кажется ввести множитель T T в формулу для L K D L KD явным образом. Однако прежде, чем мы сделаем это, давайте ещё раз внимательно посмотрим на получившийся градиент: exp exp exp exp l=1 K exp(z il /T) exp(z ik /T) − ∑ l=1 K exp(v il /T) exp(v ik /T) ), где через v i j v ij обозначены логиты, предсказанные моделью-учителем. Давайте теперь устремим T T к бесконечности. Раскладывая экспоненты в ряд Тейлора до первого слагаемого, получаем: N+∑ l=1 K z il /T 1+z ik /T − N+∑ l=1 K v il /T 1+v ik /T ) Вспомним теперь, что результат применения преобразования softmax softmax не зависит от сдвига на константу, поэтому на выходе из нейронной сети мы можем вычитать из логитов среднее значение таким образом, чтобы l=1 K z il =∑ l=1 K v il =0. В таком случае, предыдущая формула упрощается до: Из этой формулы следует два вывода. Во-первых, можно видеть, что для соблюдения баланса второе слагаемое в L K D L KD правильнее будет домножить не на T T, а на T 2 T 2 . Во-вторых, в данной формуле можно узнать градиент квадратичной функции потерь между векторами логитов. То есть при стремлении температуры T T к бесконечности градиент второго слагаемого в L K D L KD стремится к градиенту квадрата нормы разности между логитами модели-ученика и модели-учителя. Таким образом, мы приходим к финальной версии функции потерь: log log i=1 ∑ N ( j=1 ∑ K y ij logp ij +λT 2 j=1 ∑ K q ij T logp ij T ) Описанная выше статья произвела настоящий фурор в 2014 году. Дистилляция знаний путем минимизации KL-дивергенции между предсказаниями ученика и учителя хорошо зарекомендовала себя на практике и породила целый ряд исследований, направленных на использование и усовершенствование предложенного подхода. Вместе с методом прижилось и понятие теневых знаний, и его довольно часто можно встретить в статьях, посвящённых данной тематике. Кроме того, зародилась традиция изучения дистилляции знаний на примере задачи классификации картинок. Дальше по ходу параграфа мы ещё не раз столкнёмся с тем, что авторы различных методов часто прилагают результаты экспериментов на таких датасетах, как CIFAR-10, CIFAR-100, ImageNet и так далее. Тем не менее, сети для работы с данными других модальностей тоже дистиллируют, и начнем мы с разбора статьи, которая использует предложенный метод для решения задачи языкового",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 5,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на практике и породила целый ряд исследований, направленных на использование и усовершенствование предложенного подхода. Вместе с методом прижилось и понятие теневых знаний, и его довольно часто можно встретить в статьях, посвящённых данной тематике. Кроме того, зародилась традиция изучения дистилляции знаний на примере задачи классификации картинок. Дальше по ходу параграфа мы ещё не раз столкнёмся с тем, что авторы различных методов часто прилагают результаты экспериментов на таких датасетах, как CIFAR-10, CIFAR-100, ImageNet и так далее. Тем не менее, сети для работы с данными других модальностей тоже дистиллируют, и начнем мы с разбора статьи, которая использует предложенный метод для решения задачи языкового моделирования (language modelling). DistilBERT как пример хинтоновской дистилляции Одним из наиболее выдающихся примеров применения Хинтоновской дистилляции можно считать модель DistilBERT, которая сохраняет 97% качества модели BERT (согласно бенчмарку GLUE), используя при этом на 40% меньше параметров и требуя на 60% меньше времени при применении. При этом столь выдающийся результат авторы получают, используя хинтоновский подход практически без изменений. По аналогии с тем, как это делалось для модели-учителя (в роли которого выступает BERT), авторы обучают свою модель решать задачу маскированного языкового моделирования. В дополнение к хинтоновской функции потерь использовалось ещё косинусное расстояние между итоговыми векторными представлениями токенов, полученными с помощью ученика и учителя, разворачивая представлений ученика в сторону направлений, задаваемых представлениями модели-учителя. Ещё одна интересная деталь в этой статье — способ инициализации модели-ученика. Действительно, в качестве архитектуры для своей сети, авторы решили переиспользовать архитектуру самого BERT, но с уменьшенным вдвое числом слоёв для ускорения. Авторы замечают, что большинство операций, которые используются в трансформерах, уже достаточно хорошо оптимизированы во всех популярных библиотеках, поэтому изменение размера внутренних представлений оказывает существенно меньшее влияние на итоговое время применения сети, нежели изменение количества слоёв. Поэтому в статья фокусировалась на сжатии модели именно в глубину, оставляя ширину неизменной. Поскольку веса слоёв модели-ученика имеют при таком подходе такие же размерности, что и веса слоёв модели-учителя, последние можно использовать при инициализации. Ровно так авторы и поступают, копируя веса каждого второго слоя исходной модели для инициализации DistilBERT. Может показаться, что умная инициализация не критична и наихудшим следствием использования более примитивной стратегии будет всего лишь увеличение времени, требуемого для обучения модели-ученика. Но авторы провели ablation study и выяснили, что обучение без умной инициализации приводит к потере почти 4.8 4.8 процентных пункта итоговой метрики (обученная без неё модель сохраняет лишь 92.2 % 92.2% качества модели-учителя). Для сравнения, исключение из функции потерь кросс-энтропии между предсказанием ученика и истинной разметки приводит к потере лишь 0.4 0.4 процентных пункта итоговой метрики, а исключение KL-дивергенции приводит к потере 3.8 3.8 процентных пункта. Интересно, что двумя годами позднее вышла независимая статья, авторы которой показали, что хинтоновская дистилляция — это очень сложная оптимизационная задача со множеством локальных минимумов, которые сильно усложняют поиск глобального. Поскольку статья была написана независимо другими авторами, конкретный пример DistilBERT там не изучается, однако в целом авторы приходят к выводу, что умная инициализация может быть ключевым элементом для успеха дистилляции знаний. Дополнительные источники знаний для дистилляции Несмотря на широкий успех хинтоновского подхода, дистилляция знаний им не ограничивается. Одно из наиболее очевидных направлений улучшения предложенного метода —",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 6,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "0.4 процентных пункта итоговой метрики, а исключение KL-дивергенции приводит к потере 3.8 3.8 процентных пункта. Интересно, что двумя годами позднее вышла независимая статья, авторы которой показали, что хинтоновская дистилляция — это очень сложная оптимизационная задача со множеством локальных минимумов, которые сильно усложняют поиск глобального. Поскольку статья была написана независимо другими авторами, конкретный пример DistilBERT там не изучается, однако в целом авторы приходят к выводу, что умная инициализация может быть ключевым элементом для успеха дистилляции знаний. Дополнительные источники знаний для дистилляции Несмотря на широкий успех хинтоновского подхода, дистилляция знаний им не ограничивается. Одно из наиболее очевидных направлений улучшения предложенного метода — это использование дополнительных способов передачи знаний от модели-учителя к модели-ученику. Действительно, в хинтоновской постановке единственный канал передачи знаний — это выходы с последнего слоя модели-учителя. Однако в случае нейронных сетей это отнюдь не единственный доступный нам источник информации. Например, можно использовать веса модели-учителя для умной инициализации, как при обучении DistilBERT. К сожалению, поскольку дистилляция знаний практически всегда сопряжена со сжатием модели, не всегда получается непосредственно использовать веса учителя, и в каждом отдельном случае приходится изобретать специализированные трюки. По этой причине DistilBERT — это единственная известная автору этого параграфа модель, в которой удалось добиться улучшения результатов благодаря использованию весов модели-учителя для умной инициализации. Тем не менее, в нейронных сетях можно найти и другие источники информации. Хинтоновская дистилляция использует только выходы с последнего слоя сети. Почему бы нам дополнительно не использовать выходы промежуточных слоев? И действительно, исследования показывают, что использование выходов промежуточных слоев позволяет улучшить результаты дистилляции знаний. Для получения прироста качества авторы предлагают выбрать один или несколько промежуточных слоев модели-учителя, сопоставить каждому из них промежуточный слой модели-ученика, после чего использовать квадрат нормы разности выходов итоговых пар слоев в качестве дополнительного слагаемого к хинтоновской функции потерь. К сожалению, несмотря на кажущуюся прямолинейность данного подхода, здесь возникают две сложности. Сложность №1 Мы явным образом предполагаем наличие заранее выбранных пар слоёв, оставляя за бортом вопрос о том, каким образом их собственно стоит выбирать. Поскольку дополнительные слагаемые функции потерь по сути обучают модель-ученика повторять промежуточные представления модели-учителя, разумным кажется сохранять порядок слоёв: слои из середины модели-учителя сопоставлять со слоями из середины модели-ученика, а слои, находящиеся ближе к концу модели-учителя, — со слоями, находящимися ближе к концу модели-ученика. В частности, авторы оригинальной статьи просто берут средний слой в каждой из моделей и используют их в качестве своей единственной пары, однако это в большей степени связано с тем, что статья была написана в 2014 году и рассматривала достаточно маленькие по современным меркам модели. Более свежие статьи, как правило, работают с более глубокими сетями, а потому используют большее количество пар слоёв. Так, авторы следующей работы рассматривают глубокие сверточные сети с промежуточными связями (residual connections) и предлагают разбивать каждую из моделей на группы блоков с промежуточной связью таким образом, чтобы итоговое количество групп совпало. Пример такой разбивки можно видеть на картинке ниже. Здесь к каждой группе относится по три блока в модели-учителе и по два блока в модели-ученике. После этого выходы каждой такой группы можно сопоставить друг другу и использовать для дистилляции знаний. Источник После того, как пары слоёв",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 7,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "достаточно маленькие по современным меркам модели. Более свежие статьи, как правило, работают с более глубокими сетями, а потому используют большее количество пар слоёв. Так, авторы следующей работы рассматривают глубокие сверточные сети с промежуточными связями (residual connections) и предлагают разбивать каждую из моделей на группы блоков с промежуточной связью таким образом, чтобы итоговое количество групп совпало. Пример такой разбивки можно видеть на картинке ниже. Здесь к каждой группе относится по три блока в модели-учителе и по два блока в модели-ученике. После этого выходы каждой такой группы можно сопоставить друг другу и использовать для дистилляции знаний. Источник После того, как пары слоёв были выбраны, перед нами может возникнуть и второе препятствие. Сложность №2 Что, если выходы выбранных слоёв различаются по размерам? Такая ситуация запросто может случиться, ведь мы хотим, чтобы модель-ученик была поменьше, а один из способов сжатия — как раз уменьшение количества нейронов в полносвязных слоях. В таком случае авторы оригинальной статьи предлагают использовать дополнительное преобразование, чтобы придать выходам модели-ученика нужные размеры (например, линейный слой). Такие слои обучаются совместно с моделью-учеником, а после исключаются из сети при применении. В более поздних работах встречаются и другие, более продвинутые преобразования. Несмотря на кажущуюся интуитивность дистилляции промежуточных выходов, практическое применение это метода, к сожалению, осложняется необходимостью выбора целого ряда гиперпараметров. Скажем, оптимальные тактики выбора пар слоёв для дистилляции или дополнительных преобразований для выравнивания размерностей выходов до сих пор являются предметами активных исследований, точно так же, как и функции потерь для оптимизации. Иерархия методов дистилляции знаний Выше мы рассмотрели два подхода к дистилляции знаний: хинтоновскую дистилляцию и дистилляцию промежуточных представлений. Как мы уже упоминали ранее, в последние годы область применения дистилляции знаний сильно разрослась, и новые методы появляются день ото дня. Это породило довольно естественное желание систематизировать предложенные методы в некоторую иерархию. Мы рассмотрим две классификации методов: по режиму дистилляции, по области применения. Режимы дистилляции знаний Различные подходы к дистилляции знаний принято делить по так называемым режимам. Выделяют три основных режима дистилляции знаний: offline-, online- и самодистилляция. Offline-дистилляция знаний Все рассмотренные выше статьи так или иначе следуют некоторой общей схеме: в качестве учителя используется большая заранее обученная модель, знания из которой дистиллируются в ученика, в то время как сам учитель остается неизменным. Дистилляция в таком режиме получила название offline-дистилляции знаний. Но что делать, если большой предобученный учитель для вашей задачи не доступен? Что если модель-учитель не помещается на доступную нам видеокарту, из-за чего обучение или вовсе невозможно, или требует в десятки раз больше времени, по сравнению с обучением желаемой модели-ученика? Что, если набор данных, описывающий вашу задачу, невелик, и большая модель может переобучиться на нём, делая дистилляцию знаний как минимум неэффективной, а возможно и вредной для итогового качества ученика? Тут на помощь приходит online-дистилляция знаний. Online дистилляция знаний В качестве альтернативы авторы этой статьи предлагают брать в качестве учителя модель такой же архитектуры, что и ученик, и обучать обе модели одновременно. То есть вместо одной модели мы случайно инициализируем две, после чего на каждом шаге обучения для каждой из моделей мы минимизируем L K D L KD , где в качестве учителя выступает другая модель.",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 8,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сравнению с обучением желаемой модели-ученика? Что, если набор данных, описывающий вашу задачу, невелик, и большая модель может переобучиться на нём, делая дистилляцию знаний как минимум неэффективной, а возможно и вредной для итогового качества ученика? Тут на помощь приходит online-дистилляция знаний. Online дистилляция знаний В качестве альтернативы авторы этой статьи предлагают брать в качестве учителя модель такой же архитектуры, что и ученик, и обучать обе модели одновременно. То есть вместо одной модели мы случайно инициализируем две, после чего на каждом шаге обучения для каждой из моделей мы минимизируем L K D L KD , где в качестве учителя выступает другая модель. В таком случае в начале обучения градиент дистилляционного члена не будет иметь какого-то чёткого направления, а обучение обеих моделей будет происходить преимущественно за счет минимизации обычной функции потерь. На поздних же этапах обучения в дело включится и KL-дивергенция, что позволит дополнительно повысить качество каждой из моделей. Источник Почему данный подход работает? Широко известно, что в ряде задач ансамблирование нескольких одинаковых нейронных сетей, одинаково обученных на одних и тех же данных, но из разных случайных инициализаций, дает прирост в итоговой метрике. Этот факт подталкивает нас к выводу о том, что в зависимости от инициализации одна и та же нейронная сеть вычленяет из данных разные закономерности. Опираясь на данный вывод, авторы вышеупомянутой статьи утверждают, что в предложенной постановке каждая из моделей в процессе обучения может воспользоваться информацией, которая иначе была бы доступна только модели, стартовавшей из другой инициализации. Авторы проводят ряд экспериментов с моделями разных размеров, обучая их на датасетах CIFAR-100 и Market-1501, и показывают, что использование даже одной дополнительной модели позволяет добиться заметного улучшения в качестве предсказаний обучаемой модели. Так на датасете CIFAR-100 совместное обучение ансамбля из двух моделей практически во всех экспериментах дает прирост в 1 − − 2 1−−2 процентных пункта к итоговой точности предсказания, причем метод позволяет достигнуть положительного эффекта даже для самой большой из рассмотренных моделей при её совместном обучении с самой малой моделью. Кроме того, авторы проводят ряд экспериментов, в которых сравнивают offline-дистилляцию большей модели в меньшую с их совместным обучением и показывают, что предложенный метод позволяет добиться лучших результатов. Online-постановка естественным образом обобщается на случай большего числа моделей в обучаемом ансамбле. В таком случае в качестве дистилляционного слагаемого авторы предлагают минимизировать среднее значение KL-дивергенций от текущей модели до предсказаний каждой из других моделей в ансамбле, поскольку минимизация KL-дивергенции до усредненных вероятностей приводит к худшему результату. При этом авторы в своих экспериментах показывают, что увеличение числа моделей в ансамбле приводит к улучшению результатов обучения. Кроме того авторы отмечают, что для ускорения обучения можно достаточно эффективно использовать несколько видеокарт, поскольку на каждом шаге между устройствами передавать необходимо только результаты предсказания. Самодистилляция В качестве отдельного режима дистилляции знаний принято выделять также самодистилляцию (self distillation), при которой учитель и ученик являются одной и той же моделью. Самодистилляция включает в себя две основные группы методов. Первая группа методов направлена на использование информации, которая накапливается в модели во время обучения, для дополнительного улучшения качества предсказаний той же самой модели. Методы данной группы являются как бы продолжением идей online дистилляции знаний, поскольку",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 9,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "моделей в ансамбле приводит к улучшению результатов обучения. Кроме того авторы отмечают, что для ускорения обучения можно достаточно эффективно использовать несколько видеокарт, поскольку на каждом шаге между устройствами передавать необходимо только результаты предсказания. Самодистилляция В качестве отдельного режима дистилляции знаний принято выделять также самодистилляцию (self distillation), при которой учитель и ученик являются одной и той же моделью. Самодистилляция включает в себя две основные группы методов. Первая группа методов направлена на использование информации, которая накапливается в модели во время обучения, для дополнительного улучшения качества предсказаний той же самой модели. Методы данной группы являются как бы продолжением идей online дистилляции знаний, поскольку учитель и ученик обучаются одновременно. Хороший пример метода из данной группы можно найти в этой статье, где авторы пытаются заставить представления менее глубоких слоёв быть эквивалентными представлениям более глубоких слоёв. А именно, авторы предлагают разделить сеть на несколько частей ( 4 4 в статье) и после каждой такой части добавить небольшую предсказательную голову. Все такие головы обучаются путем минимизации суммы трёх слагаемых: кросс-энтропии с истинной разметкой; KL-дивергенции с предсказаниями полной сети; квадратичной функции потерь между промежуточными представлениями данной головы и выходом последней части сети. Источник Таким образом авторы добиваются от ResNet50 81.04 % 81.04% точности предсказания на тестовой выборке CIFAR-100 с минимальным замедлением обучения. Для сравнения, стандартное обучение такой же сети позволяет добиться лишь 77.68 % 77.68% точности предсказания, а дистилляция из ResNet152 (которая, в свою очередь, показывает точность предсказания в 79.21 % 79.21%) позволяет улучшить данный показатель лишь до 79.31 % 79.31%. При этом обучение в предложенном режиме занимает 5.87 5.87 часов (обычное обучение занимает 4.03 4.03 часа), а дистилляция из ResNet152 занимает уже 12.31 12.31 часов без учета обучения модели учителя (что требует дополнительных 14.67 14.67 часов). Вторая группа методов по сути заключается в offline дистилляции из обученной модели в новую модель такой же архитектуры. То есть мы выбираем некоторую архитектуру нейронной сети, обучаем одну модель стандартным образом, а затем обучаем точно такую же модель из новой случайной инициализации с использованием хинтоновской дистилляции из ранее обученной модели. Стоит заметить, что с хинтоновской точки зрения данное действие едва ли способно улучшить итоговое качество модели. Действительно, будучи точно такой же моделью, ученик обладает идентичной способностью к обучению, а значит учитель едва ли может предоставить ему какую-либо дополнительную информацию во время обучения. Поэтому такая самодистилляция изначально была предложена как метод изучения процесса хинтоновской дистилляции знаний, поскольку в такой постановке у задачи минимизации KL-дивергенции гарантированно есть глобальный минимум, причем мы даже знаем точку, в которой он достигается. В частности именно с помощью данного метода авторы ранее упомянутой статьи демонстрируют, что хинтоновская дистилляция знаний является сложной оптимизационной задачей. Тем удивительнее, что авторы статьи 2018 года обнаружили, что самодистилляция в предложенной выше постановке позволяет получить прирост в обобщающей способности итоговой модели. Так, они проводят ряд экспериментов с моделями DenseNet и Wide-ResNet на датасете CIFAR-100 и показывают, например, что самодистилляция DenseNet-112-33 позволяет повысить точность предсказания на тестовой выборке с 81.75 % 81.75% до 83.05 % 83.05%. Вопрос об источнике прироста качества в данном случае до сих пор в значительной степени открыт. Авторы статьи приписывают данный",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 10,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "точку, в которой он достигается. В частности именно с помощью данного метода авторы ранее упомянутой статьи демонстрируют, что хинтоновская дистилляция знаний является сложной оптимизационной задачей. Тем удивительнее, что авторы статьи 2018 года обнаружили, что самодистилляция в предложенной выше постановке позволяет получить прирост в обобщающей способности итоговой модели. Так, они проводят ряд экспериментов с моделями DenseNet и Wide-ResNet на датасете CIFAR-100 и показывают, например, что самодистилляция DenseNet-112-33 позволяет повысить точность предсказания на тестовой выборке с 81.75 % 81.75% до 83.05 % 83.05%. Вопрос об источнике прироста качества в данном случае до сих пор в значительной степени открыт. Авторы статьи приписывают данный эффект комбинации умного сглаживания разметки и внесения в обучение информации о взаимоотношении классов в датасете. Но на наш взгляд эксперименты, которые предъявляют в статье в качестве доказательства этих гипотез, едва ли можно назвать убедительными. Возможно, здесь в очередной раз проявляется то, что одинаковые модели могут вычленять из одних и тех же данных разные закономерности в зависимости от случайной инициализации, и дистилляция одной такой модели в другую позволяет ученику увидеть ранее недоступные ему связи. Также хочется обратить внимание на интересную статью, вышедшую в 2020 году. В ней показывается, что в случае обучения с L2-регуляризацией предложенная выше самодистилляция производит неявный отбор признаков. Ну и раз мы проходили мимо самодистилляции, здесь никак нельзя не упомянуть статью 2019 года, которая в течении практически года держала почетный статус SOTA на датасете ImageNet. Её авторы предлагают подход, который во многом очень близок к описанному выше. Они обучают модель на исходном наборе данных, после чего используют её для разметки новых данных, взятых в данном случае из стороннего обширного набора данных JFT-300M (закрытый набор данных, который нередко упоминается в статьях от Google). После этого авторы отбрасывают картинки, для которых модель дает неуверенные предсказания, чтобы избежать данных out-of-domain. Кроме того, они выравнивают размеры классов, чтобы избежать связанных с этим спецэффектов (согласно авторам статьи, модели меньшего размера показали себя более чувствительными к данной оптимизации). Таким образом, авторы получают большое количество дополнительных шумно размеченных данных, на которых, совместно с основным набором, они обучают новую модель. Эту модель, в свою очередь, можно использовать для получения новой разметки для дополнительных данных, с помощью которых обучается следующая модель, и такие итерации можно продолжать произвольное количество раз. В качестве разметки авторы предлагают использовать мягкую разметку, задаваемую моделью-учителем, но и бинаризованная разметка показывает схожие результаты на данных in-domain. Ключевая деталь здесь — что предсказание на новых данных производится без аугментаций, в то время как ученик учится воспроизводить разметку уже с высоким уровнем аугментации данных, а также с применением других техник регуляризации, таких как dropout и stochastic depth. Авторы утверждают, что ученик обучается лучше переносить свои знания на новые данные. Предложенный метод позволил авторам добиться от модели EfficientNet-L2 точности предсказания в 88.4 % 88.4% на тестовом наборе данных ImageNet, существенно улучшив результат исходной модели в 85.5 % 85.5% и обновив мировой рекорд. Области применения дистилляции знаний Сжатие генеративных состязательных сетей Подавляющее большинство рассмотренных выше статей так или иначе ограничиваются задачей классификации картинок. Такой выбор, хоть и не является случайным, всё же несёт больше исторический, нежели практический характер.",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 11,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "воспроизводить разметку уже с высоким уровнем аугментации данных, а также с применением других техник регуляризации, таких как dropout и stochastic depth. Авторы утверждают, что ученик обучается лучше переносить свои знания на новые данные. Предложенный метод позволил авторам добиться от модели EfficientNet-L2 точности предсказания в 88.4 % 88.4% на тестовом наборе данных ImageNet, существенно улучшив результат исходной модели в 85.5 % 85.5% и обновив мировой рекорд. Области применения дистилляции знаний Сжатие генеративных состязательных сетей Подавляющее большинство рассмотренных выше статей так или иначе ограничиваются задачей классификации картинок. Такой выбор, хоть и не является случайным, всё же несёт больше исторический, нежели практический характер. На самом деле, многие предложенные выше методы достаточно тривиально могут быть обобщены и на другие задачи машинного обучения. Например, метод дистилляции промежуточных представлений по сути вовсе никак не зависит от природы итоговых выходов модели, а потому может использоваться при сжатии практически любой модели. В частности данный метод может быть использован для сжатия генеративных состязательных сетей. Так авторы довольно популярной статьи в данной области демонстрируют 9 9-, 10 10- и даже 29 29-кратное ускорение для ряда популярных pix2pix генеративных сетей, при этом не теряя в качестве генерации. Как уже упоминалось ранее, авторы используют дистилляцию промежуточных представлений: модель-ученик учится минимизировать L2-расстояние между своим промежуточным представлением и промежуточным представлением модели-учителя. Но, так как данные представления имеют различное количество каналов (ученик выучивает более сжатое представление) авторы используют дополнительную свертку 1х1 над представлением ученика для сопоставления тензоров друг с другом. Помимо дистилляции промежуточных представлений, авторы также пользуются наличием модели-учителя для того, чтобы получить парную картинку в случае обучения на неспаренных данных (как это происходит, например, в CycleGAN). Парная картинка используется для минимизации L1 нормы разности с предсказанием модели. Кроме того, авторы во время обучения минимизируют и стандартную для генеративных состязательных сетей функцию потерь, при этом для модели-ученика используется такой же дискриминатор, что и для модели-учителя, что позволяет авторам инициализировать веса с помощью весов оригинального дискриминатора. Таким образом авторы предлагают следующий рецепт для сжатия генеративных состязательных сетей. Сначала необходимо обучить модель-учителя. После этого нужно сконструировать сжатый генератор-ученика, скопировать (вместе с весами) дискриминатор и обучить получившуюся систему с помощью минимизации взвешенной суммы трёх функций потерь: Стандартной функции потерь генеративных состязательных сетей. L1-расстояния между предсказанием генератора и парной картинки. При этом если в данных парной картинки нет, вместо неё используется результат генерации моделью-учителем. L2-расстояния между промежуточными представлениями двух генераторов. Этот метод хорошо себя зарекомендовал на практике и получил широкое распространение в своей нише. Дистилляция знаний при квантизации моделей Ещё одно интересное применение дистилляции знаний — улучшение результатов квантизации моделей. Техника квантизации нейронных сетей заключается в том, чтобы перевести часть весов или даже всю сеть из полной точности (как правило, float32) во float8 или даже float4. Помимо очевидной экономии памяти, такое представление нередко позволяет использовать специальные ядра современных графических ускорителей или специальные регистры процессоров для достижения существенного ускорения при применении квантизованных моделей. К сожалению, бесплатный сыр бывает только в мышеловке. Сжатое представление на то и называется сжатым, что является менее богатым, нежели полная точность. Поэтому большинство весов сети приходится изменять при сжатии, чтобы они попали на более грубую сетку.",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 12,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Ещё одно интересное применение дистилляции знаний — улучшение результатов квантизации моделей. Техника квантизации нейронных сетей заключается в том, чтобы перевести часть весов или даже всю сеть из полной точности (как правило, float32) во float8 или даже float4. Помимо очевидной экономии памяти, такое представление нередко позволяет использовать специальные ядра современных графических ускорителей или специальные регистры процессоров для достижения существенного ускорения при применении квантизованных моделей. К сожалению, бесплатный сыр бывает только в мышеловке. Сжатое представление на то и называется сжатым, что является менее богатым, нежели полная точность. Поэтому большинство весов сети приходится изменять при сжатии, чтобы они попали на более грубую сетку. Разумеется изменения каждого отдельного веса может показаться незначительным, однако когда все веса сети незначительно изменяются, итоговый результат подсчетов может оказаться вовсе неузнаваемым. Чтобы смягчить данный эффект, модель принято доучивать после квантизации. И вот здесь на помощь приходит дистилляция знаний: например, из сети полной точности в квантизованного ученика. Ровно к такой схеме приходят авторы этой статьи. Итоговая схема выглядит следующим образом: мы обучаем сеть в полной точности, квантизуем ее веса и доучиваем ее в квантизованном виде с использованием дистилляции знаний из сети в полной точности. Дистилляция знаний за пределами сжатия моделей Хочется отметить, что сжатие моделей — это хоть и основное, но всё же не единственное применение дистилляции знаний. Так, раньше в этом параграфе уже упоминалась самодистилляция, которая позволяет получать прирост в обобщающей способности обучаемой модели без использования моделей большего размера. Самодистилляцией, однако, примеры применения дистилляции знаний без сжатия модели не ограничиваются. Так, в 2020 году был предложен метод BYOL-предобучения без учителя, основанный на дистилляции знаний. Метод BYOL направлен на предобучение моделей компьютерного зрения и основан на идее так называемого контрастивного предобучения (contrastive pretraining). Суть методов данного семейства заключается в том, чтобы обучать модель выдавать схожие представления для различных аугментаций одной и той же картинки. Действительно, случайные патчи, вырезанные из фотографии автомобиля, скорее всего также являются фотографиями автомобиля. При этом, если в наших данных присутствует достаточное количество фотографий различных автомобилей, мы можем надеяться на то, что модель выучит некоторое общее понимание концепта автомобиля даже несмотря на то, что мы можем вовсе не знать, на каких конкретно картинках автомобили присутствовали, а на каких - нет. Однако если мы хотим добиться от такой модели осмысленных представлений сначала нам необходимо преодолеть проблему коллапса представлений. Действительно, у предложенной выше задачи есть тривиальное решение, в котором выход обучаемой сети не зависит от ее входа. В таком случае представления для произвольных аугментаций любой картинки будут совпадать, то есть функция потерь окажется нулевой. Тем не менее сами представления при этом окажутся совершенно бесполезными. Поэтому различные методы контрастивного обучения отличаются в первую очередь как раз способами борьбы с проблемой коллапса представлений. Так, авторы метода BYOL часто сравнивают свои результаты с довольно свежим на момент написания статьи методом SimCLR, в котором предлагается обучать модель одновременно минимизируя расстояния между парами представлений для различных аугментаций одной картинки и максимизируя расстояния между представлениями для различных картинок. При этом, для повышения эффективности такого обучения, во время генерации батча данных авторы сначала выбирают некоторое количество картинок из набора данных, затем для каждой картинки производят две",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 13,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "есть функция потерь окажется нулевой. Тем не менее сами представления при этом окажутся совершенно бесполезными. Поэтому различные методы контрастивного обучения отличаются в первую очередь как раз способами борьбы с проблемой коллапса представлений. Так, авторы метода BYOL часто сравнивают свои результаты с довольно свежим на момент написания статьи методом SimCLR, в котором предлагается обучать модель одновременно минимизируя расстояния между парами представлений для различных аугментаций одной картинки и максимизируя расстояния между представлениями для различных картинок. При этом, для повышения эффективности такого обучения, во время генерации батча данных авторы сначала выбирают некоторое количество картинок из набора данных, затем для каждой картинки производят две различные случайные аугментации, после чего полученные картинки используются для создания одной позитивной пары, расстояние между представлениями которой будет минимизироваться, а также для создания множества негативных пар с аугментациями других картинок в батче, расстояния между представлениями которых будут максимизироваться. Авторы BYOL подвергают данный подход критике, показывая, что для эффективного обучения SimCLR требует большого размера батча, а также довольно агрессивных аугментаций. В противном же случае качество обучаемых представлений заметно падает. Авторы BYOL объясняют данный эффект тем, что сам подход использования негативных пар является субоптимальным, поскольку требует аккуратного выбора негативных примеров. Поэтому свой метод авторы конструируют таким образом, чтобы модель обучалась только на позитивных парах картинок. В таком случае каким образом авторам удается решить проблему коллапса представлений? Для этого, вместо минимизации расстояния между представлениями обучаемой сети для двух аугментаций одной картинки, авторы обучают свою модель минимизировать расстояние между представлением обучаемой (online) сети для одной аугментации и представлением для второй аугментации, которое задается уже другой, целевой (target) сетью. То есть в некотором смысле здесь происходит дистилляция знаний из целевой сети в обучаемую. Последней важной деталью является природа целевой сети. Авторы BYOL замечают, что даже использование произвольно инициализированной сети в качестве целевой для предложенного выше метода обучения приводит к выучиванию обучаемой моделью осмысленных представлений. Подробнее, линейный классификатор, обученный на основе выученных таким образом представлений картинок из набора данных ImageNet показывает 18.8 % 18.8% тестовой точности предсказания, в то время как использование представлений задаваемых самой целевой сетью позволяет добиться лишь 1.4 % 1.4% точности. Мотивированные данным наблюдением, авторы предлагают в качестве целевой использовать такую же сеть, что и обучаемая. При этом: градиенты не текут через целевую модель, и она не обновляется на шаге градиентного спуска; обучаемая модель заканчивается дополнительным двухслойным перцептроном, который используется для преобразования её финальных представлений в представления целевой модели; веса целевой модели не меняются на шаге градиентного спуска, а вместо этого они обновляются между шагами с помощью экспоненциального сглаживания весов обучаемой модели: ξ↦τξ+(1−τ)θ, где, следуя обозначениям из статьи, мы обозначили через ξ ξ и τ τ веса целевой и обучаемой моделей соответственно, а τ τ — вещественный параметр. Источник Предложенный метод позволяет авторам добиться уже 79.6 % 79.6% тестовой точности от линейного классификатора на наборе данных ImageNet, заметно превосходя предложенные ранее методы self-supervised предобучения, и практически преодолевая разрыв между self-supervised и supervised обучением классификаторов на основе ResNet. Стоить заметить, что сценарии применения дистилляции знаний отнюдь не ограничиваются выше рассмотренными. На данный момент уже существует множество различных подходов и алгоритмов, так или иначе связанных",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 14,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "между шагами с помощью экспоненциального сглаживания весов обучаемой модели: ξ↦τξ+(1−τ)θ, где, следуя обозначениям из статьи, мы обозначили через ξ ξ и τ τ веса целевой и обучаемой моделей соответственно, а τ τ — вещественный параметр. Источник Предложенный метод позволяет авторам добиться уже 79.6 % 79.6% тестовой точности от линейного классификатора на наборе данных ImageNet, заметно превосходя предложенные ранее методы self-supervised предобучения, и практически преодолевая разрыв между self-supervised и supervised обучением классификаторов на основе ResNet. Стоить заметить, что сценарии применения дистилляции знаний отнюдь не ограничиваются выше рассмотренными. На данный момент уже существует множество различных подходов и алгоритмов, так или иначе связанных с дистилляцией знаний, и их количество растет день ото дня. Данный параграф не ставит своей целью полный обзор таких методов. Вместо этого всем заинтересовавшимся я рекомендую обратить внимание на довольно исчерпывающий обзор от 2020 года. Здесь можно найти множество ссылок на актуальные к тому моменту статьи, в числе которых присутствует и большинство статей, упомянутых в этом параграфе. Открытые проблемы Завершим параграф упоминанием открытых проблем в области дистилляции знаний. Действительно, несмотря на впечатляющие результаты, дистилляция знаний всё же не является идеальным методом, и ряд вопросов до сих пор остаются без ответа. Например, с ростом популярности дистилляции знаний выяснилось, что использование учителя с большей обобщающей способностью не всегда приводит к улучшению обобщающей способности ученика. В какой-то степени данный эффект можно списать на то, что компактная модель-ученик упирается в пределы своего качества предсказания, и тогда использование более умного учителя уже не приносит дополнительной пользы. Но это не объясняет, почему использование более точной модели в качестве учителя может приводить даже к ухудшению итоговой точности модели-ученика. В чём причина данного эффекта и как выбрать оптимального учителя для фиксированного ученика, до сих пор открытый вопрос. И как уже упоминалось ранее, дистилляция знаний из одной сети в точно такую же нередко приводит к росту обобщающей способности ученика по сравнению со своим учителем. С точки зрения хинтоновской теории, которая является де-факто стандартным способом объяснения дистилляции знаний, это звучит абсурдно. Модель-ученик гарантированно способна приблизить ту же функцию, что и модель-учитель. Тем не менее, этого не происходит, а модель-ученик выучивает свое собственное представление, которое нередко качественно превосходит представление учителя. Данный факт уже сложно объяснить в парадигме передачи знаний от учителя к ученику, потому что здесь ученик оказывается в состоянии получить больше знаний, нежели учитель способен передать. Несмотря на то, что на данную тему написана уже не одна статья, исчерпывающего объяснения пока нет. Так или иначе, дистилляция знаний неоспоримо работает и является основным практическим подходом к сжатию нейросетевых моделей на данный момент. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 7.1. Обучение представлений Следующий параграф 8.1. Введение в генеративное моделирование",
    "metadata": {
      "title": "Дистилляция знаний",
      "url": "https://education.yandex.ru/handbook/ml/article/distillyaciya-znanij",
      "course": "ml",
      "chapter": "7. Глубинное обучение - практика",
      "chapter_id": "7.2",
      "part": 15,
      "total_parts": 15,
      "source_file": "7.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "До этого вы изучали модели машинного обучения, которые в основном предсказывают какие-то характеристики объектов.Например, метки класса или регрессионные метки. Подобные задачи называют дискриминативным моделированием. В то же время, существуют обратные задачи, в которых по какой-то характеристике нужно создать объект или оценить плотность распределения объектов. Это называется генеративным моделированием — его нюансы мы и рассмотрим в этом разделе. Обучение генеративных моделей существенно сложнее обучения дискриминативных моделей. Последние работают с намного более простыми распределениями. Например, предсказать вероятность конкретной цифры, нарисованной на картинке, гораздо проще, чем создать картинку с нужной цифрой. При этом генеративные модели в последние годы достигли невероятных успехов и позволяют генерировать изображения, которые трудно отличить от настоящих фотографий. generative Генеративные модели помогают решать множество задач, которые мы рассмотрим далее. Самая основная задача — это приближение распределения данных и генерация новых данных. Допустим у нас есть набор картинок с нарисованными от руки числами. Будем считать, что мы получили этот набор из генеральной совокупности (то есть из всех возможных изображений). Нам бы хотелось так или иначе смоделировать распределение этой генеральной совокупности. Мы можем это сделать двумя подходами: Явное моделирование. В этом случае мы построим и как-то оценим функцию плотности распределения данных p ( x ) p(x). Из этого распределения мы сможем семплировать новые объекты. Примеры таких моделей: авторегрессионные модели (например, PixelCNN++, Video Transformer), диффузионные модели, модели на основе нормализующих потоков и вариационные автокодировщики. Неявное моделирование. При неявном моделировании мы доступ к функции плотности не получим. Но мы сможем из этого распределения сэмплировать новые объекты. В случае нашего примера с нарисованными числами мы сможем генерировать такие изображения. Примерами таких моделей являются генеративно-состязательные сети. Рассмотрим дискриминативные и генеративные задачи чуть более формально. При дискриминативном моделировании для объекта x x и характеристики y y мы обычно хотим получить плотность распределения p(y∣x). При генеративном моделировании ставится противоположная задача: восстановить плотность p ( x ) p(x) или p(x∣y). В качестве y y тут может выступать как метка класса, так и другой объект. Например, если мы хотим уметь генерировать изображения на основе текстового описания, то изображения будут являться x x, а текст — y y. Интерполяции в латентном пространстве Большинство моделей генеративного моделирования позволяют семплировать новые объекты. Как правило, в результате обучения генеративной модели мы получаем генератор — функцию, которая на выходе выдаёт объект. В таких моделях, как генеративные состязательные нейронные сети, диффузионные модели, вариационные автокодировщики, генератор на вход принимает вектор случайных значений из простого вероятностного распределения (например, нормального или равномерного). Получается, что x=G(z), где x x — объект, G G — функция генератора, а z z — вектор случайных значений. Пространство, в котором располагается z z, называется латентным. Обычно распределение z z задаётся ещё до обучения модели и не меняется в процессе. Поскольку мы знаем распределение, мы можем семплировать из него сколько угодно разных z z. Рассмотрим два вектора из латентного пространства и два соответствующих им сгенерированных объекта =G(z =G(z 2 ). Так как — это две точки в латентном пространстве, между ними можно провести линию. Точки, лежащие на этой линии, будут так же принадлежать этому пространству. Если двигаться по этой линии и использовать точки с неё",
    "metadata": {
      "title": "Введение в генеративное моделирование",
      "url": "https://education.yandex.ru/handbook/ml/article/vvedenie-v-generativnoe-modelirovanie",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.1",
      "part": 1,
      "total_parts": 3,
      "source_file": "8.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "объект, G G — функция генератора, а z z — вектор случайных значений. Пространство, в котором располагается z z, называется латентным. Обычно распределение z z задаётся ещё до обучения модели и не меняется в процессе. Поскольку мы знаем распределение, мы можем семплировать из него сколько угодно разных z z. Рассмотрим два вектора из латентного пространства и два соответствующих им сгенерированных объекта =G(z =G(z 2 ). Так как — это две точки в латентном пространстве, между ними можно провести линию. Точки, лежащие на этой линии, будут так же принадлежать этому пространству. Если двигаться по этой линии и использовать точки с неё в качестве входа для генератора, то можно получить плавно изменяющийся сгенерированный объект. Пример изображений, полученных с помощью интерполяции в латентном пространстве. Источник В примере выше мы рассмотрели движение вдоль линии, однако на практике интерполяция может быть по более сложной траектории. Манипуляции с латентным пространством позволяют не только создавать плавные переходы между объектами, но так же редактировать объекты. Обычно в таких случаях требуется найти направления в латентном пространстве, которое отвечает за нужное свойство сгенерированных объектов. Например, направление, отвечающее за цвет волос или улыбку человека. Подробнее такие методы мы рассмотрим в параграфах про конкретные модели. Применения генеративных моделей Зачем может понадобиться генерировать новые данные или восстанавливать их плотность? Самый простой пример – это аугментация набора данных, которая мешает переобучению и улучшает обобщаемость модели. Простые аугментации данных (случайные сдвиги, повороты, масштабирование, изменения цвета и контраста) активно используются почти во всех методах машинного обучения. Генеративные же модели представляют собой более сложный вид аугментации данных, который способен существенно расширить датасет, или обогатить его совершенно новыми элементами. Например, генеративную модель, которая переносит стиль одного изображения на другое (style transfer), можно использовать для обучения более робастных моделей классификации. В статье Sandfort et al. используют аугментацию генеративными нейросетями, чтобы улучшить качество сегментации компьютерной томографии. Помимо этого, у генеративных моделей есть ряд других применений для редактирования изображений. Их используют, чтобы повысить разрешение картинок (задача super-resolution). На изображении ниже оригинальную картинку (original) сначала сжали в четыре раза, а потом попробовали восстановить до исходных размеров разными методами. Видно, что метод SRGAN, метод на основе генеративных состязательных нейронных сетях работает гораздо лучше бикубической интерполяции (bicubic), которая обычно применяется по умолчанию и смазывает картинку. ссылка на источник картинки С помощью генеративных моделей можно закрашивать пропущенные куски изображений. Это полезно, когда мы хотим удалить с фото других людей, и нам нужно закрасить участки, образовавшиеся после их удаления. Эта функция представлена в некоторых современных смартфонах. ссылка на источник картинки В последние несколько лет хорошо стали работать модели, которые генерируют изображения на основе их текстового описания. Среди таких моделей: Stable Diffusion (Демо). Модель с открытым исходным кодом DALLE 2. Доступ по платному API Midjourney. Доступ через Discord Imagen stable Примеры генерации изображений из текстового описания. Модель stable diffusion Источник Появились даже специальные базы изображений, сгенерированных нейронными сетями: Lexica, Openart. Доступность таких моделей приводит к появлению множества приложений: Иллюстрации для книг Создание логотипов Создание дизайнов помещений Генерация тату Кроме этого, некоторые модели позволяют совместить несколько задач и делать закрашивание изображения на основе текстового описания. Например, удалять какую-то",
    "metadata": {
      "title": "Введение в генеративное моделирование",
      "url": "https://education.yandex.ru/handbook/ml/article/vvedenie-v-generativnoe-modelirovanie",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.1",
      "part": 2,
      "total_parts": 3,
      "source_file": "8.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "ссылка на источник картинки В последние несколько лет хорошо стали работать модели, которые генерируют изображения на основе их текстового описания. Среди таких моделей: Stable Diffusion (Демо). Модель с открытым исходным кодом DALLE 2. Доступ по платному API Midjourney. Доступ через Discord Imagen stable Примеры генерации изображений из текстового описания. Модель stable diffusion Источник Появились даже специальные базы изображений, сгенерированных нейронными сетями: Lexica, Openart. Доступность таких моделей приводит к появлению множества приложений: Иллюстрации для книг Создание логотипов Создание дизайнов помещений Генерация тату Кроме этого, некоторые модели позволяют совместить несколько задач и делать закрашивание изображения на основе текстового описания. Например, удалять какую-то область и говорить модели, что там должно быть нарисовано. Пример закрашивания части изображения на основе текстового описания. Источник На основе этой технологии появились редакторы изображений с генеративными моделями внутри: Neural love, Photoroom, ZMO. Современные генеративные модели достигли очень хорошего качества и уже стали использоваться в реальных задачах, о которых мы вам рассказали. В следующих параграфах этой главы мы рассмотрим основные методы генеративного обучения более детально. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 7.2. Дистилляция знаний Следующий параграф 8.2. Variational Autoencoder (VAE)",
    "metadata": {
      "title": "Введение в генеративное моделирование",
      "url": "https://education.yandex.ru/handbook/ml/article/vvedenie-v-generativnoe-modelirovanie",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.1",
      "part": 3,
      "total_parts": 3,
      "source_file": "8.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В машинном обучении есть довольно широкая область, посвящённая обучению генеративных моделей. Их задача — выучить распределение, из которого могли бы быть насемплированы объекты обучающей выборки. Обученная генеративная модель способна семплировать из выученного распределения новые объекты, не принадлежащие исходным данным. Чаще всего это связано с задачей генерации новых изображений: от изображений рукописных чисел до замены лиц на видео с помощью deepfake. Модель, о которой пойдёт речь в данном параграфе, называется «Вариационный автоэнкодер» или VAE (variational autoencoder). Она относится к семейству генеративных моделей. Коротко расскажем, что вас ждёт дальше. В разделах «Постановка задачи» и «Обучение VAE» мы опишем построение и обучение VAE в классическом описании. Этих двух разделов достаточно для общего представления о VAE. Раздел «Обзор статей» для первоначального понимания не обязателен, но может быть интересен тем, кто захочет узнать о недавних интересных работах, связанных с VAE. Прежде чем двинуться дальше — небольшое напоминание: большинство картинок в тексте кликабельны, и при клике вы сможете перейти к источнику, из которого была заимствована картинка. Постановка задачи Давайте представим себе, что нам нужно нарисовать лошадь. Как бы мы это сделали? Наверное, сначала наметили бы общий силуэт лошади, её размер и позу, а затем стали бы добавлять детали: гриву, хвост, копыта, выбирать окраску шерсти и так далее. Кажется, что в процессе обучения рисованию мы учимся выделять для себя основной набор каких-то факторов, наиболее важных для генерации нового изображения: общий силуэт, размер, цвет и тому подобное, а во время рисования уже просто подставляем какие-то значения факторов. При этом одинаковые сочетания одних и тех же факторов могут привести к разным картинкам — ведь нарисовать что-то два раза абсолютно одинаково вы, скорее всего, не сможете. Попробуем формализовать описанный выше процесс. Пусть у нас есть датасет D D в многомерном пространстве исходных данных X N X N , — объектов, которые мы желаем генерировать, — и пространство Z M Z M скрытых (латентных) переменных меньшей размерности, которыми кодируются скрытые факторы в данных. Тогда генеративный процесс состоит из двух последовательных стадий (см. картинку ниже): Семплирование z ∈ Z M z∈Z M из распределения p ( z ) p(z) (красное) Семплирование x ∈ X N x∈X N из распределения p(x∥z) (синее) 1. То есть, рассуждая в терминах рисования картинок с лошадками, мы сначала мысленно семплируем некоторое z z (размер, форму, цвет, ...), затем дорисовываем все необходимые детали, то есть семплируем из распределения p(x∥z), и в итоге надеемся, что получившееся будет напоминать лошадку. Таким образом, построить генеративную модель в нашем случае — значит уметь семплировать с помощью описанного двустадийного процесса объекты, близкие к объектам из обучающей выборки D D. Говоря более формально, нам бы хотелось, чтобы наша модель максимизировала правдоподобие p ( x ) p(x) элементов обучающего множества D D при описанной процедуре генерации: max ⁡ p(x)= Z M ∫ p(x∣z)p(z)dz→max Предположим, что совместное распределение p(x,z) параметризовано некоторым параметром θ ∈ Θ θ∈Θ и выражается непрерывной по θ θ функцией при каждых фиксированных x x и (x,z)=p(x,z∣θ)∈C(Θ) Тогда (x,z)=p(x∣z,θ)p(z∣θ)=p θ (x∣z)p θ (z), и мы можем записать следующую задачу оптимизации: max (x)= Z M ∫ p θ (x∣z)p θ (z)dz→ θ∈Θ max (1)",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 1,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "уметь семплировать с помощью описанного двустадийного процесса объекты, близкие к объектам из обучающей выборки D D. Говоря более формально, нам бы хотелось, чтобы наша модель максимизировала правдоподобие p ( x ) p(x) элементов обучающего множества D D при описанной процедуре генерации: max ⁡ p(x)= Z M ∫ p(x∣z)p(z)dz→max Предположим, что совместное распределение p(x,z) параметризовано некоторым параметром θ ∈ Θ θ∈Θ и выражается непрерывной по θ θ функцией при каждых фиксированных x x и (x,z)=p(x,z∣θ)∈C(Θ) Тогда (x,z)=p(x∣z,θ)p(z∣θ)=p θ (x∣z)p θ (z), и мы можем записать следующую задачу оптимизации: max (x)= Z M ∫ p θ (x∣z)p θ (z)dz→ θ∈Θ max (1) Решив её, мы построим нашу генеративную модель. Замечание 1. После приведённой выше аналогии с обучением рисованию может ошибочно показаться, что в скрытые переменные всегда заложен некоторый хорошо интерпретируемый смысл. Но на практике это всё же не обязано быть так: те скрытые переменные, которые мы найдём, могут как иметь простую интерпретацию, так и не иметь. С помощью объяснений выше мы прежде всего хотели проиллюстрировать понятие «скрытые переменные». Замечание 2. Может показаться, что p ( x ) p(x) нам откуда-то уже известно, и тогда не ясно, зачем все эти сложности с введением латентных переменных и интегралами. На самом деле, мы действительно можем построить статистическую оценку (x) по данным D D и даже пытаться генерировать новые данные с помощью таких моделей (как, например, делается тут). Но у статистических методов есть разные ограничения, наиболее серьёзным из которых представляется проклятие размерности: чем больше измерений у ваших данных, тем больше разнообразных примеров вам нужно для построения адекватной оценки (x). О проклятии размерности мы поговорим чуть подробнее далее. Замечание 3. Также может возникать вопрос — а зачем вообще нужно вводить латентные переменные, моделировать совместное распределение p(x,z), а целевое распределение p ( x ) p(x) определять как маргинализацию p(x,z) по z z? Почему такой подход в принципе должен работать? Ответ состоит в том, что, даже имея относительно простые выражения для p ( z ) p(z) и p(x∥z), можно описать достаточно сложное распределение p ( x ) p(x), что достаточно наглядно проиллюстрировано в примере ниже. Обучение VAE Прежде чем пытаться решать задачу оптимизации ( 1 ) (1) давайте подумаем, а как мы вообще могли бы посчитать такой интеграл? Первое, что приходит на ум, — попробовать получить его приближённое значение методом Монте-Карло: (x)= Z M ∫ p θ (x∣z)p θ (z)dz=E z∼p θ (z) [p θ (x∣z)]≈ (x∣z k ), где в последнем переходе мы используем сэмплы (z). Однако, если z ∈ Z M z∈Z M и M M — достаточно большое, мы столкнёмся с проклятием размерности — количество семплов, необходимых для того, чтобы хорошо покрыть Z M Z M , растёт экспоненциально с ростом M M: 3 Есть ли способ как-то сократить число необходимых семплов для подсчёта ( 1 ) (1)? На самом деле, часто оказывается, что далеко не все возможные z z отображаются в элементы D D, и вклад большинства z z в оценку (x∥z) практически нулевой. Это наводит на мысль, что для каждого x x нам может пригодиться знание распределения q(z∥x) таких z z, которые являются прообразами",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 2,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "z ∈ Z M z∈Z M и M M — достаточно большое, мы столкнёмся с проклятием размерности — количество семплов, необходимых для того, чтобы хорошо покрыть Z M Z M , растёт экспоненциально с ростом M M: 3 Есть ли способ как-то сократить число необходимых семплов для подсчёта ( 1 ) (1)? На самом деле, часто оказывается, что далеко не все возможные z z отображаются в элементы D D, и вклад большинства z z в оценку (x∥z) практически нулевой. Это наводит на мысль, что для каждого x x нам может пригодиться знание распределения q(z∥x) таких z z, которые являются прообразами x x. Мы можем предположить, что распределение q q параметризовано некоторым семейством параметров q(z∣x)=q ϕ (z∣x),ϕ∈Φ Зная распределение (z∥x), мы могли бы семлировать уже только из него, а не из всего (z), и, если распределение q q окажется достаточно хорошим, число необходимых семплов значительно сократится. О том, как построить q ϕ q ϕ , мы поговорим позже. Сейчас стоит обратить внимание на то, что процессы семплирования из распределений (z∥x) и (x∥z) взаимно обратны друг к другу: первое отображает элементы датасета в подмножество латентного пространства Z M Z M , то есть действует как энкодер, а второе отображает латентные переменные в подмножество X N X N , то есть действует как декодер: 2 Так как оба эти распределения будут участвовать в обучении VAE, возникает аналогия между VAE и моделями-автоэнкодерами, имеющими похожую структуру. Вывод функции потерь Сейчас у нас всё готово для того, чтобы записать общий вид функции потерь для обучения вариационного автоэнкодера. Напомним, что мы обучаем модель путём максимизации правдоподобия (x) по θ θ. Для удобства мы перейдём к логарифму правдоподобия: log log max ⁡ θ ∈ Θ logp θ (x)=log∫ Z M p θ (x∣z)p θ (z)dz→ θ∈Θ max Оптимизировать напрямую это выражение тяжело из-за проклятия размерности, обсуждавшегося в прошлом разделе. Чтобы победить проклятие размерности, мы хотели бы заменить семплирование из априорного распределения (z) на семплирование из (z∥x), для чего придётся осуществить некоторый трюк. Для любого (z∥x), отличного от нуля для всех z ∈ Z M z∈Z M , мы можем выписать следующую цепочку равенств: log log logp θ (x)=E q ϕ (z∣x) [logp θ (x)]= log (z∣x) [log( p θ (z∣x) p θ (x,z) )]= log (z∣x) [log( q ϕ (z∣x) p θ (x,z) p θ (z∣x) q ϕ (z∣x) )]= log log θ,ϕ (x)(ELBO) E q ϕ (z∣x) [log( q ϕ (z∣x) p θ (x,z) (z∣x)∥p θ (z∣x)) E q ϕ (z∣x) [log( p θ (z∣x) q ϕ (z∣x) )] Второе слагаемое в последнем равенстве — K L KL-дивергенция между (z∥x) и (z∥x), которая, как известно, неотрицательна: (z∣x)∥p θ (z∣x))≥0 А первое слагаемое — это величина, именуемая в английской литературе evidence lower bound (ELBO): log log θ,ϕ (x)=E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)]= log reconstruction loss regularization term = reconstruction loss E q ϕ (z∣x) [logp θ (x∣z)] − regularization term D KL (q ϕ (z∣x)∥p θ (z)) Первое слагаемое в последнем переходе обычно называют reconstruction loss, так как оно оценивает качество восстановления декодером объекта",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 3,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "E q ϕ (z∣x) [log( p θ (z∣x) q ϕ (z∣x) )] Второе слагаемое в последнем равенстве — K L KL-дивергенция между (z∥x) и (z∥x), которая, как известно, неотрицательна: (z∣x)∥p θ (z∣x))≥0 А первое слагаемое — это величина, именуемая в английской литературе evidence lower bound (ELBO): log log θ,ϕ (x)=E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)]= log reconstruction loss regularization term = reconstruction loss E q ϕ (z∣x) [logp θ (x∣z)] − regularization term D KL (q ϕ (z∣x)∥p θ (z)) Первое слагаемое в последнем переходе обычно называют reconstruction loss, так как оно оценивает качество восстановления декодером объекта x x из его латентного представления z z. А второе играет роль регуляризационного члена и подталкивает распределение, генерируемое энкодером, быть ближе к априорному распределению. Так как K L KL-дивергенция неотрицательна, ELBO является нижней границей для логарифма правдоподобия данных: log log θ,ϕ (x)=logp θ (x)−D KL (q ϕ (z∣x)∥p θ (z∣x))≤logp θ (x) Посмотрим повнимательнее на равенства, которые мы выписали. Функцию L θ , ϕ L θ,ϕ можно оптимизировать градиентным спуском (SGD), предварительно выбрав удобный вид для (x∥z), (z∥x) и (z). Максимизируя L θ , ϕ L θ,ϕ , мы растим log logp θ (x), тем самым улучшая нашу генеративную модель. Оптимизацию ELBO с помощью SGD мы будем подробно обсуждать в следующем разделе. Максимизируя L θ , ϕ L θ,ϕ , мы одновременно минимизируем (z∥x)∥p θ (z∥x)). Распределение (z∥x) оценивает, из каких z z мог бы быть сгенерирован объект x x, и заранее оно нам не известно. Но если мы выберем достаточно большую модель для (z∥x), то (z∥x) в процессе оптимизации может очень сильно приблизиться к (z∥x), и тогда мы будем напрямую оптимизировать log logp θ (x). Заодно мы получаем приятный бонус: для оценки распределения прообразов x x мы сможем использовать (z∥x) вместо невычислимого (z∥x). То есть q ϕ q ϕ , которое мы при выводе формулы ввели в рассмотрение как произвольное распределение, действительно будет играть роль энкодера для модели. Обучение VAE с помощью градиентного спуска Важное свойство ELBO в том, что его можно оптимизировать градиентным спуском относительно параметров ϕ ϕ и θ θ. Если объекты датасета D D независимы и одинаково распределены, то θ,ϕ (D) запишется как сумма (или среднее) значений θ,ϕ (x) на объектах x ∈ D x∈D: θ,ϕ (D)= x∈D ∑ L θ,ϕ (x) Значения θ,ϕ (x) и их градиенты θ,ϕ (x) в общем случае вычислить невозможно, однако можно получить их несмещённые оценки, что позволит нам использовать стохастический градиентный спуск. Оценку для градиента по параметрам θ θ получить несложно: log log θ,ϕ (x)=∇ θ E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)]= log log (z∣x) [∇ θ (logp θ (x,z)−logq ϕ (z∣x))]= log (z∣x) [∇ θ logp θ (x,z)]≈ log logp θ (x,z k ), где в последней строчке (z∥x). Однако оценку на градиент по параметрам ϕ ϕ получить сложнее, ведь они также участвуют и в семплировании: log log θ,ϕ (x)=∇ ϕ E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)] log log (z∣x) [∇ ϕ (logp θ (x,z)−logq ϕ (z∣x))] В общем случае эта проблема не",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 4,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "градиентный спуск. Оценку для градиента по параметрам θ θ получить несложно: log log θ,ϕ (x)=∇ θ E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)]= log log (z∣x) [∇ θ (logp θ (x,z)−logq ϕ (z∣x))]= log (z∣x) [∇ θ logp θ (x,z)]≈ log logp θ (x,z k ), где в последней строчке (z∥x). Однако оценку на градиент по параметрам ϕ ϕ получить сложнее, ведь они также участвуют и в семплировании: log log θ,ϕ (x)=∇ ϕ E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)] log log (z∣x) [∇ ϕ (logp θ (x,z)−logq ϕ (z∣x))] В общем случае эта проблема не разрешима. Однако некоторые распределения позволяют применить репараметризацию (reparameterization trick): представить переменную z z как обратимую дифференцируемую функцию от случайного шума, параметров ϕ ϕ и переменнной x ∈ D x∈D: z=g(ε,ϕ,x) Здесь распределение ε ∼ p ε ε∼p ε не зависит от ϕ ϕ и x x. Например, пусть ε∼N(0,I). Тогда g g может иметь следующий вид: z=g(ε,ϕ,x)=μ ϕ (x)+ε⋅σ ϕ (x)∼N(μ ϕ (x),σ ϕ 2 (x)) После такой замены мы сможем получить оценку на градиент по log log θ,ϕ (x)=∇ ϕ E q ϕ (z∣x) [logp θ (x,z)−logq ϕ (z∣x)]= log log [logp θ (x,g(ε,ϕ,x))−logq ϕ (g(ε,ϕ,x)∣x)]= log log (logp θ (x,g(ε,ϕ,x))−logq ϕ (g(ε,ϕ,x)∣x))]≈ log log (logp θ (x,g(ε k ,ϕ,x))−logq ϕ (g(ε k ,ϕ,x)∣x)), где в последней строчке . Репараметризация хорошо иллюстрируется следующей картинкой: 2 Здесь f f — функция потерь. Значения f f на обеих схемах одинаковы, но на левой картинке градиенты по ϕ ϕ рассчитать не получится, так как мы не можем дифференцировать по случайной переменной z z. Однако на правой картинке источник случайности перемещается во входные данные благодаря репараметризации, а градиенты вычисляются по детерминированным переменным. Таким образом, мы получили сетап, типичный для оптимизации с помощью SGD: там мы приближаем градиент функции потерь по случайным батчам входных данных, а здесь роль случайных батчей играют одновременно батчи из переменных x x и случайных переменных ε ε. Кроме нормального распределения, есть довольного много примеров распределений, допускающих репараметризацию. Их можно найти по ссылке в разделе \"The reparameterization trick\". Однако большая часть реализаций VAE используют именно нормальное распределение. В итоге примерный алгоритм обучения VAE такой: dataset = np.array(...) epsilon = RandomDistribution(...) # Энкодер q_phi(z|x) — нейронная сеть с параметрами phi encoder = Encoder() # Декодер p_theta(x|z) — нейронная сеть с параметрами theta decoder = Decoder() for step in range(max_steps): # Семплируем батч исходных данных и случайного шума batch_x = sample_batch(dataset) batch_noise = sample_batch(epsilon) # Считаем параметры распределения q(z | x) с помощью энкодера latent_distribution_parameters = encoder(batch_x) # Делаем репараметризацию (семплируем из q(z | x)) z = reparameterize(latent_distribution_parameters, batch_noise) # Декодер отдаёт параметры выходного распределения output_distribution_parameters = decoder(z) # Вычисляем ELBO и обновляем параметры моделей L = -ELBO( latent_distribution_parameters, output_distribution_parameters, batch_x ) L.backward() Стоит подчеркнуть, что декодер выдаёт именно параметры выходного распределения, а не конкретный семпл из этого распределения. Например, если вы моделируете выходные изображения с помощью нормального распределения N(μ(z),σ 2 (z)), то декодер на выходе предскажет некоторые (z) и (z), которые вместе с параметрами латентного распределения (выход энкодера) будут поданы в",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 5,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "= sample_batch(epsilon) # Считаем параметры распределения q(z | x) с помощью энкодера latent_distribution_parameters = encoder(batch_x) # Делаем репараметризацию (семплируем из q(z | x)) z = reparameterize(latent_distribution_parameters, batch_noise) # Декодер отдаёт параметры выходного распределения output_distribution_parameters = decoder(z) # Вычисляем ELBO и обновляем параметры моделей L = -ELBO( latent_distribution_parameters, output_distribution_parameters, batch_x ) L.backward() Стоит подчеркнуть, что декодер выдаёт именно параметры выходного распределения, а не конкретный семпл из этого распределения. Например, если вы моделируете выходные изображения с помощью нормального распределения N(μ(z),σ 2 (z)), то декодер на выходе предскажет некоторые (z) и (z), которые вместе с параметрами латентного распределения (выход энкодера) будут поданы в ELBO. Для генерации конкретной картинки на этапе инференса нужно будет либо честно провести семплирование из (z), σ ^ 2 (z)), либо, как часто делают, просто взять среднее (z) в качестве выходного изображения. В общем случае конкретный способ проведения инференса зависит от вида используемого выходного распределения. Выбор вида используемых распределений Пришло время привести примеры конкретных (x∥z), (z∥x) и (z), с которыми можно построить VAE. Для начала предположим, что (z) можно положить равным стандартному нормальному распределению: (z)=N(0,I) Заметим, что в этом случае у априорного распределения z z отсутствует зависимость от параметров θ θ. Распределение (x∥z) зависит от того, к какому распределению принадлежат ваши данные. Если ваши данные имеют непрерывное распределение, то (x∥z) можно задать, например, как гауссовское распределение: (x∣z)=N(f θ (z),σ 2 ) Вектор средних в этом примере определяется функцией f f с переменными θ θ и z z, а матрица ковариаций определяется постоянной диагональной матрицей. Функцию f f можно задать с помощью нейронной сети с параметрами θ θ. При желании, матрицу ковариаций тоже можно задавать некоторой функцией и не ограничивать её вид только постоянными матрицами. Если же ваши данные дискретны, то может подойти категориальное распределение: Categorical (x∣z)=Categorical(f θ (z)), в котором вектор вероятностей (z)=(p 1 ,…,p n ) — выход нейросети после применения softmax softmax. Если у вас бинарные данные, вы можете использовать бернуллиевское распределение: Bernoulli (x∣z)=Bernoulli(f θ (z)), где (z)=p — выход нейронной сети после применения сигмоиды. Распределение (z∥x) может, в принципе, быть любым, но в самом простом случае оно имеет вид гауссовского распределения c диагональной матрицей ковариаций: (z∣x)=N(μ ϕ (x),σ ϕ 2 (x)) Такое распределение позволяет, в частности, применить репараметризацию, обсуждавшуюся выше. Если выбрать z z двумерным, то распределения, определямые q q, хорошо визуализируются: 2 А теперь вспомним, как определяется ELBO: log θ,ϕ (x)=E q ϕ (z∣x) [logp θ (x∣z)]−D KL (q ϕ (z∣x)∥p θ (z)) Вычислим его для приведённых выше распределений. Начнём с (z∥x)∥p θ (z)). K L KL-дивергенция между распределениями N(μ,Σ) и N(0,I) равна: log ⁡ ( det (N(μ,Σ)∥N(0,I))= 2 1 (μ T μ+trΣ−M−log(detΣ)), где M M — размерность этих распределений. Вывод этого соотношения можно найти здесь. В нашем случае (x)=(μ 1 ,…,μ diag (x)=diag(σ 1 2 ,…,σ M 2 ) и (z∥x)∥p θ (z))=D KL (N(μ ϕ (x),σ ϕ 2 (x))∥N(0,I))= j=1 −1−lnσ j 2 ) Тогда ELBO будет вычисляться как: log θ,ϕ (x)=E q ϕ (z∣x) [logp θ (x∣z)]−D KL (q ϕ (z∣x)∥p θ (z))= log N(μ ϕ (x),σ ϕ 2 (x)) [logp θ (x∣z)]−",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 6,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "приведённых выше распределений. Начнём с (z∥x)∥p θ (z)). K L KL-дивергенция между распределениями N(μ,Σ) и N(0,I) равна: log ⁡ ( det (N(μ,Σ)∥N(0,I))= 2 1 (μ T μ+trΣ−M−log(detΣ)), где M M — размерность этих распределений. Вывод этого соотношения можно найти здесь. В нашем случае (x)=(μ 1 ,…,μ diag (x)=diag(σ 1 2 ,…,σ M 2 ) и (z∥x)∥p θ (z))=D KL (N(μ ϕ (x),σ ϕ 2 (x))∥N(0,I))= j=1 −1−lnσ j 2 ) Тогда ELBO будет вычисляться как: log θ,ϕ (x)=E q ϕ (z∣x) [logp θ (x∣z)]−D KL (q ϕ (z∣x)∥p θ (z))= log N(μ ϕ (x),σ ϕ 2 (x)) [logp θ (x∣z)]− 2 1 j=1 −1−lnσ log k=1 ∑ K logp θ (x∣z k )+ 2 1 j=1 ∑ M (1+lnσ где ∼N(μ ϕ (x),σ ϕ 2 (x)). Как было упомянуто в этой статье от авторов VAE в разделе 2.3, число семплирований K K можно положить равным единице при достаточно большом размере батча (например, 100). Если вы выберете биномиальное (x∥z), то log log log ⁡ Bernoulli logp θ (x∣z)= j=1 ∑ D logp θ (x j ∣z)= j=1 ∑ D logBernoulli(x log log j=1 ∑ D x j logp j +(1−x j )log(1−p j ) Если гауссовское N(f θ (z),σ 2 ), то log log log exp logp θ (x∣z)= j=1 ∑ D logp θ (x j ∣z)= j=1 ∑ D log( 2πσ 2 1 exp(− 2σ 2 (x j −f θ,j (z)) 2 ))= = − D 2 log ⁡ 2 π − D log log2π−Dlogσ− 2σ 2 1 j=1 ∑ D (x j −f θ,j (z)) 2 Пример реализации обучения и применения VAE на датасете MNIST на Keras можно найти здесь, а на PyTorch — здесь. Инференс обученной модели Когда мы обучили VAE, мы сможем генерировать новые семплы, просто подавая z∼N(0,I) на вход декодеру: ![2](https://yastatic.net/s3/education-portal/media/vae_decoder_diagram_385be2e566_6c396a28e8.svg\"> Энкодер для генерации новых семплов не нужен. Однако нам может понадобиться оценить p(x)=∫p(x∥z)p(z)dz для x x из тестового множества, чтобы понять, с какой вероятностью модель сможет сгенерировать x x. Для оценки интеграла нам нужно насемплировать некоторое количество z z, и если брать семплы из z∼N(0,I), то оценка может плохо сойтись. Но можно снова использовать ELBO как нижнюю границу для log ⁡ p ( x ) logp(x) и оценивать уже её, семплируя из распределения (z∥x). Такая оценка сойдётся быстрее и даст примерное представление о том, насколько хорошо модель справляется с конкретным примером x x. Также интересно бывает взглянуть на то, как распределены коды обучающих примеров в латентном пространстве. Так, например, может выглядеть распределение латентных кодов цифр MNIST для обученного VAE в двумерном латентном пространстве: 2 Разные типы цифр обозначены разными цветами (соответствие цифр и цветов показано на шкале сбоку). Здесь видно, что лучше всего модель различает нули и единицы, а восьмёрки и тройки — хуже всего. Стоит, конечно, отметить, что латентное пространство выбрано двумерным в целях визуализации, и при большей его размерности модель могла бы научиться различать цифры более качественно. Для двумерного латентного пространства есть ещё один интересный способ визуализировать структуру многообразия, выученного VAE. Можно взять равномерную сетку на единичном квадрате и отобразить её в латентное",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 7,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "латентном пространстве. Так, например, может выглядеть распределение латентных кодов цифр MNIST для обученного VAE в двумерном латентном пространстве: 2 Разные типы цифр обозначены разными цветами (соответствие цифр и цветов показано на шкале сбоку). Здесь видно, что лучше всего модель различает нули и единицы, а восьмёрки и тройки — хуже всего. Стоит, конечно, отметить, что латентное пространство выбрано двумерным в целях визуализации, и при большей его размерности модель могла бы научиться различать цифры более качественно. Для двумерного латентного пространства есть ещё один интересный способ визуализировать структуру многообразия, выученного VAE. Можно взять равномерную сетку на единичном квадрате и отобразить её в латентное пространство, применив к ней функцию, обратную к CDF нормального распределения. Полученные семплы можно подать в декодер и посмотреть, какие картинки будут соответствовать узлам сетки: 2 Здесь изображены примеры, сгенерированные для датасетов Frey Face и MNIST (оба доступны по ссылке). Такая визуализация позволяет увидеть плавный переход латентных кодов одних объектов в коды других, а также взаимное расположение латентных кодов. Для MNIST снова видно, в частности, что коды нулей и единиц модель разнесла далеко друг от друга, а коды троек и восьмёрок очень близки. А ещё интересно наблюдать плавный переход от шестёрок к нулям и от семёрок к единицам. Для Frey Face видно, что весёлые лица расположены далеко от грустных, а по главной диагонали квадрата можно проследить плавный переход от серьёзного лица к улыбающемуся. Ещё интересно посмотреть на то, как меняется качество генерируемых цифр в зависимости от размерности латентного пространства (на картинках просто случайные семплы из модели): 2 Заметный переход виден между размерностями 2 и 5, дальнейший рост размерности почти не оказывает значимого эффекта. Conditional VAE (CVAE) Иногда мы можем захотеть сгенерировать не просто какой-то произвольный объект из датасета, а относящийся к конкретной группе или классу. Ранее мы выписывали уравнение для log logp θ (x): log log logp θ (x)=E q ϕ (z∣x) [logp θ (x∣z)]−D KL (q ϕ (z∣x)∥p θ (z))+D KL (q ϕ (z∣x)∥p θ (z∣x)) Все распределения, участвующие в этом уравнении, мы можем сделать обусловленными по переменной y y: log log logp θ (x∣y)=E q ϕ (z∣x,y) [logp θ (x∣z,y)]−D KL (q ϕ (z∣x,y)∥p θ (z∣y))+D KL (q ϕ (z∣x,y)∥p θ (z∣x,y)) Переменная y y может быть лейблом объекта x x или вообще произвольным тензором, как-то характеризующим x x. Вместо (z), единого для всех x x из обучающей выборки, для каждого значения y y теперь будет отдельное априорное распределение (z∥y). Переменная y y может принимать и дискретные, и непрерывные значения. Она может даже, например, быть половиной изображения, которую модели предлагается дополнить. На всякий случай подчеркнём, что обучение CVAE — это не то же самое, что обучение нескольких независимых VAE, так как веса CVAE общие для всех классов. На уровне имплементации это реализуется довольно просто: нужно всего лишь сконкатенировать входы энкодера и декодера с тензором, соответствующим y y. Если y y имеет категориальные значения, то бывает полезно предварительно закодировать их one-hot векторами. Алгоритм будет примерно таким: dataset, labels = np.array(...), np.array(...) epsilon = RandomDistribution(...) # Энкодер q_phi(z|x) — нейронная сеть с параметрами phi encoder = Encoder() # Декодер p_theta(x|z) —",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 8,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "значения. Она может даже, например, быть половиной изображения, которую модели предлагается дополнить. На всякий случай подчеркнём, что обучение CVAE — это не то же самое, что обучение нескольких независимых VAE, так как веса CVAE общие для всех классов. На уровне имплементации это реализуется довольно просто: нужно всего лишь сконкатенировать входы энкодера и декодера с тензором, соответствующим y y. Если y y имеет категориальные значения, то бывает полезно предварительно закодировать их one-hot векторами. Алгоритм будет примерно таким: dataset, labels = np.array(...), np.array(...) epsilon = RandomDistribution(...) # Энкодер q_phi(z|x) — нейронная сеть с параметрами phi encoder = Encoder() # Декодер p_theta(x|z) — нейронная сеть с параметрами theta decoder = Decoder() for step in range(max_steps): # Семплируем батч исходных данных, лейблов и случайного шума batch_x = sample_batch(dataset) batch_y = sample_batch(labels) batch_noise = sample_batch(epsilon) # Подаём в энкодер конкатенацию входных данных и лейблов encoder_input = concatenate([batch_x, batch_y]) # Считаем параметры распределения z с помощью энкодера latent_distribution_parameters = encoder(encoder_input) # Делаем репараметризацию z = reparameterize(latent_distribution_parameters, batch_noise) # Конкатенируем полученный случайный вектор и лейблы decoder_input = concatenate([z, batch_y]) # Декодер отдаёт нам выходное изображение output_distribution_parameters = decoder(decoder_input) # Вычисляем ELBO и обновляем параметры L = -ELBO( latent_distribution_parameters, output_distribution_parameters, batch_x ) L.backward() Реализацию CVAE на PyTorch и Tensorflow можно найти, например, здесь. Если визуализировать распределение латентных кодов для цифр MNIST, полученных после обуславливания модели на класс цифры, то можно увидеть что-то такое: 2 Мы видим непонятную смесь из точек вместо явных кластеров, которые выделяла обычная модель VAE. Однако дело тут в том, что, вместо того, чтобы пытаться размещать все цифры в одном пространстве p(z)∼N(0,I), модель использует отдельное латентное пространство p(z∥y)∼N(0,I) для каждой цифры: 2 2 На картинке справа — априорные распределения для цифр 6 и 7, а слева — визуализация структуры выученных многообразий для этих цифр, построенная так же, как аналогичная визуализация для VAE. Качество изображений каждой отдельной цифры заметно повышается: 2 Видно, что вариабельность генерации цифр теперь тоже заметно выросла, и модель может имитировать написание цифр разными почерками. Обзор статей Кроме стандратного описания работы VAE, приведём результаты нескольких недавних интересных работ, базирующихся на идее VAE. VQ-VAE и VQ-VAE-2 Модели VQ-VAE и VQ-VAE-2 интересны тем, что в них в качестве априорных распределений были задействованы дискретные распределения. В каких ситуациях дискретные распределения могут быть более применимы, чем непрерывные? Например, если мы имеем дело с токенам в задачах NLP или фонемами в обработке речи. Картинки также можно было бы кодировать некоторым набором из целых чисел: например, одно число могло бы кодировать тип объекта, другое — его цвет, третье — цвет фона и так далее: 2 Кроме того, существуют довольно мощные алгоритмы (например, Трансформер), предназначенные для работы с дискретными данными. Выучивание хороших дискретных представлений даёт возможность эффективно использовать такие алгоритмы для, например, задачи генерации картинок. VQ-VAE Авторы VQ-VAE вводят дискретное латентное пространство в виде K K вещественных векторов ,…,e K размерности D D. Векторы из этого пространства называются кодовыми векторами или кодами. На рисунке ниже приведена примерная схема обучения предлагаемой модели. 2 Энкодер принимает на вход картинку x x и выдаёт на выходе тензор (x). На рисунке этот тензор имеет",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 9,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "бы кодировать тип объекта, другое — его цвет, третье — цвет фона и так далее: 2 Кроме того, существуют довольно мощные алгоритмы (например, Трансформер), предназначенные для работы с дискретными данными. Выучивание хороших дискретных представлений даёт возможность эффективно использовать такие алгоритмы для, например, задачи генерации картинок. VQ-VAE Авторы VQ-VAE вводят дискретное латентное пространство в виде K K вещественных векторов ,…,e K размерности D D. Векторы из этого пространства называются кодовыми векторами или кодами. На рисунке ниже приведена примерная схема обучения предлагаемой модели. 2 Энкодер принимает на вход картинку x x и выдаёт на выходе тензор (x). На рисунке этот тензор имеет размерность M × M × D M×M×D: последняя размерность совпадает с длиной кодовых векторов, а M × M M×M — это пространственная размерность выхода CNN (для простоты мы здесь не пишем явно размерность батчей). Каждый из M × M M×M векторов из (x) отображается в ближайший к нему по L 2 L 2 -расстоянию кодовый вектор. После такой процедуры тензор (x) переходит в тензор (x), состоящий из M × M M×M кодовых векторов. Декодер получает на вход тензор (x) и отображает его в исходную картинку. Для работы с речью и текстами авторы использовали двумерный тензор (x) вместо трёхмерного. Выходное распределение энкодера q(z∥x) определено здесь следующим образом: arg ⁡ min иначе q(z=k∣x)={ 1, 0, k=argmin j ∥z e (x)−e j ∥ 2 , иначе Во время обучения в качестве априорного распределения в латентном пространстве используется равномерное распределение p(z)= K 1 , поэтому слагаемое (q(z∥x)∥p(z)) оказывается постоянным и равным log ⁡ K logK: log log ⁡ K D KL (q(z∣x)∥p(z))=− k=1 ∑ K q(z=k∣x)log( q(z=k∣x) p(z) )=logK В точках, где q(z=k∥x)=0, предпоследнее выражение продолжается нулём по непрерывности. Таким образом, ELBO для таких распределений примет вид log log log ⁡ K , ELBO(x)=E q(z∣x) [logp θ (x∣z e (x))]−D KL (q(z∣x)∥p(z))=logp θ (x∣z q (x))−logK, где θ θ — параметры декодера. При оптимизации log ⁡ K logK можно не учитывать. Отображение выхода энкодера в кодовые векторы не дифференцируемо, поэтому при обучении применяется следующий трюк: при обратном проходе градиент копируется напрямую из декодера в энкодер, пропуская при этом слой, отображающий выходы энкодера в кодовые векторы. Этот трюк очень близок к приёму, известному как straight-through estimator, впервые предложенному в этой статье (а его простое описание можно найти тут). Использование straight-through estimator, однако, не позволяет обучать сами кодовые векторы, так как по ним не будут вычисляться градиенты. Поэтому лосс для обучения модели складывается из трёх компонент: L = log L=logp(x∣z q (x))+∥sg[z e (x)]−z q (x)∥ 2 2 +β∥z e (x)−sg[z q (x)]∥ 2 2 Здесь s g [ ⋅ ] sg[⋅] обозначает оператор остановки дифференцирования: через его аргумент не текут градиенты. В статье лосс записан несколько иначе: L = log L=logp(x∣z q (x))+∥sg[z e (x)]−e∥ 2 2 +β∥z e (x)−sg[e]∥ 2 2 Эти обозначения кажутся несколько путающими по двум причинам: Буква e e в нижнем индексе (x) призвана обозначить только то, что это выход энкодера, а не наличие связи между кодовыми векторами e e и параметрами энкодера. Но второе довольно легко для себя предположить. Вычитание",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 10,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "L = log L=logp(x∣z q (x))+∥sg[z e (x)]−z q (x)∥ 2 2 +β∥z e (x)−sg[z q (x)]∥ 2 2 Здесь s g [ ⋅ ] sg[⋅] обозначает оператор остановки дифференцирования: через его аргумент не текут градиенты. В статье лосс записан несколько иначе: L = log L=logp(x∣z q (x))+∥sg[z e (x)]−e∥ 2 2 +β∥z e (x)−sg[e]∥ 2 2 Эти обозначения кажутся несколько путающими по двум причинам: Буква e e в нижнем индексе (x) призвана обозначить только то, что это выход энкодера, а не наличие связи между кодовыми векторами e e и параметрами энкодера. Но второе довольно легко для себя предположить. Вычитание e e обозначает вычитание не всех элементов словаря из соответствующей позиции тензора (x), а только лишь ближайшего соседа к элементу (x) на этой позиции. То есть по факту вычитание e e в этой записи равносильно вычитанию (x). Это не уточняется в статье, но можно увидеть в официальной реализации. Первое слагаемое — это ELBO с точностью до константы. Второе слагаемое отвечает за сдвиг кодовых векторов в сторону выходов энкодера. Чтобы не получилось так, что выходы энкодера всё время меняют кодовые векторы за счёт второй компоненты лосса, а сами на каждой итерации выдают векторы, далёкие от текущих кодовых векторов, добавляется третье слагаемое. Оно отвечает за то, чтобы энкодер стремился выдавать векторы, близкие к кодовым векторам, а его значимость регулируется с помощью коэффициента β β. Однако при обучении мы потеряли регуляризационное слагаемое (q(z∥x)∥p(z)), из-за чего распределение энкодера не было обязано приближать собой априорное распределение и осталось его узким подмножеством. Из-за этого с наибольшей вероятностью при семплировании из равномерного категориального распределения мы будем получать просто шумы вместо хороших картинок: 2 Чтобы исправить эту проблему, авторы предлагают с помощью дополнительной модели выучить априорное распределение p ( z ) p(z) тех латентных переменных, которые модель научилась генерировать в процессе обучения. Поскольку любое кодовое представление можно вытянуть в последовательность, а самих кодов — конечное наперёд заданное число, то эта задача близка к задаче обучения языковой модели. Действительно, ведь там мы должны по последовательности предыдущих слов предложения предсказать следующее слово из доступного словаря, а в нашем случае — по входной последовательности дискретных латентных кодов предсказать следующий латентный код. Для картинок авторы предложили моделировать априорное распределение латентных кодов с помощью PixelCNN. Детали архитектуры и обучения этой модели можно найти в оригинальной статье, здесь мы опишем только общую идею. PixelCNN последовательно генерирует пиксели картинки, двигаясь из верхнего левого угла в правый нижний. Она проходит все ряды последовательно от верхнего до нижнего, а внутри каждого ряда движется слева направо: 2 Для цветных картинок каналы (R, G, B) также моделируются последовательно: канал B при генерации зависит от R и G, а G — только от R. При предсказании значения каждого следующего пикселя модель использует значения уже сгенерированных соседей из некоторого окружающего квадрата. Чтобы модель не могла читать пиксели, идущие после текущего предсказываемого пикселя, используется специальная маска, пример которой изображён на правой части рисунка. В случае VQ-VAE обучение PixelCNN происходит не на пикселях, а на латентных кодах. Семплирование из выученного априорного распределения выглядит гораздо лучше, чем попытки семплировать из равномерного: 2 Для аудио",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 11,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нижнего, а внутри каждого ряда движется слева направо: 2 Для цветных картинок каналы (R, G, B) также моделируются последовательно: канал B при генерации зависит от R и G, а G — только от R. При предсказании значения каждого следующего пикселя модель использует значения уже сгенерированных соседей из некоторого окружающего квадрата. Чтобы модель не могла читать пиксели, идущие после текущего предсказываемого пикселя, используется специальная маска, пример которой изображён на правой части рисунка. В случае VQ-VAE обучение PixelCNN происходит не на пикселях, а на латентных кодах. Семплирование из выученного априорного распределения выглядит гораздо лучше, чем попытки семплировать из равномерного: 2 Для аудио вместо PixelCNN авторами используется WaveNet. При обучении моделей априорных распределений есть возможность подавать метки классов, чтобы потом можно было семплировать из этих классов (принцип тот же, что и для CVAE). Результаты реконструкции картинок из ImageNet с помощью VQ-VAE выглядят довольно неплохо (под реконструкцией понимается выход полной модели, состоящей из энкодера и декодера): 2 А так выглядят результаты семплирования из VQ-VAE с априорным распределением, выученным PixelCNN: 2 VQ-VAE-2 Модель VQ-VAE-2 — это расширение VQ-VAE. Она показывает значительный скачок по качеству генерируемых изображений: 2 Впечатляет то, что на картинке именно результат семплирования из выученного моделью распределения, а не результат реконструкции. Первое основное отличие модели VQ-VAE от VQ-VAE-2 — использование иерархических латентных переменных: 2 Прежде чем перейти к описанию архитектуры, хочется сделать небольшой дисклеймер: когда в тексте далее будет говориться «тензор размера M × M M×M», то будет иметься в виду, что тензор имеет шейп (B,M,M,C), где первая размерность соответствует батчам, а последняя — каналам. На картинке показан пример двухуровневой архитектуры (хотя уровней может быть и больше). Каждому уровню соответствуют свои энкодер, декодер и набор кодовых векторов (общей размерности D D для всех уровней). Обозначим нижний и верхний энкодеры как E n c bottom Enc bottom и E n c top Enc top , а декодеры — как D e c bottom Dec bottom и D e c top Dec top . E n c bottom Enc bottom принимает на вход трёхканальную картинку размера 256 × 256 256×256 пикселей, отображает её в тензор размера 64 × 64 64×64 и передаёт на вход E n c top Enc top . E n c top Enc top выдаёт тензор размера 32 × 32 32×32, который затем отображается в тензор из кодовых векторов z top z top (квантизуется) z top z top передаётся на вход D e c top Dec top , затем выходы E n c bottom Enc bottom и D e c top Dec top конкатенируются и квантизуются в z bottom z bottom z top z top и z bottom z bottom конкатенируются и передаются на вход D e c bottom Dec bottom , который отображает их в исходную картинку Для обучения модели используется почти такой же лосс, как для VQ-VAE. Для VQ-VAE он имел вид: L = log L=logp(x∣z q (x))+∥sg[z e (x)]−z q (x)∥ 2 2 +β∥z e (x)−sg[z q (x)]∥ 2 2 Для VQ-VAE-2 первое и третье слагаемые сохраняют свой вид, а второе слагаемое заменяется на обновление кодовых векторов e i",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 12,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "c bottom Enc bottom и D e c top Dec top конкатенируются и квантизуются в z bottom z bottom z top z top и z bottom z bottom конкатенируются и передаются на вход D e c bottom Dec bottom , который отображает их в исходную картинку Для обучения модели используется почти такой же лосс, как для VQ-VAE. Для VQ-VAE он имел вид: L = log L=logp(x∣z q (x))+∥sg[z e (x)]−z q (x)∥ 2 2 +β∥z e (x)−sg[z q (x)]∥ 2 2 Для VQ-VAE-2 первое и третье слагаемые сохраняют свой вид, а второе слагаемое заменяется на обновление кодовых векторов e i e i с помощью экспоненциального скользящего среднего. Пусть E(x) (t) — выход энкодера на шаге t t, выпрямленный в двумерный тензор, последняя размерность которого равна размерности D D кодовых векторов. Пусть i,1 (t) ,…,E i,n i (t) (t) } — множество из (t) векторов, для которых на шаге t t ближайшим оказался кодовый вектор (t−1) . Тогда обновление e i e i на шаге t t происходит по следующим формулам: (t) = N i (t) m i (t) (t) =m i (t−1) ⋅γ+ j ∑ n i (t) E(x) i,j (t) (1−γ) (t) =N i (t−1) ⋅γ+n i (t) (1−γ) Здесь γ γ — некоторый вещественный параметр. Так же, как и для VQ-VAE, априорное распределение для VQ-VAE-2 выучивается отдельно уже после обучения основной модели, но в случае VQ-VAE-2 оно имеет иерархическую структуру. На картинке изображён пример такого распределения для двухуровневой архитектуры: 2 Для каждого уровня обучается отдельная модель PixelCNN: одна — на кодовых векторах первого уровня, вторая — на кодовых векторах первого и второго уровней. Обе модели также принимают на вход метку класса, изображение из которого нужно насемплировать. Семплирование из финальной модели происходит так: семплируются векторы e top e top из верхнего распределения из нижнего распределения семплируются векторы e bottom e bottom при условии векторов e top e top декодер принимает на вход векторы e top e top и e bottom e bottom и выдаёт финальную картинку Результаты семплирования из двухуровневой модели VQ-VAE-2, обученной на ImageNet: 2 А это — результаты семплирования из трёхуровневой модели VQ-VAE-2, обучавшейся на FFHQ: 2 DALL-E Одна из недавних работ, связанных с VAE, — это DALL-E от OpenAI. Они обучили модель с 12 миллиардами параметров, генерирующую картинки по их текстовому описанию. Для обучения авторами был собран датасет, состоящий из 250 миллионов пар картинок и их описаний. Вот примеры работы этой модели: 2 2 В блог-посте OpenAI, посвящённом DALL-E, есть возможность самостоятельно составлять текстовые описания из некоторого ограниченного словаря и смотреть на результаты. Осторожно, это затягивает 😃 2 DALL-E идейно основывается на результатах VQ-VAE: сначала выучиваются кодовые векторы для картинок, а затем обучается Трансформер, моделирующий совместное априорное распределение текстов и кодовых векторов. Подробнее о трансформерах мы рассказывали в главе 6.3 этого хендбука. В DALL-E задействована архитектура, основанная на декодер-части исходной архитектуры Трансформера, поэтому стоит также почитать про модель GPT-2, работающую аналогичным образом. Обучение проходит в две стадии: Сначала обучается дискретизованный VAE (dVAE) c энкодером для сжатия RGB-картинок размера 256 × 256 256×256 в тензор из 32",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 13,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "OpenAI, посвящённом DALL-E, есть возможность самостоятельно составлять текстовые описания из некоторого ограниченного словаря и смотреть на результаты. Осторожно, это затягивает 😃 2 DALL-E идейно основывается на результатах VQ-VAE: сначала выучиваются кодовые векторы для картинок, а затем обучается Трансформер, моделирующий совместное априорное распределение текстов и кодовых векторов. Подробнее о трансформерах мы рассказывали в главе 6.3 этого хендбука. В DALL-E задействована архитектура, основанная на декодер-части исходной архитектуры Трансформера, поэтому стоит также почитать про модель GPT-2, работающую аналогичным образом. Обучение проходит в две стадии: Сначала обучается дискретизованный VAE (dVAE) c энкодером для сжатия RGB-картинок размера 256 × 256 256×256 в тензор из 32 × 32 = 1024 32×32=1024 кодовых векторов. Эта стадия обучения очень напоминает VQ-VAE, но вместо добавления в лосс дополнительных слагаемых для кодовых векторов авторы DALL-E используют релаксацию Гумбеля — трюк, позволяющий проводить честное дифференцирование по параметрам энкодера. Об обучении dVAE мы будем говорить подробнее далее. Затем обучается Трансформер (точнее, только декодер-часть исходной архитектуры Трансформера), задача которого — выучить совместное распределение картинок и их текстовых описаний. Он принимает на вход конкатенацию из эмбеддингов текстовых токенов и кодовых векторов картинок и учится для каждой входной последовательности предсказывать её продолжение. О некоторых деталях обучения Трансформера также будет рассказано далее. Инференс обученной модели происходит так: эмбеддинги текстового описания картинки подаются на вход Трансформеру, и он авторегрессионно предсказывает кодовые векторы картинки, соответствующей этому описанию, а затем полученные кодовые векторы пропускаются через декодер dVAE. dVAE Обучение dVAE происходит путём максимизации ELBO для картинок x x и их дискретных латентных представлений log lnp θ (x)≥E q ϕ (z∣x) [logp θ (x∣z)]−βD KL (q ϕ (z∣x)∥p(z)), где ϕ ϕ и θ θ — параметры энкодера и декодера дискретизованного VAE, a p ( z ) p(z) — равномерное категориальное распределение над кодовыми векторами. Здесь можно заметить дополнительный коэффициент β β, который в стандартном VAE всегда равен 1. Однако авторы DALL-E ввели дополнительный параметр β β, опираясь на результаты статьи о β β-VAE. Но, в отличие от исходной статьи, в их экспериментах значение b e t a beta постепенно понижается в ходе обучения. Энкодер dVAE отображает картинки размера 256 × 256 256×256 в тензор (x) с шейпом 32 × 32 × 8192 32×32×8192, где 8192 8192 — число кодовых векторов. То есть каждой из 32 × 32 32×32 позиций энкодер сопоставляет категориальное распределение над 8192 8192 кодовыми векторами, параметризованное выходными логитами. Для получения тензора (x) из кодовых векторов можно было бы сначала применить softmax softmax к распределениям на каждой из 32 × 32 32×32 позиций, а затем сопоставить каждой позиции кодовый вектор, номеру которого соответствует максимальная вероятность (взять argmax argmax для этой позиции). Однако операция argmax argmax не дифференцируема, и, к тому же, в концепции VAE на вход декодеру должен пойти семпл из распределения, предсказываемого энкодером, а взятие argmax argmax на каждой позиции не является семплированием из предсказанного распределения. Поэтому нам потребуется применение некоторых трюков, которые позволят нам одновременно: аппроксимировать семплирование из softmax softmax сделать семплирование дифференцируемым Gumbel-Max Trick и Gumbel-Softmax Первый трюк известен в англоязычной литературе как Gumbel-Max Trick. Представим, что у нас есть логиты-выходы сетки ,…,x k ,",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 14,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "32 × 32 32×32 позиций, а затем сопоставить каждой позиции кодовый вектор, номеру которого соответствует максимальная вероятность (взять argmax argmax для этой позиции). Однако операция argmax argmax не дифференцируема, и, к тому же, в концепции VAE на вход декодеру должен пойти семпл из распределения, предсказываемого энкодером, а взятие argmax argmax на каждой позиции не является семплированием из предсказанного распределения. Поэтому нам потребуется применение некоторых трюков, которые позволят нам одновременно: аппроксимировать семплирование из softmax softmax сделать семплирование дифференцируемым Gumbel-Max Trick и Gumbel-Softmax Первый трюк известен в англоязычной литературе как Gumbel-Max Trick. Представим, что у нас есть логиты-выходы сетки ,…,x k , и мы хотим с их помощью получить семпл из категориального распределения, то есть стохастически предсказать класс. Для этого мы обычно применяем к логитам softmax softmax, чтобы получить вероятности exp ⁡ x i ∑ j exp expx j expx i , а затем из получившегося категориального распределения ,…,π k } семплируем класс. Оказывается, этим двум шагам будет эквивалентна следующая процедура: насемплировать числа ,...,g k из стандартного распределения Гумбеля, прибавить к каждому из логитов x i x i семпл g i g i , выбрать класс j j, такой что j = argmax j=argmax О том, почему это действительно так, можно почитать здесь. Однако сам по себе Gumbel-Max Trick нам не поможет — ведь операция так и не стала дифференцируемой. Поэтому придётся использовать ещё один трюк, предложенный практически одновременно в двух статьях (первая и вторая) и названный Gumbel-Softmax в одной из них. Чтобы описать этот трюк, отметим, что результат операции argmax argmax — это индекс некоторого класса j j. Такой индекс можно описать one-hot кодированием, то есть вектором длиной k k, в котором все элементы равны нулю, кроме j j-го, который равен единице. Gumbel-Softmax состоит в том, чтобы вместо взятия argmax argmax на последнем этапе Gumbel-Max Trick делать следующее: вычислить y i = exp exp j=1 k exp((x j +g j )/τ) exp((x i +g i )/τ) i=1,…,k, — аппроксимацию one-hot при помощи softmax softmax с температурой сложить кодовые векторы e i e i с весами z=∑ i y i e i выдать вектор z z в качестве латентного вектора для данной позиции На самом деле авторы DALL-E не уточняли, как выходной вектор z z агрегируется из кодовых векторов и y i y i , но такой подход применён в реализации DALL-E на PyTorch. При τ → 0 τ→0 семплирование из распределения exp exp j=1 k exp((x j +g j )/τ) exp((x i +g i )/τ) стремится к argmax argmax, и в процессе обучения dVAE авторы постепенно уменьшали значение τ τ. На следующей картинке слева — просто Gumbel-Max Trick, а справа — дифференцируемый вариант Gumbel-Max Trick: 2 Таким образом, для обучения кодовых векторов для dVAE не требуется дополнительных слагаемых в лоссе относительно ELBO, а также копирования градиентов из декодера в энкодер (как было в VQ-VAE). Кроме того, стоит отметить, что (z∣x)∥p(z)) в данном случае не вырождается в константу, а действительно действует как регурялизатор, заставляя категориальное распределение, параметризованное логитами энкодера, быть ближе к равномерному распределению над кодовыми векторами. Распределение Logit-Laplace Ещё один трюк",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 15,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i +g i )/τ) стремится к argmax argmax, и в процессе обучения dVAE авторы постепенно уменьшали значение τ τ. На следующей картинке слева — просто Gumbel-Max Trick, а справа — дифференцируемый вариант Gumbel-Max Trick: 2 Таким образом, для обучения кодовых векторов для dVAE не требуется дополнительных слагаемых в лоссе относительно ELBO, а также копирования градиентов из декодера в энкодер (как было в VQ-VAE). Кроме того, стоит отметить, что (z∣x)∥p(z)) в данном случае не вырождается в константу, а действительно действует как регурялизатор, заставляя категориальное распределение, параметризованное логитами энкодера, быть ближе к равномерному распределению над кодовыми векторами. Распределение Logit-Laplace Ещё один трюк в обучении dVAE касается выходного распределения (x∥z). Авторы DALL-E подметили проблему, возникающую при часто встречающемся выборе лапласовского и гауссовского распределений в качестве (x∥z): оба они определены на всей вещественной прямой, в то время как пиксели принимают значения из ограниченного интервала. Таким образом, часть плотности при моделировании «теряется», оказываясь вне возможных границ значений пикселей. Чтобы исправить эту проблему, авторы предлагают использовать распределение, которое они назвали “Logit-Laplace”. Его плотность определена на интервале ( 0 , 1 ) (0,1) и выражается следующей формулой: exp ⁡ ( − ∣ logit f(x∣μ,b)= 2bx(1−x) 1 exp(− b ∣logit(x)−μ∣ ), logit logit(x)= 1−x x Эта плотность соответствует случайной переменной, полученной применением сигмоиды к распределённой по Лапласу случайной переменной. Выражение для распределения Logit-Laplace можно получить по стандартной формуле для плотности случайной величины, полученной применением монотонной дифференцируемой функции к другой случайной величине (см. формулу, например, тут). Логарифм этой плотности подставляется в ELBO вместо lnp θ (x∥z). Декодер на выходе выдаёт 6 тензоров: первые три соответствуют μ μ для RGB-каналов, оставшиеся три соответствуют ln ⁡ b lnb, и эти 6 тензоров используются для подсчёта лосса. При подаче в энкодер значения картинок нормируются функцией ϕ : [ 0 , 255 ϕ:[0,255]→(ε,1−ε): 255 x + ε ϕ:x↦ 255 1−2ε x+ε Этим авторы добиваются того, чтобы декодер моделировал значения из (ε,1−ε), что позволяет нивелировать вычислительные проблемы, связанные с делением на x(1−x) в формуле плотности. Во время инференса реконструкция x ^ x ^ картинки x x вычисляется по формуле: sigmoid (sigmoid(μ)), где μ μ — первые три тензора из выхода декодера. Выходы, соответствующие ln ⁡ b lnb, при этом не используются. Априорное распределение на текстах и картинках На втором этапе авторы фиксируют параметры ϕ ϕ и θ θ и моделируют совместное распределение картинок и их текстовых описаний с помощью Sparse Transformer с 12 миллиардами параметров. На вход он получает конкатенацию из текстового описания картинки и её кодовых векторов. Картинка представляется 1024 кодовыми векторами, получаемыми из энкодера q ϕ q ϕ , причём при семплировании кодовых последовательностей используется обычный argmax argmax без добавления шума из распределения Гумбеля. Текстовое описание токенизируется с помощью процедуры BPE (см. раздел про BPE здесь), и каждому токену ставится в соответствие представляющий его вектор из вещественных чисел (эмбеддинг). Для представления текста используется не более 256 токенов, а размер используемого словаря — 16 384 токена. Задача Трансформера во время обучения — для каждого начального отрезка входной последовательности предсказать следующий за ним токен. Это может быть как текстовый токен, так и кодовый вектор",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 16,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и её кодовых векторов. Картинка представляется 1024 кодовыми векторами, получаемыми из энкодера q ϕ q ϕ , причём при семплировании кодовых последовательностей используется обычный argmax argmax без добавления шума из распределения Гумбеля. Текстовое описание токенизируется с помощью процедуры BPE (см. раздел про BPE здесь), и каждому токену ставится в соответствие представляющий его вектор из вещественных чисел (эмбеддинг). Для представления текста используется не более 256 токенов, а размер используемого словаря — 16 384 токена. Задача Трансформера во время обучения — для каждого начального отрезка входной последовательности предсказать следующий за ним токен. Это может быть как текстовый токен, так и кодовый вектор картинки. Поскольку кодовые векторы картинок всегда идут за текстовыми токенами, при генерации кодовых векторов attention-механизм учитывает также и все предыдущие текстовые токены. Кроме того, маска attention для кодовых векторов учитывает, что исходно они расположены не линейно друг за другом, а на прямоугольной сетке. В статье приводится несколько вариантов геометрических паттернов, которые использовались для attention-маски на кодовых векторах. В качестве лосса используется взвешенная сумма кросс-энтропии для текстовых токенов и кросс-энтропии для кодовых векторов картинок c весами соответственно (больший приоритет отдаётся генерации картнок, отсюда и больший вес для лосса). Конечно, огромный Трансформер обучить крайне непросто, и очень существенная часть статьи посвящена трюкам, которые авторы применили для обучения такой большой модели. 2 Инференс На этапе инференса в модель подаются токены текстового описания картинки, и на их основании модель авторегрессионно предсказывает кодовые векторы: 2 Кодовые векторы картинки подаются в декодер dVAE, который отображает их в финальную картинку: 2 Для повышения качества предсказания авторы сначала генерируют 512 картинок для каждого текстового описания, а затем выбирают лучшую картинку из предсказанных. Разные наборы кодовых векторов для одного и того же текста можно получить, например, случайно выбирая на каждом шаге генерации какой-то кодовый вектор согласно предсказанному Трансформером распределению. Ранжирование полученных 512 картинок осуществляется с помощью CLIP — большой нейросети, обучавшейся в режиме без учителя на большом количестве данных моделировать совместное распределение картинок и текстов. Заключение Итак, в этом параграфе мы поговорили о том, как устроен VAE в классическом смысле, — с непрерывным распределением латентных переменных, а также поговорили о работах, основанных на идеях использования дискретных распределений для VAE. Конечно, различные модификации VAE не исчерпываются только лишь отказом от непрерывных латентных переменных в пользу дискретных. Есть множество других возможных направлений для улучшения модели: использование иерархических латентных распределений (которые мы, кстати, видели в контексте VQ-VAE-2), использование функций потерь, отличающихся от ELBO, выбор различных форм латентных пространств, применение adversarial-обучения и многое другое. Хороший список различных статей, посвящённых модификациям VAE, можно найти здесь. Из недавних работ, связанных с применением иерархических распределений, интересной кажется NVAE — семплы из модели выглядят весьма впечатляюще. Про неё есть хороший видеообзор от Yannic Kilcher. На этом мы завершаем рассказ о VAE. Будем надеяться, что он дал вам общее представление и об исходных идеях, из которых выросла модель VAE, и о наиболее интересных последних результатах, связанных с ней. А в следующем параграфе мы поговорим о генеративно-состязательных сетях. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.1. Введение в генеративное моделирование Следующий",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 17,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "другое. Хороший список различных статей, посвящённых модификациям VAE, можно найти здесь. Из недавних работ, связанных с применением иерархических распределений, интересной кажется NVAE — семплы из модели выглядят весьма впечатляюще. Про неё есть хороший видеообзор от Yannic Kilcher. На этом мы завершаем рассказ о VAE. Будем надеяться, что он дал вам общее представление и об исходных идеях, из которых выросла модель VAE, и о наиболее интересных последних результатах, связанных с ней. А в следующем параграфе мы поговорим о генеративно-состязательных сетях. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.1. Введение в генеративное моделирование Следующий параграф 8.3. Генеративно-состязательные сети (GAN)",
    "metadata": {
      "title": "Variational Autoencoder (VAE)",
      "url": "https://education.yandex.ru/handbook/ml/article/variational-autoencoder-(vae)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.2",
      "part": 18,
      "total_parts": 18,
      "source_file": "8.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Генеративно-состязательные сети (Generative Adversarial Networks, GAN) – это большой класс генеративных моделей, общая черта которых заключается в том, что они обучаются одновременно с другой сетью, которая старается отличить сгенерированные объекты от настоящих. В этом параграфе мы рассмотрим основы основ GAN-ов, интуитивное объясним принципы их работы, а также детально погрузимся в многочисленные приёмы и модификации оригинального подхода, которые применяются в наиболее успешных моделях. Мы также приведём примеры нескольких типов практических задач, в которых применяются генеративно-состязательные сети. mnist Генеративно-состязательные сети — это неявная генеративная модель. То есть она не восстанавливает плотность данных в явном виде, но умеет сэмплировать из распределения данных. Самый простой и эффективный дизайн генеративных моделей, которые умеют только сэмплировать, но не умеют оценивать плотность, – это отображение одних случайных величин в другие. Подобного вида модель после обучения работает следующим образом: пусть x x – случайная величина, обозначающая сэмпл из распределения нужных нам данных (например, картинок с нарисованными цифрами), а z z – сэмпл из какого-то распределения, который нам легко получить (например, каждая его компонента берётся из стандартного нормального). Тогда, если у нас есть обученная функция G G, которая переводит сэмплы из p ( z ) p(z) в сэмплы из p ( x ) p(x), то процесс генерации происходит в два этапа: сначала мы случайным образом получаем вектор z∼p(z), а затем отображаем его в =G(z): z∼p(z), x ^ =G(z): x ^ ∼p(x) Ключевым вопросом в таких моделях является соотношение размерностей z z и x x. Есть генеративные модели, где dim ( z ) ≈ dim ( x ) dim(z)≈dim(x). Примером таких подходов являются, например, нормализующие потоки. В случае генеративных состязательных сетей (как и другого класса популярных генеративных моделей, вариационных автоэнкодеров), dim ( z ) ≪ dim ( x ) dim(z)≪dim(x). Поэтому работу этих моделей можно рассматривать как поиск многообразия размерности dim ( z ) dim(z) среди всех случайных примеров из домена, на котором определяется p ( x ) p(x). Например, в случае генерации цифр это соответствует поиску в домене [0,1] H×W , где H H – это ширина картинки, а W W – её высота, подмножества, в котором каждый элемент изображает какую-либо цифру. Таким образом, задача обучения генеративных состязательных сетей может рассматриваться как задача компрессии данных в низкоразмерное представление. Основы обучения GAN-ов counterfeiter Классическая аналогия того, как учатся GANы — это фальшивомонетчик и полицейский. Задача фальшивомонетчика — научиться создавать купюры, которые полицейский не сможет отличить от реальных. Задача полицейского — научиться отличать купюры фальшивомонетчика от настоящих. Чтобы понять, как обучаются GANы, надо представить себе следующий мысленный эксперимент. Допустим, фальшивомонетчик и полицейский — друзья, которые решили поучиться друг у друга. Фальшивомонетчик создаёт несколько фальшивых купюр и показывает полицейскому. Полицейский говорит фальшивомонетчику, какие из его купюр, по его мнению, поддельные, а какие — настоящие. Фальшивомонетчик запоминает отзыв полицейского и в следующий раз улучшит свои купюры на основе отзыва от полицейского. Сам полицейский при этом тоже учится: он запоминает, что купюры, которые он видел — поддельные. В нашем мысленном эксперименте представим, что фальшивомонетчик взаимодействует с полицейским много раз. Что получается в результате? С каждым разом купюры фальшивомонетчика всё труднее отличить от настоящих. И",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 1,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "GANы, надо представить себе следующий мысленный эксперимент. Допустим, фальшивомонетчик и полицейский — друзья, которые решили поучиться друг у друга. Фальшивомонетчик создаёт несколько фальшивых купюр и показывает полицейскому. Полицейский говорит фальшивомонетчику, какие из его купюр, по его мнению, поддельные, а какие — настоящие. Фальшивомонетчик запоминает отзыв полицейского и в следующий раз улучшит свои купюры на основе отзыва от полицейского. Сам полицейский при этом тоже учится: он запоминает, что купюры, которые он видел — поддельные. В нашем мысленном эксперименте представим, что фальшивомонетчик взаимодействует с полицейским много раз. Что получается в результате? С каждым разом купюры фальшивомонетчика всё труднее отличить от настоящих. И с каждым разом умение выявлять поддельные купюры у полицейского выше. Важный вопрос для понимания работы GANов: в какой момент мы можем утверждать, что фальшивомонетчик хорошо подделывает купюры? Ответ: Когда фальшивомонетчик сможет обманывать сильного полицейского. В начале нашего эксперимента полицейский плохо отличает подделку от оригинала. Поэтому обмануть его можно купюрами плохого качества. Нам же интересно получить фальшивомонетчика, который будет выдавать купюры, неотличимые от оригинала даже профессионалом. Рассмотрим задачу обучения более формально. Пусть у нас есть генератор G θ G θ (фальшивомонетчик) с параметрами θ θ, и дискриминатор D ϕ D ϕ (полицейский) с параметрами ϕ ϕ. Генератор отображает векторы z∼N(0,I) в ∼q(x), распределение которых приближает реальное распределение данных p ( x ) p(x). Дискриминатор каждому реальному сэмплу x x и фейковому x ^ x ^ ставит в соответствие вероятность D ( x ) D(x), которая оценивает степень принадлежности x x к реальным данным, т.е. он решает задачу бинарной классификации. Самый простой способ это сделать – при помощи минимизации бинарной кросс-энтропии: min log log min E x∼p(x) −logD ϕ (x)+E x ^ ∼q(x) −log[1−D ϕ ( x ^ )]. Учитывая обозначение (z), и то, что мы пытаемся максимизировать вероятность принадлежности к реальным данным, как её оценивает дискриминатор, задачу, которую решает генератор, можно расписать следующим образом (используя свойство выпуклости логарифма): θ ∗ = arg ⁡ max arg ⁡ min arg ⁡ min arg ⁡ min log arg ⁡ max log arg θ max E z∼p(z) D θ (G θ (z)) arg θ min E z∼p(z) −D θ (G θ (z)) arg θ min E z∼p(z) [1−D θ (G θ (z))] arg θ min E z∼p(z) log[1−D θ (G θ (z))] arg θ max E z∼p(z) −log[1−D θ (G θ (z))]. Это равенство позволяет записать задачи, которые решают генератор и дискриминатор, вместе. (Мы также избавимся от лишних минусов, сделав так, чтобы дискриминатор решал задачу максимизации.) min ⁡ θ max log log min ϕ max E x∼p(x) logD ϕ (x)+E z∼p(z) log[1−D ϕ (G θ (z))]. Получается, что на самом деле генератор и дискриминатор пытаются оптимизировать одну функцию: генератор её минимизирует, а дискриминатор максимизирует. Обозначим эту функцию (минус бинарную кросс-энтропию) как L θ , ϕ L θ,ϕ . Тогда эту задачу оптимизации можно записать в сокращённом виде: min ⁡ θ max min ϕ max L θ,ϕ . По параметрам дискриминатора минимум бинарной кросс-энтропии (или минимум L θ , ϕ L θ,ϕ по ϕ ϕ) достигается на следующей функции – оптимальном дискриминаторе для фиксированного генератора: (x)=",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 2,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "максимизации.) min ⁡ θ max log log min ϕ max E x∼p(x) logD ϕ (x)+E z∼p(z) log[1−D ϕ (G θ (z))]. Получается, что на самом деле генератор и дискриминатор пытаются оптимизировать одну функцию: генератор её минимизирует, а дискриминатор максимизирует. Обозначим эту функцию (минус бинарную кросс-энтропию) как L θ , ϕ L θ,ϕ . Тогда эту задачу оптимизации можно записать в сокращённом виде: min ⁡ θ max min ϕ max L θ,ϕ . По параметрам дискриминатора минимум бинарной кросс-энтропии (или минимум L θ , ϕ L θ,ϕ по ϕ ϕ) достигается на следующей функции – оптимальном дискриминаторе для фиксированного генератора: (x)= p(x)+q(x) p(x) . Её оптимальность нетрудно проверить, используя выпуклость логарифма. Учитывая это, и формулу для L L, интуицию работы метода обучения GANов со стороны генератора можно сформулировать следующим образом: Мы замеряем, насколько реалистичными являются сгенерированные сэмплы ,…, x ^ 2 , используя для этого оптимальный дискриминатор D_ϕ ∗ (x). Мы хотим увеличить отклик дискриминатора на каждом сэмпле, т.е. пытаемся модифицировать каждый предсказанный элемент так, чтобы на нём стало выше значение D_ϕ Ещё более простую интуицию для этой задачу можно сформулировать следующим образом. Как нужно модифицировать плотность q ( x ) q(x), чтобы она стала ближе к p ( x ) p(x), если к плотности распределения мы имеем доступ только через сэмплы из него? Визуализацию желаемых градиентов по случайным сэмплам для задачи сопоставления двух гауссиан можно видеть на графике ниже, где f(x)=D_ϕ ∗ (x). Artboard Направленные вниз стрелки показывают, насколько нужно уменьшить координаты точек из распределения q ( x ) q(x), чтобы получилось нечто максимально похожее на p ( x ) p(x). То есть на самом деле точки будут сдвигаться на то же самое расстояние влево. Формализуем эту интуицию, и заодно поймём, почему вообще такой метод должен работать. Подставив выражение для оптимального дискриминатора в L L, мы можем избавиться от внутренней максимизации в исходной задаче и оставить только внешнюю минимизацию по параметрам генератора. Тем самым, мы получим в явном виде функцию потерь, которую минимизирует генератор (обозначим её за D θ D θ ). Для неё мы распишем математическое ожидание через интеграл и упростим дроби: D θ = ⁣ max log log log log log log max L θ,ϕ E x∼p(x) logD ϕ ∗ (x)+E x∼q(x) log[1−D ϕ ∗ (x)] E x∼p(x) log p(x)+q(x) p(x) +E x∼q(x) log[1− p(x)+q(x) p(x) ] ∫p(x)log p(x)+q(x) p(x) dx+∫q(x)log p(x)+q(x) q(x) dx Упростим выражение для D ( x ) D(x) ещё раз, прибавив и отняв константу log ⁡ 4 log4, а также учитывая, что ∫p(x) dx=1 и ∫q(x) dx=1: D θ = − log log log log log −log4+∫p(x)log p(x)+q(x) 2p(x) dx+∫q(x)log p(x)+q(x) 2q(x) dx −log4+KL(p 2 p+q )+KL(q 2 p+q ) −log4+2⋅JSD(p∥q). Здесь KL(P∥Q) означает, как обычно, KL-дивергенцию log KL(P∥Q)=∫P(x)log Q(x) P(x) dx, которая показывает, насколько два распределения отличаются друг от друга. Через JSD(p∥q) обозначает ещё один вид дивергенции (её называют дивергенцией Йенсена-Шеннона). Получается, что при оптимальном дискриминаторе генератор, решая внешнюю задачу оптимизации, уменьшает расстояние между распределениями реальных и фейковых данных, действительно приближая их друг к другу! Исходя из этого, и в предположении достаточной capacity",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 3,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log ⁡ 4 log4, а также учитывая, что ∫p(x) dx=1 и ∫q(x) dx=1: D θ = − log log log log log −log4+∫p(x)log p(x)+q(x) 2p(x) dx+∫q(x)log p(x)+q(x) 2q(x) dx −log4+KL(p 2 p+q )+KL(q 2 p+q ) −log4+2⋅JSD(p∥q). Здесь KL(P∥Q) означает, как обычно, KL-дивергенцию log KL(P∥Q)=∫P(x)log Q(x) P(x) dx, которая показывает, насколько два распределения отличаются друг от друга. Через JSD(p∥q) обозначает ещё один вид дивергенции (её называют дивергенцией Йенсена-Шеннона). Получается, что при оптимальном дискриминаторе генератор, решая внешнюю задачу оптимизации, уменьшает расстояние между распределениями реальных и фейковых данных, действительно приближая их друг к другу! Исходя из этого, и в предположении достаточной capacity генератора и дискриминатора (т.е. предполагая, что их параметризация позволяет достичь оптимума), мы можем сформулировать первый, наивный алгоритм обучения генеративно-состязательных сетей. Решить внутреннюю задачу максимизации по ϕ ϕ, повторяя шаги ниже до сходимости по параметрам дискриминатора ϕ ϕ к оптимальному значению ϕ ∗ ϕ ∗ : — Составить мини-батч сэмплов шума ,…,z p(z). — Составить мини-батч сэмплов данных ,…,x p(x). — Обновить дикриминатор, сделав шаг вверх по его градиенту: log log i=1 ∑ n [logD ϕ (x i )+log(1−D ϕ (G θ (z i )))] Сделать шаг SGD для внешней задачи минимизации по θ θ: — Составить мини-батч сэмплов шума ,…,z n } из p ( z ) p(z). — Обновить генератор, сделав шаг вниз по его градиенту: log i=1 ∑ n log(1−D )))= n 1 i=1 ∑ n − 1−f(G f(G θ (z i )) , где через f f мы для краткости обозначили (x). Какие у этого наивного подхода могут быть недостатки? Во-первых, он очень медленный, потому что необходимо обучать дискриминатор до сходимости, чтобы сделать всего один шаг по градиенту генератора. Но вторая проблема намного серьёзнее: функция потерь генератора может насыщаться и выдавать близкие к нулю градиенты. Проиллюстрируем это на примере обучения простой модели, которая будет сэмплировать из одномерной гауссианы с заданными параметрами. Artboard Распределение p ( x ) p(x) в этом случае известно, а распределение q ( x ) q(x) мы можем получить с помощью методов оценки плотности по сэмплам. Визуализируем эти плотности, а также градиент по сэмплам из генератора (a). Видно, что в случае, когда пики распределений плохо пересекаются друг с другом, градиент будет равен нулю на большинстве сэмплов, которые выдаёт генератор, т.е. они никак не будут использоваться для обучения. Чтобы понять причину происходящего, давайте посмотрим на градиент функции потерь генератора. На точках, далёких от основной «массы» p ( x ) p(x), дискриминатор выдаёт что-то близкое к нулю, то есть знаменатель градиента практически не будет ни на что влиять, а в числителе тоже будет практически ноль: ведь если мы немного поменяем параметры генератора, то «плохие» точки по-прежнему будут далеки от p ( x ) p(x), так что изменение лосса будет пренебрежимо малым, и градиент тоже. Это приводит к тому, что обучение происходит недостаточно эффективно: мы тратим время на вычисление сэмплов, которые не делают никакой вклад в обновление параметров генератора. Но более существенная проблема возникает в вырожденном случае: если изначально два распределения практически не пересекаются своими плотностями (b): в этом случае процесс обучения практически не идёт. Часто ли",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 4,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нулю, то есть знаменатель градиента практически не будет ни на что влиять, а в числителе тоже будет практически ноль: ведь если мы немного поменяем параметры генератора, то «плохие» точки по-прежнему будут далеки от p ( x ) p(x), так что изменение лосса будет пренебрежимо малым, и градиент тоже. Это приводит к тому, что обучение происходит недостаточно эффективно: мы тратим время на вычисление сэмплов, которые не делают никакой вклад в обновление параметров генератора. Но более существенная проблема возникает в вырожденном случае: если изначально два распределения практически не пересекаются своими плотностями (b): в этом случае процесс обучения практически не идёт. Часто ли встречается такая вырожденная ситуация на практике? Довольно часто! Достаточно представить себе ситуацию, когда мы хотим генерировать реалистичные изображения лиц, а генератор в начале обучения вместо этого выдаёт случайный шум. Из-за наличия такой проблемы описанная выше функция потерь генератора называется «сатурирующей». В оригинальном подходе по обучению генеративно-состязательных сетей было предложено два решения этой проблемы. Во-первых, мы можем обучать дискриминатор на каждой итерации не до сходимости, а с небольшим фиксированным числом шагов N N (на практике чаще всего используется N ≤ 2 N≤2). Это позволяет существенно улучшить исходную ситуацию с переобучением дискриминатора. Также мы могли бы улучшить функцию потерь для генератора, сделав так, чтобы она сглаживала выходы дискриминатора около нуля. В качестве такой функции изначально был предложен логарифм. Нетрудно видеть, что оптимум улучшенной функции потерь («несатурирующий лосс») совпадает с исходной, что позволяет сохранить все описанные выше теоретические гарантии: θ ∗ = arg ⁡ min arg ⁡ min log arg ⁡ min log arg θ min L θ,ϕ arg θ min E z∼p(z) log[1−D ϕ (G θ (z))] arg θ min E z∼p(z) −logD ϕ (G θ (z)). Точка минимума у новой функции потерь та же, что у исходной, а градиенты оказываются ненулевыми на всех сгенерированных сэмплах. Artboard Помимо этого, на практике вместо обычного метода стохастического градиентного спуска используются его модификации, которые учитывают и первые, и вторые моменты градиентов например, Adam. Вообще, GAN-ы – довольно капризные модели, и настоятельно рекомендуется использовать готовые реализации с GitHub, оставляя большую часть гиперпараметров без изменений. Наиболее критичными среди них являются learning rate и расписание (то есть количество обновлений дискриминатора на одно обновление генератора). Метрики качества После успешного обучения генератора хотелось бы также понять, насколько хорошо он работает. Для этого рассмотрим на примере задачи генерации изображений типовые ошибки, которые может совершать GAN. Наиболее частая проблема – плохое качество или наличие артефактов – вызвана ограничениями, связанными с capacity генератора и несовершенством самих методов обучения. Здесь всё просто: наша генеративная модель плохо работает, и мы это видим на сгенерированных сэмплах. Более скрытым видом ошибок является так называемый mode collapse: обученный генератор выдаёт реалистично выглядящие картинки, но они не покрывают всё разнообразие распределения p ( x ) p(x). Например, если наша модель учится генерировать изображения с животными, то она может проигнорировать более редкие виды, а научиться генерировать только наиболее часто встречающиеся. Более экстремальная форма подобного поведения – это когда модель вообще выдаёт вариацию одной картинки. Иногда в литературе общее качество результатов работы нейросети, по аналогии с задачей классификации, измеряется точностью метода",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 5,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "и несовершенством самих методов обучения. Здесь всё просто: наша генеративная модель плохо работает, и мы это видим на сгенерированных сэмплах. Более скрытым видом ошибок является так называемый mode collapse: обученный генератор выдаёт реалистично выглядящие картинки, но они не покрывают всё разнообразие распределения p ( x ) p(x). Например, если наша модель учится генерировать изображения с животными, то она может проигнорировать более редкие виды, а научиться генерировать только наиболее часто встречающиеся. Более экстремальная форма подобного поведения – это когда модель вообще выдаёт вариацию одной картинки. Иногда в литературе общее качество результатов работы нейросети, по аналогии с задачей классификации, измеряется точностью метода (precision), а отсутствие mode collapse измеряется полнотой (recall). Самый простой и действенный способ измерить как precision, так и recall – сгенерировать данные и посмотреть на них, дав экспертную оценку уровня их реализма. Не стоит им пренебрегать! Формализовать этот подход в метрику можно в виде эксперимента, который в литературе называется user study. Например, мы можем сделать опрос экспертов, которым будем показывать два примера, настоящий и сгенерированный, и попросить их угадать, где фейк. Тогда процент неправильных ответов будет являться метрикой качества для нашего метода. Такой опрос в основном показывает степень реализма полученных результатов: есть ли в них какие-то заметные артефакты, соответствуют ли они реальным примерам по своей структуре, и так далее. Отчасти он также замеряет разнообразие примеров: то, насколько они хорошо покрывают носитель распределения p ( x ) p(x). Если обученная модель генерирует очень похожие друг на друга примеры (то есть имеет место существенный mode collapse), то эксперт через несколько примеров научится определять ненастоящие. С другой стороны, если сэмплы в целом разнообразные, но всё равно не полностью покрывают основу целевого распределения, то user study не позволит обнаружить эту проблему. Frechet Inception Distance Есть метрики, с помощью которых можно автоматически проводить тестирование, похожее на user study. Для изображений наиболее используемой является Frechet Inception Distance (FID). Чтобы её посчитать, нам в идеале понадобится нейросеть, предобученная на датасете, который мы генерируем, но на практике во всех случаях используется модель Inception v3, предобученная на датасете ImageNet (отсюда слово Inception в названии метрики). Для того, чтобы понять идею этой метрики, рассмотрим следующий пример: если выходом нейросети является класс (число), то его вероятность можно смоделировать мультиномиальным распределением. Гипотетически, чтобы сравнить два распределения картинок p p и q q, нам достаточно измерить расстояние между двумя мультиномиальными распределениями, построенными на выходах предобученного классификатора после прогона датасетов реальных и сгенерированных данных. Если в распределении q q примеров из каких-то классов будет меньше или больше, чем в p p, то такая метрика будет отличная от нуля. Понятно, что это слишком грубое приближение расстояния между двумя распределениями, т.к. оно практически никак не учитывает реализм получаемых картинок. Поэтому вместо выходов нейросети в FID было предложено использовать признаки с её глубоких слоёв. Они кодируют высокоуровневую семантику изображений, потому что по этим признакам модель предсказывает вероятность принадлежности картинки к тому или иному классу. При этом в них остаётся довольно много информации об исходном изображении и свойств локальных признаков (текстур), которые могут помочь распознать артефакты. Метрика FID работает таким образом, что сравнивает два распределения высокоуровневых признаков для",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 6,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "меньше или больше, чем в p p, то такая метрика будет отличная от нуля. Понятно, что это слишком грубое приближение расстояния между двумя распределениями, т.к. оно практически никак не учитывает реализм получаемых картинок. Поэтому вместо выходов нейросети в FID было предложено использовать признаки с её глубоких слоёв. Они кодируют высокоуровневую семантику изображений, потому что по этим признакам модель предсказывает вероятность принадлежности картинки к тому или иному классу. При этом в них остаётся довольно много информации об исходном изображении и свойств локальных признаков (текстур), которые могут помочь распознать артефакты. Метрика FID работает таким образом, что сравнивает два распределения высокоуровневых признаков для реальных и сгенерированных картинок, используя в качестве их приближения многомерные гауссианы (каждая размерность соответствует одному каналу). Для измерения расстояния между этими двумя распределениями используется метрика Вассерштейна: FID FID=∥μ− μ ^ ∥ 2 +Tr(Σ+ Σ ^ −2(Σ Σ ^ ) 1/2 ), где μ ∈ R C μ∈R Σ∈R C×C – это среднее и матрица ковариаций глубоких признаков C×H×W i=1 N , которые считаются по выборке из N N реальных картинок. При этом как средние, так и матрицы ковариаций считаются по объединению всех признаков со всех картинок без учёта пространственной размерности, т.е. по второй размерности матрицы F∈R C×NHW . То же самое делается для сгенерированных картинок, для них средние и ковариации обозначены как . Минимум этой метрики равен нулю, и достигается в случае, когда статистики, посчитанные по двум распределениям, совпадают. На практике эта метрика используется как для измерения реализма изображений, так и для детектирования mode collapse. Интерполяции в скрытом пространстве Ещё один способ измерения качества, который мы рассмотрим, напрямую связан с тем, что генеративно-состязательные модели эффективно занимаются кодированием потенциально высокоразмерных данных в низкоразмерное представление. Но как для нейросети с большим числом параметров проверить, занимается ли она реальным кодированием или простым запоминанием выборки? Рассмотрим следующий пример. Пусть наша генеративная модель к случайным векторам z z применяет их функцию распределения и отображает векторы в равномерно распределённые на отрезке [ 0 , 1 ] [0,1] числа u u. Упорядочим наш датасет. В качестве случайного сэмпла пусть наша модель выдаёт ту картинку, чей индекс, поделённый на размер датасета, ближе всего к u u. Другими словами, наша генеративная модель будет выдавать случайные картинки из датасета вместо генерации новых картинок. Методы оценки качества, которые мы описали выше, пропустят эту проблему: ведь «сгенерированные» картинки будут в точности совпадать с настоящими. Поэтому для полной проверки качества работы генеративной модели важно понимать, действительно ли она производит сжатие выборки в низкоразмерное представление или просто запоминает обучающие примеры. Одним из тестов на подобное поведение является интерполяция между сгенерированными примерами. Возьмём два случайных вектора p(z). Рассмотрим все векторы, которые лежат между ними z=αz 1 +(1−α)z 2 , α∈[0,1]. К каждому такому вектору z z применим наш генератор и получим x ^ x ^ для промежуточных векторов и для . Для правильно обученного GANа мы должны увидеть следующую картинку: при изменении коэффициента α α изображение x ^ x ^ должно плавно меняться и перетекать из . При этом каждая промежуточная картинка должна быть так же реалистичным сэмплом. ссылка на источник картинки Качество такой интерполяции",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 7,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "представление или просто запоминает обучающие примеры. Одним из тестов на подобное поведение является интерполяция между сгенерированными примерами. Возьмём два случайных вектора p(z). Рассмотрим все векторы, которые лежат между ними z=αz 1 +(1−α)z 2 , α∈[0,1]. К каждому такому вектору z z применим наш генератор и получим x ^ x ^ для промежуточных векторов и для . Для правильно обученного GANа мы должны увидеть следующую картинку: при изменении коэффициента α α изображение x ^ x ^ должно плавно меняться и перетекать из . При этом каждая промежуточная картинка должна быть так же реалистичным сэмплом. ссылка на источник картинки Качество такой интерполяции сложно измерить численно, но если мы видим, что промежуточные результаты меняются случайно без какой-либо связи с семантикой интерполируемых примеров, то это говорит о плохом качестве генератора. Стоит упомянуть, что для сэмплов из нормального распределения, которое обычно имеют векторы z z, намного лучше работает интерполяция по сфере (Slerp), потому что в многомерном пространстве векторы z z практически всегда будут лежать в объёме вокруг сферы диаметра d d , где d d – размерность вектора z z. Интерполяция в скрытом пространстве с недавних пор стала использоваться для генерации анимаций и видео. Ведь анимация — это последовательность кадров, плавно переходящих друг в друга. И если у нас есть обученный GAN для генерации картинок, то нам нужно лишь найти путь в скрытом пространстве таким образом, чтобы набор сгенерированных картинок складывался в анимацию. Более того, в скрытом пространстве можно находить различные интерпретируемые пути. Например, путь, при движении по которому размывается задний фон или меняется причёска. Почитать подробнее про это можно тут. ссылка на источник картинки Ближайшие соседи Ещё одним способом проверить, не запомнил ли генератор датасет, является поиск ближайших соседей по датасету. Для этого следует сгенерировать несколько изображений. Для каждого изображения нужно найти несколько ближайших соседей из датасета. В качестве признаков для картинок можно взять признаки с последних слоёв сети Inception. На соседей стоит посмотреть глазами. Если мы увидим, что ближайшие соседи из датасета визуально совпадают со сгенерированными сэмплами, то это значит, что генератор запомнил сэмплы из датасета. Базовые модели Чтобы лучше понимать современные модели, давайте сначала рассмотрим более базовые модели. Хотя они редко используются напрямую, многие идеи из них легли в основу современных моделей. DCGAN Наиболее простая версия генеративной модели для изображений — это DCGAN (Deep Convolutional GAN, 2015 год). Её до сих пор можно иногда встретить как в литературе, так и на практике. В основе DCGAN лежит простая идея: нейросети, основанные на свёртках, отлично подходят для распознавания изображений, а значит вполне могут подойти и для их генерации. Единственное отличие, которое требуется – это постепенно увеличивать внутри нейросети пространственный размер признаков, а не уменьшать. Для этого в современных нейросетях делается операция nearest upsampling, очень похожая на max pooling. В nearest upsampling пространственное разрешение карты признаков увеличивается за счёт того, что каждый вектор повторяется K K раз по горизонтали и по вертикали. К примеру, после увеличения таким образом карты признаков, состоящей из одной единицы, мы получим квадрат размера K × K K×K из единиц. На практике увеличение размерности происходит по аналогии с размерами пулинга в",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 8,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "отлично подходят для распознавания изображений, а значит вполне могут подойти и для их генерации. Единственное отличие, которое требуется – это постепенно увеличивать внутри нейросети пространственный размер признаков, а не уменьшать. Для этого в современных нейросетях делается операция nearest upsampling, очень похожая на max pooling. В nearest upsampling пространственное разрешение карты признаков увеличивается за счёт того, что каждый вектор повторяется K K раз по горизонтали и по вертикали. К примеру, после увеличения таким образом карты признаков, состоящей из одной единицы, мы получим квадрат размера K × K K×K из единиц. На практике увеличение размерности происходит по аналогии с размерами пулинга в свёрточных дискриминативных сетях и почти всегда равно K = 2 K=2. Модель DCGAN. Ссылка на источник картинки. Таким образом, генератор в случае DCGAN является последовательностью свёрток, слоёв батч нормализации, нелинейностей и слоёв upsampling, а дискриминатор – обычной классификационной нейросетью. При этом первым слоем в генераторе является линейный слой, который отображает вектор шума z z в карту признаков с начальным разрешением (как правило, размера 4 × 4 4×4). Результат работы DCGAN. Ссылка на источник картинки. Хотя результаты работы DCGAN довольно смазанные, эта модель показала большие перспективы генеративных нейросетей для изображений. Так выглядит интерполяция в скрытом пространстве для модели DCGAN. Ссылка на источник картинки. Условная генерация Допустим, что в нашем датасете есть изображения, относящиеся к разным классам, и мы хотели бы уметь генерировать изображение заданного класса. В этом случае речь идёт об условной генерации. В качестве условия может выступать не только метка класса, но и объект любой природы. Например, когда вы можете захотеть сгенерировать изображение по текстовому описанию. Далее будем обозначать условие как y y. Наша задача — построить генератор, который бы моделировал p(x∣y). Conditional GAN Самый основной метод условной генерации — конкатенация условия с вектором шума, который генератор принимает на вход. В статье Conditional GAN 2014 года, где предложили этот метод, рекомендовалось подавать условие не только в генератор, но и в дискриминатор. Модель Conditional GAN. Ссылка на источник картинки. Если мы генерируем векторные данные, то вектор на вход дискриминатора подаётся конкатенированным с y y. При этом если если y y — это метка класса, то стоит её закодировать с помощью one-hot encoding. Если же мы работаем с изображениями, то нам из вектора условия следует сделать изображение. Например, если картинки из датасета имеют размер H × W H×W, то следует размножить вектор y y, создав из него тензор размера H×W×d y , где d y d y — размерность вектора y y. Далее полученное «изображение» конкатенируется с входным изображением. Современные модели Теперь на примере наиболее успешных моделей мы расскажем об улучшениях, которые во многом отходят от оригинального подхода к обучению GAN-ов и при этом значительно улучшают практические результаты, а значит расширяют практическую применимость. В этом разделе мы рассмотрим state-of-the-art систему генерации изображений StyleGAN, методы её обращения (т.е., поиска векторов шума, соответствующих произвольной картинке), а также методы манипуляции семантикой изображений. После этого мы рассмотрим несколько примеров условных генеративных моделей, которые вместо шума принимают на вход изображения. Такие модели используются как для задачи повышения разрешения (super resolution), так и стилизации (например, превращение пейзажей в",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 9,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Далее полученное «изображение» конкатенируется с входным изображением. Современные модели Теперь на примере наиболее успешных моделей мы расскажем об улучшениях, которые во многом отходят от оригинального подхода к обучению GAN-ов и при этом значительно улучшают практические результаты, а значит расширяют практическую применимость. В этом разделе мы рассмотрим state-of-the-art систему генерации изображений StyleGAN, методы её обращения (т.е., поиска векторов шума, соответствующих произвольной картинке), а также методы манипуляции семантикой изображений. После этого мы рассмотрим несколько примеров условных генеративных моделей, которые вместо шума принимают на вход изображения. Такие модели используются как для задачи повышения разрешения (super resolution), так и стилизации (например, превращение пейзажей в картины Моне). Мы сфокусируемся на изображениях, так как в этой области сконцентрирован как основной прогресс, так и наиболее впечатляющие применения генеративных моделей. StyleGAN Самой известной генеративно-состязательной моделью, работающей с изображениями, по праву считается StyleGAN, который до сих пор активно развивается и имеет большое количество расширений (например, существуют разнообразные методы его обращения). Progressive Growing Архитектура StyleGAN переняла progressive growing из модели Progressive Growing of GANs. Суть данной техники заключается в том, чтобы не сразу генерировать изображение высокого разрешения, а постепенно. Давайте рассмотрим это подробнее. Иллюстрация работы генератора и дискриминатора в модели Progressive Growing GANs. Ссылка на источник картинки. Мы хотим получить генератор, который генерирует изображения размера 1024x1024. Обучить такой генератор очень сложно. Поэтому мы начинаем с разрешения 4x4. У генератора мы оставляем только первый блок слоёв, который позволяет из шума получить изображение размера 4x4. У дискриминатора мы оставим, наоборот, только последний, который принимает на вход изображение размером 4x4. Такой GAN мы обучаем на изображениях из датасета (предварительно уменьшив их в размере). Спустя сколько-то итераций мы понимаем, что сеть уже умеет генерировать маленькие изображения. В этот момент мы добавляем к генератору один блок, чтобы на выходе у неё получалось изображение размера 8x8. Так же мы добавляем один блок в начало дискриминатора, чтобы он на вход принимал изображения размера 8x8. Теперь генератор и дискриминатор состоят из двух блоков, которые мы и обучаем. Такой процесс мы повторяем несколько раз, пока в итоге не дойдём до нужного нам разрешения 1024x1024. Эта схема в итоге показала себя действенным способом генерации реалистичных изображений высокого разрешения. Подача шума в нейросеть Ключевой частью StyleGAN является используемый в нём способ подачи шума z z в нейросеть, и именно из-за него метод и получил своё название. Для того чтобы понять, что конкретно в нём особенного, давайте подробнее посмотрим на архитектуру сети (рисунок из предыдущего раздела, модель StyleGAN справа). Слева: традиционный генератор. Справа: генератор модели StyleGAN. Ссылка на источник картинки. Во-первых, вместо того, чтобы подавать вектор шума z z только в самом начале генератора, нейросеть обуславливают на него много раз на разных разрешениях признаков. Исторически, впервые похожим методом решалась задача переноса стиля одной картинки на другую, отсюда и название: a style-based generator. В качестве метода обуславливания используются так называемые адаптивные слои. Это модификация обычных слоёв нейросетей, в которых часть параметров предсказывается другой нейросетью. Вообще говоря, адаптивным можно сделать любой вид нормализации, включая батч нормализацию, но наиболее известным примером такого слоя является адаптивная инстанс нормализация (adaptive instance normalization), и именно она использовалась в",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 10,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Справа: генератор модели StyleGAN. Ссылка на источник картинки. Во-первых, вместо того, чтобы подавать вектор шума z z только в самом начале генератора, нейросеть обуславливают на него много раз на разных разрешениях признаков. Исторически, впервые похожим методом решалась задача переноса стиля одной картинки на другую, отсюда и название: a style-based generator. В качестве метода обуславливания используются так называемые адаптивные слои. Это модификация обычных слоёв нейросетей, в которых часть параметров предсказывается другой нейросетью. Вообще говоря, адаптивным можно сделать любой вид нормализации, включая батч нормализацию, но наиболее известным примером такого слоя является адаптивная инстанс нормализация (adaptive instance normalization), и именно она использовалась в первой версии StyleGAN. Вспомним, как именно работает неадаптивная версия этого слоя. Пусть у нас есть батч B×C×H×W , элементы которого будем обозначать как bchw in ∈R. Здесь B B обозначает размер мини-батча, C C – количество признаков, а H H и W W – высоту и ширину. Тогда внутри слоя инстанс нормализации выполняется следующая операция: F out out = σ F in −μ ⋅γ+β.μ,σ∈R B×C ,γ,β∈R C . Здесь μ μ и σ σ обозначают матрицы средних и стандартных отклонений, которые считаются отдельно для каждого элемента мини-батча и для каждого признака: h=1 ∑ H w=1 ∑ W f bchw ,σ bc = HW 1 h=1 ∑ H w=1 ∑ W (f bchw −μ bc ) 2 . При этом γ γ и β β являются параметрами слоя, которые настраиваются в процессе обучения. Особенностью этого слоя является то, что, в отличие от батч нормализации, он применяется одинаковым образом как при обучении, так и во время инференса. То есть вместо того, что приближать средние и стандартные отклонения по батчу при помощи скользящих средних, как это делается в батч нормализации, мы честно каждый раз считаем эти статистики для каждой новой картинки отдельно от всех остальных. Это делает инстанс нормализацию очень популярной в области обработки и генерации изображений, где зачастую бывает невозможным обучение с большим размером мини-батча, а значит и использование батч нормализации. Адаптивной инстанс нормализацией (AdaIN) называется слой, где γ γ и β β являются не обучаемыми параметрами, а нейросетями, которые предсказывают эти векторы из какого-то общего для всех слоёв адаптивной инстанс нормализации входа (обозначим его через w w): F out out = σ F in −μ ⋅γ(w)+β(w). Это означает, что вместо оптимизации по векторам γ γ и β β будет происходить оптимизация по параметрам этих двух нейросетей. Также это означает, что у адаптивной инстанс нормализации добавляется ещё один вход помимо набора признаков F in F in , который определяет её поведение: некоторый вектор w w, который также называют вектором стиля. Как правило, в качестве γ ( w ) γ(w) и β ( w ) β(w) используется нейросеть с одним линейным слоем или неглубокий персептрон. Нетрудно видеть, что если в качестве вектора w w подавать сгенерированный шум z z, то это будет хорошим способом многократного обуславливания нашего генератора на вектор шума. Это позволило бы глубоким слоям нейросети выучивать лишь часть той информации о выходном изображении, которая содержится в векторе w w, например, глобальные признаки картинки. А информация о локальных признаках выходного изображения (текстурах)",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 11,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "F in F in , который определяет её поведение: некоторый вектор w w, который также называют вектором стиля. Как правило, в качестве γ ( w ) γ(w) и β ( w ) β(w) используется нейросеть с одним линейным слоем или неглубокий персептрон. Нетрудно видеть, что если в качестве вектора w w подавать сгенерированный шум z z, то это будет хорошим способом многократного обуславливания нашего генератора на вектор шума. Это позволило бы глубоким слоям нейросети выучивать лишь часть той информации о выходном изображении, которая содержится в векторе w w, например, глобальные признаки картинки. А информация о локальных признаках выходного изображения (текстурах) может появляться уже ближе к последним слоям на более высоком разрешении промежуточных признаков. Таким образом, у генератора нет необходимости хранить во всех своих картах признаков всю информацию о сгенерированной картинке, как это происходит в случае DCGAN: он может декодировать её напрямую из вектора шума по мере необходимости, что существенно облегчает обучение таких моделей и улучшает качество результатов. Авторы StyleGAN пошли даже дальше: в качестве дополнительной регуляризации они специально заставляли нейросеть использовать информацию из вектора шума частями. А именно, во время обучения все слои адаптивной нормализации случайным образом делятся на две последовательно идущие группы: первая группа обуславливается при помощи одной части вектора шума z 1 z 1 , а вторая – при помощи другой части z 2 z 2 . На практике это приводит к следующему эффекту: нейросеть учиться декодировать часть признаков изображения, используя вектор z 1 z 1 , а часть – используя z 2 z 2 . Это позволяет после обучения напрямую манипулировать выходами нейросети, смешивая разные векторы стилей z z. Обучение нового латентного пространства Второе ключевое открытие авторов StyleGAN связано с задачей поиска семантически значимого редактирования векторов из выученного низкоразмерного многообразия p ( z ) p(z). Зачем это нужно на практике мы уже упоминали ранее: на этом низкоразмером многообразии значительно проще семантически редактировать изображения, чем на уровне пикселей. Например, для задачи генерации лиц на многообразии z z за изменение возраста или гендера может отвечать простой аддитивный сдвиг вектора z z на Δ z Δz. Если же мы попытаемся приблизить такую операцию в пространстве пикселей, то для этого уже понадобится большая нейросеть с сотнями тысяч или даже миллионами параметров. При этом, как правило, мы хотим использовать наиболее простые операции редактирования. В идеале, мы бы хотели ограничить класс преобразований редактирования (а) сдвигами на какой-то вектор и (б) линейной (или сферически-линейной) интерполяцией двух векторов. С одной стороны, кажется, что так задача редактирования векторов существенно усложняется: этот класс преобразований даже менее выразителен, чем линейные операции. Но, с другой стороны, для таких простых преобразований легче гарантировать, что они не выведут нас за пределы многообразия, в котором у распределения p ( z ) p(z) большая «масса», т.е. того множества векторов z z, которые генератор чаще всего видел во время обучения. Для нормального распределения, как было сказано ранее, это многообразие можно приблизить сферой радиуса d d . Но будет ли легко найти хорошо работающие преобразования на многообразии случайных векторов z z, взятых из нормального распределения? Авторы StyleGAN обнаружили, что если сначала пропустить векторы z z",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 12,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "редактирования векторов существенно усложняется: этот класс преобразований даже менее выразителен, чем линейные операции. Но, с другой стороны, для таких простых преобразований легче гарантировать, что они не выведут нас за пределы многообразия, в котором у распределения p ( z ) p(z) большая «масса», т.е. того множества векторов z z, которые генератор чаще всего видел во время обучения. Для нормального распределения, как было сказано ранее, это многообразие можно приблизить сферой радиуса d d . Но будет ли легко найти хорошо работающие преобразования на многообразии случайных векторов z z, взятых из нормального распределения? Авторы StyleGAN обнаружили, что если сначала пропустить векторы z z через многослойный персептрон f f, и подавать на вход свёрточного генератора его выходы w=f(z), то редактировать латентные векторы на выученном многообразии w ∈ W w∈W станет намного проще. Это объясняется тем, что функция f f имеет возможность выучить достаточно сложное распределение для переменной w w, которое упростило бы задачу генерации картинки для свёрточной части генератора. И на практике оказывается, что такое выученное представление W W улучшает не только качество генерируемых картинок, но и качество результатов для семантического редактирования векторов. Примеры генерации StyleGAN. Ссылка на источник картинки. Truncation trick Последняя важная деталь, которая тем не менее очень сильно помогла авторам StyleGAN получить настолько хорошие результаты – это так называемый truncation trick. Он был впервые предложен в более ранних работах и продолжает оказывает огромное влияние на качество результатов. Его суть состоит в том, чтобы после обучения сэмплировать те примеры из латентного пространства, которые чаще всего видел генератор во время обучения. Например, если мы во время обучения брали вектор z z из нормального распределения, то при использовании truncation trick после обучения мы бы его сэмплировали из нормального распределения с обрезанными хвостами. Тем самым, интуитивно, мы убираем из сгенерированной выборки те примеры входных векторов, которые генератор реже видел во время обучения. Однако, нетрудно заметить что такая процедура приводит к потере разнообразия в выходных картинках. Например, если мы обрежем нормальное распределение вплоть до его среднего значения, то тогда нейросеть сможет выдавать лишь один пример. При всём при этом потеря разнообразия выходов – не такая большая проблема, т.к. обученный генератор всё ещё может часто ошибаться и выдавать маргинальные примеры. Фильтрация таких плохих примеров по какому-то выставленному порогу – в этом и есть суть применения truncation trick. В случае StyleGAN, авторам хотелось бы применять этот трюк непосредственно на распределении в выученном латентном пространстве W W. Для этого они применяют простой трюк: сначала считают центр масс W W, усредняя векторы w w для большой выборки сэмплов z∼p(z) [f(z)], а затем сдвигают каждый сгенерированный вектор w w по направлению к этому центру: +ψ(w− w ˉ ), где ψ < 1 ψ<1 – это параметр, который задаёт trade-off между качеством результатов и их разнообразием. StyleGAN-2 Хотя работа StyleGAN показала довольно хорошие результаты, авторы статьи про StyleGAN-2 Analyzing and Improving the Image Quality of StyleGAN заметили, что в некоторых случаях она может выдавать некачественные изображения. В частности, StyleGAN в некоторых случая может выдавать изображения с артефактами. Примеры артефактов StyleGAN. Ссылка на источник картинки. Основной причиной этих артефактов оказалась адаптивная нормализация. Изначально,",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 13,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "усредняя векторы w w для большой выборки сэмплов z∼p(z) [f(z)], а затем сдвигают каждый сгенерированный вектор w w по направлению к этому центру: +ψ(w− w ˉ ), где ψ < 1 ψ<1 – это параметр, который задаёт trade-off между качеством результатов и их разнообразием. StyleGAN-2 Хотя работа StyleGAN показала довольно хорошие результаты, авторы статьи про StyleGAN-2 Analyzing and Improving the Image Quality of StyleGAN заметили, что в некоторых случаях она может выдавать некачественные изображения. В частности, StyleGAN в некоторых случая может выдавать изображения с артефактами. Примеры артефактов StyleGAN. Ссылка на источник картинки. Основной причиной этих артефактов оказалась адаптивная нормализация. Изначально, адаптивная нормализация состояла из двух частей: нормализация (на рисунке обозначена как Norm) и модуляция (на рисунке обозначена как Mod). В нормализации мы вычитали среднее и делили на стандартное отклонение. В модуляции мы умножали на новое выученное стандартное отклонение и прибавляли новое выученное среднее. Авторы StyleGAN2 предложили несколько модификаций для этапа нормализации. Каждое изменение в статье добавляли последовательно, следя за изменением общего качества генерации. Как из нормализации, так и из модуляции убрали вычитание/прибавление среднего. Нормализация и модуляция теперь выполняются независимо друг от друга и были перемещены в начало/конец стилевых блоков (см рисунок ниже, (c) Revised architecture). Нормализацию из предыдущего пункта заменили на демодуляцию весов. По сути это та же нормализация, только теперь нормализуются веса свёрток, а не входные данные (см рисунок ниже, (d) Weight demodulation. Обратите внимание на Изменения, которые добавили в StyleGAN. Ссылка на источник картинки. В StyleGAN используется техника progressive growing (см раздел про StyleGAN). Из-за этого StyleGAN появляются артефакты, возникающие при исследовании латентного пространства с помощью интерполяций. Некоторые объекты лиц (глаза, зубы), которые должны вращаться при вращении головы, оставались на месте. Чтобы побороть эти артефакты, вместо progressive growing в StyleGAN2 стали использовать residual connections. Артефакты из-за прогрессивной генерации в StyleGAN. Ссылка на источник картинки. Эти изменения позволили улучшить качество генерируемых изображений и избавиться от артефактов StyleGAN. Вот, например, некоторые примеры сгенерированных изображений модели StyleGAN2: Примеры изображений, сгенерированных с помощью StyleGAN2. Ссылка на источник картинки. StyleGAN-ADA Следующий шаг в развитии архитектуры StyleGAN — это статья StyleGAN-ADA. ADA расшифровывается как Adaptive Discriminator Augmentation. В данной статье авторы предложили механизм аугментации данных, который позволяет стабилизировать обучение и избежать переобучения дискриминатора. На левом рисунке (b) изображено, куда добавляется аугментация (синие блоки). На правом рисунке (c) изображена степень аугментации в зависимости от контролирующего её параметра p. Ссылка на источник картинки. Всего в статье использовали 18 разных аугментаций. В статье также предложили некоторую эвристику того, как понимать, насколько переобучился дискриминатор. Эвристика нужна для того, чтобы адаптивно контролировать параметр аугментации p p в зависимости от степени переобучения. Основная идея алгоритма контроля p p в процессе обучения следующая. Изначально этот параметр равен нулю. Его значение изменяется на фиксированную величину каждые четыре мини-батча (авторы пишут, что частота обновлений не влияет на результат). Если наблюдается, что дискриминатор слишком переобучился, то параметр p p увеличивается. И наоборот, при низкой степени переобучении дискриминатора значение p p уменьшается. Аугментация, как показали авторы, действительно помогает стабилизировать обучение при маленьком количестве данных. Однако, большой набор реальных данных всегда будет выигрывать у аугментации. StyleGAN-T",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 14,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "того, как понимать, насколько переобучился дискриминатор. Эвристика нужна для того, чтобы адаптивно контролировать параметр аугментации p p в зависимости от степени переобучения. Основная идея алгоритма контроля p p в процессе обучения следующая. Изначально этот параметр равен нулю. Его значение изменяется на фиксированную величину каждые четыре мини-батча (авторы пишут, что частота обновлений не влияет на результат). Если наблюдается, что дискриминатор слишком переобучился, то параметр p p увеличивается. И наоборот, при низкой степени переобучении дискриминатора значение p p уменьшается. Аугментация, как показали авторы, действительно помогает стабилизировать обучение при маленьком количестве данных. Однако, большой набор реальных данных всегда будет выигрывать у аугментации. StyleGAN-T Большинство современных моделей, которые показали впечатляющие результаты для генерации изображений, работают по схеме text to image. То есть текст является входом для нейросети, изображение — выходом. Обычно текст на входе называют prompt. По такой схеме работают модели Stable Diffusion, DALLE 2, Midjourney. Все эти модели являются диффузионными. Однако, пока что списывать GANы со счетов не стоит. Хотя качество у GANов не такое высокое, как у диффузионных моделей, а обучать их сложнее, у них есть неоспоримое преимущество — быстрая генерация изображений. На момент написания этого параграфа самая свежая статья про генерацию изображений с помощью GANов — StyleGAN-T. Её авторы решили на основе StyleGAN сделать модель для генерации изображений из текста. Архитектура модели StyleGAN-T очень похожа на архитектуру модели StyleGAN (за основу авторы взяли StyleGAN-XL — версию StyleGAN для больших обучающих выборок). В качестве кодировщика текста была использована предобученная модель CLIP. Архитектура модели StyleGAN-T. Ссылка на источник картинки. На что стоит обратить внимание в данной архитектуре: Текст, закодированный CLIP text encoder, подаётся на вход как генератору, так и дискриминатору. Дискриминатор в данном случае классифицирует не отдельное изображение, а пару текст/изображение. Сгенерированное изображение пропускается через фиксированный кодировщик изображений, также взятый из модели CLIP (CLIP image encoder на рисунке архитектуры). Полученное представление изображения должно быть близко с представлением текста, полученным с помощью CLIP text encoder. Это достигается за счёт добавление CLIP guidance loss в общую функцию потерь. Для разрешения выше 64x64 авторы берут случайные кропы размера 64x64 на изображении, чтобы посчитать CLIP guidance loss. Основное новшество модели StyleGAN-T — это лучшая GAN-модель для генерации изображений из текста. До этого большинство хорошо работающих моделей позволяли генерировать изображения для заданного класса или вообще без условий. Связать текст с изображением — гораздо более сложная задача. Результаты генерации Поскольку на вход модель принимает не только шум, но и закодированный текст, она позволяет делать интерполяции по пространству текста. Примеры сгенерированных изображений и интерполяций по текстовому пространству вы можете видеть на рисунке ниже. Результаты генерации моделью StyleGAN-T. Ссылка на источник картинки. Качество изображений StyleGAN-T отстаёт от диффузионных моделей, таких как Stable Diffusion или DALLE 2, о чём пишут сами авторы. Однако данная модель сильно выигрывает по скорости: на одной и той же видеокарте Stable Diffusion генерирует изображение за 3.7 секунды, в то время как StyleGAN-T за 0.02 секунды. Disclaimer Выше были перечислены лишь основные особенности данного класса генеративных моделей, на которые стоит обратить внимание. Эти соображения нашли применение в других задачах помимо генерации картинок из шума. На самом деле, список",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 15,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "сгенерированных изображений и интерполяций по текстовому пространству вы можете видеть на рисунке ниже. Результаты генерации моделью StyleGAN-T. Ссылка на источник картинки. Качество изображений StyleGAN-T отстаёт от диффузионных моделей, таких как Stable Diffusion или DALLE 2, о чём пишут сами авторы. Однако данная модель сильно выигрывает по скорости: на одной и той же видеокарте Stable Diffusion генерирует изображение за 3.7 секунды, в то время как StyleGAN-T за 0.02 секунды. Disclaimer Выше были перечислены лишь основные особенности данного класса генеративных моделей, на которые стоит обратить внимание. Эти соображения нашли применение в других задачах помимо генерации картинок из шума. На самом деле, список трюков и нюансов, необходимых для успешного обучения такой модели, намного обширнее. На практике для генеративных моделей настоятельно рекомендуется отталкиваться от готовых кодовых баз, внося минимальные и контролируемые изменения в процесс обучения. Особенно чувствительны генеративные модели бывают к архитектуре генератора и дискриминатора, к параметрам оптимизации (learning rate, количество обновлений весов дискриминатора на одно обновление генератора, и т.д.), а также к значениям весов лоссов (например, к весу R1 регуляризации, которую мы тут не обсуждали). Применения генеративных состязательных нейросетей До этого мы рассмотрели основные особенности генеративных состязательных нейросетей, а также их применение в задаче генерации изображений. В этом разделе мы рассмотрим, какие ещё задачи можно решать с помощью таких моделей. Отметим, что задачи, которые мы рассмотрим ниже, можно решать и другими способами без ГАНов. Зачастую диффузионные модели (MidJourney, Stable Diffusion) показывают лучшие результаты в этих задачах. Тем не менее в данном же разделе мы рассмотрим именно методы на основе генеративных состязательных нейросетей. Inpainting Представьте, что вы хотите удалить с фотографии людей на заднем плане. Встаёт вопрос, чем их заменить? Для этого существует задача инпеинтинга (inpainting). Она заключается в том, чтобы восстановить часть изображения, которая была выделена маской. Если выделить людей или объекты на фотографии маской, то нейросеть для инпеинтинга будет способна зарисовать эти участки чем-то подходящим для конкретной фотографии. Пример работы модели инпеинтинга. Ссылка на источник картинки. Обычно генератор модели GANs для инпеинтинга представляют собой image-to-image модели. То есть изображение подаётся как на вход, так и на выход. То, что происходит внутри генератора, зависит от архитектуры модели. Как правило, используются U-Net-подобные архитектуры с какими-то дополнениями. Так, например, в одной из последних работ по инпеинтигу на основе GANs Resolution-robust Large Mask Inpainting with Fourier Convolutions используются Fast Fourier Convolutions. Чтобы обучить модель инпеинтинга, нужно подготовить данные в формате пар <изображение с маской, изображение без маски>. Сделать это не сложно. Достаточно на существующем наборе изображений случайным образом выделить участки для удаления, после чего обучать нейросеть их восстанавливать. Outpainting Задачу inpainting можно так же превратить в задачу outpainting, то есть дорисовки изображения по краям. Для этого нужно в качестве маски подать пиксели, которые находятся за рамками изображения. При этом само исходное изображение можно уменьшить, если того требуют размерности нейросети. Пример outpainting, сделанный моделью In&Out. Ссылка на источник картинки. Задача outpainting может быть полезна, когда хочется расширить изображение, например, чтобы увеличить его разрешение. Редактирование изображений До этого мы рассматривали, как можно редактировать латентное пространство обученной состязательной модели, чтобы это отражалось на сгенерированных изображениях. В 2023 году вышла",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 16,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "случайным образом выделить участки для удаления, после чего обучать нейросеть их восстанавливать. Outpainting Задачу inpainting можно так же превратить в задачу outpainting, то есть дорисовки изображения по краям. Для этого нужно в качестве маски подать пиксели, которые находятся за рамками изображения. При этом само исходное изображение можно уменьшить, если того требуют размерности нейросети. Пример outpainting, сделанный моделью In&Out. Ссылка на источник картинки. Задача outpainting может быть полезна, когда хочется расширить изображение, например, чтобы увеличить его разрешение. Редактирование изображений До этого мы рассматривали, как можно редактировать латентное пространство обученной состязательной модели, чтобы это отражалось на сгенерированных изображениях. В 2023 году вышла работа Drag Your GAN, которая основана на этом принципе, и позволяет редактировать изображения перетаскиванием одной точки в другую. Пример работы Drag Your GAN.cсылка на источник изображения Метод Drag Your GAN основан на модели StyleGAN2. Ему на вход подаётся набор изначальных точек и набор конечных точек. Внутри метода поочерёдно выполняются следующие два шага: Обновление латентного пространства и обновления изображения с помощью оптимизации; Обновление координат точек (трекинг точек). Изначально метод работает только со сгенерированным изображениями. Однако, нет проблем в том, чтобы добавить кодировщик, который бы переводил реальные изображения в латентное пространство модели. В таком случае можно будет редактировать и реальные изображения. Демо Drag Your GAN доступно по ссылке. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.2. Variational Autoencoder (VAE) Следующий параграф 8.4. Нормализующие потоки",
    "metadata": {
      "title": "Генеративно-состязательные сети (GAN)",
      "url": "https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan)",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.3",
      "part": 17,
      "total_parts": 17,
      "source_file": "8.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В главе Генеративный подход к классификации мы уже познакомились с типом моделей, которые оценивают совместное распределение p(X,Y). Такие модели называют генеративными. Для простоты предположим, что мы имеем всего один класс, тогда задача моделирования P(X,Y) сводится к задаче моделирования p ( X ) p(X). Научившись моделировать это распределение, мы сможем: генерировать объекты x∼p θ data , где θ θ – параметры модели; оценивать вероятность встретить данный объект x x среди набора наблюдаемых данных D D; выучивать скрытые представления для объекта x x. Известными примерами генеративных моделей являются: Авторегрессионные модели: (x)= i=1 x∈R n , Вариационные автокодировщики: (x)=∫p θ (x,z)dz, z ∈ R m . z∈R m . Но оба эти метода не позволяют одновременно: получать скрытые представления для объектов точно вычислять функцию правдоподобия Нормализующие потоки способны решить обе эти задачи. Мотивация flow Пусть x∼p x (x), где (x) неизвестно, а z∼p z (z)=N(0,I). Мы хотим найти отображение , для которого x=f θ (z) и z=f θ −1 (x). Отображение f f преобразует базовую функцию плотности p z p z к более сложной p x p x . С его помощью мы можем генерировать сложный объект путем сэмплинга простого объекта z z (скрытой переменной) из распределения p z p z и применения «генератора» f(z)=x. Обратное отображение f − 1 f −1 «нормализует» сложное распределение p x p x , приводя его к простому p z p z . Найдя такое отображение f f, мы сможем генерировать новые объекты x x, а оценить плотность (x) поможет формула преобразования плотности случайной величины. Давайте её вспомним. Формула замены переменной Пусть x∼p x (x), z∼p z (z), при этом отображение f:R n →R n дифференцируемо, обратимо и x=f(z). Тогда: det (x)=p z (f −1 (x))⋅ det(J f −1 ) , где det det(J f −1 ) – якобиан отображения Определение Итак, нормализующий поток – это обратимое дифференцируемое отображение , которое переводит исходные представления объектов в скрытые: x=f θ (z) и z=f θ −1 (x). При этом функция правдоподобия (x) вычисляется по формуле: det (x)=p z (f −1 (x))⋅ det(J f −1 ) Умея вычислять функцию правдоподобия, мы можем обучать нашу модель f θ f θ методом максимума правдоподобия (ММП): log log log ⁡ ∣ det log p x (D;θ)= i=1 ∑ M log p z (f θ −1 (x))+log det(J f −1 ) . где D={x (i) } i=1 M – выборка наблюдаемых данных из распределения p x p x . Обычно модель нормализующего потока составляет композицию из K K не очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной: f=f 1 ∘f 2 ∘⋯∘f K Тогда якобиан вычисляется по формуле: det det det(J f −1 )= i=1 ∏ K det(J f i −1 ) Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение f f, подсчет якобиана которого был бы эффективен! В отличе от VAE и GANов, нормализующие потоки требуют вычисления функции правдоподобия, воэтому важно уметь эффективно вычислять функцию правдоподобия. Метод максимального",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 1,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "композицию из K K не очень сложных отображений, чтобы она была, с одной стороны, достаточно контролируемой, а с другой – достаточно выразительной: f=f 1 ∘f 2 ∘⋯∘f K Тогда якобиан вычисляется по формуле: det det det(J f −1 )= i=1 ∏ K det(J f i −1 ) Но вычисление якобиана является очень затратной операцией. Для того, чтобы мы могли обучать модели эффективно на высокоразмерных данных (аудио, изображения), необходимо использовать такое отображение f f, подсчет якобиана которого был бы эффективен! В отличе от VAE и GANов, нормализующие потоки требуют вычисления функции правдоподобия, воэтому важно уметь эффективно вычислять функцию правдоподобия. Метод максимального правдоподобия позволяет обучать нормализующие потоки стабильнее в сравнении с GAN-ами, а возможность быстро и точно вычислять значение функции правдоподобия выделяет нормализующие потоки на фоне VAE и диффузионных моделей. Примером такого отображения является планарный поток (Planar Flow), где отображение f f принадлежит следующему семейству функций: x=f θ (z)=z+u θ h(w θ ⊤ z+b θ ), где – обучаемые параметры, а h h – гладкая нелинейная функция, например, tanh tanh. Якобиан такого отображения можно будет посчитать за O ( n ) O(n). Обозначим ψ(z)=h ′ (w θ ⊤ z+b θ )w θ . Тогда ∣ det det det(J f ) = det(I+u θ ψ(z) ⊤ ) = (1+u θ ⊤ ψ(z)) . planar Действие планарного нормализующего потока на нормальное и равномерное распределение (ссылка на статью). Развитие идеи В планарных потоках нам удалось быстро посчитать якобиан, потому что матрица имела специальный вид (сумма единичной и низкоранговой). Но мы знаем и другие случаи, когда определитель можно посчитать быстрее – треугольные матрицы. Их определитель равен произведению элементов на диагонали. Следующие модели активно использовали этот трюк. NICE: Non-linear Independent Component Estimation и RealNVP Авторы модели NICE предложили использовать в качестве f θ f θ следующее семейство преобразований: x=f θ (z)={ x 1:d =z 1:d x d+1:n =z d+1:n +m θ (z 1:d ) , где 1 < d < n 1<d<n, а m θ m θ – произвольная нейросеть с d d входами и n − d n−d выходами. Такое преобразование называют аддитивным связыванием (additive coupling). Обратное преобразование вычисляется с такой же легкостью, а якобиан равен 1 1. То есть, (x)=p z (f −1 (x)), что является довольно сильным ограничением модели. Далее, из-за того, что 1:d =z 1:d , первые d d каналов вектора x x совпадают с координатами нормального шума z z, то есть моделирования этих каналов x x не происходит. Из-за этого выразительная способность модели NICE была относительно невысокой. Позже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов x x, что стало основой работы RealNVP. Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием (z); при этом градиент перестановки вычисляется легко. exp x=f θ (z)={ x 1:d =z 1:d x d+1:n =exp(s θ (z 1:d ))⊙z d+1:n +m θ (z 1:d ) , где ⊙ ⊙ – поэлементное умножение, а s θ s θ – нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и m θ m θ . Такое преобразование",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 2,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "модели NICE была относительно невысокой. Позже авторы NICE позже предложили использовать между слоями нормализующих потоков зафиксированные перестановки признаков/каналов x x, что стало основой работы RealNVP. Использование перестановок позволяет добиться того того, чтобы все выходные каналы оказались затронуты преобразованием (z); при этом градиент перестановки вычисляется легко. exp x=f θ (z)={ x 1:d =z 1:d x d+1:n =exp(s θ (z 1:d ))⊙z d+1:n +m θ (z 1:d ) , где ⊙ ⊙ – поэлементное умножение, а s θ s θ – нейросеть, которая может быть произвольной, но, как правило, выбирается такой же архитектуры, как и m θ m θ . Такое преобразование называют аффинным связыванием (affine coupling). Получившееся отображение тоже легко обращается, а его якобиан равен: det exp det(J f −1 )=exp i=d+1 ∑ n (s θ (z 1:d )) i Заметим, что, как и в случае аддитивного связывания, значительная часть каналов остается неизменной при использовании аффинного связывания. Для того, чтобы преобразование (x) моделировало распределение x x во всех каналах, на разных слоях неизменными оставляют разные подмножества из d d каналов. Чтобы улучшить сходимость глубоких ( K > 1 K>1) нормализующих потоков, авторы предложили использовать Batch Normalization. Данное преобразование тоже является обратимым, а его якобиан вычисляется крайне просто. В результате, выразительная способность модели сильно повысилась, и она стала способна выучивать сложные распределения: realnvp Masked Autoregressive Flows Ссылка на статью Данный вид нормализующих потоков также обладает нижнетреугольным якобианом, но он использует другое семейство функций: x i = z i exp exp(f α i (x 1:i−1 ))+f μ i (x 1:i−1 ), где 1:i−1 ) и 1:i−1 ) – нейросети произвольной архитектуры. Как видно из формулы, x i x i напрямую зависит от 1:i−1 . Таким образом, элементы генерируются авторегрессивно, что и дало название архитектуре. Якобиан такого преобразования вычисляется по следующей формуле: det exp det(J f −1 )=exp(− i=1 1:i−1 )) Таким образом, шаг генерации выглядит следующим образом: z∼N(0,I) x 1 = z 1 exp exp(α 1 )+μ exp exp(f α 2 (x 1 ))+f μ 2 (x 1 ) ... Однако вычисление скрытых переменных z z не является авторегрессивным: exp =(x i −f μ i (x 1:i−1 ))exp(−f α i (x 1:i−1 )) Несмотря на то, что данная разновидность нормализующих потоков кажется более мощной моделью, её трудно применить на практике к данным высокой размерности. Это происходит из-за того, что генерация нового объекта осуществляется авторегрессивно по координатам, что становится слишком затратным при обучении на высокоразмерных данных, например, на изображениях. Inverse Autoregressive Flows Ссылка на статью Чтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные: exp =(x i −f μ i (z 1:i−1 ))exp(−f α i (z 1:i−1 )) Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объекты x x быстрее: exp ⋅exp(f α i (z 1:i−1 ))+f μ i (z 1:i−1 ) Но вычисление z z, а значит и правдоподобия, становится долгим, и обучение занимает больше времени. Звук Нормализующие потоки стали наиболее актуальны в задаче генерации звука,",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 3,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "статью Чтобы быстро генерировать объекты из сложного распределения, мы можем избавиться от авторегрессивности на шаге генерации, поставив в авторегрессивную зависимость не наблюдаемые, а латентные переменные: exp =(x i −f μ i (z 1:i−1 ))exp(−f α i (z 1:i−1 )) Можем заметить, что проблема долгого вычисления авторегрессивных выражений никуда не уходит. Мы лишь изменяем построение модели таким образом, чтобы генерировать объекты x x быстрее: exp ⋅exp(f α i (z 1:i−1 ))+f μ i (z 1:i−1 ) Но вычисление z z, а значит и правдоподобия, становится долгим, и обучение занимает больше времени. Звук Нормализующие потоки стали наиболее актуальны в задаче генерации звука, поскольку они обладают достаточно высокой выразительностью и эффективностью, чтобы быстро генерировать аудиозаписи высокого качества. В этом контексте, модель нормализующего потока должна генерировать аудио, получая на вход описание того, что ей необходимо сгенерировать. То есть модель обуславливается на дополнительные признаки. Нормализующие потоки могут быть обусловлены на входные данные путем использования дополнительных входных данных в качестве переменной, от которой зависят преобразования, применяемые к данным. Обусловливающей переменной может быть любая дополнительная информация, имеющая отношение к задаче генерации, такая как текстовые описания, изображения или другие характеристики данных. В контексте генерации аудио обуславливающей переменной обычно служит mel-спектрограмма, которая позволяет отобразить интенсивность различных частот аудио-сигнала в разные моменты времени. melspec Нормализующий поток учится генерировать сигнал в виде waveform-а на основе спектрограммы путем обратного преобразования. waveform Чтобы генерировать более длинные фрагменты звука, модель генерирует короткие звуковые кадры (фреймы) за раз, которые затем объединяются для формирования полного waveform-а. Artboard Теперь мы готовы узнать про применение нормализующих потоков в генерации аудио! Probability Density Distillation и Parallel WaveNet Ссылка на статью Архитектура Inverse Autoregressive Flow (IAF) была изначально предложена для задачи генерации аудио. Она позволяет генерировать объекты крайне эффективно, но обучение методом максимального правдоподобия занимает много времени из-за авторегрессивности вычислений. Метод Probability Density Estimation позволяет решить эту проблему с помощью использования второй предобученной авторегрессивной модели в качестве учителя. IAF обучается в качестве модели-студента, минимизируя KL дивергенцию KL(p S ∣∣p T ), где – распределения студента и учителя соответственно. Ключевым достижением данного подхода является то, что вычисление функции потерь требует вычисления кросс-энтропии между учителем и студентом, а не правдоподобия, что позволяет максимально распараллелить все вычисления ввиду отсутствия авторегрессивности в вычислениях. Вместе с тем в данной работе в качестве учителя выбирается не случайная модель, а оригинальная авторегрессивная модель WaveNet, которая в 2016 году позволила достичь state-of-the-art качества генерации аудио. Эта модель является не нормализующим потоком, а обыкновенной авторегрессивной моделью, которая обучается предсказывать следующий кусочек аудио (фрейм) длиной в несколько миллисекунд. wavenet Таким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества! Artboard На картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примером задачи условной генерации, где на вход модели подается спектрограмма, сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе про Waveglow Glow Исследователи из OpenAI в 2018 году опубликовали работу Glow: Generative Flow with Invertible 1×1 Convolutions,",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 4,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "следующий кусочек аудио (фрейм) длиной в несколько миллисекунд. wavenet Таким образом, с помощью IAF и Probability Density Distillation авторам удалось ускорить генерацию более чем в 1000 раз без потери качества! Artboard На картинке выше мы видим, что модель использует лингвистические признаки для генерации аудио. Эта задача является примером задачи условной генерации, где на вход модели подается спектрограмма, сгенерированная отдельной моделью по тексту, а на выход ожидается речь в аудио-формате (waveform). О том, как модель использует дополнительную информацию для обуславливания, поговорим в главе про Waveglow Glow Исследователи из OpenAI в 2018 году опубликовали работу Glow: Generative Flow with Invertible 1×1 Convolutions, которая значительно улучшает результаты модели RealNVP. Опишем два главых улучшения. Во-первых, для перемешивания каналов Glow использует обратимые свертки с ядром 1x1 вместо фиксированной матрицы перестановок каналов в RealNVP; Это нововведение является по-настоящему красивым, так как в нем предлагается способ вычисления якобиана 2D-свертки за O ( n ) O(n). А именно, логарифм якобиана такой 1x1-свертки с числом каналов n n для тензора размера h × w × n h×w×n равен h w ⋅ log ⁡ ∣ det ⁡ ( W ) ∣ hw⋅log∣det(W)∣, где W W – матрица свёртки 1х1. Авторы предлагают использовать следующий вариант LU-разложения для матрицы diag ( s ) ) , W=PL(U+diag(s)), где P P – фиксированная матрица перестановок, L L – нижнетреугольная матрица с единицами на диагонали, U U – верхнетреугольная матрица с нулями на диагонали, а s s – обучаемый вектор. Нетрудно показать, что log ⁡ ∣ det sum ( log ⁡ ( s ) ) log∣det(W)∣=sum(log(s)) Благодаря этому авторам удалось снизить сложность вычислений якобиана с O ( n 3 ) O(n 3 ) до O ( n ) O(n) Кроме того, для улучшения сходимости использовали собственно разработанный actnorm-слой (activation normalization). Поскольку нормализующие потоки требуют много вычислительных ресурсов, для обучения используются мини-батчи маленького размера, из-за чего батч-нормализация работает не очень хорошо. Авторы предлагают использовать следующий тип нормализации – actnorm: i,j ′ =s⊙x i,j +b Нормализуем входной тензор (промежуточное изображение) по размерности каналов; Инициализируем параметры смещения b b и разброса s s статистиками с первого батча; Далее обучаем их в качестве обычных параметров. Таким образом, один блок нормализующего потока выглядит так: Artboard WaveGlow Ссылка на статью Вторым важным с практической точки зрения применением нормализующих потоков стала модель WaveGlow. Она представляет собой версию модели Glow, адаптированную для генерации речи по тексту. Как мы помним, эта задача также является примером задачи условной генерации: log log log ⁡ ∣ det log p x∣c (D;θ)= i=1 ∑ M log p z∣c (f θ −1 (x,c)∣c)+log det(J f −1 ) . На практике это приводит к тому, что все распределения в нашей формуле становятся условными. Таким образом, при генерации мы также сэмплируем из условного распределения z∼p z∣c (z∣c), а в слоях нормализующих потоков используем преобразования i−1 ,c). В качестве обуславливающего фактора c c для WaveGlow мы имеем сгенерированную по тексту mel-спектрограмму, а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 5,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "log p x∣c (D;θ)= i=1 ∑ M log p z∣c (f θ −1 (x,c)∣c)+log det(J f −1 ) . На практике это приводит к тому, что все распределения в нашей формуле становятся условными. Таким образом, при генерации мы также сэмплируем из условного распределения z∼p z∣c (z∣c), а в слоях нормализующих потоков используем преобразования i−1 ,c). В качестве обуславливающего фактора c c для WaveGlow мы имеем сгенерированную по тексту mel-спектрограмму, а на выходе ожидаем получить соответствующую тексту и спектрограмме аудио-запись. Как мы видим на изображении и в формулах ниже, mel-спектрограмма используется как дополнительный признак для нейросети, генерирующей параметры афинного преобразования. В качестве модели, которая производит параметры афинного преобразования, используется похожая на WaveNet архитектура с dilated-свертками. Правая часть схемы ниже более подробно показывает строение слоя affine coupling: Artboard split ( x ) ( log mel-spectrogram concat )=split(x) (logs,t)=WN(x a ,mel-spectrogram) x b ′ =s⊙x a +t f coupling −1 (x)w=concat(x a ,x b ′ ) Операция s p l i t split разделяет тензор x x пополам на два тензора меньшей размерности для их последующего участия в слое аффинного связывания (affine coupling). Пример генерации: Источник Ground truth WaveNet WaveGlow Out-of-distribution detection Может показаться, что способность точно и эффективно вычислять функцию правдоподобия может позволить без труда обнаруживать аномалии в данных, что может пригодиться во многих приложениях. Однако в работе Kirichenko et al. на примере задачи генерации изображений было показано, что нормализующие потоки выучивают отображение картинок в латентное пространство, основываясь на локальных корреляциях пикселей и графических деталях, а не на семантическом контенте. Из-за этого правдоподобие OOD-объектов может быть выше, чем правдоподобие in-distribution сэмплов. flows Однако позже было предложено использовать ряд эвристик для того, чтобы улучшить способность к детекции аномалий за счет подсчета значения функции правдоподобия: Использовать значение правдоподобия второй модели потока, обученного на отличном от исходного датасете (например, ImageNet при исходном CelebA). А затем вычислять отношенение этих двух значений для вынесения вердикта об аномальности объекта. Schirrmeister et al. В работе Serrà et al. показали, что проблема качества нормализующих потоков в задаче детекции аномалий связана с чрезмерным влиянием сложности входных данных на значение функции правдоподобия. Поэтому авторы предложили использовать в качестве поправки размер сжатого изображения с помощью одного из алгоритмов компрессии (JPEG2000/PNG). flows Сравнение с другими типами генеративных моделей Обратимся к статье Bond-Taylor et al., в которой приводится количественный анализ всех существующих семейств генеративных моделей в задаче генерации изображений из датасета CIFAR-10. nf В таблице выше указано, насколько представители каждого из популярных семейств генеративных моделей эффективны в следующих аспектах решения задачи: скорость обучения; скорость генерации; число обучаемых параметров; разрешение генерируемого изображения; ограничение на форму якобиана; возможность вычислять правдоподобие объекта; FID (Fréchet Inception Distance) тестовой выборки; Отрицательный логарифм правдоподобия тестовой выборки. За расшифровкой обратимся к таблице ниже: nf Подведя итог, можно сказать, что нормализующие потоки: требуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления; имеют скорость генерации, сравнимую с GAN-ами; менее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели; позволяют быстро вычислять точное значение функции правдоподобия объекта; обладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям. Итак, нормализующие",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 6,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "эффективны в следующих аспектах решения задачи: скорость обучения; скорость генерации; число обучаемых параметров; разрешение генерируемого изображения; ограничение на форму якобиана; возможность вычислять правдоподобие объекта; FID (Fréchet Inception Distance) тестовой выборки; Отрицательный логарифм правдоподобия тестовой выборки. За расшифровкой обратимся к таблице ниже: nf Подведя итог, можно сказать, что нормализующие потоки: требуют очень много времени на обучение, так как при обучении проводятся нетривиальные неоптимизированные вычисления; имеют скорость генерации, сравнимую с GAN-ами; менее эффективны по соотношению качество/число параметров, чем GAN-ы и диффузионные модели; позволяют быстро вычислять точное значение функции правдоподобия объекта; обладают сравнительно неплохим качеством генерации, проигрывающим GAN-ам и диффузионным моделям. Итак, нормализующие потоки явно выделяются среди других семейств генеративных моделей своими свойствами – обратимостью и способностью вычислять правдоподобие объекта. Но если для решения задачи они не требуются, то имеет смысл попробовать другие модели – в первую очередь, GAN-ы и диффузионные модели. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.3. Генеративно-состязательные сети (GAN) Следующий параграф 8.5. Диффузионные модели",
    "metadata": {
      "title": "Нормализующие потоки",
      "url": "https://education.yandex.ru/handbook/ml/article/normalizuyushie-potoki",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.4",
      "part": 7,
      "total_parts": 7,
      "source_file": "8.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "В этом параграфе мы снова попробуем решить задачу генерации, когда нам дана выборка объектов из распределения ∼q(x), и хотим научиться генерировать новые объекты из распределения , которых нет в нашей выборке. Вероятно, вы уже знакомы с другими генеративными моделями, например VAE или GAN-ы. Здесь же мы познакомим вас с еще одним видом генеративных моделей: диффузионные модели, которые стали крайне популярны в последнее время благодаря своему высокому качеству генерации объектов из заданного распределения. В общий чертах, они работают следующим образом: берем шум из N(0,I) и шаг за шагом удаляем компоненты шума до тех пор, пока не получим объект x 0 x 0 из распределения, см. иллюстрацию ниже. Screenshot Более детально Для детального понимания стоит объяснить, что такое прямой и обратный диффузионные процессы. Прямой процесс заключается в постепенном зашумлении картинки с помощью распределения q q, а обратный, наоборот, в расшумлении с помощью распределения p p. Их можно схематично изобразить следующим образом: Artboard Прямой диффузионный процесс определяется как апостериорное распределение q(x 1:T ∣x 0 ). Это распределение также является Марковской цепочкой, которая постепенно добавляет гауссовский шум к объекту x 0 x 0 . На каждом шаге шум добавляется с различной магнитудой, которая определяется расписанием дисперсий ,...,β T }. При правильном выборе расписания в пределе по числу шагов T T мы должны сойтись к шуму из N(0,I). В качестве распределений q q берут нормальные распределения: q(x t ∣x t−1 ):=N(x t ; 1−β t x t−1 ,β t I), q(x 1:T ∣x 0 )= t=1 ∏ T q(x t ∣x t−1 ) Теперь перейдем к обратному процессу и к самой диффузионной модели. Диффузионная модель - это вероятностная модель с латентными переменными вида ):=∫p θ (x 0:T )dx 1:T , где промежуточные состояния ,...,x T соответствуют зашумленным объектам, a x 0 x 0 - объект из распределения. Совместное распределение 0:T ) называет обратным диффузионным процессом, который представляет собой Марковскую цепочку из гауссовских распределений i−1 p(x 0:T )=p(x 0 ) t=1 ∏ T p θ (x t−1 ∣x t ) p θ (x T )=N(x T ∣0,I) t−1 ∣x t ):=N(x t−1 ;μ θ (x t ,t),Σ θ (x t ,t)) Таким образом, обратный процесс параметризуется моделью θ θ, которая по зашумленному объекту x t x t и шагу t t предсказывает среднее ,t) и дисперсию ,t). Обучение диффузионной модели Диффузионный модели обучаются, максимизируя вариационную нижнюю оценку (ELBO) логарифма правдоподобия log logp θ (x 0 ). По тому же принципу обучаются VAE, с тем лишь отличием, что у диффузионных моделей другая форма модели с латентными переменными. Итак, давайте выведем ELBO для диффузии: − log log log log log log log log Let L VLB log log −logp θ (x 0 ) Let L VLB ≤−logp θ (x 0 )+D KL (q(x 1:T ∣x 0 )∥p θ (x 1:T ∣x 0 )) =−logp θ (x 0 )+E x 1:T ∼q(x 1:T ∣x 0 ) [log p θ (x 0:T ) q(x 1:T =−logp θ (x 0 )+E q [log p θ (x 0:T ) q(x 1:T ∣x 0 ) +logp [log p θ (x 0:T ) q(x",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 1,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "что у диффузионных моделей другая форма модели с латентными переменными. Итак, давайте выведем ELBO для диффузии: − log log log log log log log log Let L VLB log log −logp θ (x 0 ) Let L VLB ≤−logp θ (x 0 )+D KL (q(x 1:T ∣x 0 )∥p θ (x 1:T ∣x 0 )) =−logp θ (x 0 )+E x 1:T ∼q(x 1:T ∣x 0 ) [log p θ (x 0:T ) q(x 1:T =−logp θ (x 0 )+E q [log p θ (x 0:T ) q(x 1:T ∣x 0 ) +logp [log p θ (x 0:T ) q(x 1:T ∣x 0 ) ] =E q(x 0:T ) [log p θ (x 0:T ) q(x 1:T ∣x 0 ) ]≥−E q(x 0 ) logp θ (x 0 ) Комментарий Если вы знакомы с VAE, то вывод L V L B L VLB должен быть вам понятен, однако ниже приведен вывод с помощью неравенства Йенсена log log log log log log VLB L CE =−E q(x 0 ) logp θ (x 0 ) =−E q(x 0 ) log(∫p θ (x 0:T )dx 1:T ) =−E q(x 0 ) log(∫q(x 1:T ∣x 0 ) q(x 1:T 0:T ) dx 1:T ) =−E q(x 0 ) log(E q(x 1:T ∣x 0 ) q(x 1:T 0:T ) ) ≤−E q(x 0:T ) log q(x 1:T 0:T ) =E q(x 0:T ) [log p θ (x 0:T ) q(x 1:T ∣x 0 ) ]=L VLB Теперь вернемся к распределению q(x t ∣x t−1 ). Для того чтобы получить x t x t , придется итеративно получать ,...,x t−1 . Однако это можно сделать более эффективно благодаря нормальным распределениям. Для этого обозначим :=1−β :=∏ i=1 t α i , тогда q(x t ∣x 0 )=N(x ,(1− α ˉ t )I) Формальный вывод этого факта где где q(x t−1 + 1−α t z t−1 ; где z t−1 ,z t−2 ,⋯∼N(0,I) = α t ( α t−1 x t−2 + 1−α t−1 z t−2 )+ 1−α t z t−1 = α t α t−1 x t−2 + 1−α t α t−1 z ˉ t−2 ; где z ˉ t−2 ∼N(0,I) (∗) =N(x ,(1− α ˉ t )I) (*) Пояснение ко второму переходу. У нас выходит где (1−α t−1 ) z t−2 + 1−α t z t−1 = α t (1−α t−1 )+(1−α t ) z ˉ t−2 = 1−α t α t−1 z ˉ t−2 ; где z t−1 ,z t−2 , z ˉ t−2 ∼N(0,I) Тогда L V L B L VLB может быть переписано как log VLB (q(x T ∣x 0 )∥p θ (x T )) ++ t=2 ∑ T L t−1 D KL (q(x t−1 ∣x t ,x 0 )∥p θ (x t−1 ∣x t )) L 0 −logp Долгий вывод Серым в скобках комментарий к последующему переходу. L VLB log (расписываем совместное распределение) = E q [ log (берем логарифм) = E q [ − log log (отщепляем члены суммы) = E q [ − log log log (*) = E q",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 2,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "где z t−1 ,z t−2 , z ˉ t−2 ∼N(0,I) Тогда L V L B L VLB может быть переписано как log VLB (q(x T ∣x 0 )∥p θ (x T )) ++ t=2 ∑ T L t−1 D KL (q(x t−1 ∣x t ,x 0 )∥p θ (x t−1 ∣x t )) L 0 −logp Долгий вывод Серым в скобках комментарий к последующему переходу. L VLB log (расписываем совместное распределение) = E q [ log (берем логарифм) = E q [ − log log (отщепляем члены суммы) = E q [ − log log log (*) = E q [ − log log log (лог произведения раскрываем) = E q [ − log log log log (от второй суммы останется только 1ый и последний член) = E q [ − log log log log (комбинируем 1 и 3 член, 3 и 4 член) = E q [ log log log log VLB =E q(x 0:T ) [log p θ (x 0:T ) q(x 1:T ∣x 0 ) ](расписываем совместное распределение) =E q [log p θ (x T )∏ t=1 T p θ (x t−1 ∣x t ) ∏ t=1 T q(x t ∣x t−1 ) ] (берем логарифм) =E q [−logp θ (x T )+ t=1 ∑ T log p θ (x t−1 ∣x t ) q(x t ∣x t−1 ) ](отщепляем члены суммы) =E q [−logp θ (x T )+ t=2 ∑ T log p θ (x t−1 ∣x t ) q(x t ∣x t−1 ) +log q(x 1 ∣x 0 ) ](*) =E q [−logp θ (x T )+ t=2 ∑ T log( p θ (x t−1 ∣x t ) q(x t−1 q(x t−1 ∣x 0 ) q(x t ∣x 0 ) )+log q(x 1 ∣x 0 ) ](лог произведения раскрываем) =E q [−logp θ (x T )+ t=2 ∑ T log p θ (x t−1 ∣x t ) q(x t−1 t=2 ∑ T log q(x t−1 ∣x 0 ) q(x t ∣x 0 ) +log q(x 1 ∣x 0 ) ](от второй суммы останется только 1ый и последний член) =E q [−logp θ (x T )+ t=2 ∑ T log p θ (x t−1 ∣x t ) q(x t−1 ∣x t ,x 0 ) +log q(x 1 ∣x 0 ) q(x T ∣x 0 ) +log q(x 1 ∣x 0 ) ](комбинируем 1 и 3 член, 3 и 4 член) =E q [log p θ (x T ) q(x T ∣x 0 ) + t=2 ∑ T log p θ (x t−1 ∣x t ) q(x t−1 ∣x t ,x 0 ) −logp (q(x T ∣x 0 )∥p θ (x T )) + t=2 ∑ T L t−1 D KL (q(x t−1 ∣x t ,x 0 )∥p θ (x t−1 ∣x t )) L 0 −logp Пояснение (*). Пользуемся тем, что у нас Марковский процесс, и теоремой Байеса: q(x t ∣x t−1 )=q(x t ∣x t−1 ,x 0 )= q(x t−1 ∣x 0 ) q(x t−1 ∣x t ,x 0 )q(x t ∣x 0 ) Таким",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 3,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "T ) q(x T ∣x 0 ) + t=2 ∑ T log p θ (x t−1 ∣x t ) q(x t−1 ∣x t ,x 0 ) −logp (q(x T ∣x 0 )∥p θ (x T )) + t=2 ∑ T L t−1 D KL (q(x t−1 ∣x t ,x 0 )∥p θ (x t−1 ∣x t )) L 0 −logp Пояснение (*). Пользуемся тем, что у нас Марковский процесс, и теоремой Байеса: q(x t ∣x t−1 )=q(x t ∣x t−1 ,x 0 )= q(x t−1 ∣x 0 ) q(x t−1 ∣x t ,x 0 )q(x t ∣x 0 ) Таким образом во время обучения, на каждой итерации параллельно оптимизируются случайные член L t L t с помощью градиентного спуск (сэмлируем t∼U{1,...,T}). Поскольку все распределения нормальные, то KL между ними можно выписать в явной форме (см. ниже). Формула KL между двумя нормальными KL(N 1 ∣∣ N 2 )= 2 1 (Tr(Σ 2 −1 Σ 1 )+(μ +ln det(Σ 1 ) det(Σ 2 ) −d) Если I, Σ KL(N 1 ∣∣ N +ln σ 1 σ 2 ) Осталось только выписать q(x t−1 ∣x t ,x 0 ) . Мы знаем, поскольку у нас все распределения нормальные, то и q(x t−1 ∣x t ,x 0 ) будет нормальным. Обозначим q(x t−1 ∣x t ,x 0 )=N(x t−1 Вывод q(x t−1 ∣x t ,x 0 ) Применим формулу Байеса и распишем. Тут мы просто пытаемся понять, как будут выглядеть среднее и дисперсия, выделяя квадратичную форму в показателе экспоненты q(x t−1 ∣x t ,x 0 )=q(x t ∣x t−1 ,x 0 ) q(x t ∣x 0 ) q(x t−1 ∣x 0 ) ∝ ∝ exp ∝exp(− t−1 t−1 (x t−1 − α ˉ t−1 exp =exp(− t−1 +α t x t−1 2 + 1− α ˉ t−1 x t−1 2 −2 α ˉ t−1 x 0 x t−1 + α ˉ t−1 ))= = exp =exp(− t−1 1 )x t−1 t−1 2 α ˉ t−1 x 0 )x t−1 +C(x t ,x 0 ))) Далее перепишем красные и синие выражения в более красивой форме =1/( t−1 1 )=1/( β t (1− α ˉ t−1 t−1 )=( t−1 α ˉ t−1 x 0 )/( t−1 t−1 α ˉ t−1 t−1 (1− α ˉ t−1 t−1 β t x 0 Другой лосс. Предсказываем шум В прошлой подсекции наша модель предсказывала среднее и дисперсию нормального распределения. Давайте зафиксируем ,t)=σ t 2 I. Обычно берут или t−1 β t . Тогда L t − 1 L t−1 из предыдущей секции можно переписать как ]+const(θ) Это первый момент, как меняется функционал, если мы не хотим предсказывать ,t), а фиксируем её. Теперь вспомним, что q(x t ∣x 0 )=N(x ,(1− α ˉ t )I), но благодаря тому, что у нас гауссовское распределение, это можно переписать в виде ,ϵ)= ϵ, ϵ∼N(0,I) Выразим отсюда x 0 x 0 и получим, что ϵ), тогда подставим это выражение в формулу для ) (из подсекции «Вывод q(x t−1 ∣x t ,x 0 )») и получим Теперь скажем, что наша модель будет предсказывать ϵ ϵ. И",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 4,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "t . Тогда L t − 1 L t−1 из предыдущей секции можно переписать как ]+const(θ) Это первый момент, как меняется функционал, если мы не хотим предсказывать ,t), а фиксируем её. Теперь вспомним, что q(x t ∣x 0 )=N(x ,(1− α ˉ t )I), но благодаря тому, что у нас гауссовское распределение, это можно переписать в виде ,ϵ)= ϵ, ϵ∼N(0,I) Выразим отсюда x 0 x 0 и получим, что ϵ), тогда подставим это выражение в формулу для ) (из подсекции «Вывод q(x t−1 ∣x t ,x 0 )») и получим Теперь скажем, что наша модель будет предсказывать ϵ ϵ. И просто будем «подставлять» его в выражение для μ ~ μ ~ выше. Обозначим предсказание модели как ,t) — предсказанный шум ϵ ϵ. Тогда лосс L t L t превратиться в (1− ∥ϵ−ϵ +(1− α ˉ t )ϵ,t)∥ 2 ] Тем не менее лосс можно еще больше упростить и просто обучать с помощью MSE на simple =E x 0 ,ϵ,t [∥ϵ−ϵ +(1− α ˉ t )ϵ,t)∥ 2 ] Итак, алгоритмы обучения и сэмплирования выглядят вот так (на картинке z : = ϵ z:=ϵ). Алгоритм обучения и сэмплирования диффузионной модели (Изображение взято из: Ho et al. 2020) Стоит отметить, что важным недостатком диффузионных моделей является низкая скорость сэмплирования. Согласно Song et al. 2020: «Требуется 20 часов на генерацию 50 тысяч картинок размера 32х32, используя DDPM, и меньше минуты, используя GAN» (Nvidia 2080 Ti GPU). Тем не менее, в данном направлении был достигнут значительный прогресс и в целом проблема медленного сэмплирования была частично решена: Jiaming Song et al. (2021), Kong & Ping (2021), Bond-Taylor et al. (2021) Давайте зафиксируем, какие функции потерь можно использовать. Для всех них справедлив тот факт, что мы сэмплируем шаг равномерно во время обучение t∼U{1,...,T}) и оптимизируем соответствующий L t L t . Оптимизируя член из суммы L V L B L VLB . Это KL дивергенция между двумя нормальными распределениями log VLB (q(x T ∣x 0 )∥p θ (x T )) ++ t=2 ∑ T L t−1 D KL (q(x t−1 ∣x t ,x 0 )∥p θ (x t−1 ∣x t )) L 0 −logp При фиксированной дисперсии Σ θ Σ θ можно оптимизировать взвешенную MSE между средними нормальных распределений ]+const(θ) При фиксированной дисперсии и при предсказании шума с помощью взвешенной MSE. Или просто MSE. simple является самым популярным вариантом, который на практике дает лучшие результаты. (1− ∥ϵ−ϵ +(1− α ˉ t )ϵ,t)∥ 2 ] L t simple =E x 0 ,z [∥ϵ−ϵ +(1− α ˉ t )ϵ,t)∥ 2 ] Выбор расписания β t β t Расписание является гиперпараметром, основными требованиями на который являются невозрастание ≤...≤β T ) и чтобы прямой процесс сходился к N(0,I) в пределе по T T. Второе может гарантироваться тем, что →0. Вспомним, q(x t ∣x 0 )=N(x ,(1− α ˉ t )I) Однако на практике оно также проверяется, чтобы (q(x T ∣x 0 )∣∣N(0,I)) было близко к 0. Также стоит упомянуть, что обычно берут T = 1000 T=1000. Но также важно помнить про требования выше, ведь расписание шума непосредственно зависит от T T.",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 5,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "=E x 0 ,z [∥ϵ−ϵ +(1− α ˉ t )ϵ,t)∥ 2 ] Выбор расписания β t β t Расписание является гиперпараметром, основными требованиями на который являются невозрастание ≤...≤β T ) и чтобы прямой процесс сходился к N(0,I) в пределе по T T. Второе может гарантироваться тем, что →0. Вспомним, q(x t ∣x 0 )=N(x ,(1− α ˉ t )I) Однако на практике оно также проверяется, чтобы (q(x T ∣x 0 )∣∣N(0,I)) было близко к 0. Также стоит упомянуть, что обычно берут T = 1000 T=1000. Но также важно помнить про требования выше, ведь расписание шума непосредственно зависит от T T. Чаще всего используют линейное расписание, где 0.02 β 1 =10 −4 , β T =0.02. У данных констант нет никакой мотивации, кроме той, которая описана выше. Они были предложены в Ho et al. (2020). В Nichol & Dhariwal (2021) было предложено косинусное расписание, которое помогло диффузионным моделям достичь лучшего NLL (negative loglikelihood): β t = clip 0.999 where f ( t ) = cos =clip(1− α ˉ t−1 α ˉ t ,0.999) α ˉ t = f(0) f(t) where f(t)=cos( 1+s t/T+s ⋅ 2 π ) Авторы обнаружили, что линейное расписание плохо работает на картинках 64х64 и меньше. А именно, последнии шаги прямого прохода были шумными и малоинформатиыными (просто зашумляем шум еще больше): Пример зашумления картинки для линейного (сверху) и косинусного (снизу) расписания. Также они обнаружили, что если обучать модель с линейным расписанием только на 80% первых шагов, то модель не становится сильно хуже, что подтверждает неиформативность последних шагов. Далее, они подобрали расписание так, чтобы убывало линейно на большей части отрезка (от 0 до T T) и почти не менялось рядом с 0 и T T. Разницу в для разных расписаний можно увидеть на картинке ниже: Изображение взято из Nichol & Dhariwal, 2021 Детали Также они ограничивают β t β t числом 0.999, чтобы в конце процесса не было проблем с численной устойчивостью. Коэффициент s s используется, чтобы β t β t не были слишком малы рядом с нулем. Он равен 0.008. Такое число было выбрано так, чтобы « β 0 β 0 была немного меньше, чем размер бина одного пикселя, то есть 1 / 127.5 1/127.5» Classifier guidance В Nichol & Dhariwal (2021) был предложен метод условной генерации, который повышает качество генерируемых картинок, при этом уменьшая их разнообразие. Для этого предобучается «шумный» классификатор на зашумленных картинках, то есть (y∣x t ). Затем он используется во время сэмплирования, корректируя предсказанное среднее на ∇ x log logp ϕ (y∣x t ). В Nichol & Dhariwal (2021) (Секция 4.1) показывают, что данная добавка позволяет превратить распределение i−1 ∣x i ) в i−1 ∣x i ,y). Важно, что исходная диффузионная модель никак не меняется, что делает трюк еще более привлекательным. Алгоритм сэмплирования можно видеть на картинке ниже. Коэффициент s s отвечает за силу guidance. Мотивация У генеративной модели GAN есть способ, который позволяет «балансировать» между разнообразием картинок и их качеством — truncation trick. Он заключается в сэмплировании латентного вектора truncated normal distibution. Данный трюк был хорошо описан и исследован в статье про BigGAN.",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 6,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "среднее на ∇ x log logp ϕ (y∣x t ). В Nichol & Dhariwal (2021) (Секция 4.1) показывают, что данная добавка позволяет превратить распределение i−1 ∣x i ) в i−1 ∣x i ,y). Важно, что исходная диффузионная модель никак не меняется, что делает трюк еще более привлекательным. Алгоритм сэмплирования можно видеть на картинке ниже. Коэффициент s s отвечает за силу guidance. Мотивация У генеративной модели GAN есть способ, который позволяет «балансировать» между разнообразием картинок и их качеством — truncation trick. Он заключается в сэмплировании латентного вектора truncated normal distibution. Данный трюк был хорошо описан и исследован в статье про BigGAN. Поэтому в диффузионных моделях тоже хотелось бы иметь метод, который позволяет балансировать между качеством и разнообразием. Авторы предложили classifier guidance, сравнили его с truncation trick и показали, что их метод строго лучше. Изображение взято из Nichol & Dhariwal, 2021 Classifier-free guidance Ho & Salimans (2021) предложили метод, в котором guidance достигается без использования дополнительной модели, поскольку это достаточно затратно. Для этого они обучали условную модель ∣y), у которой во время обучения реальная метка y y заменялась с какой-то фиксированной вероятностью (10%) на пустую метку ( y = ∅ y=∅). Это по сути позволяет нам обучать безусловную модель ) одновременно с условной ∣y)Тогда во время сэмплирования делаем так, чтобы предсказание немного менялось в сторону ∣y), а именно: ∣y)=ϵ θ (x t ∣∅)+s⋅(ϵ θ (x t ∣y)−ϵ θ (x t ∣∅)) Мотивация этой формулы следовала из формулы Байеса: log log log log log log log p(y∣x t )∝ p(x t ) p(x t ∣y) ⟹logp(y∣x t )∝logp(x t ∣y)−logp(x t ) ⟹∇ x t logp(y∣x t )∝∇ x t logp(x t ∣y)−∇ x t logp(x t ) ⟹∇ x t logp(y∣x t )∝ϵ(x t ∣y)−ϵ(x t ) Тогда мы можем просто подставить ∇ x t log logp(y∣x t ) в формулу для classifier guidance из предыдущей подсекции и получить желаемое равенство с точностью до коэффициента s s. Овервью ключевых работ на сегодняшний день Jonathan Ho et al. «Denoising diffusion probabilistic models.» arxiv Preprint arxiv:2006.11239 (2020) Основная работа, в которой диффузионные модели (Denoising Diffusion Probabilistic Models, DDPMs) были применены для генерации картинок. Параграф в основном построен на ней. Jiaming Song et al. «Denoising diffusion implicit models.» arxiv Preprint arxiv:2010.02502 (2020) Одна из первых попыток ускорить генерацию объектов. Идея следущая: давайте изменим прямой диффузионный процесс так, чтобы используя предобученную DDPM, приближать новый обратный процесс за меньшее число шагов. Чтобы не обучать новую модель, нам нужен прямой диффузионный процесс, у которого будет такая же (суррогатная) функция потерь, а обратный процесс все еще останется Марковским. Оказалось, что существует целое семейство не-Марковских прямых процессов, удовлетворяющих этим требования. Это семейство имеет следующий вид: 1:T ∣x 0 ):=q t=2 ∏ T q σ (x t−1 ∣x t ,x 0 ), где )=N( α t x 0 ,(1−α t )I) и для всех t > 1 , t>1, t−1 ∣x t ,x 0 )=N( α t−1 x 0 + 1−α t−1 −σ t 2 ⋅ 1−α Среднее было выбрано так, чтобы )=N( α t x 0 ,(1−α t )I) для",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 7,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "нужен прямой диффузионный процесс, у которого будет такая же (суррогатная) функция потерь, а обратный процесс все еще останется Марковским. Оказалось, что существует целое семейство не-Марковских прямых процессов, удовлетворяющих этим требования. Это семейство имеет следующий вид: 1:T ∣x 0 ):=q t=2 ∏ T q σ (x t−1 ∣x t ,x 0 ), где )=N( α t x 0 ,(1−α t )I) и для всех t > 1 , t>1, t−1 ∣x t ,x 0 )=N( α t−1 x 0 + 1−α t−1 −σ t 2 ⋅ 1−α Среднее было выбрано так, чтобы )=N( α t x 0 ,(1−α t )I) для всех t t. (см. Лемму 1 в Приложении B к статье). То есть важно лишь то, чтобы маргинальное распределение ) не менялось по сравнению с обычным Марковским случаем. Прямой процесс может быть получен с помощью теоремы Байеса: t−1 t−1 t−1 Тут σ σ контролирует степень стохастичности прямого процесса. Можно заметить, что в отличии от исходного диффузионного процесса, предложенный прямой процесс больше не является Марковским, так как каждый x t x t теперь зависит и от x t − 1 x t−1 и от x 0 x 0 . Схематично, это можно изобразить как на картинке справа. (Слева исходный диффузионный процесс для сравнения) Screen Заметка Авторы обращают внимание, что функция потерь в DDPM зависит от q(x t ∣x 0 ), а не от q(x 1 :x T ∣x 0 ) напрямую. Это означает, что нам нужно выбрать любой другой прямой диффузионный процесс, у которого q(x t ∣x 0 ) остались те же. Далее, мы можем переписать обратный процесс в данном виде: t−1 = α t−1 \"predictedx 1−α t ϵ θ (t) (x t ) + \"directionpointingtox t \" 1−α t−1 −σ t 2 ⋅ϵ θ (t) (x t ) + randomnoise σ t ϵ t Заметим, что при (1−α t−1 )(1−α t ) 1−α t /α t−1 прямой процесс становится марковским, а обратный как у DDPM (обычное сэмплирование, описанное в основной секции). При =0 процесс сэмплирования становится детерминистичным (данный способ и называется DDIM). Ускорение сэмплирования достигается засчет использования лишь какого-то подмножества шагов ( 0≤τ 1 ≤...≤τ S ≤T, S<T). Также одним из плюсов детерминистичного сэмплирования является возможность делать семантическую интерполяцию в латентном пространстве (как у GANов). Alex Nichol & Prafulla Dhariwal. «Improved denoising diffusion probabilistic models» arxiv Preprint arxiv:2102.09672 (2021) Улучшение DDPM, в котором был предложен новое расписание шума, что улучшило NLL. Также был изучен вариант, в котором дисперсия ,t) предсказывается моделью. Prafula Dhariwal & Alex Nichol. «Diffusion Models Beat GANs on Image Synthesis.» arxiv Preprint arxiv:2105.05233 (2021). Статья, в которой показывается, что DDPM могут генерировать более качественные картинки по сравнению с GANами. Также был предложен метод conditional сэмплирования. Для этого предобучается классификатор на зашумленных сэмплах, а во время сэмплирования среднее нормального распределения «корректируется» на градиент классификатора. Jacob Austin et al. «Structured Denoising Diffusion Models in Discrete State-Spaces».arXiv:2107.03006 (2021) Диффузионные модели на дискретных данных (например, текст). Вместо нормальных распределений используются категориальные. Также была обобщена мультиномиальная диффузия с помощью «матриц перехода», которые задают способ зашумления дискретных данных. Более подробно: у нас есть",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 8,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "моделью. Prafula Dhariwal & Alex Nichol. «Diffusion Models Beat GANs on Image Synthesis.» arxiv Preprint arxiv:2105.05233 (2021). Статья, в которой показывается, что DDPM могут генерировать более качественные картинки по сравнению с GANами. Также был предложен метод conditional сэмплирования. Для этого предобучается классификатор на зашумленных сэмплах, а во время сэмплирования среднее нормального распределения «корректируется» на градиент классификатора. Jacob Austin et al. «Structured Denoising Diffusion Models in Discrete State-Spaces».arXiv:2107.03006 (2021) Диффузионные модели на дискретных данных (например, текст). Вместо нормальных распределений используются категориальные. Также была обобщена мультиномиальная диффузия с помощью «матриц перехода», которые задают способ зашумления дискретных данных. Более подробно: у нас есть ∈{1,...,K} — дискретная величина на всех шагах диффузии, тогда для каждого шага t t определена матрица прямого перехода Q t Q t такая, что =q(x t =j∣x t−1 =i). То есть строки матрицы суммируются в единицу. Тогда если обозначить через one-hot-закодированную версию x t x t , то прямой процесс можно описать через категориальные распределения: q(x t ∣x t−1 )=Cat(x t ;p=x t−1 Q t ) Как и в нормальных распределениях, можем выписать где q(x t ∣x 0 )=Cat(x t ;p=x 0 Q ˉ t ), где ...Q q(x t−1 ∣x t ,x 0 )=Cat(x t−1 ;p= t−1 ) Поскольку тут нет такой хорошей параметризации через ϵ ϵ, как у нормальных распределений, то единственный способ обучать — с помощью KL дивергенции (членами L V L B L VLB ). Остается только понять, как выбирать Q t Q t . Помимо того, чтобы сумма в каждой строчке была один, требуется, чтобы сходилось (при t → ∞ t→∞) к равномерному распределению в каждой строчке (аналог нормального шума). За конкретными примерами стоит обратиться к статье. Серия работ про text-conditional diffusions: GLIDE, ImaGen, DALLE-2 Опишем работу метода GLIDE. Стоит задача генерировать картинки по заданному текстовому описанию. Для этого используется classifier-free guided diffusion model или CLIP. Это два разных варианта модели, которые авторы сравнивают. В первом случае модель обуславливается на эмбеддинги текста, которые были получены из обучаемого трансформера. Во втором случае guidance осуществляется за счет ⟨f(x t ),g(c)⟩ (это по сути градиент лосса метода CLIP) . Тут f f — это картиночный энкодер (на зашумленных картинках), а g g — это энкодер текстового входа. В целом, авторы получили, что classifier-free guidance генерирует более качественные картинки. Song et al. «Score-Based Generative Modeling through Stochastic Differential Equations» Способ описать диффузионные модели через стохастические дифференциальные уравнения. What are Diffusion Models?. Прекрасный блог от Lilian Weng (OpenAI). Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.4. Нормализующие потоки Следующий параграф 8.6. Языковые модели",
    "metadata": {
      "title": "Диффузионные модели",
      "url": "https://education.yandex.ru/handbook/ml/article/diffuzionnye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.5",
      "part": 9,
      "total_parts": 9,
      "source_file": "8.5.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "imgonline_com_ua_Black_White_E_Xr_Oil_BVM_Bi_Ya_c484034b32.webp Евгений Косарев В 2023 году ChatGPT стал самой узнаваемой языковой моделью машинного обучения во всём мире — причём как среди специалистов, так и среди обычных людей. Способность вести осмысленный диалог, отвечать на практически любые вопросы и быть применимыми без дообучения в большом спектре задач с высоким качеством — вот залог их популярности. В этом параграфе мы расскажем, что такое языковые модели, как они устроены, как развивались, а также как изменились за последнее время. Что такое языковые модели? Говоря простым языком, языковые модели — это алгоритмы, способные продолжать тексты. Если чуть усложнить, то это вероятностные алгоритмы, и к ним сразу можно задать эмпирический критерий качества: хорошая модель даёт разумные продолжения данных ей текстов. Artboard Давайте разберём пример выше. Модель высчитывает вероятность возможных продолжений текста и предлагает их нам. Слово «фрукт» — наименее разумное продолжение нашей фразы, в то время как слово «наука» — наиболее разумное. И действительно, это часть определения машинного обучения, которое мы давали в начале этого учебника. Таким образом, нам осталось лишь научить алгоритм моделировать эти вероятности и максимизировать их для разумных предложений. Но как это сделать? По ходу развития языковых моделей подходы менялись, мы расскажем о каждом из них в хронологическом порядке. Начнём с краткого экскурса в историю — поговорим о статистических моделях, рекуррентных нейронных сетях и трансформерах. А затем перейдём к современным — GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT и LLaMa. Развитие языковых моделей Статистические модели Идея модели лежит на поверхности, много где применяется в самых разных вариациях даже в ХХ веке, поэтому сложно назвать авторов или точную дату создания. Однако этот метод популярен до сих пор — используется в клавиатурах смартфонов для исправления опечаток и быстрого набора текстов через Т9. Теперь подробнее о методе. Напомним вероятностную формулировку цепей Маркова в общем виде: P(w 1 ,w 2 ,...,w N )=P(w N ∣w 1 ,w 2 ,...,w N−1 )×P(w 1 ,w 2 ,...,w N−1 )=P(w N ∣w 1 ,w 2 ,...,w N−1 )×P(w N−1 ∣w 1 ,w 2 ,...,w N−2 )×...×P(w 2 ∣w 1 )×P(w 1 ) Если представить, что w i w i — это слово, а набор этих омега — это предложение, то по формуле становится возможным посчитать вероятность предложения ,...,w N С практической точки зрения всё чуть сложнее, ведь распределение слов в реальном языке (какое, с какими и как часто встречается), вообще говоря, неизвестно. Его принято аппроксимировать на основе корпуса текстов (например, всего интернета) — в этом случае считаются совстречаемости слов друг с другом, и по ним считаются вероятности. В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении P(w N ∣w 1 ,w 2 ,...,w N−1 ) длина контекста равна N − 1 N−1. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин: Сложность в подсчёте и хранении каждого возможного уникального контекста длины K K. Если корпус текстов состоит из N N различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна N K N K , что очень много при больших K K. Большой контекст реже встречается. То есть",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 1,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "по ним считаются вероятности. В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении P(w N ∣w 1 ,w 2 ,...,w N−1 ) длина контекста равна N − 1 N−1. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин: Сложность в подсчёте и хранении каждого возможного уникального контекста длины K K. Если корпус текстов состоит из N N различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна N K N K , что очень много при больших K K. Большой контекст реже встречается. То есть слова «яблоку», «негде» и «упасть» поодиночке встречаются чаще, чем их комбинация «яблоку негде упасть». Отсюда достаточность статистик падает с ростом длины контекста. В учебном примере предлагается ограничиться шириной контекста размера 1: P(w 1 ,w 2 ,...,w N )=P(w N ∣w N−1 )×P(w N−1 ∣w N−2 )×...×P(w 2 ∣w 1 )×P(w 1 ) Artboard Интересно, что такой подход достаточно популярен до сих пор. Например, он используется в умных клавиатурах, чтобы подсказать следующее слово. Достоинства статистических моделей: Простота имплементации. Высокая скорость работы алгоритма. Низкая вычислительная стоимость обучения и инференса. Недостатки статистических моделей: Не сможет сгенерировать слова, которые не шли подряд в обучающем корпусе. Очень маленький контекст. Длинные последовательности равновероятны ≈ нулю (в цепях Маркова для длинных последовательностей много множителей меньше нуля, поэтому их произведение уже практически равно нулю для любых множителей). Отсюда алгоритм не может выдавать разумные продолжения большой длины. Токенизация Языковые модели, да и вообще все модели, которые оперируют текстом, используют понятие токена. Токен — это единица текста, которую понимают алгоритмы. В примере выше токен — это отдельное слово w i w i (этот подход называется мешком слов), однако текст можно разбивать на токены и иначе. Artboard Раньше предложение разбивалось на слова по пробелам, знакам препинания, исключались стоп-слова и так далее (назовем это CountVectorizer). Но у этого подхода возникали две проблемы с разными словоформами. Они: Либо обозначались разными токенами, что не совсем верно, ведь слово-то одно и то же. И получалось, что похожим смыслом обладало сразу несколько токенов. Либо приводились к начальной форме — и в итоге терялся падеж, время, число. Современные токенизаторы построены на алгоритме BPE (Byte Pair Encoding; об устройстве BPE более подробно можно прочитать в учебнике Лены Войта). Решение требует фиксации определённого числа токенов. Как только это сделано, в словарь добавляются все символы из текста, ищутся самые частые их сочетания и снова добавляются. Этот процесс продолжается до тех пор, пока число токенов не станет равно заданному значению. Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет _ перед соответствующим токеном) и не построен на логике разбиения слов по разделителям. Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать здесь. 💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 2,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "пор, пока число токенов не станет равно заданному значению. Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет _ перед соответствующим токеном) и не построен на логике разбиения слов по разделителям. Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать здесь. 💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в отдельный токен, так как чисел бесконечное количество. Сейчас используется идея о разбиении всех чисел на цифры, чтобы множеством из десяти токенов представить всё многообразие чисел. Рекуррентные нейронные сети (RNN) Появились после статистических моделей, подробнее о хронологии здесь. Рекуррентные нейронные сети концептуально можно описать формулой, где: А А — некоторая модель; h t h t — внутреннее состояние модели на момент времени — токен, который сейчас обрабатывается. Тогда следующий токен x t + 1 x t+1 получается так: t+1 =g(h t ); h t =A(h t−1 ,x t ) ссылка на источник картинки Подробно об устройстве RNN мы рассказываем в параграфе Нейросети для работы с последовательностями. Здесь же коротко отметим, что существуют различные модификации рекуррентных сетей, которые усложняют структуру алгоритма А А, даже добавляют механизм внимания Attention. Если коротко, то он позволяет лучше оценивать взаимосвязи токенов в тексте. Все они в разной степени помогают модели усваивать более длинные и сложные последовательности токенов. Достоинства RNN: Высокая скорость инференса и сравнительно низкая стоимость. Более качественный текст, чем у моделей на статистиках. Теоретически понимает контекст в сотни слов (а с Attention ещё больше). Точно учитывает весь контекст документа. Недостатки RNN: Невозможность параллельного обучения на многих устройствах, отсюда не получится просто так обучить большую RNN. Модель «хорошо помнит» лишь несколько последних токенов контекста (без Attention). Проблемы с обучением (exploading/vanishing gradients). Трансформеры Более подробно трансформеры и их устройство описаны в параграфе Трансформеры. Последней и наиболее успешной с точки зрения качества оказалась архитектура трансформеров. Она состоит из двух частей: encoder (на изображении слева) и decoder (на изображении справа). ссылка на источник картинки Изначально был популярен подход обучать части отдельно. Так на базе encoder-блоков были построены BERT-модели. Идея обучения звучит несложно: давайте из входного текста замаскируем токеном MASK 15% имеющихся токенов и обучим модель угадывать, какие именно токены были скрыты. Тогда, если модель обучится это делать, она сможет очень хорошо понимать текст. Таким образом, энкодеры обладают следующими особенностями: Анализируют входной текст и связи между токенами. Выделяют важные токены для определённой задачи. Ничего не генерируют. На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель: генерирует токен за токеном. смотрит на весь контекст, архитектурно, нет забывания токенов. имеет возможность (как и BERT-модели) обучаться параллельно. обладает достаточно высокой вычислительной стоимостью инференса. 💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 3,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "очень хорошо понимать текст. Таким образом, энкодеры обладают следующими особенностями: Анализируют входной текст и связи между токенами. Выделяют важные токены для определённой задачи. Ничего не генерируют. На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель: генерирует токен за токеном. смотрит на весь контекст, архитектурно, нет забывания токенов. имеет возможность (как и BERT-модели) обучаться параллельно. обладает достаточно высокой вычислительной стоимостью инференса. 💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно за понимание контекста отвечает блок Attention, и размеры матриц в нём как раз определяют размер контекста. Размер матриц конечен: чем они больше, тем сложнее вычислять блок внимания, поэтому контекст существенно ограничен. На момент написания параграфа разработаны различные модификации Attention, позволяющие растить понимаемый контекст, однако они имеют ряд проблем, с которыми предлагаем ознакомиться читателю самостоятельно. Современные подходы GPT-1 & GPT-2 Начнём немного издалека, с моделей GPT-1 и GPT-2. Первая была обучена в 2018 году на 7000 книг и имела размер контекста в 512 токенов. И она сразу получилась довольно сильной: после дообучения на специализированные задачи (бенчмарки) показывала на них лучшее на то время качество. Так, в задачах CoLA (бенчмарк классификационный, в нём надо определить грамматическую корректность предложения) результат вырос до 45,4 против прежнего результата в 35,0 у RNN. А в GLUE — с 72,8 до 68,9. Вторая модель была обучена в 2019 году. Она состояла из рекордных для того времени 1,5 млрд параметров (то есть была в ~10 раз больше первой), имела контекст в 1024 токена и была обучена на 40 ГБ текстовых данных. GPT-2 снова побеждала предыдущие подходы, включая GPT-1, на многих бенчмарках. По сравнению с первой версией модели у второй произошел качественный рост: теперь она могла генерировать разумные тексты — а не только предложения. Правда, не всегда и не с первой попытки. GPT-3 GPT-3 стала революцией с точки зрения качества и размеров. В 2020 году была получена модель размером в 175 млрд параметров, она обучалась на 570 ГБ текстовых данных с контекстом в 2048 токенов. Модель могла решать целый спектр задач, включая перевод, суммаризацию и ответы на вопросы, с качеством, близким к человеческому уровню, а также отличалась высокой способностью генерировать креативный контент. Демонстрацию работы модели лучше посмотреть в этой статье на 28 странице и далее. Модель Число обучающих данных Контекст Число параметров Decoder-слои Hidden-size (размерность тензоров внутри модели) Train batchsize (размер батча при обучении) GPT 7000 книг 512 117 млн 12 768 64 GPT-2 40 ГБ текстовых данных 1024 1,5 млрд 48 1600 512 GPT-3 570 ГБ текстовых данных 2048 175 млрд 96 12 288 3 200 000 Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу. Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря — графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 4,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "batchsize (размер батча при обучении) GPT 7000 книг 512 117 млн 12 768 64 GPT-2 40 ГБ текстовых данных 1024 1,5 млрд 48 1600 512 GPT-3 570 ГБ текстовых данных 2048 175 млрд 96 12 288 3 200 000 Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу. Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря — графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно много. Что с этим делать? Использовать подводки. Подводки Few-shot обучение Оказывается, что обучать большие языковые модели решать определённые задачи не всегда нужно (как мы говорили ранее, это ресурсоёмко): можно составить few-shot подводку. Подводка — словесное описание поставленной задачи, составленное определенным образом. Представим, что мы хотим осуществить перевод с английского на французский. Для обучения нам необходимо было бы составить пары ( X , y ) (X,y), где X X — слово на английском, а y y — на французском. Сделаем иначе — опишем задание на естественном языке: ссылка на источник картинки Здесь на английском языке сформулировано задание и предлагается слово «cheese» перевести на французский. Назовем такую конструкцию zero-shot-примером. Такой запрос GPT-3, возможно, поймёт, но работать будет плохо. Давайте увеличим количество примеров в подводке и назовем эту конструкцию one-shot: ссылка на источник картинки Или больше, и это будет few-shot: ссылка на источник картинки При этом приёме не тратятся ресурсы на обучение модели, она лишь смотрит на контекст и генерирует продолжение. Оказывается, этого достаточно, чтобы сравняться с downstream-обучением. Продемонстрируем преимущество такого подхода на двух бенчмарках. TriviaQA — вопросно-ответный бенчмарк, составленный на основе Википедии. Он помогает оценивать знания модели и ее ответы на вопросы. Lambada — оценивает меморизацию длинного контекста модели. Чем выше скор, тем лучше модель на обоих бенчмарках. ссылка на источник картинки ссылка на источник картинки Графики выше демонстрируют несколько особенностей: Few-shot позволяет получать качество, сравнимое с дообучением на определённом датасете, и стремится к человеческому качеству. С ростом числа обучаемых параметров модели растет её качество. На правом графике few-shot-примеры начинают работать лучше zero-shot-примеров лишь с некоторого размера модели. Это говорит о том, что модель начинает демонстрировать «умные» свойства лишь начиная с некоторого размера. 💡 На самом деле последний пункт достаточно часто встречается в языковых моделях. Случается так, что определённые приёмы не работают с маленькими моделями, но показывают себя лишь на больших. Это можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает большую обобщающую способность. Формулировка имеет значение Few-shot действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо. Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья: Artboard Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы этой статьи заметили, что сама по себе конструкция few-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от: Текстового описания задачи. Числа примеров в подводке. Порядка,",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 5,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает большую обобщающую способность. Формулировка имеет значение Few-shot действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо. Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья: Artboard Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы этой статьи заметили, что сама по себе конструкция few-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от: Текстового описания задачи. Числа примеров в подводке. Порядка, в котором примеры следуют друг за другом в подводке. Формате составления few-shot. Чтобы улучшить качество решения задачи, авторы предлагают осуществлять калибровку подводок. В статье они заметили, что модели смещены относительно подводок, то есть переформулировка запроса ведёт к смещению в ответе модели, а также к росту разброса ответов. Например, модели задают вопрос и её задача — ответить «да» или «нет». Если few-shot состоит из четырёх примеров и они идут в порядке «да», «да», «нет», «нет», то, вероятнее всего, дальше модель ответит «нет» на любой вход, просто потому что слово «нет» встречалось последним. Калибровать модель предлагается с помощью выученного линейного преобразования: =softmax(W p ^ +b) В этом преобразовании: W W и b b — обучаемые; p ^ p ^ — вероятности на выходе модели; q ^ q ^ — откалиброванные вероятности; Обучающие данные собираются так: ссылка на источник картинки Для различных задач собираем подводки и добавляем нейтральное слово N/A. В этом примере несмещённая модель должна давать с вероятностью 50% ответ «positive» или «negative». Чтобы добиться такого распределения ответов у смещённой модели, представим: W=diag( p ^ ) −1 , b=0 Также все few-shot-примеры стандартизуются в специальный формат вопрос — ответ, как на картинке выше. Этот метод (синий график) по сравнению со стандартными few-shot-примерами (красный график) помог повысить качество и уменьшить разброс результата. Таким образом, оптимизировав всего 4 параметра, авторы существенно улучшили итоговый результат. ссылка на источник картинки Promt-tuning Качество работы модели зависит от подводки, и few-shot просто один из способов её построения. Эксперименты показывают, что грамотный подбор промта позволяет экономить на обучении и решать задачи с высоким качеством. Проблема в обучении больших моделей — нехватка оперативной памяти на GPU, поэтому не будем оптимизировать все параметры модели. Пусть необходимо решить задачу А А, к ней имеется обучающее множество вида ( X , y ) (X,y). Введём дополнительные токены, которых не было в словаре: >,<P 2 >,…,<P k > — и будем добавлять в каждый текст из X согласно какому-то правилу. Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец. Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо оптимизировать лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели. ссылка на источник картинки 💡 Эффект от такого трюка достаточно многогранен: Меньше обучаемых параметров — меньше памяти занимает модель. Меньше обучаемых параметров — быстрее происходит обучение. Обычно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 6,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "2 >,…,<P k > — и будем добавлять в каждый текст из X согласно какому-то правилу. Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец. Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо оптимизировать лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели. ссылка на источник картинки 💡 Эффект от такого трюка достаточно многогранен: Меньше обучаемых параметров — меньше памяти занимает модель. Меньше обучаемых параметров — быстрее происходит обучение. Обычно нужно сильно меньше обучающих данных, чем при традиционном обучении всей модели для достижения высокого качества. Высокое качество результата. Получается, что можно оптимизировать подводку, или, другими словами, находить наиболее оптимальный промт, который лучше прочих решает поставленную задачу. Как повысить качество решения задач из разных категорий Языковые модели призваны решать самый широкий спектр текстовых задач — вопросно-ответные, суммаризацию, диалоговость, перевод и многие другие. Получается, что модель должна после некого обучения (подбора подводки или оптимизации вообще всех параметров под каждую задачу) решать каждую из них на высоком уровне. Однако модель обычно учится на текстах из интернета, книгах и других доступных ресурcах. И формат задачи, который обычно требуется от модели, не соответствует тому, что алгоритм привык видеть на обучении. К этому стоит добавить, что среди веб-документов просьба что-то сократить или определить тональность документа встречается не очень часто. Исправить этот недостаток призваны подходы по генерализации языковых моделей: FLAN и T0. Инструкции даются на естественном языке и для подготовки качественного обучающего множества предлагается произвести следующие действия: Каждой отдельной задаче (будь то перевод, написание отзывов или суммаризация) пишется по несколько различных подводок, отражающих смысл задания. Итоговый датасет составляется из отдельных задач, все строчки датасета перемешиваются случайным образом. Авторы стараются собрать как можно более разнообразные задачи в обучающее множество. ссылка на источник картинки ссылка на источник картинки ссылка на источник картинки Две картинки сверху демонстрируют FLAN- и T0- подходы по созданию датасета, а картинка снизу — рост усреднённого качества модели после обучения на смеси. Таким образом с некоторого размера модели наблюдается повышение метрик качества при дальнейших дообучениях генерализованной модели на отложенных задачах. Chain-of-Thought Предыдущий подход со смесью датасетов помогает решать многие задачи в среднем заметно лучше. Однако есть задачи, где качество результатов модели всё ещё низкое. Например, предложить эффективный код, решающий некую алгоритмическую задачу, найти минимум некоторой аналитической функции потерь, посчитать производную фукнции в точке и так далее. Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле Chain-of-Thought (CoT): ссылка на источник картинки CoT-подводка состоит из трёх обязательных элементов: Формулировки задачи на естественном языке. Подробного пошагового решения. Ответа на задачу. Формирование такого промта, особенно на few-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторы этой статьи сравнили на двух математических бенчмарках способность модели решать сложные задачи. MultiArith — проверяет умение решать простые арифметически задачки. GSM8K — более сложные. Результаты демонстрируют, что наличие CoT в подводке увеличивает способность решать математические задачки у больших языковых моделей. Artboard InstructGPT",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 7,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле Chain-of-Thought (CoT): ссылка на источник картинки CoT-подводка состоит из трёх обязательных элементов: Формулировки задачи на естественном языке. Подробного пошагового решения. Ответа на задачу. Формирование такого промта, особенно на few-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторы этой статьи сравнили на двух математических бенчмарках способность модели решать сложные задачи. MultiArith — проверяет умение решать простые арифметически задачки. GSM8K — более сложные. Результаты демонстрируют, что наличие CoT в подводке увеличивает способность решать математические задачки у больших языковых моделей. Artboard InstructGPT Наконец, обсудив, как готовить обучающие данные, перейдем к прародителю ChatGPT. Инструкционная модель — это та, которая обучена отвечать на пользовательские запросы в режиме zero-shot (а вообще, и few-shot, и любой человекочитаемый формат) с высоким качеством. ссылка на источник картинки InstructGPT — это модель, и она интересна с точки зрения выработки концепции обучения всех инструкционных моделей (InstructGPT, ChatGPT, GPT-4 и других). С некоторыми нюансами обучение состоит из четырех этапов: Подготовка качественного претрейна. Языковая модель должна содержать в себе как можно больше знаний о мире, чтобы иметь возможность в последующем решать произвольные задачи с высоким качеством. На этом этапе необходимо озаботиться наибольшим разнообразием, чистотой и полнотой обучающих данных. Подробнее об этом мы поговорим в последнем разделе этого параграфа. SFT (supervised finetuning) — обучение модели следовать инструкциям. Этот пункт мы подробно обсудили в предыдущей части параграфа (T0, FLAN, CoT). На этом этапе важно составить грамотный инструкционный датасет, где инструкция содержит произвольные запросы к модели, а ответ на неё — подробный текст, которым будущий пользователь будет доволен. Грамотный сбор таких данных довольно дорогостоящий процесс, но от него напрямую зависит, каким образом модель будет взаимодействовать с пользователем. Обучение reward-модели. Каждый ответ алгоритма можно оценить с точки зрения вежливости, подробности или персонажности. Персонажность позволяет модели считать себя, например, капитаном Джеком Воробьем и общаться на пиратском говоре. Также есть менее формализуемые критерии качества ответов, их даже сложно описать словами. Например, что в основном людям ответ 1 нравится больше чем ответ 2. Reward-модель агрегирует эти кртитерии в число — меру качества. Чем оно выше, тем качественнее ответ модели. Для выравнивания поведения модели обычно важно уметь оценивать тысячи текстов, а вручную это делать дорого и долго, поэтому обучается специальная модель-оценщик. Про то, как обучать reward-модель, будет рассказано далее. Этап Reinforcement Learning (RL). На нём языковая модель обучается генерировать такие ответы, которые имели бы наивысшую оценку относительно reward-модели. Про то, как делать RL, будет рассказано далее. ChatGPT Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в: Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных. Размере и качестве инструкционного датасета. Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL. К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели. Как обучить свою LLM? Обсудим детально на примере доступных в open-source моделей семейства LLaMA.",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 8,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "как делать RL, будет рассказано далее. ChatGPT Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в: Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных. Размере и качестве инструкционного датасета. Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL. К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели. Как обучить свою LLM? Обсудим детально на примере доступных в open-source моделей семейства LLaMA. LLaMa В качестве примера возьмём самую свежую архитектуру трансформеров на первую половину 2023 года — LLaMa, а также способы превращать её в чатовую модель, проводить Alignment на примере LLaMa-2. Вторая модель архитектурно не отличается от первой (кроме увеличенного контекста до 4096 токенов), поэтому содержание статей можно объединить в один рассказ. Претрейн Для обучения с нуля качественной языковой модели необходимы: мощный кластер на сотни видеокарт, на котором можно производить параллельное обучение модели. Больше GPU — больше модель можно обучить и быстрее по времени обучения; терабайты текстовых данных для тренировки на них; архитектура, которая лучшим образом может моделировать язык. Поговорим подробнее о двух последних пунктах. Текстовые данные Текстовые данные можно брать из открытых источников, таких как CommonCrawl, C4, Taiga и прочее. Важно обеспечить: чистоту данных — например, убрать html-тэги, устранить дублирование текстов; полноту — чтобы модель одинаково хорошо решала математические задачи, писала код или сочиняла стихотворения, текстов соответствующих доменов должно быть в достатке в обучающем корпусе; разнообразие данных. Существуют эмпирические законы обученности модели, но здесь остановимся на числе пройденных за обучение токенов. В LLaMa-моделях это значение варьируется от 1T до 2Т. Ниже приведены основные параметры по числу размерности внутренних эмбедингов, числу голов Attention, слоёв и параметров обучения разных моделей: ссылка на источник картинки Архитектура У LLaMa-моделей предлагается целый ряд архитектурных изменений. Так как в учебнике рассматривался лишь базовая архитектура трансформеров, то опишем, что в ней необходимо изменить, чтобы получить LLaMa-модель. Pre-нормализация. Обычно используется LayerNorm, а в LLaMa — RMSNorm. Пусть x∈R m ,y∈R n . Тогда нелинейное преобразование в общем виде выглядит так: j=1 =f(a i +b i ) И LayerNorm можно описать следующими формулами: i=1 ∑ n a i , σ= n 1 i=1 ∑ n (a i −μ) 2 y i =f( В свою очередь экспериментально RMSNorm демонстрирует лучшие результаты в сравнении с LayerNorm и высчитывается так: =f( n 1 ∑ i=1 SwiGLU-активация используется вместо ReLU. ⊗ ⊗ — значок поэлементного умножения матриц. ReLU(x)=max(0,x) SwiGLU(x,W,V,b,c,β)=Swish β (xW+b)⊗(xV+c) Swish β (x)=xσ(βx) Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о трансформерах) или относительными (кодируется расстояние между каждой парой токенов). Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 9,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "∑ i=1 SwiGLU-активация используется вместо ReLU. ⊗ ⊗ — значок поэлементного умножения матриц. ReLU(x)=max(0,x) SwiGLU(x,W,V,b,c,β)=Swish β (xW+b)⊗(xV+c) Swish β (x)=xσ(βx) Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о трансформерах) или относительными (кодируется расстояние между каждой парой токенов). Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое модели при подсчёте Attention, а не только перед первым слоем. Это позволяет на каждом слое явно обрабатывать информацию об относительном расположении токенов. Роторные эмбединги показывают лучшее качество на многих задачах и являются стандартом для обучения языковых моделей. Подробнее о них можно почитать в этой статье. Существуют также техники ускорения обучения моделей и оптимизации использования памяти, но с этим предлагаем читателям ознакомиться самостоятельно. SFT (supervised finetuning) Второй этап обучения инструкционных языковых моделей требует множество инструкций. Рецепт как их готовить был подробно описан в середине этого параграфа. Снова проговорим, что для написания инструкций или сбора датасета необходимо, чтобы инструкции были: разнообразными; качественными; имели одинаковый формат, чтобы чатовая модель могла обучиться диалоговости (где вопрос пользователя, где ее ответ); информативными; подробными; Chain-of-Thought (CoT), few-shot и так далее. Reward-модель Третий этап в создании инструкционных моделей. Есть несколько способов собрать датасет для обучения reward-модели. Он должен содержать тексты и метки к ним. Если меток много (например, в случае балльной оценки), можно использовать разновидности ранжирующих лоссов. Разберем способ обучения модели на бинарную оценку. Пусть модели подается на вход инструкция x x. Поменяв температуру, способ сэмплирования или использовав разные чек-пойнты модели, возможно получить два разнообразных ответа . Не ограничивая общность, предположим, что, согласно некоторым предпочтениям, асессоры или пользователи установили, что первый ответ лучше второго. Проделаем эту операцию много раз и получим обучающее множество, состоящее из i=1 N . Тогда reward-модель можно обучать минимизацией следующей функции потерь: ranking =−log(σ(r θ (x,y 1 )−r θ (x,y 2 )−m(r))) Где: r θ r θ — reward-модель с обучаемыми параметрами тета; m ( r ) m(r) — некий margin, который определяет, насколько сильно модель должна отделять хороший и плохой ответы друг от друга. RL (Reinforcement Learning) На четвёртом этапе, этапе выравнивания модели, можно воспользоваться разными алгоритмами. LLaMa-2 Chat была обучена последовательно сначала на Rejection Sampling fine-tuning (RL «для бедных») и Proximal Policy Optimization (PPO). Rejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция x x. Сгенерируем для неё N N ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше N N, тем больше reward-score у лучшего ответа. Собрав пары инструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели. ссылка на источник картинки Proximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященный RL. Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 10,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Optimization (PPO). Rejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция x x. Сгенерируем для неё N N ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше N N, тем больше reward-score у лучшего ответа. Собрав пары инструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели. ссылка на источник картинки Proximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященный RL. Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция оценивает средний reward в текущем состоянии (обычно это та же самая модель с линейным слоем поверх). Формализуем термины из RL для задачи выравнивания языковой модели: Политика π π — обучаемая языковая модель. Value-функция ) — обычно та же самая модель с линейным слоем поверх, оценивает средний reward, если действовать из состояния s t s t согласно политике — состояние в момент времени t t. Это весь контекст t t-токенов, которые модель успела сгенерировать к текущему моменту. a t a t — действие из текущего состояния в момент времени t t. Обозначает следующий токен, который будет сгенерирован. τ τ — траектория, т. е. тройки i=0 ∞ , — это состояния генерируемого токена и награды за него Сразу можно сделать вывод, что в языковых моделях t+1 =concat([s t ,a t ]), Q(s t ,a t )=V(s t+1 ). Также, в RL символом ∞ ∞ обозначается вся последовательность токенов, то есть на практике сюда можно подставлять количество сгенерированных токенов. Инициализируем — начальные веса политики и value-функции Для n=0,1,2,…,N: Соберем коллекцию траекторий ={τ i } , следуя политике π ( θ n ) π(θ n ). Посчитаем A(s t ,a t )=Q(s t ,a t )−V(s t ). Эта формула отражает разницу между финальной наградой за выбранное действие a t a t в текущем состоянии s t s t и средней финальной наградой, которую можно было бы получить в этом состоянии. Вообще говоря, с помощью метода Generalized Advantage Estimation (GAE) её можно аппроксимировать следующим выражением: +γV θ n (s t+1 )−V θ n (s t ) Обновляем веса политики согласно одному из лоссов PPO. Например, используем такой: θ n + 1 = argmaxθ n+1 = θ argmax n−1 ]−βKL(π θ n−1 (⋅∣s t ),π θ n (⋅∣s t )) С помощью MSE лосса оптимизируем значение value-функции: L(ψ)= E t ^ [∣∣V l=0 ∑ ∞ γ l r t+l Итог Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.5. Диффузионные модели Следующий параграф 9.1. Введение в рекомендательные системы А что, если попробовать дообучить уже сейчас? Хочешь применить знания о больших языковых моделях на практике? В Yandex Cloud ты можешь дообучить YandexGPT или Llama на своих данных и интегрировать их в свои проекты. Всё это — с помощью",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 11,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "γ l r t+l Итог Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.5. Диффузионные модели Следующий параграф 9.1. Введение в рекомендательные системы А что, если попробовать дообучить уже сейчас? Хочешь применить знания о больших языковых моделях на практике? В Yandex Cloud ты можешь дообучить YandexGPT или Llama на своих данных и интегрировать их в свои проекты. Всё это — с помощью Yandex Cloud AI Studio.",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/yazykovye-modeli",
      "course": "ml",
      "chapter": "8. Генеративные модели",
      "chapter_id": "8.6",
      "part": 12,
      "total_parts": 12,
      "source_file": "8.6.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "imgonline_com_ua_Black_White_E_Xr_Oil_BVM_Bi_Ya_c484034b32.webp Евгений Косарев В 2023 году ChatGPT стал самой узнаваемой языковой моделью машинного обучения во всём мире — причём как среди специалистов, так и среди обычных людей. Способность вести осмысленный диалог, отвечать на практически любые вопросы и быть применимыми без дообучения в большом спектре задач с высоким качеством — вот залог их популярности. В этом параграфе мы расскажем, что такое языковые модели, как они устроены, как развивались, а также как изменились за последнее время. Что такое языковые модели? Говоря простым языком, языковые модели — это алгоритмы, способные продолжать тексты. Если чуть усложнить, то это вероятностные алгоритмы, и к ним сразу можно задать эмпирический критерий качества: хорошая модель даёт разумные продолжения данных ей текстов. Artboard Давайте разберём пример выше. Модель высчитывает вероятность возможных продолжений текста и предлагает их нам. Слово «фрукт» — наименее разумное продолжение нашей фразы, в то время как слово «наука» — наиболее разумное. И действительно, это часть определения машинного обучения, которое мы давали в начале этого учебника. Таким образом, нам осталось лишь научить алгоритм моделировать эти вероятности и максимизировать их для разумных предложений. Но как это сделать? По ходу развития языковых моделей подходы менялись, мы расскажем о каждом из них в хронологическом порядке. Начнём с краткого экскурса в историю — поговорим о статистических моделях, рекуррентных нейронных сетях и трансформерах. А затем перейдём к современным — GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT и LLaMa. Развитие языковых моделей Статистические модели Идея модели лежит на поверхности, много где применяется в самых разных вариациях даже в ХХ веке, поэтому сложно назвать авторов или точную дату создания. Однако этот метод популярен до сих пор — используется в клавиатурах смартфонов для исправления опечаток и быстрого набора текстов через Т9. Теперь подробнее о методе. Напомним вероятностную формулировку цепей Маркова в общем виде: P(w 1 ,w 2 ,...,w N )=P(w N ∣w 1 ,w 2 ,...,w N−1 )×P(w 1 ,w 2 ,...,w N−1 )=P(w N ∣w 1 ,w 2 ,...,w N−1 )×P(w N−1 ∣w 1 ,w 2 ,...,w N−2 )×...×P(w 2 ∣w 1 )×P(w 1 ) Если представить, что w i w i — это слово, а набор этих омега — это предложение, то по формуле становится возможным посчитать вероятность предложения ,...,w N С практической точки зрения всё чуть сложнее, ведь распределение слов в реальном языке (какое, с какими и как часто встречается), вообще говоря, неизвестно. Его принято аппроксимировать на основе корпуса текстов (например, всего интернета) — в этом случае считаются совстречаемости слов друг с другом, и по ним считаются вероятности. В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении P(w N ∣w 1 ,w 2 ,...,w N−1 ) длина контекста равна N − 1 N−1. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин: Сложность в подсчёте и хранении каждого возможного уникального контекста длины K K. Если корпус текстов состоит из N N различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна N K N K , что очень много при больших K K. Большой контекст реже встречается. То есть",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 1,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "по ним считаются вероятности. В условной вероятности число переменных, от которых зависит распределение следующего слова, называется контекстом. Например, в выражении P(w N ∣w 1 ,w 2 ,...,w N−1 ) длина контекста равна N − 1 N−1. На практике же редко считают вероятности с контекстом больше трёх, на это есть несколько причин: Сложность в подсчёте и хранении каждого возможного уникального контекста длины K K. Если корпус текстов состоит из N N различных слов, то стоимость хранения счётчиков встречаемости для выбранной длины контекста равна N K N K , что очень много при больших K K. Большой контекст реже встречается. То есть слова «яблоку», «негде» и «упасть» поодиночке встречаются чаще, чем их комбинация «яблоку негде упасть». Отсюда достаточность статистик падает с ростом длины контекста. В учебном примере предлагается ограничиться шириной контекста размера 1: P(w 1 ,w 2 ,...,w N )=P(w N ∣w N−1 )×P(w N−1 ∣w N−2 )×...×P(w 2 ∣w 1 )×P(w 1 ) Artboard Интересно, что такой подход достаточно популярен до сих пор. Например, он используется в умных клавиатурах, чтобы подсказать следующее слово. Достоинства статистических моделей: Простота имплементации. Высокая скорость работы алгоритма. Низкая вычислительная стоимость обучения и инференса. Недостатки статистических моделей: Не сможет сгенерировать слова, которые не шли подряд в обучающем корпусе. Очень маленький контекст. Длинные последовательности равновероятны ≈ нулю (в цепях Маркова для длинных последовательностей много множителей меньше нуля, поэтому их произведение уже практически равно нулю для любых множителей). Отсюда алгоритм не может выдавать разумные продолжения большой длины. Токенизация Языковые модели, да и вообще все модели, которые оперируют текстом, используют понятие токена. Токен — это единица текста, которую понимают алгоритмы. В примере выше токен — это отдельное слово w i w i (этот подход называется мешком слов), однако текст можно разбивать на токены и иначе. Artboard Раньше предложение разбивалось на слова по пробелам, знакам препинания, исключались стоп-слова и так далее (назовем это CountVectorizer). Но у этого подхода возникали две проблемы с разными словоформами. Они: Либо обозначались разными токенами, что не совсем верно, ведь слово-то одно и то же. И получалось, что похожим смыслом обладало сразу несколько токенов. Либо приводились к начальной форме — и в итоге терялся падеж, время, число. Современные токенизаторы построены на алгоритме BPE (Byte Pair Encoding; об устройстве BPE более подробно можно прочитать в учебнике Лены Войта). Решение требует фиксации определённого числа токенов. Как только это сделано, в словарь добавляются все символы из текста, ищутся самые частые их сочетания и снова добавляются. Этот процесс продолжается до тех пор, пока число токенов не станет равно заданному значению. Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет _ перед соответствующим токеном) и не построен на логике разбиения слов по разделителям. Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать здесь. 💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 2,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "пор, пока число токенов не станет равно заданному значению. Токенизатор SentencePiece в определённом смысле совершеннее, чем BPE, — он наследует логику Unigram- и BPE-токенизаторов, иначе работает с пробелами (добавляет _ перед соответствующим токеном) и не построен на логике разбиения слов по разделителям. Поэтому, в отличие от BPE, он способен работать с такими языками, как японский или китайский. Подробнее о его устройстве можно прочитать здесь. 💡 Токенизаторы не разделяют входной поток по значимости. Например, число 12345 BPE могут разбить на два токена — 1 и 2345, что явно не соответствует логике написанного выражения. Также они будут неправильно выделять всё число в отдельный токен, так как чисел бесконечное количество. Сейчас используется идея о разбиении всех чисел на цифры, чтобы множеством из десяти токенов представить всё многообразие чисел. Рекуррентные нейронные сети (RNN) Появились после статистических моделей, подробнее о хронологии здесь. Рекуррентные нейронные сети концептуально можно описать формулой, где: А А — некоторая модель; h t h t — внутреннее состояние модели на момент времени — токен, который сейчас обрабатывается. Тогда следующий токен x t + 1 x t+1 получается так: t+1 =g(h t ); h t =A(h t−1 ,x t ) ссылка на источник картинки Подробно об устройстве RNN мы рассказываем в параграфе Нейросети для работы с последовательностями. Здесь же коротко отметим, что существуют различные модификации рекуррентных сетей, которые усложняют структуру алгоритма А А, даже добавляют механизм внимания Attention. Если коротко, то он позволяет лучше оценивать взаимосвязи токенов в тексте. Все они в разной степени помогают модели усваивать более длинные и сложные последовательности токенов. Достоинства RNN: Высокая скорость инференса и сравнительно низкая стоимость. Более качественный текст, чем у моделей на статистиках. Теоретически понимает контекст в сотни слов (а с Attention ещё больше). Точно учитывает весь контекст документа. Недостатки RNN: Невозможность параллельного обучения на многих устройствах, отсюда не получится просто так обучить большую RNN. Модель «хорошо помнит» лишь несколько последних токенов контекста (без Attention). Проблемы с обучением (exploading/vanishing gradients). Трансформеры Более подробно трансформеры и их устройство описаны в параграфе Трансформеры. Последней и наиболее успешной с точки зрения качества оказалась архитектура трансформеров. Она состоит из двух частей: encoder (на изображении слева) и decoder (на изображении справа). ссылка на источник картинки Изначально был популярен подход обучать части отдельно. Так на базе encoder-блоков были построены BERT-модели. Идея обучения звучит несложно: давайте из входного текста замаскируем токеном MASK 15% имеющихся токенов и обучим модель угадывать, какие именно токены были скрыты. Тогда, если модель обучится это делать, она сможет очень хорошо понимать текст. Таким образом, энкодеры обладают следующими особенностями: Анализируют входной текст и связи между токенами. Выделяют важные токены для определённой задачи. Ничего не генерируют. На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель: генерирует токен за токеном. смотрит на весь контекст, архитектурно, нет забывания токенов. имеет возможность (как и BERT-модели) обучаться параллельно. обладает достаточно высокой вычислительной стоимостью инференса. 💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 3,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "очень хорошо понимать текст. Таким образом, энкодеры обладают следующими особенностями: Анализируют входной текст и связи между токенами. Выделяют важные токены для определённой задачи. Ничего не генерируют. На базе декодеров сделаны GPT-модели. Они обучаются предсказывать следующий токен на основе предыдущих. На инференсе, когда очередной токен сгенерирован, он добавляется в контекст, и уже на основе него выбирается новый токен. Таким образом модель: генерирует токен за токеном. смотрит на весь контекст, архитектурно, нет забывания токенов. имеет возможность (как и BERT-модели) обучаться параллельно. обладает достаточно высокой вычислительной стоимостью инференса. 💡 Контекст в случае трансформеров определяется числом токенов, которые они могут обработать за раз. Архитектурно за понимание контекста отвечает блок Attention, и размеры матриц в нём как раз определяют размер контекста. Размер матриц конечен: чем они больше, тем сложнее вычислять блок внимания, поэтому контекст существенно ограничен. На момент написания параграфа разработаны различные модификации Attention, позволяющие растить понимаемый контекст, однако они имеют ряд проблем, с которыми предлагаем ознакомиться читателю самостоятельно. Современные подходы GPT-1 & GPT-2 Начнём немного издалека, с моделей GPT-1 и GPT-2. Первая была обучена в 2018 году на 7000 книг и имела размер контекста в 512 токенов. И она сразу получилась довольно сильной: после дообучения на специализированные задачи (бенчмарки) показывала на них лучшее на то время качество. Так, в задачах CoLA (бенчмарк классификационный, в нём надо определить грамматическую корректность предложения) результат вырос до 45,4 против прежнего результата в 35,0 у RNN. А в GLUE — с 72,8 до 68,9. Вторая модель была обучена в 2019 году. Она состояла из рекордных для того времени 1,5 млрд параметров (то есть была в ~10 раз больше первой), имела контекст в 1024 токена и была обучена на 40 ГБ текстовых данных. GPT-2 снова побеждала предыдущие подходы, включая GPT-1, на многих бенчмарках. По сравнению с первой версией модели у второй произошел качественный рост: теперь она могла генерировать разумные тексты — а не только предложения. Правда, не всегда и не с первой попытки. GPT-3 GPT-3 стала революцией с точки зрения качества и размеров. В 2020 году была получена модель размером в 175 млрд параметров, она обучалась на 570 ГБ текстовых данных с контекстом в 2048 токенов. Модель могла решать целый спектр задач, включая перевод, суммаризацию и ответы на вопросы, с качеством, близким к человеческому уровню, а также отличалась высокой способностью генерировать креативный контент. Демонстрацию работы модели лучше посмотреть в этой статье на 28 странице и далее. Модель Число обучающих данных Контекст Число параметров Decoder-слои Hidden-size (размерность тензоров внутри модели) Train batchsize (размер батча при обучении) GPT 7000 книг 512 117 млн 12 768 64 GPT-2 40 ГБ текстовых данных 1024 1,5 млрд 48 1600 512 GPT-3 570 ГБ текстовых данных 2048 175 млрд 96 12 288 3 200 000 Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу. Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря — графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 4,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "batchsize (размер батча при обучении) GPT 7000 книг 512 117 млн 12 768 64 GPT-2 40 ГБ текстовых данных 1024 1,5 млрд 48 1600 512 GPT-3 570 ГБ текстовых данных 2048 175 млрд 96 12 288 3 200 000 Модель демонстрировала действительно впечатляющие результаты: собрав обучающие данные, можно было с высоким качеством решить практически любую текстовую задачу. Однако для применения таких решений остаётся проблема со стоимостью их обучения. Для обучения GPT-2 авторы использовали 16 GPU (иначе говоря — графических процессоров, видеокарт), а для GPT-3 уже 3200. Для дообучения модели под определенную задачу, конечно, понадобится меньше ресурсов, но всё равно достаточно много. Что с этим делать? Использовать подводки. Подводки Few-shot обучение Оказывается, что обучать большие языковые модели решать определённые задачи не всегда нужно (как мы говорили ранее, это ресурсоёмко): можно составить few-shot подводку. Подводка — словесное описание поставленной задачи, составленное определенным образом. Представим, что мы хотим осуществить перевод с английского на французский. Для обучения нам необходимо было бы составить пары ( X , y ) (X,y), где X X — слово на английском, а y y — на французском. Сделаем иначе — опишем задание на естественном языке: ссылка на источник картинки Здесь на английском языке сформулировано задание и предлагается слово «cheese» перевести на французский. Назовем такую конструкцию zero-shot-примером. Такой запрос GPT-3, возможно, поймёт, но работать будет плохо. Давайте увеличим количество примеров в подводке и назовем эту конструкцию one-shot: ссылка на источник картинки Или больше, и это будет few-shot: ссылка на источник картинки При этом приёме не тратятся ресурсы на обучение модели, она лишь смотрит на контекст и генерирует продолжение. Оказывается, этого достаточно, чтобы сравняться с downstream-обучением. Продемонстрируем преимущество такого подхода на двух бенчмарках. TriviaQA — вопросно-ответный бенчмарк, составленный на основе Википедии. Он помогает оценивать знания модели и ее ответы на вопросы. Lambada — оценивает меморизацию длинного контекста модели. Чем выше скор, тем лучше модель на обоих бенчмарках. ссылка на источник картинки ссылка на источник картинки Графики выше демонстрируют несколько особенностей: Few-shot позволяет получать качество, сравнимое с дообучением на определённом датасете, и стремится к человеческому качеству. С ростом числа обучаемых параметров модели растет её качество. На правом графике few-shot-примеры начинают работать лучше zero-shot-примеров лишь с некоторого размера модели. Это говорит о том, что модель начинает демонстрировать «умные» свойства лишь начиная с некоторого размера. 💡 На самом деле последний пункт достаточно часто встречается в языковых моделях. Случается так, что определённые приёмы не работают с маленькими моделями, но показывают себя лишь на больших. Это можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает большую обобщающую способность. Формулировка имеет значение Few-shot действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо. Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья: Artboard Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы этой статьи заметили, что сама по себе конструкция few-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от: Текстового описания задачи. Числа примеров в подводке. Порядка,",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 5,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "можно назвать фазовым переходом, когда языковая модель вместе с увеличением размера и числа пройденных текстов на обучении обретает большую обобщающую способность. Формулировка имеет значение Few-shot действительно полезен и помогает получать от модели нужный результат без обучения, но всё же недостаточно хорошо. Предположим, мы хотим узнать у модели, как приготовить любимое блюдо. Пусть это будет лазанья: Artboard Можно заметить, что запрос к модели можно задать по-разному, но ответ ожидается обычно какой-то конкретный. Авторы этой статьи заметили, что сама по себе конструкция few-shot-примера не приводит к стабильному результату. Качество решения задачи очень зависит от: Текстового описания задачи. Числа примеров в подводке. Порядка, в котором примеры следуют друг за другом в подводке. Формате составления few-shot. Чтобы улучшить качество решения задачи, авторы предлагают осуществлять калибровку подводок. В статье они заметили, что модели смещены относительно подводок, то есть переформулировка запроса ведёт к смещению в ответе модели, а также к росту разброса ответов. Например, модели задают вопрос и её задача — ответить «да» или «нет». Если few-shot состоит из четырёх примеров и они идут в порядке «да», «да», «нет», «нет», то, вероятнее всего, дальше модель ответит «нет» на любой вход, просто потому что слово «нет» встречалось последним. Калибровать модель предлагается с помощью выученного линейного преобразования: =softmax(W p ^ +b) В этом преобразовании: W W и b b — обучаемые; p ^ p ^ — вероятности на выходе модели; q ^ q ^ — откалиброванные вероятности; Обучающие данные собираются так: ссылка на источник картинки Для различных задач собираем подводки и добавляем нейтральное слово N/A. В этом примере несмещённая модель должна давать с вероятностью 50% ответ «positive» или «negative». Чтобы добиться такого распределения ответов у смещённой модели, представим: W=diag( p ^ ) −1 , b=0 Также все few-shot-примеры стандартизуются в специальный формат вопрос — ответ, как на картинке выше. Этот метод (синий график) по сравнению со стандартными few-shot-примерами (красный график) помог повысить качество и уменьшить разброс результата. Таким образом, оптимизировав всего 4 параметра, авторы существенно улучшили итоговый результат. ссылка на источник картинки Promt-tuning Качество работы модели зависит от подводки, и few-shot просто один из способов её построения. Эксперименты показывают, что грамотный подбор промта позволяет экономить на обучении и решать задачи с высоким качеством. Проблема в обучении больших моделей — нехватка оперативной памяти на GPU, поэтому не будем оптимизировать все параметры модели. Пусть необходимо решить задачу А А, к ней имеется обучающее множество вида ( X , y ) (X,y). Введём дополнительные токены, которых не было в словаре: >,<P 2 >,…,<P k > — и будем добавлять в каждый текст из X согласно какому-то правилу. Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец. Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо оптимизировать лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели. ссылка на источник картинки 💡 Эффект от такого трюка достаточно многогранен: Меньше обучаемых параметров — меньше памяти занимает модель. Меньше обучаемых параметров — быстрее происходит обучение. Обычно",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 6,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "2 >,…,<P k > — и будем добавлять в каждый текст из X согласно какому-то правилу. Правило может быть таким: имеем 20 спецтокенов, добавим токены 1–10 в начало строки, а 11–20 в конец. Тогда, можно «заморозить» все параметры в модели, кроме этих токенов, и сэкономить на обучении. Если токенов 100 и каждый из них имеет размерность в 1024, то необходимо оптимизировать лишь 100 тысяч параметров вместо 175 млрд в случае обучения всей модели. ссылка на источник картинки 💡 Эффект от такого трюка достаточно многогранен: Меньше обучаемых параметров — меньше памяти занимает модель. Меньше обучаемых параметров — быстрее происходит обучение. Обычно нужно сильно меньше обучающих данных, чем при традиционном обучении всей модели для достижения высокого качества. Высокое качество результата. Получается, что можно оптимизировать подводку, или, другими словами, находить наиболее оптимальный промт, который лучше прочих решает поставленную задачу. Как повысить качество решения задач из разных категорий Языковые модели призваны решать самый широкий спектр текстовых задач — вопросно-ответные, суммаризацию, диалоговость, перевод и многие другие. Получается, что модель должна после некого обучения (подбора подводки или оптимизации вообще всех параметров под каждую задачу) решать каждую из них на высоком уровне. Однако модель обычно учится на текстах из интернета, книгах и других доступных ресурcах. И формат задачи, который обычно требуется от модели, не соответствует тому, что алгоритм привык видеть на обучении. К этому стоит добавить, что среди веб-документов просьба что-то сократить или определить тональность документа встречается не очень часто. Исправить этот недостаток призваны подходы по генерализации языковых моделей: FLAN и T0. Инструкции даются на естественном языке и для подготовки качественного обучающего множества предлагается произвести следующие действия: Каждой отдельной задаче (будь то перевод, написание отзывов или суммаризация) пишется по несколько различных подводок, отражающих смысл задания. Итоговый датасет составляется из отдельных задач, все строчки датасета перемешиваются случайным образом. Авторы стараются собрать как можно более разнообразные задачи в обучающее множество. ссылка на источник картинки ссылка на источник картинки ссылка на источник картинки Две картинки сверху демонстрируют FLAN- и T0- подходы по созданию датасета, а картинка снизу — рост усреднённого качества модели после обучения на смеси. Таким образом с некоторого размера модели наблюдается повышение метрик качества при дальнейших дообучениях генерализованной модели на отложенных задачах. Chain-of-Thought Предыдущий подход со смесью датасетов помогает решать многие задачи в среднем заметно лучше. Однако есть задачи, где качество результатов модели всё ещё низкое. Например, предложить эффективный код, решающий некую алгоритмическую задачу, найти минимум некоторой аналитической функции потерь, посчитать производную фукнции в точке и так далее. Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле Chain-of-Thought (CoT): ссылка на источник картинки CoT-подводка состоит из трёх обязательных элементов: Формулировки задачи на естественном языке. Подробного пошагового решения. Ответа на задачу. Формирование такого промта, особенно на few-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторы этой статьи сравнили на двух математических бенчмарках способность модели решать сложные задачи. MultiArith — проверяет умение решать простые арифметически задачки. GSM8K — более сложные. Результаты демонстрируют, что наличие CoT в подводке увеличивает способность решать математические задачки у больших языковых моделей. Artboard InstructGPT",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 7,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Такие вопросы требуют рассуждения, которое модель не может просто так провести из-за своей архитектуры. Выход — составить подводки в стиле Chain-of-Thought (CoT): ссылка на источник картинки CoT-подводка состоит из трёх обязательных элементов: Формулировки задачи на естественном языке. Подробного пошагового решения. Ответа на задачу. Формирование такого промта, особенно на few-shot, заставляет модель рассуждать, как можно правильно решить задачу. Авторы этой статьи сравнили на двух математических бенчмарках способность модели решать сложные задачи. MultiArith — проверяет умение решать простые арифметически задачки. GSM8K — более сложные. Результаты демонстрируют, что наличие CoT в подводке увеличивает способность решать математические задачки у больших языковых моделей. Artboard InstructGPT Наконец, обсудив, как готовить обучающие данные, перейдем к прародителю ChatGPT. Инструкционная модель — это та, которая обучена отвечать на пользовательские запросы в режиме zero-shot (а вообще, и few-shot, и любой человекочитаемый формат) с высоким качеством. ссылка на источник картинки InstructGPT — это модель, и она интересна с точки зрения выработки концепции обучения всех инструкционных моделей (InstructGPT, ChatGPT, GPT-4 и других). С некоторыми нюансами обучение состоит из четырех этапов: Подготовка качественного претрейна. Языковая модель должна содержать в себе как можно больше знаний о мире, чтобы иметь возможность в последующем решать произвольные задачи с высоким качеством. На этом этапе необходимо озаботиться наибольшим разнообразием, чистотой и полнотой обучающих данных. Подробнее об этом мы поговорим в последнем разделе этого параграфа. SFT (supervised finetuning) — обучение модели следовать инструкциям. Этот пункт мы подробно обсудили в предыдущей части параграфа (T0, FLAN, CoT). На этом этапе важно составить грамотный инструкционный датасет, где инструкция содержит произвольные запросы к модели, а ответ на неё — подробный текст, которым будущий пользователь будет доволен. Грамотный сбор таких данных довольно дорогостоящий процесс, но от него напрямую зависит, каким образом модель будет взаимодействовать с пользователем. Обучение reward-модели. Каждый ответ алгоритма можно оценить с точки зрения вежливости, подробности или персонажности. Персонажность позволяет модели считать себя, например, капитаном Джеком Воробьем и общаться на пиратском говоре. Также есть менее формализуемые критерии качества ответов, их даже сложно описать словами. Например, что в основном людям ответ 1 нравится больше чем ответ 2. Reward-модель агрегирует эти кртитерии в число — меру качества. Чем оно выше, тем качественнее ответ модели. Для выравнивания поведения модели обычно важно уметь оценивать тысячи текстов, а вручную это делать дорого и долго, поэтому обучается специальная модель-оценщик. Про то, как обучать reward-модель, будет рассказано далее. Этап Reinforcement Learning (RL). На нём языковая модель обучается генерировать такие ответы, которые имели бы наивысшую оценку относительно reward-модели. Про то, как делать RL, будет рассказано далее. ChatGPT Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в: Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных. Размере и качестве инструкционного датасета. Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL. К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели. Как обучить свою LLM? Обсудим детально на примере доступных в open-source моделей семейства LLaMA.",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 8,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "как делать RL, будет рассказано далее. ChatGPT Одна из самых нашумевших языковых моделей в мире наследует логику обучения Instruct GPT. Основные отличия от последней заключаются в: Диалоговости. Модель обучена работать с диалогами, держать их в контексте и помнить историю того, что требовал пользователь. Обучение производится посредством сбора/написания диалоговых данных. Размере и качестве инструкционного датасета. Том, что больше внимания уделено разметке и обучению reward-модели и этапу с RL. К сожалению, OpenAI не предоставили детали обучения ChatGPT, а предложили лишь общий ход действий. Также неизвестны архитектурные параметры модели. Как обучить свою LLM? Обсудим детально на примере доступных в open-source моделей семейства LLaMA. LLaMa В качестве примера возьмём самую свежую архитектуру трансформеров на первую половину 2023 года — LLaMa, а также способы превращать её в чатовую модель, проводить Alignment на примере LLaMa-2. Вторая модель архитектурно не отличается от первой (кроме увеличенного контекста до 4096 токенов), поэтому содержание статей можно объединить в один рассказ. Претрейн Для обучения с нуля качественной языковой модели необходимы: мощный кластер на сотни видеокарт, на котором можно производить параллельное обучение модели. Больше GPU — больше модель можно обучить и быстрее по времени обучения; терабайты текстовых данных для тренировки на них; архитектура, которая лучшим образом может моделировать язык. Поговорим подробнее о двух последних пунктах. Текстовые данные Текстовые данные можно брать из открытых источников, таких как CommonCrawl, C4, Taiga и прочее. Важно обеспечить: чистоту данных — например, убрать html-тэги, устранить дублирование текстов; полноту — чтобы модель одинаково хорошо решала математические задачи, писала код или сочиняла стихотворения, текстов соответствующих доменов должно быть в достатке в обучающем корпусе; разнообразие данных. Существуют эмпирические законы обученности модели, но здесь остановимся на числе пройденных за обучение токенов. В LLaMa-моделях это значение варьируется от 1T до 2Т. Ниже приведены основные параметры по числу размерности внутренних эмбедингов, числу голов Attention, слоёв и параметров обучения разных моделей: ссылка на источник картинки Архитектура У LLaMa-моделей предлагается целый ряд архитектурных изменений. Так как в учебнике рассматривался лишь базовая архитектура трансформеров, то опишем, что в ней необходимо изменить, чтобы получить LLaMa-модель. Pre-нормализация. Обычно используется LayerNorm, а в LLaMa — RMSNorm. Пусть x∈R m ,y∈R n . Тогда нелинейное преобразование в общем виде выглядит так: j=1 =f(a i +b i ) И LayerNorm можно описать следующими формулами: i=1 ∑ n a i , σ= n 1 i=1 ∑ n (a i −μ) 2 y i =f( В свою очередь экспериментально RMSNorm демонстрирует лучшие результаты в сравнении с LayerNorm и высчитывается так: =f( n 1 ∑ i=1 SwiGLU-активация используется вместо ReLU. ⊗ ⊗ — значок поэлементного умножения матриц. ReLU(x)=max(0,x) SwiGLU(x,W,V,b,c,β)=Swish β (xW+b)⊗(xV+c) Swish β (x)=xσ(βx) Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о трансформерах) или относительными (кодируется расстояние между каждой парой токенов). Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 9,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "∑ i=1 SwiGLU-активация используется вместо ReLU. ⊗ ⊗ — значок поэлементного умножения матриц. ReLU(x)=max(0,x) SwiGLU(x,W,V,b,c,β)=Swish β (xW+b)⊗(xV+c) Swish β (x)=xσ(βx) Роторные эмбединги. Информацию о том, в каком порядке следуют токены внутри модели, хранят в себе позиционные эмбединги. Они могут быть абсолютными (кодирование синусами и косинусами, как описано в параграфе о трансформерах) или относительными (кодируется расстояние между каждой парой токенов). Роторные эмбединги позволяют вычислять относительную связь между парой токенов на этапе вычисления Attention, также они выигрывают по сравнению с относительными в совместимости kernel-ов. То есть, одно из понятных не технических отличий их от других — вычисление позиционной информации на каждом слое модели при подсчёте Attention, а не только перед первым слоем. Это позволяет на каждом слое явно обрабатывать информацию об относительном расположении токенов. Роторные эмбединги показывают лучшее качество на многих задачах и являются стандартом для обучения языковых моделей. Подробнее о них можно почитать в этой статье. Существуют также техники ускорения обучения моделей и оптимизации использования памяти, но с этим предлагаем читателям ознакомиться самостоятельно. SFT (supervised finetuning) Второй этап обучения инструкционных языковых моделей требует множество инструкций. Рецепт как их готовить был подробно описан в середине этого параграфа. Снова проговорим, что для написания инструкций или сбора датасета необходимо, чтобы инструкции были: разнообразными; качественными; имели одинаковый формат, чтобы чатовая модель могла обучиться диалоговости (где вопрос пользователя, где ее ответ); информативными; подробными; Chain-of-Thought (CoT), few-shot и так далее. Reward-модель Третий этап в создании инструкционных моделей. Есть несколько способов собрать датасет для обучения reward-модели. Он должен содержать тексты и метки к ним. Если меток много (например, в случае балльной оценки), можно использовать разновидности ранжирующих лоссов. Разберем способ обучения модели на бинарную оценку. Пусть модели подается на вход инструкция x x. Поменяв температуру, способ сэмплирования или использовав разные чек-пойнты модели, возможно получить два разнообразных ответа . Не ограничивая общность, предположим, что, согласно некоторым предпочтениям, асессоры или пользователи установили, что первый ответ лучше второго. Проделаем эту операцию много раз и получим обучающее множество, состоящее из i=1 N . Тогда reward-модель можно обучать минимизацией следующей функции потерь: ranking =−log(σ(r θ (x,y 1 )−r θ (x,y 2 )−m(r))) Где: r θ r θ — reward-модель с обучаемыми параметрами тета; m ( r ) m(r) — некий margin, который определяет, насколько сильно модель должна отделять хороший и плохой ответы друг от друга. RL (Reinforcement Learning) На четвёртом этапе, этапе выравнивания модели, можно воспользоваться разными алгоритмами. LLaMa-2 Chat была обучена последовательно сначала на Rejection Sampling fine-tuning (RL «для бедных») и Proximal Policy Optimization (PPO). Rejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция x x. Сгенерируем для неё N N ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше N N, тем больше reward-score у лучшего ответа. Собрав пары инструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели. ссылка на источник картинки Proximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященный RL. Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 10,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Optimization (PPO). Rejection Sampling fine-tuning. Этот подход основан на довольно простой стратегии. Пусть имеется инструкция x x. Сгенерируем для неё N N ответов и выберем тот, который получает наивысшую оценку у reward-модели. График ниже демонстрирует, что чем больше N N, тем больше reward-score у лучшего ответа. Собрав пары инструкция — лучший ответ, можно обучить на них языковую модель и провести таким образом выравнивание поведения модели. ссылка на источник картинки Proximal Policy Optimization. Для лучшего понимания происходящего советуем прочесть параграф, посвященный RL. Основная задача, как обычно, следовать некой политике, которая лучшим образом отражает human feedback. Политика — наша итоговая модель, value-функция оценивает средний reward в текущем состоянии (обычно это та же самая модель с линейным слоем поверх). Формализуем термины из RL для задачи выравнивания языковой модели: Политика π π — обучаемая языковая модель. Value-функция ) — обычно та же самая модель с линейным слоем поверх, оценивает средний reward, если действовать из состояния s t s t согласно политике — состояние в момент времени t t. Это весь контекст t t-токенов, которые модель успела сгенерировать к текущему моменту. a t a t — действие из текущего состояния в момент времени t t. Обозначает следующий токен, который будет сгенерирован. τ τ — траектория, т. е. тройки i=0 ∞ , — это состояния генерируемого токена и награды за него Сразу можно сделать вывод, что в языковых моделях t+1 =concat([s t ,a t ]), Q(s t ,a t )=V(s t+1 ). Также, в RL символом ∞ ∞ обозначается вся последовательность токенов, то есть на практике сюда можно подставлять количество сгенерированных токенов. Инициализируем — начальные веса политики и value-функции Для n=0,1,2,…,N: Соберем коллекцию траекторий ={τ i } , следуя политике π ( θ n ) π(θ n ). Посчитаем A(s t ,a t )=Q(s t ,a t )−V(s t ). Эта формула отражает разницу между финальной наградой за выбранное действие a t a t в текущем состоянии s t s t и средней финальной наградой, которую можно было бы получить в этом состоянии. Вообще говоря, с помощью метода Generalized Advantage Estimation (GAE) её можно аппроксимировать следующим выражением: +γV θ n (s t+1 )−V θ n (s t ) Обновляем веса политики согласно одному из лоссов PPO. Например, используем такой: θ n + 1 = argmaxθ n+1 = θ argmax n−1 ]−βKL(π θ n−1 (⋅∣s t ),π θ n (⋅∣s t )) С помощью MSE лосса оптимизируем значение value-функции: L(ψ)= E t ^ [∣∣V l=0 ∑ ∞ γ l r t+l Итог Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.5. Диффузионные модели Следующий параграф 9.1. Введение в рекомендательные системы А что, если попробовать дообучить уже сейчас? Хочешь применить знания о больших языковых моделях на практике? В Yandex Cloud ты можешь дообучить YandexGPT или Llama на своих данных и интегрировать их в свои проекты. Всё это — с помощью",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 11,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "γ l r t+l Итог Мы с вами обсудили, как развивались языковые модели, какие приёмы и техники необходимы для успешного обучения инструкционных моделей. Также на примере архитектуры LLaMa разобрали, как самостоятельно обучить языковые модели с нуля. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 8.5. Диффузионные модели Следующий параграф 9.1. Введение в рекомендательные системы А что, если попробовать дообучить уже сейчас? Хочешь применить знания о больших языковых моделях на практике? В Yandex Cloud ты можешь дообучить YandexGPT или Llama на своих данных и интегрировать их в свои проекты. Всё это — с помощью Yandex Cloud AI Studio.",
    "metadata": {
      "title": "Языковые модели",
      "url": "https://education.yandex.ru/handbook/ml/article/intro-recsys",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.1",
      "part": 12,
      "total_parts": 12,
      "source_file": "9.1.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Допустим, мы работаем в сервисе рекомендаций фильмов и перед нами стоит задача подобрать для каждого пользователя набор наиболее релевантных фильмов. Пользователь может разными способами провзаимодействовать с фильмом: посмотреть его, оставить отзыв, поставить оценку (например, от 1 до 5). В этом параграфе мы будем строить рекомендации на основе матрицы оценок user-item. Её строки соответствуют объектам, а столбцы – пользователям. На ( i , j ) (i,j)-й позиции матрицы мы ставим либо пропуск, либо оценку, выставленную i i-му объекту j j-м пользователем. Разумеется, не все оценки нам известны: вряд ли каждый пользователь имел возможность ознакомиться с каждым объектом. В процессе решения задачи мы будем пытаться восстановить оценки на местах пропусков. Сделав это, мы сможем, например, порекомендовать пользователю те объекты, которые он ещё не смотрел, но предсказанная оценка которых для этого пользователя максимальна. user Все типы взаимодействия пользователей с объектами мы можем рассматривать как пользовательский фидбек. Обычно различают явный (explicit) и неявный (implicit) виды фидбека. Фидбек называется явным, если он отражает степень интереса пользователя к объекту. Например, к этому типу относят рейтинги, лайки и дизлайки. Такого фидбека обычно мало, он поступает только от тех пользователей, которые соглашаются нам его дать. Обычно гораздо больше информации имеется о неявных предпочтениях – просмотры, клики, добавление в закладки. Но если пользователь, например, посмотрел фильм, мы ещё не можем сделать вывод, что он ему понравился. Мы можем лишь утверждать, что до просмотра этот фильм казался ему достаточно интересным. Поэтому обычно неявная обратная связь более шумная, чем явная. Для начала научимся работать с явным фидбеком. Связь с задачей матричной факторизации Вернёмся к задаче восстановления матрицы оценок и предположим, что каждый пользователь и объект можно закодировать набором из S S скрытых признаков, а оценка i i-го объекта u u-м пользователем равна скалярному произведению соответствующих векторов скрытых представлений . Тогда если бы наша матрица оценок была заполнена полностью, её можно было бы представить в виде произведений двух матриц X X и Y Y, составленных по столбцам из скрытых представлений пользователей и объектов: U=X T ⋅Y Decomp31 Правда, в таком случае нам бы и не требовалось ничего решать: мы могли бы просто рекомендовать пользователю объекты с самыми высокими оценками в соответствующей строке. Но суровая реальность такова, что зачастую матрица оценок сильно разрежена. Мы можем поступить следующим образом: восстановить латентные векторы для пользователей и объектов по имеющемуся набору оценок, после чего предсказать оценки для всех отсутствующих позиций. В параграфе, посвящённом матричной факторизации, мы уже обсуждали способы решения данной задачи с помощью SVD и стохастического градиентного спуска. У SVD есть существенные недостатки: из-за большого количества пропусков в матрице полученное решение будет слишком шумным, а кроме того, его придется каждый раз рассчитывать заново при добавлении новых пользователей или объектов. Градиентный спуск не имеет данных проблем, но тоже не очень практичен. В этом параграфе мы рассмотрим более эффективный алгоритм, называемый Alternating Least Squares (ALS). Постановка задачи Пусть, как и раньше, – скрытые представления пользователей и объектов соответственно размерности T T. Запишем эти векторы по строкам в матрицы X X и Y Y размера S × N S×N и S × D S×D соответственно, где N N",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 1,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "стохастического градиентного спуска. У SVD есть существенные недостатки: из-за большого количества пропусков в матрице полученное решение будет слишком шумным, а кроме того, его придется каждый раз рассчитывать заново при добавлении новых пользователей или объектов. Градиентный спуск не имеет данных проблем, но тоже не очень практичен. В этом параграфе мы рассмотрим более эффективный алгоритм, называемый Alternating Least Squares (ALS). Постановка задачи Пусть, как и раньше, – скрытые представления пользователей и объектов соответственно размерности T T. Запишем эти векторы по строкам в матрицы X X и Y Y размера S × N S×N и S × D S×D соответственно, где N N – количество пользователей, а D D – количество объектов. Обозначим через R R множество таких пар ( u , i ) (u,i) пользователей и объектов, для которых имеются явно проставленные оценки. Предсказывать рейтинги мы будем как скалярное произведение скрытых представлений: В результате мы приходим к следующей задаче оптимизации. Мы хотим научиться как можно лучше приближать известные рейтинги: min min (u,i)∈R Добавив регуляризацию получаем следующую функцию потерь: min min (u,i)∈R ∣∣x ∣∣y i ∣∣ 2 C i Alternating Least Squares (ALS) Оптимальные параметры можно найти с помощью хорошо знакомого нам градиентного спуска, но есть более быстрые и надёжные способы. Если мысленно заморозить параметры, соответствующие латентным факторам пользователей, задача оптимизации латентных представлений объектов сведётся к задаче наименьших квадратов, для которой мы знаем точное решение. Итоговый процесс оптимизации функции потерь будет иметь следующий вид. В цикле до сходимости: Фиксируем матрицу X X (скрытые представления пользователей); Решаем задачу L2-регуляризованной регрессии для каждого товара и находим оптимальную матрицу Y Y; Фиксируем матрицу Y Y (скрытые представления объектов); Решаем задачу L2-регуляризованной регрессии для каждого пользователя и находим оптимальную матрицу X X; Решение, получаемое путём попеременного вычисления точных аналитических решений, обычно точнее тех, что получаются с помощью наивного градиентного спуска. Более того, данное решение имеет эффективную реализацию, позволяющую использовать преимущества параллельных вычислений. Для лучшего понимания распишем каждый шаг данного алгоритма оптимизации: ALS - шаг по (одному) x u x u : argminxu argmin (u,i)∈R ∣∣x ∣∣y i ∣∣ 2 C i Раскроем квадратичный член: argminxu argmin (u,i)∈R ∑ r ui 2 −2 (u,i)∈R (u,i)∈R ∣∣x ∣∣y i ∣∣ 2 C i В первой сумме константы, они уходят. Из второй и третьей возьмём только те слагаемые, в которых участвует x u x u . Из четвёртой остается только член с x u x u , так как все x v x v независимы. Последняя сумма пропадает, так как независимы: argminxu argmin −2 i:(u,i)∈R i:(u,i)∈R +λC В первой сумме индекс u u фиксирован, поэтому x u x u можно вынести за знак суммы: argminxu argmin −2x u T (u,i)∈R (u,i)∈R +λC Объединим второй и третий члены формулы, вынесем умножение на x u x u за скобки: argminxu argmin −2x u T ( (u,i)∈R ∑ r ui y i )+x u T ( (u,i)∈R +λC u )x u = Теперь воспользуемся тем, что argminxu argmin −2x и выпишем ответ: i:(u,i)∈R +λC i I) −1 ( j:(i,j)∈R Таким образом, мы получили аналитическое выражение для вычисления каждого x u x u на",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 2,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "независимы: argminxu argmin −2 i:(u,i)∈R i:(u,i)∈R +λC В первой сумме индекс u u фиксирован, поэтому x u x u можно вынести за знак суммы: argminxu argmin −2x u T (u,i)∈R (u,i)∈R +λC Объединим второй и третий члены формулы, вынесем умножение на x u x u за скобки: argminxu argmin −2x u T ( (u,i)∈R ∑ r ui y i )+x u T ( (u,i)∈R +λC u )x u = Теперь воспользуемся тем, что argminxu argmin −2x и выпишем ответ: i:(u,i)∈R +λC i I) −1 ( j:(i,j)∈R Таким образом, мы получили аналитическое выражение для вычисления каждого x u x u на шаге алгоритма. Отметим, что каждый вектор x u x u мы можем вычислить независимо от других x v x v . Данное наблюдение позволяет нам использовать всю мощь параллельных вычислений для эффективного решения оптимизационной задачи. Распределив данные так, что на каждой вычислительной машине хранятся все y i y i для некоторого подмножества x u x u , на одной итерации алгоритма мы можем параллельно вычислить все x u x u . На следующей итерации аналогичным образом вычисляем все y i y i . IALS (Implicit ALS) Оригинальная статья Раньше мы работали с матрицей R R как с матрицей рейтингов, явно проставленных пользователем. Как мы говорили выше, такого фидбека обычно довольно мало, а куда больше неявного фидбека. При этом количество данных может быть критичным при работе с такими разреженными структурами, как матрицы рейтингов, поэтому хочется научиться работать и с неявным фидбеком тоже. Неявным фидбеком является в том числе и факт взаимодействия, поэтому мы можем заполнить всю матрицу user-item целиком: на тех позициях, где пользователь положительно взаимодействовал с объектом, поставим 1 1, а на тех, где взаимодействие было негативным или его вообще не произошло, поставим 0 0. Эта компонента фидбека называется предпочтением (preference): или r u i не определено ≤0 или r ui не определено Тем самым мы избавились от пропусков в матрице, но использовали не всю информацию. Согласитесь, если один пользователь посмотрел часовое видео польностью, а другой выключил после 5 минут, несправедливо считать, что это видео им понравилось в одинаковой степени. Введём ещё степень уверенности (confidence), отражающую уверенность в оценке пользователя: степень уверенности в =1+α∣r ui ∣ ( степень уверенности в p ui ), где α α – некоторая константа. На местах пропусков мы явно проставляем =0. На остальных позициях мы можем сами регулировать степень уверенности в зависимости от фидбека пользователя. Рассмотрим следующую функцию потерь: ∀u,i ∣∣x ∣∣y Она позволяет: Учитывать неявный фидбек, которого обычно на порядок больше, чем явного, Регулировать степень уверенности в действиях пользователей. IALS: оптимизация Распишем нашу функцию потерь по аналогии с ALS и приведем к форме −2x argminxu argminxu argminxu argminxi argmin u,i ∣∣x ∣∣y argmin +λC argmin −2x +λC argmin −2x )+x +λC +λC Разобьём сумму на 2 части. В первой будет сумма по тем элементам, с которыми у пользователя не было положительного взаимодействия. Во второй – сумма по всем остальным элементам. Также заметим, что во втором множителе суммирование имеет смысл только по ненулевым элементам: ∀i:p ∀i:p +λC u I) −1 ( ∀i:p Заметим, что в",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 3,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "неявный фидбек, которого обычно на порядок больше, чем явного, Регулировать степень уверенности в действиях пользователей. IALS: оптимизация Распишем нашу функцию потерь по аналогии с ALS и приведем к форме −2x argminxu argminxu argminxu argminxi argmin u,i ∣∣x ∣∣y argmin +λC argmin −2x +λC argmin −2x )+x +λC +λC Разобьём сумму на 2 части. В первой будет сумма по тем элементам, с которыми у пользователя не было положительного взаимодействия. Во второй – сумма по всем остальным элементам. Также заметим, что во втором множителе суммирование имеет смысл только по ненулевым элементам: ∀i:p ∀i:p +λC u I) −1 ( ∀i:p Заметим, что в первой сумме все c u i c ui будут равны 1 (так как везде =0). Прибавим и вычтем единицу к c u i c ui во второй сумме и разобьем её на две компоненты. Вторый из них будет сумма по всем , где =0. Объединив её с первой суммой, получим просто Y+λC u I+ ∀i:p −1)y ∀i:p Заметим, что произведение Y T Y Y T Y никак не зависит от u u. Мы можем посчитать его один раз для всех x u x u перед очередной итерацией. В остальном же мы точно так же, как и в случае с обычным ALS, можем распределить данные так, чтобы на одной машине содержались все y j y j , необходимые для обновления x v x v , хранящихся на этой машине, и сделать следующий шаг оптимизации нашей функции потерь. Обобщения ALS и IALS Обе модели: и ALS, и Imlicit ALS – можно несколько усложнить, вместо рассмотрев +μ. В таком случае играют роль некоторых априорных усреднённых оценок пользователя и объекта соответственно, а μ μ является глобальной априорной константой. В модели IALS мы обычно полагаем элементы p u i p ui равными 1 1 во всех случаях, когда имело место взаимодействие, но можем использовать и другие значения, в том числе зависящие от того, что ещё нам известно о пользователях и объектах. Для уверенности =1+α∥r ui ∥ для IALS необязательно использовать 1 1 в качестве значения по умолчанию. Например, события «пользователь не посмотрел популярный фильм» и «пользователь не посмотрел редкий фильм» могут иметь для нас разный вес. FunkSVD Этот подход получил широкую известность после конкурса Netflix Prize в 2006 году. Пост Саймона Фанка про участие в Netflize Prize Фанк предложил моделировать рейтинг как =μ+b . Однако, в отличие от ALS, оптимизация производилась с помощью стохастического градиентного спуска. Правила обновления весов выглядели следующим образом: +η(e ui y i −λx +η(e ui x u −λy +η(e ui −λb +η(e ui −λb i ) Этот подход не получил большой популярности, так как градиентный спуск, в отличие от ALS, намного сложнее распараллелить. Singular Value Decomposition with implicit feedback (SVD++) Оригинальная статья Ранее мы отдельно рассматривали факторизации для явного и неявного фидбека. Но, ограничиваясь только одним типом фидбека, мы теряем много информации. Если мы работаем над стриминговым сервисом, то в качестве неявного фидбека мы можем взять, например, историю фильмов, взятых в прокат. Такие данные не предоставляют нам явных оценок пользователей, но позволяют выявить неявные предпочтения. Учесть неявный фидбек в модели можно",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 4,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "i −λx +η(e ui x u −λy +η(e ui −λb +η(e ui −λb i ) Этот подход не получил большой популярности, так как градиентный спуск, в отличие от ALS, намного сложнее распараллелить. Singular Value Decomposition with implicit feedback (SVD++) Оригинальная статья Ранее мы отдельно рассматривали факторизации для явного и неявного фидбека. Но, ограничиваясь только одним типом фидбека, мы теряем много информации. Если мы работаем над стриминговым сервисом, то в качестве неявного фидбека мы можем взять, например, историю фильмов, взятых в прокат. Такие данные не предоставляют нам явных оценок пользователей, но позволяют выявить неявные предпочтения. Учесть неявный фидбек в модели можно следующим образом: ≈(x u + ∣{j∣p uj  =0}∣ 1 ∀j:p В данной модели пользователь представлен скрытым представлением x u x u , а также слагаемым, отражающим историю неявных взаимодей с айтемами: ∣{j∣p uj  =0}∣ 1 ∀j:p Важно отметить, что вектора y ^ j y j не совпадают с векторами y i y i . Это своего рода «неявные» вектора айтемов. Collaborative Filtering with Temporal Dynamics (timeSVD++) Оригинальная статья Особенностью всех рассмотренных на данный момент разложений является отсутствие учёта порядка просмотра объектов. Однако, как показывает практика, со временем пользователь может менять своё мнение о тех или иных айтемах. Тогда, отсортировав взаимодействия по времени, мы можем разбить события на бакеты и модифицировать приведённую выше функцию потерь, в которой таргет выражается следующим образом: (t)≈(x u (t)+ ∣{j∣p uj  =0}∣ 1 ∀j:p (t)+b i (t)+μ SLIM (Sparse Linear Methods) Оригинальная статья Описанные выше методы демонстрируют хорошее качество, однако требуют больших усилий для эффективной работы в онлайн сервисах. Возникает потребность в лёгких моделях, эффективность которых значительно выше, но качество которых не сильно хуже. Для этого была предложена линейная разреженная модель. Итак, пусть A A – бинарная матрица N × D N×D user-item взаимодействий, например, матрица кликов/показов. Будем определять ответ алгоритма a u i a ui как взвешивание событий из истории пользователя: При этом наложим ограничение ≥0. В такой постановке мы будем учить модель находить «похожие» объекты. Добавим ещё условие =0, которое позволит нам избежать элементарного решения – единичной матрицы W = I W=I.В результате вес w i j w ij выступает в качестве некоторой меры схожести i i-го и j j-го объектов. Осталось определиться с методом оптимизации данных параметров. Для оптимизации используется функция потерь MSE с L 1 L 1 - и L 2 L 2 -регуляризаторами: min ⁡ W 2 1 u,i i,j i,j min Можно заметить, что задачу можно разбить на D D независимых по строкам матрицы min ,...,w iD min (∀i) Данную задачу можно решать покоординатным спуском: Фиксируем все строки W W, кроме одной координаты переходим в оптимум по переходим к следующей координате; повторять до сходимости. Применение данной модели выглядит следующим образом: Рассчитываем вектор взаимодействий пользователя i=1 D ; Считаем a t a u i ata ui для всех непросмотренных объектов; Отбираем топ k k непросмотренных объектов по Так как в задаче оптимизации мы пользуемся L 1 L 1 -регуляризацией, матрица W W получается разреженной. Матрица просмотров A A тоже разреженная (по определению). Эти обстоятельства позволяют заметно улучшить эффективность",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 5,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "на D D независимых по строкам матрицы min ,...,w iD min (∀i) Данную задачу можно решать покоординатным спуском: Фиксируем все строки W W, кроме одной координаты переходим в оптимум по переходим к следующей координате; повторять до сходимости. Применение данной модели выглядит следующим образом: Рассчитываем вектор взаимодействий пользователя i=1 D ; Считаем a t a u i ata ui для всех непросмотренных объектов; Отбираем топ k k непросмотренных объектов по Так как в задаче оптимизации мы пользуемся L 1 L 1 -регуляризацией, матрица W W получается разреженной. Матрица просмотров A A тоже разреженная (по определению). Эти обстоятельства позволяют заметно улучшить эффективность применения модели. Итоги В этом параграфе мы рассмотрели некоторые рекомендательные модели на основе матричных факторизаций. Такие модели редко используется в чистом виде для формирования рекомендательной выдачи. Обычно результаты матричной факторизации используются для генерации кандидатов в рекомендации, когда из сотен тысяч и миллионов объектов необходимо отобрать небольшое количество (например, сотни) самых релевантных. Для генерации кандидатов требуется перемножить вектор пользователя с вектором каждого из сотен тысяч объектов и отобрать топ самых релевантных. В онлайн-сервисах, когда время формирования рекомендаций составляет несколько сотен миллисекунд, нет возможности при каждом запросе рассчитывать релевантность каждого объекта для данного пользователя. Оптимизировать поиск можно с помощью инструментов для поиска ближайших соседей. Для любой функции близости, в том числе и для скалярного произведения, можно построить индекс – структуру данных, с помощью которой для любого пользователя мы сможем быстро приближённо, но зато быстро искать «ближайшие» объекты. В результате, принцип работы выглядит следующим образом: обучаются эмбеддинги объектов и пользователей; для представлений эмбеддингов строится индекс; в рантайме по вектору пользователя происходит приближённый поиск n n самых релевантных объектов; таким образом генерируется список кандидатов в рекомендации; дальше список кандидатов обрабатывается с помощью более хитрых методов машинного обучения. Подробнее о том, как быстро искать ближайших соседей, вы можете узнать в параграфе посвященном метрическим методам Помимо генерации кандидатов, полученные представления можно использовать в качестве признаков в более сложных моделях. Основной недостаток методов, основанных на матричной факторизации, состоит в том, что они используют лишь информацию о взаимодействии пользователей и объектов, но не о них самих. В следующем параграфе мы рассмотрим контентные методы, которые используют атрибуты объектов и пользователей. Список литературы Статья про Implicit ALS Статья про SVD++ Статья про TimeSVD++ Статья про SLIM Пост Саймона Фанка про участие в конкурсе Netflix Prize Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 9.1. Введение в рекомендательные системы Следующий параграф 9.3. Контентные рекомендации",
    "metadata": {
      "title": "Рекомендации на основе матричных разложений",
      "url": "https://education.yandex.ru/handbook/ml/article/rekomendacii-na-osnove-matrichnyh-razlozhenij",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.2",
      "part": 6,
      "total_parts": 6,
      "source_file": "9.2.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Все рекомендательные системы можно поделить на три типа в зависимости от того, какую информацию они используют для построения рекомендаций: Контентные; Коллаборативые; Гибридные. В данном разделе мы подробнее рассмотрим основные алгоритмы построения контентных рекомендаций. Основная идея контентных рекомендаций состоит в том, что для их построения будут использоваться атрибуты объектов и пользователей. На основе данных атрибутов мы можем найти релевантные данному пользователю объекты и рекомендовать их. Представим, например, что мы работаем в музыкальном онлайн-сервисе и хотим подбирать наиболее релевантную музыку нашим пользователям. Допустим у нас есть пользователь Иван, который интересуется русским роком. Тогда наша система может рекомендовать Ивану музыку этого или подобных жанров. Можно придумать много различных атрибутов трека: жанр, автор, год выхода, продолжительность и так далее. Также можно использовать дополнительную информацию о пользователе: возраст, уровень дохода и тому подобные. Какими бывают контентные признаки Допустим, мы работаем в музыкальном сервисе. Тогда в качестве признаков объектов можно использовать: Стандартные статистики объекта: количество лайков, кликов, полных прослушиваний; Признаки автора: количество слушателей, жанр; Неструктурированные данные: названия треков, обложки альбомов или даже предобученные эмбеддинги треков целиком. В качестве признаков пользователей можно использовать: Информацию про пользователя, если она нам доступна: возраст, пол, язык, насколько долго пользуется сервисом; Информацию про контекст запроса: с какого устройства был сделан, в какое время. Информацию про друзей пользователя и их взаимодействия. Например, усреднённый эмбеддинг всех треков, которые слушал каждый из друзей. Или же можно обучить RNN или Transformer на истории и результат конкатенировать к остальным признакам. Факторизационные машины Начнём с постановки задачи. Пусть I – множество объектов (айтемов), U - множество пользователей. Для каждой пары объект-пользователь построим вектор размерности ∣U∣+∣I∣ взаимодействия этой пары, в котором единицы стоят на месте соответствующих пользователя и объекта: one Предсказывать будем пользовательские рейтинги объектов a ( x ) a(x). Можно рассмотреть простейшую регрессионную модель: a(x)=w 0 + t=1 ∑ ∣U∣+∣I∣ w t x t Заметим, что к этой модели легко добавить любые фичи объектов, пользователей или пар объект-пользователь: factorization Дальше будем обозначать через n n общее число фичей. Модель можно обогатить признаками, отвечающими за взаимодействия второго порядка: a(x)=w 0 + t=1 r=1 ∑ n s=r+1 Матрицу W=(w rs ) можно считать симметричной: в любом случае, мы используем только её верхний треугольник. Из-за использования попарных взаимодействий пользователей и объектов в полученной модели будет n(n+1) +n+1 параметр, и так как n⩾∣U∣+∣I∣ может быть очень большим, работать с такой моделью может оказаться непросто. Для решения этой проблемы можно использовать следующий трюк. Сопоставим каждому признаку x t x t вектор для некоторого не очень большого k k и представим модель в виде: a(x)=w 0 + t=1 r=1 ∑ n s=r+1 Таким образом, мы заменяем симметричную матрицу коэффициентов W W на её низкоранговое приближение V T V V T V, где V V – матрица n × k n×k с векторами v i v i по столбцам. Число параметров модели при этом можно снизить до nk+n+1. На практике матрица W W разреженная, и, как правило, даже при небольшом k k получается её неплохо приблизить. В то же время, при небольших k k модель обладает лучшей обобщающей способностью. Вычислить r=1 n ∑ s=1",
    "metadata": {
      "title": "Контентные рекомендации",
      "url": "https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.3",
      "part": 1,
      "total_parts": 5,
      "source_file": "9.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "некоторого не очень большого k k и представим модель в виде: a(x)=w 0 + t=1 r=1 ∑ n s=r+1 Таким образом, мы заменяем симметричную матрицу коэффициентов W W на её низкоранговое приближение V T V V T V, где V V – матрица n × k n×k с векторами v i v i по столбцам. Число параметров модели при этом можно снизить до nk+n+1. На практике матрица W W разреженная, и, как правило, даже при небольшом k k получается её неплохо приблизить. В то же время, при небольших k k модель обладает лучшей обобщающей способностью. Вычислить r=1 n ∑ s=1 по можно за O ( n k ) O(nk): r=1 ∑ n s=r+1 r=1 ∑ n s=1 r=1 r=1 ∑ n s=1 ∑ n f=1 r=1 ∑ n f=1 f=1 ∑ k ( r=1 )⋅( s=1 r=1 f=1 ∑ k (( r=1 r=1 Итоговая модель имеет вид a(x)=w 0 + r=1 r=1 r=1 ∑ n ∣∣v Данная модель и называется факторизационной машиной. Первоначально факторизационные машины использовали только коллаборативный сигнал, но, как мы уже видели, в такую модель можно естественным образом добавить и контентную информацию. Факторизацонную машину можно обучать для решения разных задач. Например: Предсказание рейтинга. Ответ модели a ( x ) a(x) можно интерпретировать, как вещественный рейтинг, и решать задачу регрессии. Бинарную классификацию рекомендовать/не рекомендовать. Тогда a ( x ) a(x) имеет смысл логита, и мы можем оптимизировать оптимизировать log loss или hinge loss. Ранжирование объектов. Тогда a ( x ) a(x) – это ранжирующая функция. Модель обычно обучается градиентным спуском. FFM – Field-aware Factorization Machines Оригинальная статья Статья про практическое применение Как следующий этап развития факториационных машин, появилась идея иметь несколько различных латентных представлений для каждой из фичей. Пример: есть три разных по своей природе признака: год выпуска, цвет и марка автомобиля. В факторизационной машине для учёта взаимодействия год-цвет и год-марка используется один и тот же вектор для года. Но так как эти признаки разные по смыслу, то и характер их взаимодействия может отличаться. Идея: использовать 2 разных вектора для признака «год выпуска» при учёте взаимодействий год-цвет и год-марка. Таким образом, модель принимает вид: a(x)=w 0 + t=1 r=1 ∑ n s=r+1 ∑ m <v r,s ,v s,r >x r x s FFM Авторы статьи выложили исходный код своей библиотеки libffm, с помощью которой они смогли войти в топ-3 сразу в трёх соревнованиях на kaggle (Criteo, Avazu, Outbrain). Подробнее об этом можно почитать вот тут. DSSM (deep sematic similiarity model) Теперь рассмотрим ещё одну популярную модель, которая использует контентную информацию для построения рекомендаций – DSSM. Оригинальная статья В оригинальной статье DSSM была использована для нахождения «схожести» между поисковым запросом и документом. Для этого она использовала текст запроса и текст документа. DSSM представляет из себя «двуногую» (two-tower) нейронную сеть. В исходной постановке на первый вход подаётся текст запроса, а на второй – текст документа. Далее, независимо для текста запроса и текста документа строятся эмбеддинги. Итоговая «схожесть» вычисляется, как косинусная мера близости между ними. На схеме ниже Q – это запрос (query), а D – документ (document). DSSM Некоторые авторы пытались",
    "metadata": {
      "title": "Контентные рекомендации",
      "url": "https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.3",
      "part": 2,
      "total_parts": 5,
      "source_file": "9.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "model) Теперь рассмотрим ещё одну популярную модель, которая использует контентную информацию для построения рекомендаций – DSSM. Оригинальная статья В оригинальной статье DSSM была использована для нахождения «схожести» между поисковым запросом и документом. Для этого она использовала текст запроса и текст документа. DSSM представляет из себя «двуногую» (two-tower) нейронную сеть. В исходной постановке на первый вход подаётся текст запроса, а на второй – текст документа. Далее, независимо для текста запроса и текста документа строятся эмбеддинги. Итоговая «схожесть» вычисляется, как косинусная мера близости между ними. На схеме ниже Q – это запрос (query), а D – документ (document). DSSM Некоторые авторы пытались в качестве меры близости рассматривать вместо косинусной меры обучаемый MLP, но это оказалось гиблой идеей. Эта архитектура оказалась крайне удобной при использовании на практике, так как эмбеддинги пользователя и объекта можно предподсчитать независимо и дальше хранить сразу готовые представления для них, а при запросе к рекомендациям просто пересчитывать меру близости, что ускоряет применение модели. Данная идея хорошо обобщается на построение рекомендаций. Поиск релевантных объектов можно представить, как задачу ранжирования, где вместо текстов запроса и документа мы будем иметь некоторую контентную информацию о пользователе и объекте. Обучение DSSM Давайте считать, что мы для каждого запроса q q предсказываем один релевантный документ. Обозначим через построенные моделью эмбеддинги запроса q q и документа d d соответственно. Будем вычислять условную вероятность клика по документу d d при условии запроса q q следующим образом: exp exp P(d∣q)= ∑ i=1 D exp(b 0 R(q,d i )) exp(b 0 R(q,d)) где cos R(q,d)=cos(y q ,y d )= ∣∣y q ∣∣⋅∣∣y Здесь b 0 b 0 – коэффициент сглаживания, который подбирается эмпирически, а D D – число всех документов. Если в качестве функции потерь мы выбираем кросс-энтропию, то на паре запрос-кликнутый документ (q,d + ) она принимает вид L(q,d + )=−log(P(d + ∣q)). Но вычислять градиент такого функционала для каждого примера дорого, ведь для этого придётся для каждого запроса находить вероятность клика по всем документам. Что же делать? На помощь приходит negative sampling. Заметим, что среди документов d d в знаменателе P(d∣q) есть лишь один кликнутый, а остальные тысячи и миллионы являются отрицательными примерами. Есть смысл на каждом шаге оптимизации рассматривать не все из них, а только небольшую выборку, вместо полной суммы ∑ i = 1 D exp i=1 ∑ D exp(b 0 R(q,d i )) беря exp exp exp(b 0 R(q,d + ))+ i=1 ∑ k exp(b 0 R(q,d i − )), где ,…,d k − – подобранные для запроса q q негативные примеры. Генерировать их можно по-разному; на практике чаще всего используют одну из следующих стратегий: Равновероятно выбирать подмножество документов из некликнутых. В оригинальной статье предлагают брать позитивные и негативные в соотношении 4 : 1 4:1. С большей вероятностью выбирать те из некликнутых документов, популярность которых выше. На каждой эпохе обучения выбирать некликнутые документы, получившие максимальный скор для этого запроса на предыдущей эпохе. Другие функции потерь Pairwise loss Задачу построения рекомендаций можно решать, как задачу ранжирования. Например, это можно делать с помощью попарного лосса. А именно, рассмотрим пару объектов, в которой i 1 i 1 –",
    "metadata": {
      "title": "Контентные рекомендации",
      "url": "https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.3",
      "part": 3,
      "total_parts": 5,
      "source_file": "9.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "подобранные для запроса q q негативные примеры. Генерировать их можно по-разному; на практике чаще всего используют одну из следующих стратегий: Равновероятно выбирать подмножество документов из некликнутых. В оригинальной статье предлагают брать позитивные и негативные в соотношении 4 : 1 4:1. С большей вероятностью выбирать те из некликнутых документов, популярность которых выше. На каждой эпохе обучения выбирать некликнутые документы, получившие максимальный скор для этого запроса на предыдущей эпохе. Другие функции потерь Pairwise loss Задачу построения рекомендаций можно решать, как задачу ранжирования. Например, это можно делать с помощью попарного лосса. А именно, рассмотрим пару объектов, в которой i 1 i 1 – релевантный, а i 2 i 2 не релевантный для пользователя u u. Тогда мы можем использовать один из двух вариантов функции потерь: CrossEntropy ( 1.0 L(R(u,i 1 ),R(u,i 2 ))=CrossEntropy(1.0,σ(R(u,i 1 )−R(u,i 2 ))). Тем самым модель будет учиться ранжировать положительные примеры выше отрицательных. max L(R(u,i 1 ),R(u,i 2 ))=max(0,α−R(u,i 1 )+R(u,i 2 )) (triplet loss). При этом модель обучается так, чтобы положительный и отрицательный примеры как можно больше отличались. Эта функция потерь довольно популярна не только в DSSM сетках, но и в целом в задачах, где нужно обучить парные представления ) объектов ( q , d ) (q,d) из разных доменов так, чтобы для релевантных друг другу q q и d d эмбеддинги оказывались близкими, а для не релевантных далёкими. Full Product Softmax loss Рассмотрим батч ),…,(u M ,i M ,r M ) размера M M, где u t u t – пользователь, i t i t – пользователю, а r t r t – таргет, степень релевантности объекта пользователю. Построим по ним: матрицу эмбеддингов пользователей U∈R M×D ; матрицу эмбеддингов объектов W∈R M×D ; вектор таргетов r ∈ R M r∈R M . Рассмотрим матрицу softmax softmax(αUW T +β),UW T ∈R M×M , где softmax берётся по строкам DSSM Рассмотрим функцию потерь вид log ⁡ ( diag ( softmax L=−I{r>0} T ⋅log(diag(softmax(αUW T +β))) Эта функция потерь старается сделать так, чтобы для релевантных друг другу (с r > 0 r>0) пар ( u , i ) (u,i) скалярное произведение эмбеддингов ⟩ было максимальным. Трансформеры для рекомендаций В 2018 году появилась архитектура трансформеров на основе механизма внимания. Модели на основе трансформеров показали state-of-the-art результаты на большом числе NLP задач, а впоследствии оказалось, что они отлично подходят и для задач компьютерного зрения. С их помощью можно решать и задачи рекомендаций. Аналогия заключается в следующем: если в NLP трансформеры работают с последовательностями токенов, то в рекомендациях в качестве последовательности можно взять историю событий пользователя. Каждый элемент последовательности – это взаимодействие пользователя с объектом, например, клик на объект. Классические модели рекомендаций часто игнорируют тот факт, что история пользователя – это направленная последовательность, в которой порядок событий имеет значение. Трансформеры позволяют учитывать как порядок событий, так и сложные паттерны в поведении и интересах пользователя. Например, исследователи из Alibaba представили модель, которую назвали Behaviour Sequence Transformer. Авторы заявляют, что модель используется в продакшене. Модель решает задачу Click Through Rate (CTR) prediction – предсказание вероятности клика по объекту. transformer На вход модели подается история кликов",
    "metadata": {
      "title": "Контентные рекомендации",
      "url": "https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.3",
      "part": 4,
      "total_parts": 5,
      "source_file": "9.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "последовательностями токенов, то в рекомендациях в качестве последовательности можно взять историю событий пользователя. Каждый элемент последовательности – это взаимодействие пользователя с объектом, например, клик на объект. Классические модели рекомендаций часто игнорируют тот факт, что история пользователя – это направленная последовательность, в которой порядок событий имеет значение. Трансформеры позволяют учитывать как порядок событий, так и сложные паттерны в поведении и интересах пользователя. Например, исследователи из Alibaba представили модель, которую назвали Behaviour Sequence Transformer. Авторы заявляют, что модель используется в продакшене. Модель решает задачу Click Through Rate (CTR) prediction – предсказание вероятности клика по объекту. transformer На вход модели подается история кликов пользователя, на основе которой нужно предсказать вероятность клика по заданному объекту. Роль архитектуры трансформера здесь в том, чтобы качественно закодировать представление пользователя, после чего применяется обычный multi layer perceptron (MLP) для предсказания вероятности. Помимо архитектур, которые специально разрабатываются под задачи рекомендаций, трансформеры можно использовать и как обособленные предобученные модели для построения векторых представлений текстов или изображений, которые затем подаются как признаки для решения downstream задач в домене рекомендаций. Несмотря на очевидные преимущества трансформеров с точки зрения качества, их использование в продакшене часто ограничивается имеющимися вычислительными ресурсами. Это особенно актуально для рекомендаций, где модели важно применять непосредственно в момент запроса пользователя. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 9.2. Рекомендации на основе матричных разложений Следующий параграф 9.4. Хорошие свойства рекомендательных систем",
    "metadata": {
      "title": "Контентные рекомендации",
      "url": "https://education.yandex.ru/handbook/ml/article/kontentnye-rekomendacii",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.3",
      "part": 5,
      "total_parts": 5,
      "source_file": "9.3.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "Предположим, выдача нашей рекомендательной системы имеет высокие значения метрик ранжирования. Значит ли это, что система действительно хорошая? Не всегда просто ответить на этот вопрос. Оптимизируя определенные метрики, можно выкрутить кликбейт, и пользователи будут охотно кликать в моменте, но больше не станут пользоваться таким сервисом. Соответственно, нужно как-то измерять «счастье пользователей», попытаться формализовать свойства, которыми должна обладать хорошая рекомендательная система. Однозначного ответа на этот вопрос нет, всё зависит от контекста применения рекомендательной системы. В этом разделе мы поговорим о наиболее распространённых критериях, которые довольно часто оказываются важными. Полнота (Coverage) Под полнотой в данном контексте понимается доля рекомендованных объектов recommended среди всех объектов Coverage= ∣I∣ ∣I recommended ∣ Эта метрика была предложена в статье Ge, M., Delgado-Battenfeld, C., Jannach, D. (2010, September). Beyond accuracy: evaluating recommender systems by coverage and serendipity. In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260). Данную метрику имеет смысл оценивать в разных временных интервалах, при этом принимая во внимание возможные ограничения, связанные с объемом данных. Например, нас может интересовать значение полноты за первый день работы рекомендательной системы, а может – за неделю. Целевое поведение полноты будет различаться в зависимости от доменных областей и бизнес деталей конкретного случая. Например, в рекомендациях музыки может быть полезно периодически повторно рекомендовать треки, которые пользователю в наибольшей степени нравятся, так как пользователь может захотеть послушать их еще раз. В то же время в рекомендациях фильмов это реже оказывается осмысленным: обычно проходит много времени, прежде чем пользователь захочет пересмотреть фильм. Таким образом, во втором случае полнота будет расти быстрее за счет отсутствия повторов. Еще одним фактором, влияющим на полноту, является алгоритм холодного старта, который может использоваться для того чтобы найти подходящие объекты для нового пользователя или подходящих пользователей для нового объекта. Часто пользователям на этапе холодного старта показывают самые популярные объекты. Из-за этого свежедобавленные объекты (например, музыкальные треки) могут неявно пессимизироваться алгоритмом. Один из способов решения проблемы – бустить свежие объекты в течение определённого времени, чтобы они показывались чаще. Настройки логики холодного старта могут сильно повлиять на метрику полноты. Среди других актуальных вопросов, которыми стоит задаваться: Cколько нужно дней, чтобы полнота достигала заданного значения p p? Возможно ли достичь такого значения в принципе, используя текущий алгоритм? Чтобы ответить на эти вопросы, нужно принимать во внимание ряд факторов: Какой объём трафика у системы рекомендаций? Есть ли у бизнеса ограничения, влияющие на конечный список рекомендаций? Имеет ли алгоритм рекомендаций достаточную степень персонализации? Можно ли регулировать режимы exploration и exploitation во время работы рекомендательной системы? Каждый из этих факторов может по-разному влиять на динамику полноты. Бизнес ограничения и слабая степень персонализации могут сдерживать рост полноты. Напротив, если модель высокоперсонализированная и учитывает много пользовательских факторов, то она способна рекомендовать больше уникальных объектов из хвоста распределения, которые тоже могут ему понравиться, тем самым обеспечивая рост полноты. Новизна (Novelty) Один из способов оценить новизну рекомендательной системы – использовать статистическую меру собственной информации объекта (self information), которая используется в теории информации и тесно связана с понятием энтропии. Значение собственной информации для события X X равняется логарифму вероятности наступления данного события. Согласно теории, чем меньше вероятность наступления события,",
    "metadata": {
      "title": "Хорошие свойства рекомендательных систем",
      "url": "https://education.yandex.ru/handbook/ml/article/horoshie-svojstva-rekomendatelnyh-sistem",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.4",
      "part": 1,
      "total_parts": 3,
      "source_file": "9.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "рекомендательной системы? Каждый из этих факторов может по-разному влиять на динамику полноты. Бизнес ограничения и слабая степень персонализации могут сдерживать рост полноты. Напротив, если модель высокоперсонализированная и учитывает много пользовательских факторов, то она способна рекомендовать больше уникальных объектов из хвоста распределения, которые тоже могут ему понравиться, тем самым обеспечивая рост полноты. Новизна (Novelty) Один из способов оценить новизну рекомендательной системы – использовать статистическую меру собственной информации объекта (self information), которая используется в теории информации и тесно связана с понятием энтропии. Значение собственной информации для события X X равняется логарифму вероятности наступления данного события. Согласно теории, чем меньше вероятность наступления события, тем больше потенциальной информации принесет это событие при его наступлении. Единицей информации при использовании логарифма по основании 2 2 является бит. Теперь если переносить идею собственной информации в парадигму рекомендательных систем, то получается, что чем менее популярен объект, тем более вероятно, что он будет новым для пользователя. А значит мера информации у такого объекта будет выше. Для каждого рекомендованного объекта i i считаем вероятность, с которой его порекомендуют случайному пользователю: , где m i m i – количество пользователей, которым был показан i i-й объект, а N N – общее число пользователей. Для заданного пользователя усредняем значение собственной информации по списку его рекомендаций R R и получаем итоговое значение метрики: Novelty user = ∣R∣ 1 i∈R ∑ −log(P(i)) Разнообразие (Diversity) Разнообразие – это способность модели рекомендовать разные по содержанию объекты. Такое свойство очень важно для долгосрочного успеха сервисов, основанных на рекомендательных системах. Действительно, если модель постоянно рекомендует похожие друг на друга объекты, то рано или поздно пользователю наскучат такие рекомендации. diversity Разнообразие можно рассчитывать на основе комбинаций метрик полноты и новизны. Также мерой разнообразия может быть дисперсия рекомендаций за заданный промежуток времени. Помимо этого популярны подходы, использующие эмбединги объектов для оценки попарной похожести объектов и расчёта на основе неё значения разнообразия. Одна из таких метрик – Intra List Similarity (ILS). Чтобы ее посчитать, нужно иметь эмбединги объектов рекомендаций, находящиеся в едином векторном пространстве. Для расчёта разнообразия для одного пользователя нужно усреднить попарную схожесть sim sim между рекомендованными объектами: sim ILS user = R 1 iϵR ∑ jϵR ∑ sim(i,j), где R R – это набор рекомендованных пользователю объектов. Для того чтобы добиться большего разнообразия, метрику нужно минимизировать. Мера схожести должна быть больше для более похожих объектов. Чаще всего используется косинусная близость (cosine similarity). Serendipity Одно из самых желанных свойств для любой рекомендательной системы. У слова serendipity нет четкого перевода, в 2008 году оно даже попало в список самых неподдающихся переводу слов в мире. На русский иногда оно переводится как «интуитивная прозорливость». Serendipity – это способность рекомендовать такие объекты, которые не только релевантны для пользователя, но ещё и существенно отличаются от того, с какими объектами пользователь взаимодействовал в прошлом. serendipity Serendipity – довольно субъективное свойство и его сложно формализовать. Более того рекомендации, удовлетворяющие этому свойству, встречаются редко, что усложняет интерпретацию и измерение serendipity. Нет консенсуса о том, какой метрикой можно оценить его. Мы расскажем о способе, предложенном в статье T. Murakami, K. Mori, R. Orihara, Metrics for evaluating the serendipity of",
    "metadata": {
      "title": "Хорошие свойства рекомендательных систем",
      "url": "https://education.yandex.ru/handbook/ml/article/horoshie-svojstva-rekomendatelnyh-sistem",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.4",
      "part": 2,
      "total_parts": 3,
      "source_file": "9.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  },
  {
    "text": "2008 году оно даже попало в список самых неподдающихся переводу слов в мире. На русский иногда оно переводится как «интуитивная прозорливость». Serendipity – это способность рекомендовать такие объекты, которые не только релевантны для пользователя, но ещё и существенно отличаются от того, с какими объектами пользователь взаимодействовал в прошлом. serendipity Serendipity – довольно субъективное свойство и его сложно формализовать. Более того рекомендации, удовлетворяющие этому свойству, встречаются редко, что усложняет интерпретацию и измерение serendipity. Нет консенсуса о том, какой метрикой можно оценить его. Мы расскажем о способе, предложенном в статье T. Murakami, K. Mori, R. Orihara, Metrics for evaluating the serendipity of recommendation lists, in: New Frontiers in Artificial Intelligence, Vol. 4914, Springer Berlin Heidelberg, Berlin, Heidelberg, 2008, pp. 40–46. Пусть R u R u – список рекомендаций для пользователя, (i) – предсказание модели, для каждого объекта из списка, а Prim u ( i ) Prim u (i) – предсказание примитивной модели (в качестве примитивной можно брать модель на основе эвристик без машинного обучения или простую неперсональную модель), а rel rel – известная релевантность объекта для пользователя. Тогда Serendipity рассчитывается следующим образом: Prim rel u ( i ) Serendipity user = iϵR ∑ max(Pr u (i)−Prim u (i),0)⋅rel u (i) Значение метрики можно усреднить по всем пользователям тестовой выборки. Чем больше значение, тем больше модель удовлетворяет свойству Serendipity. Ключевая идея формулы такова: если уверенность персонализированной модели в том, что пользователю понравится i i-ый айтем, больше, чем уверенность неперсональной модели (примитивной), это значит, что данному пользователю может особенно понравиться i i-й айтем. Отдельный вопрос – как оптимизировать Serendipity. Нужно улучшать способность модели к персонализации: добавлять больше фичей для пар (пользователь, объект); взвешивать таргеты, чтобы более тонко учитывать необычные клики/просмотры; писать кастомные функции потерь, которые будут поощрять модель за буст неожиданныйх объектов (которые в большей степени удовлетворяют свойству serendipity). Кроме того, имеет смысл оптимизировать модель по метрике serendipity на офлайн тестовой выборке. Заключение В этом разделе мы рассмотрели ключевые свойства рекомендательных систем и метрики для их оценки. Рекомендательные системы – сложная область, где нет готовых рецептов оценки качества. Ключевые метрики всегда идут от продуктовых деталей применения рекомендательной системы. Полезно смотреть на несколько метрик одновременно, чтобы оценить разные свойства моделей. В какой момент нужно начинать следить за метриками из данного раздела? Несмотря на их ценность, на начальном этапе стоит концентрироваться на более простых и интуитивно понятных с точки зрения бизнеса метриках: конверсии, среднем времени визита и так далее. А вот как только базовые метрики будут на удовлетворительном уровне, стоит начинать мониторить и оптимизировать метрики, разобранные в этом разделе. Знак вопроса Пройдите квиз по параграфу Чтобы закрепить пройденный материал Сообщить об ошибке Предыдущий параграф 9.3. Контентные рекомендации Следующий параграф 10.1. Кластеризация Методы кластеризации: K-Means, агломеративная кластеризация, DBSCAN. Оценка качества кластеризации",
    "metadata": {
      "title": "Хорошие свойства рекомендательных систем",
      "url": "https://education.yandex.ru/handbook/ml/article/horoshie-svojstva-rekomendatelnyh-sistem",
      "course": "ml",
      "chapter": "9. Рекомендательные системы",
      "chapter_id": "9.4",
      "part": 3,
      "total_parts": 3,
      "source_file": "9.4.txt",
      "description": "Материалы курса Яндекс по машинному обучению"
    }
  }
]