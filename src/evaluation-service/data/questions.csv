Номер,Вопрос,Эталонный ответ,Ответ модели,Оценка эксперта,Оценка ИИ
1,Опишите жадный алгоритм по построению решающих деревьев?,"Алгоритм:
Начинаем с корня — берём весь обучающий датасет
Выбираем лучший признак и значение для сплита (критерия ветвления)
Разбиваем данные на две части по этому сплиту
Повторяем шаги для каждой части — рекурсивно строим поддеревья
Останавливаемся, если выполнен критерий остановки; тогда вершина становится листом
Критерии ветвления:
gini или entropy - классификация
mse или mae - регрессия
Критерий остановки:
достигнута максимальная глубина
в узле мало или много объектов
прирост информации от нового сплита слишком мал",,,
2,В чём разница между классификацией и регрессией в деревьях?,"Классификация:
предсказываем метки классов
Используем gini или энтропию
самый популярный класс по объектам в листе (мода)
Регрессия:
предсказываем числовое значение
Оптимизируем mse/mae во время сплита
Используем среднее / медиану в листе как предсказание",,,
3,Как оценить важность признаков по дереву?,"Обычно важность признака измеряется как суммарное снижение impurity (Gini или энтропии), которое произошло благодаря разбиениям по этому признаку, по всем уровням дерева.
В ансамблях деревьев (например, в Random Forest) эти значения усредняются по всем деревьям.",,,
4,Как можно регуляризировать решающее дерево?,"Ограничить максимальную глубину дерева (max_depth)
Задать минимальное количество объектов в узле (min_samples_split, min_samples_leaf)
Ограничить максимальное число признаков для выбора сплита (max_features)
Использовать pruning (обрезку лишних веток после обучения)
max_leaf_nodes — ограничивает общее число листьев в дереве
min_impurity_decrease - минимальный прирост критерия сплита",,,
5,Что такое бустинг?,"Бустинг — это метод ансамблирования, в котором модели обучаются последовательно. Каждая последующая модель пытается исправить ошибки предыдущих. Например, слабые модели (например, слабые деревья) обучаются на ошибках предыдущих моделей, усиливая их предсказания на тяжёлых объектах.",,,
6,Почему градиентный бустинг называется градиентным? Где там градиент?,"Когда мы говорим, что обучаемся на ошибках предыдущих моделей, мы:
Мы фиксируем функцию потерь. Она зависит от целевых значений и от текущей композиции
L(y, F(x))
Добавляя новый базовых алгоритм в наш ансамбль / композицию, хотим минимизировать функцию потерь
Для этого можем посчитать градиент этой функции потерь (градиент берем по предсказаниям ансамбля)
И обучать следующее дерево на антиградиент
Таким образом мы обучаемся на ошибках, но не совсем на остатках (разницы между предсказанием и таргетом), а на значение антиградиента, что, например, для функции потерь mse одно и то же.     Формула градиента                                                                                                                      ",,,
7,Какие есть плюсы и минусы метрических моделей и knn в особенности?,"Плюсы:
Простота — легко реализовать, легко объяснить.
Нет обучения — не нужно долго тренировать модель.
Интерпретируемость — можно объяснить решение через похожие примеры: “мы присвоили этот класс, потому что рядом был вот этот объект”.
Работают на любых признаках (если выбрать подходящую метрику): числовые, бинарные, категориальные.

Минусы:
Медленные предсказания — при большом количестве данных или признаков тормозят.
Чувствительность к масштабу признаков — без нормализации всё ломается.
Проклятие размерности — в высоких размерностях расстояния теряют смысл.
Чувствительность к шуму и выбросам — особенно при маленьком k.
Хранение всей выборки — нужны память и ресурсы для хранения и поиска.",,,
8,В задаче классификации дисбаланс классов 95:5. Как решить проблему?,"from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split

# Пример несбалансированных данных
X = df[['A', 'C']]
y = pd.Series([0, 0, 0, 0, 1])  # 4:1

# SMOTE (oversampling)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Undersampling
rus = RandomUnderSampler(random_state=42)
X_under, y_under = rus.fit_resample(X, y)",,,
9,Как обнаружить выбросы в данных?,"from scipy import stats

# Метод z-score
z_scores = np.abs(stats.zscore(df[['A', 'C']]))
outliers_z = (z_scores > 3).any(axis=1)

# IQR метод
Q1 = df[['A', 'C']].quantile(0.25)
Q3 = df[['A', 'C']].quantile(0.75)
IQR = Q3 - Q1
outliers_iqr = ((df[['A', 'C']] < (Q1 - 1.5 * IQR)) | 
                (df[['A', 'C']] > (Q3 + 1.5 * IQR))).any(axis=1)",,,
10,Как закодировать категориальный признак с множеством уникальных значений?,"from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Label Encoding для ordinal признаков
le = LabelEncoder()
df['B_encoded'] = le.fit_transform(df['B'])

# One-Hot Encoding для nominal признаков
ohe = OneHotEncoder(sparse_output=False, drop='first')
B_ohe = ohe.fit_transform(df[['B']])
df_ohe = pd.DataFrame(B_ohe, columns=ohe.get_feature_names_out(['B']))",,,
11,В DataFrame есть колонки с пропусками (числовые и категориальные). Как вы будете их обрабатывать?,"import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Создадим пример данных
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5],
    'B': ['cat', 'dog', np.nan, 'cat', 'dog'],
    'C': [10, np.nan, np.nan, 40, 50]
})

# Для числовых - медиана
num_imputer = SimpleImputer(strategy='median')
df[['A', 'C']] = num_imputer.fit_transform(df[['A', 'C']])

# Для категориальных - мода
cat_imputer = SimpleImputer(strategy='most_frequent')
df[['B']] = cat_imputer.fit_transform(df[['B']])",,,
12,Определить важность признаков в модели RandomForestClassifier?,"from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# Важность признаков
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

plt.barh(importances['feature'], importances['importance'])
plt.xlabel('Feature Importance')
plt.show()",,,
13,Напишите метод определения оптимизации гиперпараметров модели используя библиотеку sklearn?,"from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

model = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(model, param_grid, 
                          cv=3, 
                          scoring='accuracy',
                          n_jobs=-1)
grid_search.fit(X, y)

print(f'Best params: {grid_search.best_params_}')
print(f'Best score: {grid_search.best_score_:.3f}')",,,
14,Какие метрики вы будете использовать для бинарной классификации? Напишите метод,"from sklearn.metrics import classification_report, confusion_matrix, roc_curve

# Предположим, что у нас есть предсказания
y_true = [0, 1, 0, 1, 0]
y_pred = [0, 0, 0, 1, 1]
y_prob = [0.1, 0.4, 0.2, 0.8, 0.7]

print(classification_report(y_true, y_pred))
print(f'Confusion Matrix:\n{confusion_matrix(y_true, y_pred)}')

# ROC-AUC
fpr, tpr, _ = roc_curve(y_true, y_prob)
roc_auc = auc(fpr, tpr)
print(f'ROC-AUC: {roc_auc:.3f}')",,,
15,"Задача: В банке система мониторинга мошеннических операций срабатывает с точностью 99% (вероятность обнаружить мошенническую операцию = 0.99). При этом вероятность ложного срабатывания (система сработала на легальную операцию) = 1%. Известно, что лишь 0.1% всех операций являются мошенническими.
Вопрос: Если система сработала на конкретную операцию, какова вероятность того, что операция действительно мошенническая?","Решение (Теорема Байеса):
Обозначим:
M = операция мошенническая
A = система сработала (аларм)

Дано:
P(A|M) = 0.99 (чувствительность)
P(A|не M) = 0.01 (ложное срабатывание)
P(M) = 0.001 (базовая распространенность)
Нужно найти P(M|A).
По формуле Байеса:
P(M|A) = [P(A|M) * P(M)] / P(A)
P(A) = P(A|M)P(M) + P(A|не M)P(не M)
= (0.99 * 0.001) + (0.01 * 0.999)
= 0.00099 + 0.00999 = 0.01098
Тогда:
P(M|A) = (0.99 * 0.001) / 0.01098 ≈ 0.09016 ≈ 9.02%
Вывод: Несмотря на высокую точность системы, из-за низкой базовой распространенности мошенничества, лишь ~9% срабатываний являются истинно положительными. Это классический пример важности учёта априорной вероятности.",,,
16,"Задача: В A/B-тесте новая рекомендательная система (вариант B) показала конверсию 12.5% на выборке из 1000 пользователей, тогда как старая система (A) — 10% на выборке из 1200 пользователей.
Вопрос: Статистически значимо ли улучшение на уровне доверия 95%? Рассчитайте p-value.","Решение (Z-тест для долей):
p₁ = 0.125, n₁ = 1000
p₂ = 0.10, n₂ = 1200
Объединенная доля:
p̂ = (x₁ + x₂)/(n₁ + n₂) = (125 + 120)/(1000 + 1200) = 245/2200 ≈ 0.11136
Стандартная ошибка разности:
SE = √[ p̂*(1-p̂)*(1/n₁ + 1/n₂) ]
= √[0.11136*0.88864*(1/1000 + 1/1200)]
≈ √[0.09898 * 0.001833] ≈ √0.0001814 ≈ 0.01347
Z-статистика:
Z = (p₁ - p₂) / SE = (0.125 - 0.10) / 0.01347 ≈ 1.856
Для двустороннего теста на уровне α=0.05, критическое значение Z ≈ 1.96.
Наш Z ≈ 1.856 < 1.96 → различие не является статистически значимым на уровне 95%.
p-value ≈ 2 * P(Z > 1.856) ≈ 2 * 0.0317 ≈ 0.0634 (т.е. > 0.05).",,,
17,"Задача: В методе главных компонент (PCA) для датасета с 10 признаками мы получили следующие собственные значения ковариационной матрицы: [4.2, 2.1, 1.8, 1.1, 0.5, 0.3, 0.2, 0.1, 0.05, 0.05].
Вопрос: Сколько главных компонент нужно выбрать, чтобы сохранить 90% дисперсии данных?","Решение:
Общая дисперсия = сумма всех собственных значений = 4.2+2.1+1.8+1.1+0.5+0.3+0.2+0.1+0.05+0.05 = 10.4.

Накапливаем доли:
1-я компонента: 4.2/10.4 ≈ 40.38%
2-я: (4.2+2.1)/10.4 ≈ 60.58%
3-я: (4.2+2.1+1.8)/10.4 ≈ 77.88%
4-я: +1.1 → (8.2)/10.4 ≈ 78.85% (ошибка в расчетах, пересчитаем)

Правильно:
1-я: 4.2/10.4 = 0.4038
2-я: (4.2+2.1)/10.4 = 6.3/10.4 = 0.6058
3-я: (6.3+1.8)/10.4 = 8.1/10.4 = 0.7788
4-я: (8.1+1.1)/10.4 = 9.2/10.4 = 0.8846
5-я: (9.2+0.5)/10.4 = 9.7/10.4 = 0.9327

Ответ: Нужно 5 компонент, так как на 4-й компоненте накопленная дисперсия ≈ 88.5% (<90%), а на 5-й ≈ 93.3% (>90%).",,,
18,Почему в Random Forest при выборе признаков на каждом сплите используют случайное подмножество признаков?,"Это делается для того, чтобы снизить корреляцию между деревьями в лесу и улучшить общую производительность модели. Если все деревья будут использовать одни и те же признаки, они будут более похожи друг на друга, что приведёт к менее разнообразным решениям.

С точки зрения bias-variance decomposition мы хотим строить максимально разнообразные деревья и как можно больше их. То есть мы строим экспертов в своей области (по подмножеству данных и признаков), а потом их усредняем.",,,
19,Где деревья глубже бустинг/бэггинг?,"В бэггинге обычно деревья глубже, а бустинге с ограничением, чтобы не переобучаться.
Если в бустинге в какой-то момент будет переобучение - вырулить из этого никак не получится.
А в бэггинге даже если будет несколько переобученных деревьев, эффект от них может быть некритичным, так как мы берем несколько алгоритмов и усредняем их предсказаниях.",,,
20,Как работает k-nn?,"1. Обучение:
Обучения как такового нет.
Модель просто запоминает все обучающие объекты и их метки классов (или значения, если это регрессия). 
Чтобы классифицировать новый объект, модель ищет k ближайших объектов в обучающей выборке и голосует по их меткам.
Для регрессии — берется среднее значение у k ближайших соседей.

2. Предсказание:
Когда приходит новый объект, происходит следующее:
Вычисляется расстояние до всех объектов из обучающей выборки
— обычно Евклидово, но может быть любое (манхэттенское, косинусное и т.п.)
Выбираются k ближайших соседей
Классификация:
Класс — тот, за который проголосовало большинство соседей (majority vote)
Можно взвешивать по расстоянию: ближние — весомее

Регрессия:
Предсказывается среднее значение по соседям
Или взвешенное среднее (опять же — ближние весомее)",,,
21,Как ускорить K-NN для большого датасета?,"Понижение размерности перед k-NN
Используй PCA, UMAP, TSNE, TruncatedSVD и т.п.
Уменьшаешь размерность данных → быстрее считается расстояние
Главное — сохранить ""структуру близости"" между точками
Можно использовать Approximate Nearest Neighbors. Annoy, HNSW (Approximate Nearest Neighbors)
Для приближенного поиска ближайших — быстрее, но с небольшой потерей точности
Например, Annoy: Можем представить обучающую выборку в виде дерева и считаем уже не по всей области, а по части выборки. Другие методы используют другие методы ограничения выборки для подсчета.
Очень хорошо работают для больших объемов данных",,,
22,"Напиши скрипт на SQL. В таблице sales есть колонки order_id, user_id, amount, order_date. Напишите запрос, который покажет для каждого пользователя: общую сумму покупок, количество заказов и средний чек, но только для заказов за последние 30 дней.","SELECT 
    user_id,
    SUM(amount) as total_amount_last_30d,
    COUNT(order_id) as orders_count_last_30d,
    AVG(amount) as avg_check_last_30d
FROM sales
WHERE order_date >= CURRENT_DATE - INTERVAL '30 days'
GROUP BY user_id
ORDER BY total_amount_last_30d DESC;",,,
23,"Напишите скрипт на SQL. Найдите 3 последних заказа для каждого пользователя из таблицы orders с колонками order_id, user_id, order_date, amount.","WITH ranked_orders AS (
    SELECT 
        order_id,
        user_id,
        order_date,
        amount,
        ROW_NUMBER() OVER (
            PARTITION BY user_id 
            ORDER BY order_date DESC
        ) as rn
    FROM orders
)
SELECT *
FROM ranked_orders
WHERE rn <= 3;",,,
24,"Напишите скрипт на SQL.В таблице users есть дублирующиеся email (колонки id, email, created_at). Найдите все дубликаты и оставьте только самую раннюю запись для каждого email.","SELECT 
    email,
    COUNT(*) as duplicate_count,
    MIN(created_at) as first_created,
    MAX(created_at) as last_created
FROM users
GROUP BY email
HAVING COUNT(*) > 1;",,,
25,"Напишите скрипт на SQL. Найдите продукты, цена которых выше средней цены в их категории (таблица products с колонками product_id, category_id, price).","SELECT 
    product_id,
    category_id,
    price,
    category_avg_price
FROM (
    SELECT 
        product_id,
        category_id,
        price,
        AVG(price) OVER (PARTITION BY category_id) as category_avg_price
    FROM products
) t
WHERE price > category_avg_price;",,,
26,Реализуйте кастомную метрику для задачи регрессии: Symmetric Mean Absolute Percentage Error (SMAPE),"import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy import stats

def smape(y_true, y_pred):
    """"""
    Symmetric Mean Absolute Percentage Error
    SMAPE = 100% * Σ(|y_true - y_pred| / (|y_true| + |y_pred|)) / n
    """"""
    # Защита от деления на 0
    denominator = np.abs(y_true) + np.abs(y_pred)
    # Для случая, когда и истинное значение, и предсказание равны 0
    denominator = np.where(denominator == 0, 1, denominator)
    
    smape_val = 100 * np.mean(2 * np.abs(y_pred - y_true) / denominator)
    return smape_val

def mape(y_true, y_pred):
    """"""Mean Absolute Percentage Error""""""
    # Защита от нулей в y_true
    mask = y_true != 0
    if not np.any(mask):
        return np.nan
    return 100 * np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))

# Пример использования с обработкой edge cases
y_true = np.array([100, 200, 300, 0, 500])
y_pred = np.array([110, 190, 320, 10, 480])

print(f""SMAPE: {smape(y_true, y_pred):.2f}%"")
print(f""MAPE: {mape(y_true, y_pred):.2f}%"")
print(f""MAE: {mean_absolute_error(y_true, y_pred):.2f}"")
print(f""RMSE: {np.sqrt(mean_squared_error(y_true, y_pred)):.2f}"")

# Дополнительно: реализация weighted RMSE
def weighted_rmse(y_true, y_pred, weights=None):
    """"""RMSE с весами для наблюдений""""""
    if weights is None:
        weights = np.ones_like(y_true)
    
    # Нормализуем веса
    weights = weights / np.sum(weights) * len(weights)
    
    mse_weighted = np.mean(weights * (y_true - y_pred) ** 2)
    return np.sqrt(mse_weighted)

# Пример с весами
weights = np.array([1.0, 0.5, 2.0, 0.1, 1.5])
print(f""Weighted RMSE: {weighted_rmse(y_true, y_pred, weights):.2f}"")",,,
27,"Что такое иерархическая агломеративная кластеризация, чем отличается от дивизионных алгоритмов?
","Это тип кластеризации, при котором мы постепенно объединяем объекты в кластеры, начиная с того, что каждая точка — отдельный кластер, и в итоге объединяем всё в один.
Алгоритм:
Старт: каждая точка — это свой кластер.
Вычисляется расстояние между всеми кластерами.
Находим два ближайших кластера и объединяем их.
Обновляем матрицу расстояний.
Повторяем шаги 2–4, пока не останется один кластер (или не достигнут критерий остановки).",,,
28,Почему AUC-ROC не всегда подходит для многоклассовых задач без модификаций?,Потому что ROC определен только для бинарной классификации,,,
29,Что такое векторизация TF/IDF?,"TF/IDF (Term Frequency/Inverse Document Frequency) – это метод векторизации текстовых данных, который используется для оценки важности слов в документе или коллекции документов. Он основывается на двух понятиях: частоте терма (TF) и обратной частоте документа (IDF).
Частота терма (TF) – это мера того, насколько часто определенное слово встречается в документе. Чем чаще слово встречается, тем больше его вес в документе.
Обратная частота документа (IDF) – это мера того, насколько уникально слово является в коллекции документов. Слова, которые встречаются редко в коллекции, имеют более высокий IDF и, следовательно, более высокую важность.
Векторизация TF/IDF преобразует текстовые документы в числовые векторы, где каждое слово представлено весом, основанным на его TF и IDF. Это позволяет использовать текстовые данные в алгоритмах машинного обучения, которые требуют числовых входных данных.",,,
30,"Вам дали набор данных твитов, задача – предсказать их тональность (положительная или отрицательная). Как бы вы проводили предобработку?","Поскольку твиты наполнены хэштегами, которые могут представлять важную информацию, и, возможно, создать набор признаков, закодированных унитарным кодом (one-hot encoding), в котором ‘1’ будет означать наличие хэштега, а ‘0’ – его отсутствие. То же самое можно сделать с символами ‘@’ (может быть важно, какому аккаунту адресован твит). В твитах особенно часто встречаются сокращения (поскольку есть лимит количества символов), так что в текстах наверняка будет много намеренно неправильно записанных слов, которые придется восстанавливать. Возможно, само количество неправильно написанных слов также представляет полезную информацию: разозленные люди обычно пишут больше неправильных слов.

Удаление пунктуации, хоть оно и является стандартным для NLP, в данном случае можно пропустить, поскольку восклицательные знаки, вопросы, точки и пр. могут нести важную информацию, в сочетании с текстом, в котором они применяются. Можно создать три или большее количество столбцов, в которых будет указано количество восклицательных знаков, вопросительных знаков и точек. Однако перед передачей данных в модель пунктуацию следует убрать из текста.

Затем нужно провести лемматизацию и токенизацию текста. В модель следует передать не только чистый текст, но и информацию о хэштегах, ‘@’, неправильно написанных словах и пунктуации. Все это, вероятно, повысит точность предсказаний.",,,